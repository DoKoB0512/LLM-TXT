Directory structure:
└── docs/
    ├── api_refs/
    │   ├── README.md
    │   ├── blacklisted-entrypoints.json
    │   ├── next.config.js
    │   ├── package.json
    │   ├── postcss.config.js
    │   ├── tailwind.config.ts
    │   ├── tsconfig.json
    │   ├── typedoc.base.json
    │   ├── vercel.json
    │   ├── .eslintrc.json
    │   ├── .gitignore
    │   ├── scripts/
    │   │   ├── create-entrypoints.js
    │   │   ├── typedoc-plugin.js
    │   │   └── update-typedoc-css.js
    │   └── src/
    │       └── app/
    └── core_docs/
        ├── README.md
        ├── babel.config.js
        ├── docusaurus.config.js
        ├── package.json
        ├── sidebars.js
        ├── vercel.json
        ├── .eslintrc.js
        ├── .gitignore
        ├── .prettierignore
        ├── data/
        │   ├── ls_few_shot_example_dataset.json
        │   └── people.yml
        ├── docs/
        │   ├── community.mdx
        │   ├── introduction.mdx
        │   ├── packages.mdx
        │   ├── people.mdx
        │   ├── security.md
        │   ├── _static/
        │   │   └── css/
        │   │       └── custom.css
        │   ├── additional_resources/
        │   │   └── tutorials.mdx
        │   ├── concepts/
        │   │   ├── agents.mdx
        │   │   ├── architecture.mdx
        │   │   ├── callbacks.mdx
        │   │   ├── chat_history.mdx
        │   │   ├── chat_models.mdx
        │   │   ├── document_loaders.mdx
        │   │   ├── embedding_models.mdx
        │   │   ├── evaluation.mdx
        │   │   ├── example_selectors.mdx
        │   │   ├── few_shot_prompting.mdx
        │   │   ├── index.mdx
        │   │   ├── key_value_stores.mdx
        │   │   ├── lcel.mdx
        │   │   ├── messages.mdx
        │   │   ├── multimodality.mdx
        │   │   ├── output_parsers.mdx
        │   │   ├── prompt_templates.mdx
        │   │   ├── rag.mdx
        │   │   ├── retrieval.mdx
        │   │   ├── retrievers.mdx
        │   │   ├── runnables.mdx
        │   │   ├── streaming.mdx
        │   │   ├── structured_outputs.mdx
        │   │   ├── t.ipynb
        │   │   ├── text_llms.mdx
        │   │   ├── text_splitters.mdx
        │   │   ├── tokens.mdx
        │   │   ├── tool_calling.mdx
        │   │   ├── tools.mdx
        │   │   ├── tracing.mdx
        │   │   ├── vectorstores.mdx
        │   │   └── why_langchain.mdx
        │   ├── contributing/
        │   │   ├── code.mdx
        │   │   ├── faq.mdx
        │   │   ├── index.mdx
        │   │   ├── integrations.mdx
        │   │   ├── repo_structure.mdx
        │   │   ├── testing.mdx
        │   │   └── documentation/
        │   │       ├── _category_.yml
        │   │       └── style_guide.mdx
        │   ├── how_to/
        │   │   ├── agent_executor.ipynb
        │   │   ├── assign.ipynb
        │   │   ├── binding.ipynb
        │   │   ├── caching_embeddings.mdx
        │   │   ├── callbacks_attach.ipynb
        │   │   ├── callbacks_constructor.ipynb
        │   │   ├── callbacks_custom_events.ipynb
        │   │   ├── callbacks_runtime.ipynb
        │   │   ├── callbacks_serverless.ipynb
        │   │   ├── cancel_execution.ipynb
        │   │   ├── character_text_splitter.ipynb
        │   │   ├── chat_model_caching.mdx
        │   │   ├── chat_models_universal_init.mdx
        │   │   ├── chat_streaming.ipynb
        │   │   ├── chat_token_usage_tracking.mdx
        │   │   ├── chatbots_memory.ipynb
        │   │   ├── chatbots_retrieval.ipynb
        │   │   ├── chatbots_tools.ipynb
        │   │   ├── code_splitter.ipynb
        │   │   ├── contextual_compression.mdx
        │   │   ├── convert_runnable_to_tool.ipynb
        │   │   ├── custom_callbacks.ipynb
        │   │   ├── custom_chat.ipynb
        │   │   ├── custom_llm.ipynb
        │   │   ├── custom_retriever.mdx
        │   │   ├── custom_tools.ipynb
        │   │   ├── debugging.mdx
        │   │   ├── document_loader_csv.mdx
        │   │   ├── document_loader_custom.mdx
        │   │   ├── document_loader_directory.mdx
        │   │   ├── document_loader_html.ipynb
        │   │   ├── document_loader_markdown.ipynb
        │   │   ├── document_loader_pdf.mdx
        │   │   ├── document_loaders_json.mdx
        │   │   ├── embed_text.mdx
        │   │   ├── ensemble_retriever.mdx
        │   │   ├── example_selectors.ipynb
        │   │   ├── example_selectors_langsmith.ipynb
        │   │   ├── example_selectors_length_based.mdx
        │   │   ├── example_selectors_similarity.mdx
        │   │   ├── extraction_examples.ipynb
        │   │   ├── extraction_long_text.ipynb
        │   │   ├── extraction_parse.ipynb
        │   │   ├── fallbacks.mdx
        │   │   ├── few_shot.mdx
        │   │   ├── few_shot_examples.ipynb
        │   │   ├── few_shot_examples_chat.ipynb
        │   │   ├── filter_messages.ipynb
        │   │   ├── functions.ipynb
        │   │   ├── generative_ui.mdx
        │   │   ├── graph_constructing.ipynb
        │   │   ├── graph_mapping.ipynb
        │   │   ├── graph_prompting.ipynb
        │   │   ├── graph_semantic.ipynb
        │   │   ├── index.mdx
        │   │   ├── indexing.mdx
        │   │   ├── installation.mdx
        │   │   ├── lcel_cheatsheet.ipynb
        │   │   ├── llm_caching.mdx
        │   │   ├── llm_token_usage_tracking.mdx
        │   │   ├── logprobs.ipynb
        │   │   ├── merge_message_runs.ipynb
        │   │   ├── message_history.ipynb
        │   │   ├── migrate_agent.ipynb
        │   │   ├── multi_vector.mdx
        │   │   ├── multimodal_inputs.ipynb
        │   │   ├── multimodal_prompts.ipynb
        │   │   ├── multiple_queries.ipynb
        │   │   ├── output_parser_fixing.ipynb
        │   │   ├── output_parser_json.ipynb
        │   │   ├── output_parser_structured.ipynb
        │   │   ├── output_parser_xml.ipynb
        │   │   ├── parallel.mdx
        │   │   ├── parent_document_retriever.mdx
        │   │   ├── passthrough.ipynb
        │   │   ├── prompts_composition.ipynb
        │   │   ├── prompts_partial.mdx
        │   │   ├── qa_chat_history_how_to.ipynb
        │   │   ├── qa_citations.ipynb
        │   │   ├── qa_per_user.ipynb
        │   │   ├── qa_sources.ipynb
        │   │   ├── qa_streaming.ipynb
        │   │   ├── query_constructing_filters.ipynb
        │   │   ├── query_few_shot.ipynb
        │   │   ├── query_high_cardinality.ipynb
        │   │   ├── query_multiple_queries.ipynb
        │   │   ├── query_multiple_retrievers.ipynb
        │   │   ├── query_no_queries.ipynb
        │   │   ├── recursive_text_splitter.ipynb
        │   │   ├── reduce_retrieval_latency.mdx
        │   │   ├── routing.mdx
        │   │   ├── self_query.ipynb
        │   │   ├── sequence.ipynb
        │   │   ├── split_by_token.ipynb
        │   │   ├── sql_large_db.mdx
        │   │   ├── sql_prompting.mdx
        │   │   ├── sql_query_checking.mdx
        │   │   ├── stream_agent_client.mdx
        │   │   ├── stream_tool_client.mdx
        │   │   ├── streaming.ipynb
        │   │   ├── streaming_llm.mdx
        │   │   ├── structured_output.ipynb
        │   │   ├── time_weighted_vectorstore.mdx
        │   │   ├── tool_artifacts.ipynb
        │   │   ├── tool_calling.ipynb
        │   │   ├── tool_calling_parallel.ipynb
        │   │   ├── tool_calls_multimodal.ipynb
        │   │   ├── tool_choice.ipynb
        │   │   ├── tool_configure.ipynb
        │   │   ├── tool_results_pass_to_model.ipynb
        │   │   ├── tool_runtime.ipynb
        │   │   ├── tool_stream_events.ipynb
        │   │   ├── tool_streaming.ipynb
        │   │   ├── tools_builtin.ipynb
        │   │   ├── tools_error.ipynb
        │   │   ├── tools_few_shot.ipynb
        │   │   ├── tools_prompting.ipynb
        │   │   ├── trim_messages.ipynb
        │   │   ├── vectorstore_retriever.mdx
        │   │   └── vectorstores.mdx
        │   ├── integrations/
        │   │   ├── callbacks/
        │   │   │   ├── datadog_tracer.mdx
        │   │   │   └── upstash_ratelimit_callback.mdx
        │   │   ├── chat/
        │   │   │   ├── alibaba_tongyi.mdx
        │   │   │   ├── anthropic.ipynb
        │   │   │   ├── arcjet.ipynb
        │   │   │   ├── azure.ipynb
        │   │   │   ├── baidu_qianfan.mdx
        │   │   │   ├── baidu_wenxin.mdx
        │   │   │   ├── bedrock.ipynb
        │   │   │   ├── bedrock_converse.ipynb
        │   │   │   ├── cerebras.ipynb
        │   │   │   ├── cloudflare_workersai.ipynb
        │   │   │   ├── cohere.ipynb
        │   │   │   ├── deep_infra.mdx
        │   │   │   ├── deepseek.ipynb
        │   │   │   ├── fake.mdx
        │   │   │   ├── fireworks.ipynb
        │   │   │   ├── friendli.mdx
        │   │   │   ├── google_generativeai.ipynb
        │   │   │   ├── google_vertex_ai.ipynb
        │   │   │   ├── groq.ipynb
        │   │   │   ├── ibm.ipynb
        │   │   │   ├── index.mdx
        │   │   │   ├── llama_cpp.mdx
        │   │   │   ├── minimax.mdx
        │   │   │   ├── mistral.ipynb
        │   │   │   ├── moonshot.mdx
        │   │   │   ├── ni_bittensor.mdx
        │   │   │   ├── novita.ipynb
        │   │   │   ├── ollama.ipynb
        │   │   │   ├── ollama_functions.mdx
        │   │   │   ├── openai.ipynb
        │   │   │   ├── perplexity.ipynb
        │   │   │   ├── premai.mdx
        │   │   │   ├── prompt_layer_openai.mdx
        │   │   │   ├── tencent_hunyuan.mdx
        │   │   │   ├── togetherai.ipynb
        │   │   │   ├── web_llm.mdx
        │   │   │   ├── xai.ipynb
        │   │   │   ├── yandex.mdx
        │   │   │   └── zhipuai.mdx
        │   │   ├── document_compressors/
        │   │   │   ├── cohere_rerank.mdx
        │   │   │   ├── ibm.ipynb
        │   │   │   └── mixedbread_ai.mdx
        │   │   ├── document_loaders/
        │   │   │   ├── index.mdx
        │   │   │   ├── file_loaders/
        │   │   │   │   ├── chatgpt.mdx
        │   │   │   │   ├── csv.ipynb
        │   │   │   │   ├── directory.ipynb
        │   │   │   │   ├── docx.mdx
        │   │   │   │   ├── epub.mdx
        │   │   │   │   ├── index.mdx
        │   │   │   │   ├── json.mdx
        │   │   │   │   ├── jsonlines.mdx
        │   │   │   │   ├── multi_file.mdx
        │   │   │   │   ├── notion_markdown.mdx
        │   │   │   │   ├── openai_whisper_audio.mdx
        │   │   │   │   ├── pdf.ipynb
        │   │   │   │   ├── pptx.mdx
        │   │   │   │   ├── subtitles.mdx
        │   │   │   │   ├── text.ipynb
        │   │   │   │   └── unstructured.ipynb
        │   │   │   └── web_loaders/
        │   │   │       ├── airtable.mdx
        │   │   │       ├── apify_dataset.mdx
        │   │   │       ├── assemblyai_audio_transcription.mdx
        │   │   │       ├── azure_blob_storage_container.mdx
        │   │   │       ├── azure_blob_storage_file.mdx
        │   │   │       ├── browserbase.mdx
        │   │   │       ├── college_confidential.mdx
        │   │   │       ├── confluence.mdx
        │   │   │       ├── couchbase.mdx
        │   │   │       ├── figma.mdx
        │   │   │       ├── firecrawl.ipynb
        │   │   │       ├── gitbook.mdx
        │   │   │       ├── github.mdx
        │   │   │       ├── google_cloud_storage.mdx
        │   │   │       ├── google_cloudsql_pg.mdx
        │   │   │       ├── hn.mdx
        │   │   │       ├── imsdb.mdx
        │   │   │       ├── index.mdx
        │   │   │       ├── jira.mdx
        │   │   │       ├── langsmith.ipynb
        │   │   │       ├── notionapi.mdx
        │   │   │       ├── pdf.ipynb
        │   │   │       ├── recursive_url_loader.ipynb
        │   │   │       ├── s3.mdx
        │   │   │       ├── searchapi.mdx
        │   │   │       ├── serpapi.mdx
        │   │   │       ├── sitemap.mdx
        │   │   │       ├── sonix_audio_transcription.mdx
        │   │   │       ├── sort_xyz_blockchain.mdx
        │   │   │       ├── spider.mdx
        │   │   │       ├── taskade.mdx
        │   │   │       ├── web_cheerio.ipynb
        │   │   │       ├── web_playwright.mdx
        │   │   │       ├── web_puppeteer.ipynb
        │   │   │       └── youtube.mdx
        │   │   ├── document_transformers/
        │   │   │   ├── html-to-text.mdx
        │   │   │   ├── mozilla_readability.mdx
        │   │   │   └── openai_metadata_tagger.mdx
        │   │   ├── llm_caching/
        │   │   │   ├── azure_cosmosdb_nosql.mdx
        │   │   │   └── index.mdx
        │   │   ├── llms/
        │   │   │   ├── ai21.mdx
        │   │   │   ├── aleph_alpha.mdx
        │   │   │   ├── arcjet.ipynb
        │   │   │   ├── aws_sagemaker.mdx
        │   │   │   ├── azure.ipynb
        │   │   │   ├── bedrock.ipynb
        │   │   │   ├── chrome_ai.mdx
        │   │   │   ├── cloudflare_workersai.ipynb
        │   │   │   ├── cohere.ipynb
        │   │   │   ├── deep_infra.mdx
        │   │   │   ├── fireworks.ipynb
        │   │   │   ├── friendli.mdx
        │   │   │   ├── google_vertex_ai.ipynb
        │   │   │   ├── gradient_ai.mdx
        │   │   │   ├── huggingface_inference.mdx
        │   │   │   ├── ibm.ipynb
        │   │   │   ├── index.mdx
        │   │   │   ├── jigsawstack.mdx
        │   │   │   ├── layerup_security.mdx
        │   │   │   ├── llama_cpp.mdx
        │   │   │   ├── mistral.ipynb
        │   │   │   ├── ni_bittensor.mdx
        │   │   │   ├── ollama.ipynb
        │   │   │   ├── openai.ipynb
        │   │   │   ├── prompt_layer_openai.mdx
        │   │   │   ├── raycast.mdx
        │   │   │   ├── replicate.mdx
        │   │   │   ├── together.ipynb
        │   │   │   ├── writer.mdx
        │   │   │   └── yandex.mdx
        │   │   ├── memory/
        │   │   │   ├── astradb.mdx
        │   │   │   ├── aurora_dsql.mdx
        │   │   │   ├── azure_cosmos_mongo_vcore.mdx
        │   │   │   ├── azure_cosmosdb_nosql.mdx
        │   │   │   ├── cassandra.mdx
        │   │   │   ├── cloudflare_d1.mdx
        │   │   │   ├── convex.mdx
        │   │   │   ├── dynamodb.mdx
        │   │   │   ├── file.mdx
        │   │   │   ├── firestore.mdx
        │   │   │   ├── google_cloudsql_pg.mdx
        │   │   │   ├── ipfs_datastore.mdx
        │   │   │   ├── mem0_memory.mdx
        │   │   │   ├── momento.mdx
        │   │   │   ├── mongodb.mdx
        │   │   │   ├── motorhead_memory.mdx
        │   │   │   ├── planetscale.mdx
        │   │   │   ├── postgres.mdx
        │   │   │   ├── redis.mdx
        │   │   │   ├── upstash_redis.mdx
        │   │   │   ├── xata.mdx
        │   │   │   ├── zep_memory.mdx
        │   │   │   └── zep_memory_cloud.mdx
        │   │   ├── platforms/
        │   │   │   ├── anthropic.mdx
        │   │   │   ├── aws.mdx
        │   │   │   ├── google.mdx
        │   │   │   ├── index.mdx
        │   │   │   ├── microsoft.mdx
        │   │   │   └── openai.mdx
        │   │   ├── retrievers/
        │   │   │   ├── arxiv-retriever.mdx
        │   │   │   ├── azion-edgesql.ipynb
        │   │   │   ├── bedrock-knowledge-bases.ipynb
        │   │   │   ├── bm25.ipynb
        │   │   │   ├── chaindesk-retriever.mdx
        │   │   │   ├── chatgpt-retriever-plugin.mdx
        │   │   │   ├── dria.mdx
        │   │   │   ├── exa.ipynb
        │   │   │   ├── hyde.mdx
        │   │   │   ├── index.mdx
        │   │   │   ├── kendra-retriever.ipynb
        │   │   │   ├── metal-retriever.mdx
        │   │   │   ├── supabase-hybrid.mdx
        │   │   │   ├── tavily.ipynb
        │   │   │   ├── time-weighted-retriever.mdx
        │   │   │   ├── vespa-retriever.mdx
        │   │   │   ├── zep-cloud-retriever.mdx
        │   │   │   ├── zep-retriever.mdx
        │   │   │   └── self_query/
        │   │   │       ├── chroma.ipynb
        │   │   │       ├── hnswlib.ipynb
        │   │   │       ├── index.mdx
        │   │   │       ├── memory.ipynb
        │   │   │       ├── pinecone.ipynb
        │   │   │       ├── qdrant.ipynb
        │   │   │       ├── supabase.ipynb
        │   │   │       ├── vectara.ipynb
        │   │   │       └── weaviate.ipynb
        │   │   ├── stores/
        │   │   │   ├── cassandra_storage.mdx
        │   │   │   ├── file_system.ipynb
        │   │   │   ├── in_memory.ipynb
        │   │   │   ├── index.mdx
        │   │   │   ├── ioredis_storage.mdx
        │   │   │   ├── upstash_redis_storage.mdx
        │   │   │   └── vercel_kv_storage.mdx
        │   │   ├── text_embedding/
        │   │   │   ├── alibaba_tongyi.mdx
        │   │   │   ├── azure_openai.ipynb
        │   │   │   ├── baidu_qianfan.mdx
        │   │   │   ├── bedrock.ipynb
        │   │   │   ├── bytedance_doubao.ipynb
        │   │   │   ├── cloudflare_ai.ipynb
        │   │   │   ├── cohere.ipynb
        │   │   │   ├── deepinfra.mdx
        │   │   │   ├── fireworks.ipynb
        │   │   │   ├── google_generativeai.ipynb
        │   │   │   ├── google_vertex_ai.ipynb
        │   │   │   ├── gradient_ai.mdx
        │   │   │   ├── hugging_face_inference.mdx
        │   │   │   ├── ibm.ipynb
        │   │   │   ├── index.mdx
        │   │   │   ├── jina.mdx
        │   │   │   ├── llama_cpp.mdx
        │   │   │   ├── minimax.mdx
        │   │   │   ├── mistralai.ipynb
        │   │   │   ├── mixedbread_ai.mdx
        │   │   │   ├── nomic.mdx
        │   │   │   ├── ollama.ipynb
        │   │   │   ├── openai.ipynb
        │   │   │   ├── pinecone.ipynb
        │   │   │   ├── premai.mdx
        │   │   │   ├── tencent_hunyuan.mdx
        │   │   │   ├── tensorflow.mdx
        │   │   │   ├── togetherai.ipynb
        │   │   │   ├── transformers.mdx
        │   │   │   ├── voyageai.mdx
        │   │   │   └── zhipuai.mdx
        │   │   ├── toolkits/
        │   │   │   ├── connery.mdx
        │   │   │   ├── ibm.ipynb
        │   │   │   ├── index.mdx
        │   │   │   ├── json.mdx
        │   │   │   ├── openapi.ipynb
        │   │   │   ├── sfn_agent.mdx
        │   │   │   ├── sql.ipynb
        │   │   │   └── vectorstore.ipynb
        │   │   ├── tools/
        │   │   │   ├── aiplugin-tool.mdx
        │   │   │   ├── azure_dynamic_sessions.mdx
        │   │   │   ├── connery.mdx
        │   │   │   ├── dalle.mdx
        │   │   │   ├── discord.mdx
        │   │   │   ├── duckduckgo_search.ipynb
        │   │   │   ├── exa_search.ipynb
        │   │   │   ├── gmail.mdx
        │   │   │   ├── google_calendar.mdx
        │   │   │   ├── google_places.mdx
        │   │   │   ├── google_routes.mdx
        │   │   │   ├── google_scholar.ipynb
        │   │   │   ├── google_trends.mdx
        │   │   │   ├── index.mdx
        │   │   │   ├── jigsawstack.mdx
        │   │   │   ├── lambda_agent.mdx
        │   │   │   ├── pyinterpreter.mdx
        │   │   │   ├── searchapi.mdx
        │   │   │   ├── searxng.mdx
        │   │   │   ├── serpapi.ipynb
        │   │   │   ├── stackexchange.mdx
        │   │   │   ├── stagehand.mdx
        │   │   │   ├── tavily_search.ipynb
        │   │   │   ├── webbrowser.mdx
        │   │   │   ├── wikipedia.mdx
        │   │   │   ├── wolframalpha.mdx
        │   │   │   └── zapier_agent.mdx
        │   │   └── vectorstores/
        │   │       ├── analyticdb.mdx
        │   │       ├── astradb.mdx
        │   │       ├── azion-edgesql.ipynb
        │   │       ├── azure_aisearch.mdx
        │   │       ├── azure_cosmosdb_mongodb.mdx
        │   │       ├── azure_cosmosdb_nosql.mdx
        │   │       ├── cassandra.mdx
        │   │       ├── chroma.ipynb
        │   │       ├── clickhouse.mdx
        │   │       ├── closevector.mdx
        │   │       ├── cloudflare_vectorize.mdx
        │   │       ├── convex.mdx
        │   │       ├── couchbase.mdx
        │   │       ├── elasticsearch.ipynb
        │   │       ├── faiss.ipynb
        │   │       ├── google_cloudsql_pg.ipynb
        │   │       ├── googlevertexai.mdx
        │   │       ├── hanavector.mdx
        │   │       ├── hnswlib.ipynb
        │   │       ├── index.mdx
        │   │       ├── lancedb.mdx
        │   │       ├── libsql.mdx
        │   │       ├── mariadb.ipynb
        │   │       ├── memory.ipynb
        │   │       ├── milvus.mdx
        │   │       ├── momento_vector_index.mdx
        │   │       ├── mongodb_atlas.ipynb
        │   │       ├── myscale.mdx
        │   │       ├── neo4jvector.mdx
        │   │       ├── neon.mdx
        │   │       ├── opensearch.mdx
        │   │       ├── pgvector.ipynb
        │   │       ├── pinecone.ipynb
        │   │       ├── prisma.mdx
        │   │       ├── qdrant.ipynb
        │   │       ├── redis.ipynb
        │   │       ├── rockset.mdx
        │   │       ├── singlestore.mdx
        │   │       ├── supabase.ipynb
        │   │       ├── tigris.mdx
        │   │       ├── turbopuffer.mdx
        │   │       ├── typeorm.mdx
        │   │       ├── typesense.mdx
        │   │       ├── upstash.ipynb
        │   │       ├── usearch.mdx
        │   │       ├── vectara.mdx
        │   │       ├── vercel_postgres.mdx
        │   │       ├── voy.mdx
        │   │       ├── weaviate.ipynb
        │   │       ├── xata.mdx
        │   │       ├── zep.mdx
        │   │       └── zep_cloud.mdx
        │   ├── mdx_components/
        │   │   ├── integration_install_tooltip.mdx
        │   │   └── unified_model_params_tooltip.mdx
        │   ├── troubleshooting/
        │   │   └── errors/
        │   │       ├── index.mdx
        │   │       ├── INVALID_PROMPT_INPUT.mdx
        │   │       ├── INVALID_TOOL_RESULTS.ipynb
        │   │       ├── MESSAGE_COERCION_FAILURE.mdx
        │   │       ├── MODEL_AUTHENTICATION.mdx
        │   │       ├── MODEL_NOT_FOUND.mdx
        │   │       ├── MODEL_RATE_LIMIT.mdx
        │   │       └── OUTPUT_PARSING_FAILURE.mdx
        │   ├── tutorials/
        │   │   ├── chatbot.ipynb
        │   │   ├── classification.ipynb
        │   │   ├── extraction.ipynb
        │   │   ├── graph.ipynb
        │   │   ├── index.mdx
        │   │   ├── llm_chain.ipynb
        │   │   ├── qa_chat_history.ipynb
        │   │   ├── rag.ipynb
        │   │   ├── retrievers.ipynb
        │   │   ├── sql_qa.ipynb
        │   │   └── summarization.ipynb
        │   └── versions/
        │       ├── release_policy.mdx
        │       ├── migrating_memory/
        │       │   ├── chat_history.ipynb
        │       │   ├── conversation_buffer_window_memory.ipynb
        │       │   ├── conversation_summary_memory.ipynb
        │       │   └── index.mdx
        │       ├── v0_2/
        │       │   ├── index.mdx
        │       │   └── migrating_astream_events.mdx
        │       └── v0_3/
        │           └── index.mdx
        ├── scripts/
        │   ├── append_related_links.py
        │   ├── check-broken-links.js
        │   ├── code-block-loader.js
        │   ├── quarto-build.js
        │   └── vercel_build.sh
        ├── src/
        │   ├── css/
        │   │   └── custom.css
        │   ├── pages/
        │   │   └── index.js
        │   └── theme/
        │       ├── ChatModelTabs.js
        │       ├── EmbeddingTabs.js
        │       ├── FeatureTables.js
        │       ├── Feedback.js
        │       ├── NotFound.js
        │       ├── Npm2Yarn.js
        │       ├── People.js
        │       ├── RedirectAnchors.js
        │       ├── VectorStoreTabs.js
        │       ├── CodeBlock/
        │       │   └── index.js
        │       ├── DocItem/
        │       │   └── Paginator/
        │       │       └── index.js
        │       ├── DocPaginator/
        │       │   └── index.js
        │       └── DocVersionBanner/
        │           └── index.js
        └── static/
            ├── llms.txt
            ├── robots.txt
            ├── .nojekyll
            ├── fonts/
            │   ├── Manrope-VariableFont_wght.ttf
            │   └── PublicSans-VariableFont_wght.ttf
            ├── img/
            │   ├── graph_chain.webp
            │   ├── langchain_stack_feb_2024.webp
            │   └── brand/
            ├── js/
            │   └── google_analytics.js
            └── svg/

================================================
FILE: docs/api_refs/README.md
================================================
# Auto-generated API documentation for LangChainJS

Do not edit the contents of this directory directly.

## Usage

To build the API refs run `yarn build` from the root of this directory, then `yarn dev` or `yarn start` to serve the docs locally.
This app uses [Typedoc](https://typedoc.org/) to generate API references from the source code. The generated HTML is then placed inside the `/public` directory, which is served by [Next.js](https://nextjs.org/).
There is a default redirect when requests are made to `/` which redirects to `/index.html`.

The API references are gitignored by default, so they will not be committed to the repo.



================================================
FILE: docs/api_refs/blacklisted-entrypoints.json
================================================
[
  "../../langchain/src/load.ts",
  "../../langchain/src/load/serializable.ts",
  "../../langchain/src/agents/toolkits/connery.ts",
  "../../langchain/src/tools/aws_lambda.ts",
  "../../langchain/src/tools/aws_sfn.ts",
  "../../langchain/src/tools/connery.ts",
  "../../langchain/src/tools/gmail.ts",
  "../../langchain/src/tools/google_places.ts",
  "../../langchain/src/tools/google_trends.ts",
  "../../langchain/src/embeddings/bedrock.ts",
  "../../langchain/src/embeddings/cloudflare_workersai.ts",
  "../../langchain/src/embeddings/ollama.ts",
  "../../langchain/src/embeddings/cohere.ts",
  "../../langchain/src/embeddings/tensorflow.ts",
  "../../langchain/src/embeddings/hf.ts",
  "../../langchain/src/embeddings/hf_transformers.ts",
  "../../langchain/src/embeddings/huggingface_transformers.ts",
  "../../langchain/src/embeddings/googlevertexai.ts",
  "../../langchain/src/embeddings/googlepalm.ts",
  "../../langchain/src/embeddings/minimax.ts",
  "../../langchain/src/embeddings/voyage.ts",
  "../../langchain/src/embeddings/llama_cpp.ts",
  "../../langchain/src/embeddings/gradient_ai.ts",
  "../../langchain/src/llms/ai21.ts",
  "../../langchain/src/llms/aleph_alpha.ts",
  "../../langchain/src/llms/cloudflare_workersai.ts",
  "../../langchain/src/llms/cohere.ts",
  "../../langchain/src/llms/hf.ts",
  "../../langchain/src/llms/raycast.ts",
  "../../langchain/src/llms/ollama.ts",
  "../../langchain/src/llms/replicate.ts",
  "../../langchain/src/llms/fireworks.ts",
  "../../langchain/src/llms/googlevertexai.ts",
  "../../langchain/src/llms/googlevertexai/web.ts",
  "../../langchain/src/llms/googlepalm.ts",
  "../../langchain/src/llms/gradient_ai.ts",
  "../../langchain/src/llms/sagemaker_endpoint.ts",
  "../../langchain/src/llms/watsonx_ai.ts",
  "../../langchain/src/llms/bedrock.ts",
  "../../langchain/src/llms/bedrock/web.ts",
  "../../langchain/src/llms/llama_cpp.ts",
  "../../langchain/src/llms/writer.ts",
  "../../langchain/src/llms/portkey.ts",
  "../../langchain/src/llms/yandex.ts",
  "../../langchain/src/vectorstores/clickhouse.ts",
  "../../langchain/src/vectorstores/analyticdb.ts",
  "../../langchain/src/vectorstores/cassandra.ts",
  "../../langchain/src/vectorstores/convex.ts",
  "../../langchain/src/vectorstores/elasticsearch.ts",
  "../../langchain/src/vectorstores/cloudflare_vectorize.ts",
  "../../langchain/src/vectorstores/closevector/web.ts",
  "../../langchain/src/vectorstores/closevector/node.ts",
  "../../langchain/src/vectorstores/chroma.ts",
  "../../langchain/src/vectorstores/googlevertexai.ts",
  "../../langchain/src/vectorstores/hnswlib.ts",
  "../../langchain/src/vectorstores/hanavector.ts",
  "../../langchain/src/vectorstores/faiss.ts",
  "../../langchain/src/vectorstores/weaviate.ts",
  "../../langchain/src/vectorstores/lancedb.ts",
  "../../langchain/src/vectorstores/mariadb.ts",
  "../../langchain/src/vectorstores/momento_vector_index.ts",
  "../../langchain/src/vectorstores/mongodb_atlas.ts",
  "../../langchain/src/vectorstores/pinecone.ts",
  "../../langchain/src/vectorstores/qdrant.ts",
  "../../langchain/src/vectorstores/supabase.ts",
  "../../langchain/src/vectorstores/opensearch.ts",
  "../../langchain/src/vectorstores/pgvector.ts",
  "../../langchain/src/vectorstores/milvus.ts",
  "../../langchain/src/vectorstores/neo4j_vector.ts",
  "../../langchain/src/vectorstores/prisma.ts",
  "../../langchain/src/vectorstores/typeorm.ts",
  "../../langchain/src/vectorstores/myscale.ts",
  "../../langchain/src/vectorstores/redis.ts",
  "../../langchain/src/vectorstores/rockset.ts",
  "../../langchain/src/vectorstores/typesense.ts",
  "../../langchain/src/vectorstores/singlestore.ts",
  "../../langchain/src/vectorstores/tigris.ts",
  "../../langchain/src/vectorstores/usearch.ts",
  "../../langchain/src/vectorstores/vectara.ts",
  "../../langchain/src/vectorstores/vercel_postgres.ts",
  "../../langchain/src/vectorstores/voy.ts",
  "../../langchain/src/vectorstores/xata.ts",
  "../../langchain/src/vectorstores/zep.ts",
  "../../langchain/src/memory/zep.ts",
  "../../langchain/src/document_transformers/html_to_text.ts",
  "../../langchain/src/document_transformers/mozilla_readability.ts",
  "../../langchain/src/chat_models/portkey.ts",
  "../../langchain/src/chat_models/bedrock.ts",
  "../../langchain/src/chat_models/bedrock/web.ts",
  "../../langchain/src/chat_models/cloudflare_workersai.ts",
  "../../langchain/src/chat_models/googlevertexai.ts",
  "../../langchain/src/chat_models/googlevertexai/web.ts",
  "../../langchain/src/chat_models/googlepalm.ts",
  "../../langchain/src/chat_models/fireworks.ts",
  "../../langchain/src/chat_models/baiduwenxin.ts",
  "../../langchain/src/chat_models/iflytek_xinghuo.ts",
  "../../langchain/src/chat_models/iflytek_xinghuo/web.ts",
  "../../langchain/src/chat_models/ollama.ts",
  "../../langchain/src/chat_models/minimax.ts",
  "../../langchain/src/chat_models/llama_cpp.ts",
  "../../langchain/src/chat_models/yandex.ts",
  "../../langchain/src/callbacks/handlers/llmonitor.ts",
  "../../langchain/src/retrievers/amazon_kendra.ts",
  "../../langchain/src/retrievers/supabase.ts",
  "../../langchain/src/retrievers/zep.ts",
  "../../langchain/src/retrievers/metal.ts",
  "../../langchain/src/retrievers/chaindesk.ts",
  "../../langchain/src/retrievers/databerry.ts",
  "../../langchain/src/retrievers/vectara_summary.ts",
  "../../langchain/src/retrievers/tavily_search_api.ts",
  "../../langchain/src/retrievers/vespa.ts",
  "../../langchain/src/stores/doc/in_memory.ts",
  "../../langchain/src/stores/message/cassandra.ts",
  "../../langchain/src/stores/message/convex.ts",
  "../../langchain/src/stores/message/cloudflare_d1.ts",
  "../../langchain/src/stores/message/in_memory.ts",
  "../../langchain/src/stores/message/dynamodb.ts",
  "../../langchain/src/stores/message/firestore.ts",
  "../../langchain/src/stores/message/momento.ts",
  "../../langchain/src/stores/message/mongodb.ts",
  "../../langchain/src/stores/message/redis.ts",
  "../../langchain/src/stores/message/ioredis.ts",
  "../../langchain/src/stores/message/upstash_redis.ts",
  "../../langchain/src/stores/message/planetscale.ts",
  "../../langchain/src/stores/message/xata.ts",
  "../../langchain/src/storage/convex.ts",
  "../../langchain/src/storage/ioredis.ts",
  "../../langchain/src/storage/vercel_kv.ts",
  "../../langchain/src/storage/upstash_redis.ts",
  "../../langchain/src/graphs/neo4j_graph.ts",
  "../../langchain/src/util/convex.ts",
  "../../langchain/src/runnables.ts",
  "../../libs/langchain-community/src/chat_models/yandex.ts",
  "../../libs/langchain-community/src/llms/yandex.ts",
  "../../langchain/src/schema/output_parser.ts",
  "../../langchain/src/document.ts",
  "../../langchain/src/callbacks/index.ts"
]



================================================
FILE: docs/api_refs/next.config.js
================================================
/** @type {import('next').NextConfig} */
const nextConfig = {
  async redirects() {
    return [
      {
        source: "/",
        destination: "/index.html",
        permanent: false,
      },
    ];
  },
};

module.exports = nextConfig;



================================================
FILE: docs/api_refs/package.json
================================================
{
  "name": "api_refs",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "dev": "next dev -p 3001",
    "typedoc:build": "npx typedoc --options typedoc.json",
    "build:scripts": "node ./scripts/create-entrypoints.js && yarn typedoc:build && node ./scripts/update-typedoc-css.js",
    "build": "yarn clean && yarn build:scripts && next build",
    "start": "yarn build && next start -p 3001",
    "lint": "next lint",
    "format": "prettier --write \"**/*.{js,jsx,ts,tsx}\"",
    "format:check": "prettier --check \"**/*.{js,jsx,ts,tsx}\"",
    "clean": "rm -rf .next .turbo public/ && mkdir public"
  },
  "dependencies": {
    "next": "14.0.1",
    "react": "^18",
    "react-dom": "^18"
  },
  "devDependencies": {
    "@types/node": "^20",
    "@types/react": "^18",
    "@types/react-dom": "^18",
    "autoprefixer": "^10.0.1",
    "eslint": "^8",
    "eslint-config-next": "14.0.1",
    "glob": "^10.3.10",
    "postcss": "^8",
    "prettier": "^2.8.3",
    "tailwindcss": "^3.3.0",
    "ts-morph": "^23.0.0",
    "typedoc": "^0.26.1",
    "typedoc-plugin-expand-object-like-types": "^0.1.2",
    "typescript": "~5.1.6"
  }
}



================================================
FILE: docs/api_refs/postcss.config.js
================================================
module.exports = {
  plugins: {
    tailwindcss: {},
    autoprefixer: {},
  },
};



================================================
FILE: docs/api_refs/tailwind.config.ts
================================================
import type { Config } from "tailwindcss";

const config: Config = {
  content: [
    "./src/pages/**/*.{js,ts,jsx,tsx,mdx}",
    "./src/components/**/*.{js,ts,jsx,tsx,mdx}",
    "./src/app/**/*.{js,ts,jsx,tsx,mdx}",
  ],
  theme: {
    extend: {
      backgroundImage: {
        "gradient-radial": "radial-gradient(var(--tw-gradient-stops))",
        "gradient-conic":
          "conic-gradient(from 180deg at 50% 50%, var(--tw-gradient-stops))",
      },
    },
  },
  plugins: [],
};
export default config;



================================================
FILE: docs/api_refs/tsconfig.json
================================================
{
  "compilerOptions": {
    "target": "es5",
    "lib": ["dom", "dom.iterable", "esnext"],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "preserve",
    "incremental": true,
    "plugins": [
      {
        "name": "next"
      }
    ],
    "paths": {
      "@/*": ["./src/*"]
    }
  },
  "include": ["next-env.d.ts", "**/*.ts", "**/*.tsx", ".next/types/**/*.ts"],
  "exclude": ["node_modules"]
}



================================================
FILE: docs/api_refs/typedoc.base.json
================================================
{
  "$schema": "https://typedoc.org/schema.json",
  "includeVersion": true
}


================================================
FILE: docs/api_refs/vercel.json
================================================
{
  "buildCommand": "yarn build",
  "trailingSlash": false,
  "redirects": [
    {
      "source": "/:path*/langchain_load:rest",
      "destination": "/:path*/langchain.load:rest"
    },
    {
      "source": "/:path*/langchain_agents:rest",
      "destination": "/:path*/langchain.agents:rest"
    },
    {
      "source": "/:path*/langchain_tools:rest",
      "destination": "/:path*/langchain.tools:rest"
    },
    {
      "source": "/:path*/langchain_chains:rest",
      "destination": "/:path*/langchain.chains:rest"
    },
    {
      "source": "/:path*/langchain_chat_models:rest",
      "destination": "/:path*/langchain.chat_models:rest"
    },
    {
      "source": "/:path*/langchain_embeddings:rest",
      "destination": "/:path*/langchain.embeddings:rest"
    },
    {
      "source": "/:path*/langchain_vectorstores:rest",
      "destination": "/:path*/langchain.vectorstores:rest"
    },
    {
      "source": "/:path*/langchain_text_splitter:rest",
      "destination": "/:path*/langchain.text_splitter:rest"
    },
    {
      "source": "/:path*/langchain_memory:rest",
      "destination": "/:path*/langchain.memory:rest"
    },
    {
      "source": "/:path*/langchain_document:rest",
      "destination": "/:path*/langchain.document:rest"
    },
    {
      "source": "/:path*/langchain_document_loaders:rest",
      "destination": "/:path*/langchain.document_loaders:rest"
    },
    {
      "source": "/:path*/langchain_document_transformers:rest",
      "destination": "/:path*/langchain.document_transformers:rest"
    },
    {
      "source": "/:path*/langchain_sql_db:rest",
      "destination": "/:path*/langchain.sql_db:rest"
    },
    {
      "source": "/:path*/langchain_callbacks:rest",
      "destination": "/:path*/langchain.callbacks:rest"
    },
    {
      "source": "/:path*/langchain_output_parsers:rest",
      "destination": "/:path*/langchain.output_parsers:rest"
    },
    {
      "source": "/:path*/langchain_retrievers:rest",
      "destination": "/:path*/langchain.retrievers:rest"
    },
    {
      "source": "/:path*/langchain_cache:rest",
      "destination": "/:path*/langchain.cache:rest"
    },
    {
      "source": "/:path*/langchain_stores:rest",
      "destination": "/:path*/langchain.stores:rest"
    },
    {
      "source": "/:path*/langchain_storage:rest",
      "destination": "/:path*/langchain.storage:rest"
    },
    {
      "source": "/:path*/langchain_hub:rest",
      "destination": "/:path*/langchain.hub:rest"
    },
    {
      "source": "/:path*/langchain_util:rest",
      "destination": "/:path*/langchain.util:rest"
    },
    {
      "source": "/:path*/langchain_experimental:rest",
      "destination": "/:path*/langchain.experimental:rest"
    },
    {
      "source": "/:path*/langchain_evaluation:rest",
      "destination": "/:path*/langchain.evaluation:rest"
    },
    {
      "source": "/:path*/langchain_smith:rest",
      "destination": "/:path*/langchain.smith:rest"
    },
    {
      "source": "/:path*/langchain_runnables:rest",
      "destination": "/:path*/langchain.runnables:rest"
    },
    {
      "source": "/:path*/langchain_indexes:rest",
      "destination": "/:path*/langchain.indexes:rest"
    },
    {
      "source": "/:path*/langchain_schema:rest",
      "destination": "/:path*/langchain.schema:rest"
    },
    {
      "source": "/:path*/langchain_core_:rest",
      "destination": "/:path*/_langchain_core.:rest"
    },
    {
      "source": "/:path*/langchain_core.:rest",
      "destination": "/:path*/_langchain_core.:rest"
    },
    {
      "source": "/:path*/langchain_anthropic_experimental(_|\\.):rest",
      "destination": "/:path*/_langchain_anthropic.experimental.:rest"
    },
    {
      "source": "/:path*/langchain_anthropic.ChatAnthropic.:rest",
      "destination": "/:path*/_langchain_anthropic.index.ChatAnthropic.:rest"
    },
    {
      "source": "/:path*/langchain_anthropic.ChatAnthropicMessages.:rest",
      "destination": "/:path*/_langchain_anthropic.index.ChatAnthropicMessages.:rest"
    },
    {
      "source": "/:path*/langchain_anthropic.AnthropicInput.:rest",
      "destination": "/:path*/_langchain_anthropic.index.AnthropicInput.:rest"
    },
    {
      "source": "/:path*/langchain_anthropic.ChatAnthropicCallOptions.:rest",
      "destination": "/:path*/_langchain_anthropic.index.ChatAnthropicCallOptions.:rest"
    },
    {
      "source": "/:path*/langchain_aws(_|\\.):rest",
      "destination": "/:path*/_langchain_aws.:rest"
    },
    {
      "source": "/:path*/langchain_azure_cosmosdb(_|\\.):rest",
      "destination": "/:path*/_langchain_azure_cosmosdb.:rest"
    },
    {
      "source": "/:path*/langchain_azure_dynamic_sessions(_|\\.):rest",
      "destination": "/:path*/_langchain_azure_dynamic_sessions.:rest"
    },
    {
      "source": "/:path*/langchain_baidu_qianfan(_|\\.):rest",
      "destination": "/:path*/_langchain_baidu_qianfan.:rest"
    },
    {
      "source": "/:path*/langchain_cloudflare_langgraph_checkpointers(_|\\.):rest",
      "destination": "/:path*/_langchain_cloudflare.langgraph_checkpointers.:rest"
    },
    {
      "source": "/:path*/langchain_cloudflare.ChatCloudflareWorkersAI:rest",
      "destination": "/:path*/_langchain_cloudflare.index.ChatCloudflareWorkersAI:rest"
    },
    {
      "source": "/:path*/langchain_cloudflare.CloudflareD1MessageHistory:rest",
      "destination": "/:path*/_langchain_cloudflare.index.CloudflareD1MessageHistory:rest"
    },
    {
      "source": "/:path*/langchain_cloudflare.CloudflareKVCache:rest",
      "destination": "/:path*/_langchain_cloudflare.index.CloudflareKVCache:rest"
    },
    {
      "source": "/:path*/langchain_cloudflare.CloudflareVectorizeStore:rest",
      "destination": "/:path*/_langchain_cloudflare.index.CloudflareVectorizeStore:rest"
    },
    {
      "source": "/:path*/langchain_cloudflare.CloudflareWorkersAI:rest",
      "destination": "/:path*/_langchain_cloudflare.index.CloudflareWorkersAI:rest"
    },
    {
      "source": "/:path*/langchain_cloudflare.CloudflareWorkersAIEmbeddings:rest",
      "destination": "/:path*/_langchain_cloudflare.index.CloudflareWorkersAIEmbeddings:rest"
    },
    {
      "source": "/:path*/langchain_cloudflare.ChatCloudflareWorkersAICallOptions:rest",
      "destination": "/:path*/_langchain_cloudflare.index.ChatCloudflareWorkersAICallOptions:rest"
    },
    {
      "source": "/:path*/langchain_cloudflare.CloudflareWorkersAIEmbeddingsParams:rest",
      "destination": "/:path*/_langchain_cloudflare.index.CloudflareWorkersAIEmbeddingsParams:rest"
    },
    {
      "source": "/:path*/langchain_cloudflare.CloudflareWorkersAIInput:rest",
      "destination": "/:path*/_langchain_cloudflare.index.CloudflareWorkersAIInput:rest"
    },
    {
      "source": "/:path*/langchain_cloudflare.VectorizeLibArgs:rest",
      "destination": "/:path*/_langchain_cloudflare.index.VectorizeLibArgs:rest"
    },
    {
      "source": "/:path*/langchain_cloudflare.CloudflareD1MessageHistoryInput:rest",
      "destination": "/:path*/_langchain_cloudflare.index.CloudflareD1MessageHistoryInput:rest"
    },
    {
      "source": "/:path*/langchain_cloudflare.VectorizeDeleteParams:rest",
      "destination": "/:path*/_langchain_cloudflare.index.VectorizeDeleteParams:rest"
    },
    {
      "source": "/:path*/langchain_cohere(_|\\.):rest",
      "destination": "/:path*/_langchain_cohere.:rest"
    },
    {
      "source": "/:path*/langchain_community_:rest",
      "destination": "/:path*/_langchain_community.:rest"
    },
    {
      "source": "/:path*/langchain_exa(_|\\.):rest",
      "destination": "/:path*/_langchain_exa.:rest"
    },
    {
      "source": "/:path*/langchain_google_common_types(_|\\.):rest",
      "destination": "/:path*/_langchain_google_common.types.:rest"
    },
    {
      "source": "/:path*/langchain_google_common_utils(_|\\.):rest",
      "destination": "/:path*/_langchain_google_common.utils.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.AbstractGoogleLLMConnection.:rest",
      "destination": "/:path*/_langchain_google_common.index.AbstractGoogleLLMConnection.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.ApiKeyGoogleAuth.:rest",
      "destination": "/:path*/_langchain_google_common.index.ApiKeyGoogleAuth.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.BaseGoogleEmbeddings.:rest",
      "destination": "/:path*/_langchain_google_common.index.BaseGoogleEmbeddings.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.ChatGoogleBase.:rest",
      "destination": "/:path*/_langchain_google_common.index.ChatGoogleBase.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.ComplexJsonStream.:rest",
      "destination": "/:path*/_langchain_google_common.index.ComplexJsonStream.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.GoogleAIConnection.:rest",
      "destination": "/:path*/_langchain_google_common.index.GoogleAIConnection.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.GoogleAbstractedFetchClient.:rest",
      "destination": "/:path*/_langchain_google_common.index.GoogleAbstractedFetchClient.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.GoogleBaseLLM.:rest",
      "destination": "/:path*/_langchain_google_common.index.GoogleBaseLLM.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.GoogleConnection.:rest",
      "destination": "/:path*/_langchain_google_common.index.GoogleConnection.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.GoogleHostConnection.:rest",
      "destination": "/:path*/_langchain_google_common.index.GoogleHostConnection.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.JsonStream.:rest",
      "destination": "/:path*/_langchain_google_common.index.JsonStream.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.ReadableJsonStream.:rest",
      "destination": "/:path*/_langchain_google_common.index.ReadableJsonStream.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.BaseGoogleEmbeddingsOptions.:rest",
      "destination": "/:path*/_langchain_google_common.index.BaseGoogleEmbeddingsOptions.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.BaseGoogleEmbeddingsParams.:rest",
      "destination": "/:path*/_langchain_google_common.index.BaseGoogleEmbeddingsParams.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.ChatGoogleBaseInput.:rest",
      "destination": "/:path*/_langchain_google_common.index.ChatGoogleBaseInput.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.GoogleAbstractedClient.:rest",
      "destination": "/:path*/_langchain_google_common.index.GoogleAbstractedClient.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.GoogleEmbeddingsInstance.:rest",
      "destination": "/:path*/_langchain_google_common.index.GoogleEmbeddingsInstance.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.GoogleEmbeddingsResponse.:rest",
      "destination": "/:path*/_langchain_google_common.index.GoogleEmbeddingsResponse.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.GoogleAbstractedClientOps.:rest",
      "destination": "/:path*/_langchain_google_common.index.GoogleAbstractedClientOps.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.GoogleAbstractedClientOpsMethod.:rest",
      "destination": "/:path*/_langchain_google_common.index.GoogleAbstractedClientOpsMethod.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.GoogleAbstractedClientOpsResponseType.:rest",
      "destination": "/:path*/_langchain_google_common.index.GoogleAbstractedClientOpsResponseType.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.aiPlatformScope.:rest",
      "destination": "/:path*/_langchain_google_common.index.aiPlatformScope.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.complexValue.:rest",
      "destination": "/:path*/_langchain_google_common.index.complexValue.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.convertToGeminiTools.:rest",
      "destination": "/:path*/_langchain_google_common.index.convertToGeminiTools.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.copyAIModelParams.:rest",
      "destination": "/:path*/_langchain_google_common.index.copyAIModelParams.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.copyAIModelParamsInto.:rest",
      "destination": "/:path*/_langchain_google_common.index.copyAIModelParamsInto.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.copyAndValidateModelParamsInto.:rest",
      "destination": "/:path*/_langchain_google_common.index.copyAndValidateModelParamsInto.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.ensureAuthOptionScopes.:rest",
      "destination": "/:path*/_langchain_google_common.index.ensureAuthOptionScopes.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.jsonSchemaToGeminiParameters.:rest",
      "destination": "/:path*/_langchain_google_common.index.jsonSchemaToGeminiParameters.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.modelToFamily.:rest",
      "destination": "/:path*/_langchain_google_common.index.modelToFamily.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.removeAdditionalProperties.:rest",
      "destination": "/:path*/_langchain_google_common.index.removeAdditionalProperties.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.simpleValue.:rest",
      "destination": "/:path*/_langchain_google_common.index.simpleValue.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.validateModelParams.:rest",
      "destination": "/:path*/_langchain_google_common.index.validateModelParams.:rest"
    },
    {
      "source": "/:path*/langchain_google_common.zodToGeminiParameters.:rest",
      "destination": "/:path*/_langchain_google_common.index.zodToGeminiParameters.:rest"
    },
    {
      "source": "/:path*/langchain_google_genai(_|\\.):rest",
      "destination": "/:path*/_langchain_google_genai.:rest"
    },
    {
      "source": "/:path*/langchain_google_vertexai_types(_|\\.):rest",
      "destination": "/:path*/_langchain_google_vertexai.types.:rest"
    },
    {
      "source": "/:path*/langchain_google_vertexai_utils(_|\\.):rest",
      "destination": "/:path*/_langchain_google_vertexai.utils.:rest"
    },
    {
      "source": "/:path*/langchain_google_vertexai.ChatVertexAI.:rest",
      "destination": "/:path*/_langchain_google_vertexai.index.ChatVertexAI.:rest"
    },
    {
      "source": "/:path*/langchain_google_vertexai.VertexAI.:rest",
      "destination": "/:path*/_langchain_google_vertexai.index.VertexAI.:rest"
    },
    {
      "source": "/:path*/langchain_google_vertexai.VertexAIEmbeddings.:rest",
      "destination": "/:path*/_langchain_google_vertexai.index.VertexAIEmbeddings.:rest"
    },
    {
      "source": "/:path*/langchain_google_vertexai.ChatVertexAIInput.:rest",
      "destination": "/:path*/_langchain_google_vertexai.index.ChatVertexAIInput.:rest"
    },
    {
      "source": "/:path*/langchain_google_vertexai.GoogleVertexAIEmbeddingsInput.:rest",
      "destination": "/:path*/_langchain_google_vertexai.index.GoogleVertexAIEmbeddingsInput.:rest"
    },
    {
      "source": "/:path*/langchain_google_vertexai.VertexAIInput.:rest",
      "destination": "/:path*/_langchain_google_vertexai.index.VertexAIInput.:rest"
    },
    {
      "source": "/:path*/langchain_google_vertexai_web_types(_|\\.):rest",
      "destination": "/:path*/_langchain_google_vertexai_web.types.:rest"
    },
    {
      "source": "/:path*/langchain_google_vertexai_web_utils(_|\\.):rest",
      "destination": "/:path*/_langchain_google_vertexai_web.utils.:rest"
    },
    {
      "source": "/:path*/langchain_google_vertexai_web.ChatVertexAI.:rest",
      "destination": "/:path*/_langchain_google_vertexai_web.index.ChatVertexAI.:rest"
    },
    {
      "source": "/:path*/langchain_google_vertexai_web.VertexAI.:rest",
      "destination": "/:path*/_langchain_google_vertexai_web.index.VertexAI.:rest"
    },
    {
      "source": "/:path*/langchain_google_vertexai_web.VertexAIEmbeddings.:rest",
      "destination": "/:path*/_langchain_google_vertexai_web.index.VertexAIEmbeddings.:rest"
    },
    {
      "source": "/:path*/langchain_google_vertexai_web.ChatVertexAIInput.:rest",
      "destination": "/:path*/_langchain_google_vertexai_web.index.ChatVertexAIInput.:rest"
    },
    {
      "source": "/:path*/langchain_google_vertexai_web.GoogleVertexAIEmbeddingsInput.:rest",
      "destination": "/:path*/_langchain_google_vertexai_web.index.GoogleVertexAIEmbeddingsInput.:rest"
    },
    {
      "source": "/:path*/langchain_google_vertexai_web.VertexAIInput.:rest",
      "destination": "/:path*/_langchain_google_vertexai_web.index.VertexAIInput.:rest"
    },
    {
      "source": "/:path*/langchain_groq(_|\\.):rest",
      "destination": "/:path*/_langchain_groq.:rest"
    },
    {
      "source": "/:path*/langchain_mistralai(_|\\.):rest",
      "destination": "/:path*/_langchain_mistralai.:rest"
    },
    {
      "source": "/:path*/langchain_mixedbread_ai(_|\\.):rest",
      "destination": "/:path*/_langchain_mixedbread_ai.:rest"
    },
    {
      "source": "/:path*/langchain_mongodb(_|\\.):rest",
      "destination": "/:path*/_langchain_mongodb.:rest"
    },
    {
      "source": "/:path*/langchain_nomic(_|\\.):rest",
      "destination": "/:path*/_langchain_nomic.:rest"
    },
    {
      "source": "/:path*/langchain_ollama(_|\\.):rest",
      "destination": "/:path*/_langchain_ollama.:rest"
    },
    {
      "source": "/:path*/langchain_openai(_|\\.):rest",
      "destination": "/:path*/_langchain_openai.:rest"
    },
    {
      "source": "/:path*/langchain_pinecone(_|\\.):rest",
      "destination": "/:path*/_langchain_pinecone.:rest"
    },
    {
      "source": "/:path*/langchain_qdrant(_|\\.):rest",
      "destination": "/:path*/_langchain_qdrant.:rest"
    },
    {
      "source": "/:path*/langchain_redis(_|\\.):rest",
      "destination": "/:path*/_langchain_redis.:rest"
    },
    {
      "source": "/:path*/langchain_textsplitters(_|\\.):rest",
      "destination": "/:path*/_langchain_textsplitters.:rest"
    },
    {
      "source": "/:path*/langchain_weaviate(_|\\.):rest",
      "destination": "/:path*/_langchain_weaviate.:rest"
    },
    {
      "source": "/:path*/langchain_yandex(_|\\.):rest",
      "destination": "/:path*/_langchain_yandex.:rest"
    }
  ]
}



================================================
FILE: docs/api_refs/.eslintrc.json
================================================
{
  "extends": "next/core-web-vitals"
}



================================================
FILE: docs/api_refs/.gitignore
================================================
# See https://help.github.com/articles/ignoring-files/ for more about ignoring files.

# dependencies
/node_modules
/.pnp
.pnp.js
.yarn/install-state.gz

# testing
/coverage

# next.js
/.next/
/out/

# production
/build

# misc
.DS_Store
*.pem

# debug
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# local env files
.env*.local

# vercel
.vercel

# typescript
*.tsbuildinfo
next-env.d.ts

# Autogenerated docs
/public/*
/langchain
/langchain-core



================================================
FILE: docs/api_refs/scripts/create-entrypoints.js
================================================
const fs = require("fs");
const path = require("path");
const { execSync } = require("child_process");

const BASE_TYPEDOC_CONFIG = {
  $schema: "https://typedoc.org/schema.json",
  out: "public",
  sort: [
    "kind",
    "visibility",
    "instance-first",
    "required-first",
    "alphabetical",
  ],
  plugin: [
    "./scripts/typedoc-plugin.js",
    "typedoc-plugin-expand-object-like-types",
  ],
  tsconfig: "../../tsconfig.json",
  excludePrivate: true,
  excludeInternal: true,
  excludeExternals: false,
  excludeNotDocumented: false,
  includeVersion: true,
  sourceLinkTemplate:
    "https://github.com/langchain-ai/langchainjs/blob/{gitRevision}/{path}#L{line}",
  logLevel: "Error",
  name: "LangChain.js",
  skipErrorChecking: true,
  exclude: ["dist"],
  hostedBaseUrl: "https://v03.api.js.langchain.com/",
  entryPointStrategy: "packages",
};

/**
 *
 * @param {string} relativePath
 * @param {any} updateFunction
 */
const updateJsonFile = (relativePath, updateFunction) => {
  const contents = fs.readFileSync(relativePath).toString();
  const res = updateFunction(JSON.parse(contents));
  fs.writeFileSync(relativePath, JSON.stringify(res, null, 2) + "\n");
};

const workspacesListBreakStr = `"}
{"`;
const workspacesListJoinStr = `"},{"`;
const BLACKLISTED_WORKSPACES = [
  "@langchain/azure-openai",
  "@langchain/google-gauth",
  "@langchain/google-webauth",
];

/**
 * @returns {Array<string>} An array of paths to all workspaces in the monorepo.
 */
function getYarnWorkspaces() {
  const stdout = execSync("yarn workspaces list --json");
  const workspaces = JSON.parse(
    `[${stdout
      .toString()
      .split(workspacesListBreakStr)
      .join(workspacesListJoinStr)}]`
  );
  const cleanedWorkspaces = workspaces.filter(
    (ws) =>
      ws.name === "langchain" ||
      (ws.name.startsWith("@langchain/") &&
        !BLACKLISTED_WORKSPACES.find((blacklisted) => ws.name === blacklisted))
  );
  return cleanedWorkspaces.map((ws) => `../../${ws.location}`);
}

async function main() {
  const workspaces = fs
    .readdirSync("../../libs/")
    .filter((dir) => dir.startsWith("langchain-"))
    .map((dir) => path.join("../../libs/", dir, "/langchain.config.js"))
    .filter((configPath) => fs.existsSync(configPath));
  const configFiles = [
    "../../langchain/langchain.config.js",
    "../../langchain-core/langchain.config.js",
    ...workspaces,
  ]
    .map((configFile) => path.resolve(configFile))
    .filter((configFile) => !configFile.includes("/langchain-scripts/"));

  /** @type {Array<string>} */
  const blacklistedEntrypoints = JSON.parse(
    fs.readFileSync("./blacklisted-entrypoints.json")
  );

  for await (const configFile of configFiles) {
    const langChainConfig = await import(configFile);
    if (!("entrypoints" in langChainConfig.config)) {
      throw new Error(
        `The config file "${configFile}" does not contain any entrypoints.`
      );
    } else if (
      langChainConfig.config.entrypoints === null ||
      langChainConfig.config.entrypoints === undefined
    ) {
      continue;
    }
    const { config } = langChainConfig;

    const entrypointDir = path.relative(
      process.cwd(),
      configFile.split("/langchain.config.js")[0]
    );

    const deprecatedNodeOnly =
      "deprecatedNodeOnly" in config ? config.deprecatedNodeOnly : [];

    const workspaceEntrypoints = Object.values(config.entrypoints)
      .filter((key) => !deprecatedNodeOnly.includes(key))
      .filter(
        (key) =>
          !blacklistedEntrypoints.find(
            (blacklistedItem) =>
              blacklistedItem === `${entrypointDir}/src/${key}.ts`
          )
      )
      .map((key) => `src/${key}.ts`);

    const typedocPath = path.join(entrypointDir, "typedoc.json");

    if (!fs.existsSync(typedocPath)) {
      fs.writeFileSync(typedocPath, "{}\n");
    }

    updateJsonFile(typedocPath, (existingConfig) => ({
      ...existingConfig,
      entryPoints: workspaceEntrypoints,
      extends: typedocPath.includes("/libs/")
        ? ["../../docs/api_refs/typedoc.base.json"]
        : ["../docs/api_refs/typedoc.base.json"],
    }));
  }

  // Check if the `./typedoc.json` file exists, since it is gitignored by default
  if (!fs.existsSync("./typedoc.json")) {
    fs.writeFileSync("./typedoc.json", "{}\n");
  }

  const yarnWorkspaces = getYarnWorkspaces();

  updateJsonFile("./typedoc.json", () => ({
    ...BASE_TYPEDOC_CONFIG,
    entryPoints: yarnWorkspaces,
  }));
}

async function runMain() {
  try {
    await main();
  } catch (error) {
    console.error("An error occurred while creating the entrypoints.");
    throw error;
  }
}

runMain();



================================================
FILE: docs/api_refs/scripts/typedoc-plugin.js
================================================
const {
  Application,
  Converter,
  Context,
  ReflectionKind,
  DeclarationReflection,
  RendererEvent,
  UrlMapping,
  Reflection,
} = require("typedoc");
const fs = require("fs");
const path = require("path");
const { glob } = require("glob");
const { Project, ClassDeclaration } = require("ts-morph");

// Chat model methods which _should_ be included in the documentation
const WHITELISTED_CHAT_MODEL_INHERITED_METHODS = [
  "invoke",
  "stream",
  "batch",
  "streamLog",
  "streamEvents",
  "bind",
  "bindTools",
  "asTool",
  "pipe",
  "withConfig",
  "withRetry",
  "assign",
  "getNumTokens",
  "getGraph",
  "pick",
  "withFallbacks",
  "withStructuredOutput",
  "withListeners",
  "transform",
];

// Reflection types to check for methods that should not be documented.
// e.g methods prefixed with `_` or `lc_`
const REFLECTION_KINDS_TO_HIDE = [
  ReflectionKind.Property,
  ReflectionKind.Accessor,
  ReflectionKind.Variable,
  ReflectionKind.Method,
  ReflectionKind.Function,
  ReflectionKind.Class,
  ReflectionKind.Interface,
  ReflectionKind.Enum,
  ReflectionKind.TypeAlias,
];

const BASE_OUTPUT_DIR = "./public";

// Script to inject into the HTML to add a CMD/CTRL + K 'hotkey' which focuses
// on the search input element.
const SCRIPT_HTML = `<script>
  document.addEventListener('keydown', (e) => {
    if ((e.metaKey || e.ctrlKey) && e.keyCode === 75) { // Check for CMD + K or CTRL + K
      const input = document.getElementById('tsd-search-field'); // Get the search input element by ID
      input.focus(); // Focus on the search input element
      document.getElementById('tsd-search').style.display = 'block'; // Show the div wrapper with ID tsd-search
    }
  }, false); // Add event listener for keydown events
</script>`;

// Injected into each page's HTML to add a dropdown to switch between versions.
const VERSION_DROPDOWN_HTML = `<div class="version-select">
<select id="version-dropdown" onchange="window.location.href=this.value;">
  <option selected value="">v0.3</option>
  <option value="https://v02.api.js.langchain.com/">v0.2</option>
  <option value="https://v01.api.js.langchain.com/">v0.1</option>
</select>
</div>`;

/**
 * HTML injected into sections where there is a `@deprecated` JSDoc tag.
 * This provides a far more visible warning to the user that the feature is
 * deprecated.
 *
 * @param {string | undefined} deprecationText
 * @returns {string}
 */
const DEPRECATION_HTML = (deprecationText) => `<div class="deprecation-warning">
<h2>⚠️ Deprecated ⚠️</h2>
${deprecationText ? `<p>${deprecationText}</p>` : ""}
<p>This feature is deprecated and will be removed in the future.</p>
<p>It is not recommended for use.</p>
</div>`;

/**
 * Uses ts-morph to check if the class is a subclass of `BaseChatModel` or
 * `SimpleChatModel`.
 *
 * @param {ClassDeclaration} classDeclaration
 * @returns {boolean}
 */
function isBaseChatModelOrSimpleChatModel(classDeclaration) {
  let currentClass = classDeclaration;
  while (currentClass) {
    const baseClassName = currentClass.getBaseClass()?.getName();
    if (
      baseClassName === "BaseChatModel" ||
      baseClassName === "SimpleChatModel"
    ) {
      return true;
    }
    currentClass = currentClass.getBaseClass();
  }
  return false;
}

/**
 * Uses ts-morph to load all chat model files, and extract the names of the
 * classes. This is then used to remove unwanted properties from showing up
 * in the documentation of those classes.
 *
 * @returns {Array<string>}
 */
function getAllChatModelNames() {
  const communityChatModelPath =
    "../../libs/langchain-community/src/chat_models/*";
  const communityChatModelNestedPath =
    "../../libs/langchain-community/src/chat_models/**/*";
  const partnerPackageGlob =
    "../../libs/!(langchain-community)/**/chat_models.ts";
  const partnerPackageFiles = glob.globSync(partnerPackageGlob);

  const tsMorphProject = new Project();
  const sourceFiles = tsMorphProject.addSourceFilesAtPaths([
    communityChatModelPath,
    communityChatModelNestedPath,
    ...partnerPackageFiles,
  ]);

  const chatModelNames = [];
  for (const sourceFile of sourceFiles) {
    const exportedClasses = sourceFile.getClasses();
    for (const exportedClass of exportedClasses) {
      if (isBaseChatModelOrSimpleChatModel(exportedClass)) {
        chatModelNames.push(exportedClass.getName());
      }
    }
  }
  return chatModelNames.flatMap((n) => (n ? [n] : []));
}

/**
 * Takes in a reflection and an array of all chat model class names.
 * Then performs checks to see if the given reflection should be removed
 * from the documentation.
 * E.g a class method on chat models which is
 * not intended to be documented.
 *
 * @param {DeclarationReflection} reflection
 * @param {Array<string>} chatModelNames
 * @returns {boolean} Whether or not the reflection should be removed
 */
function shouldRemoveReflection(reflection, chatModelNames) {
  const kind = reflection.kind;

  if (
    reflection.parent &&
    chatModelNames.find((name) => name === reflection.parent.name) &&
    reflection.name !== "constructor"
  ) {
    if (kind === ReflectionKind.Property) {
      return true;
    }
    if (
      !WHITELISTED_CHAT_MODEL_INHERITED_METHODS.find(
        (n) => n === reflection.name
      )
    ) {
      return true;
    }
    if (kind === ReflectionKind.Accessor && reflection.name === "callKeys") {
      return true;
    }
  }

  if (REFLECTION_KINDS_TO_HIDE.find((kindToHide) => kindToHide === kind)) {
    if (reflection.name.startsWith("_") || reflection.name.startsWith("lc_")) {
      // Remove all reflections which start with an `_` or `lc_`
      return true;
    }
  }
}

/**
 * @param {Application} application
 * @returns {void}
 */
function load(application) {
  let allChatModelNames = [];
  try {
    allChatModelNames = getAllChatModelNames();
  } catch (err) {
    console.error("Error while getting all chat model names");
    throw err;
  }

  application.converter.on(
    Converter.EVENT_CREATE_DECLARATION,
    resolveReflection
  );

  application.renderer.on(RendererEvent.END, onEndRenderEvent);

  /**
   * @param {Context} context
   * @param {DeclarationReflection} reflection
   * @returns {void}
   */
  function resolveReflection(context, reflection) {
    const { project } = context;

    if (shouldRemoveReflection(reflection, allChatModelNames)) {
      project.removeReflection(reflection);
    }
  }

  /**
   * @param {Context} context
   */
  function onEndRenderEvent(context) {
    const htmlToSplitAtSearchScript = `<div class="tsd-toolbar-contents container">`;
    const htmlToSplitAtVersionDropdown = `<div id="tsd-toolbar-links">`;
    const deprecatedHTML = "<h4>Deprecated</h4>";

    const { urls } = context;
    for (const { url } of urls) {
      const indexFilePath = path.join(BASE_OUTPUT_DIR, url);
      let htmlFileContent = fs.readFileSync(indexFilePath, "utf-8");

      if (htmlFileContent.includes(deprecatedHTML)) {
        // If any comments are added to the `@deprecated` JSDoc, they'll
        // be inside the following <p> tag.
        const deprecationTextRegex = new RegExp(
          `${deprecatedHTML}<p>(.*?)</p>`
        );
        const deprecationTextMatch =
          htmlFileContent.match(deprecationTextRegex);

        /** @type {string | undefined} */
        let textInsidePTag;

        if (deprecationTextMatch) {
          textInsidePTag = deprecationTextMatch[1];
          const newTextToReplace = `${deprecatedHTML}<p>${textInsidePTag}</p>`;
          htmlFileContent = htmlFileContent.replace(
            newTextToReplace,
            DEPRECATION_HTML(textInsidePTag)
          );
        } else {
          htmlFileContent = htmlFileContent.replace(
            deprecatedHTML,
            DEPRECATION_HTML(undefined)
          );
        }
      }

      const [part1, part2] = htmlFileContent.split(htmlToSplitAtSearchScript);
      const htmlWithScript = part1 + SCRIPT_HTML + part2;
      const htmlWithDropdown = htmlWithScript.replace(
        htmlToSplitAtVersionDropdown,
        htmlToSplitAtVersionDropdown + VERSION_DROPDOWN_HTML
      );
      fs.writeFileSync(indexFilePath, htmlWithDropdown);
    }
  }
}

module.exports = { load };



================================================
FILE: docs/api_refs/scripts/update-typedoc-css.js
================================================
const { readFile, writeFile } = require("fs/promises");

const CSS = `\n.tsd-navigation {
  word-break: break-word;
}

.page-menu {
  display: none;
}

.deprecation-warning {
  background-color: #ef4444;
  border-radius: 0.375rem;
  display: flex;
  flex-direction: column;
  padding: 12px;
  text-align: left;
}

.version-select {
  display: inline-block;
  margin-left: 10px;
  z-index: 1;
}

.version-select select {
  padding: 2.5px 5px;
  font-size: 14px;
  border: 1px solid #ccc;
  border-radius: 4px;
  background-color: #fff;
  color: #333;
  cursor: pointer;
}

.version-select select:hover {
  border-color: #999;
}

.version-select select:focus {
  outline: none;
  box-shadow: 0 0 3px rgba(0, 0, 0, 0.2);
}
`;

async function main() {
  let cssContents = await readFile("./public/assets/style.css", "utf-8");
  cssContents += CSS;
  await writeFile("./public/assets/style.css", cssContents);
}

main();




================================================
FILE: docs/core_docs/README.md
================================================
# Website

This website is built using [Docusaurus 2](https://docusaurus.io/), a modern static website generator.

### Installation

First, make sure you have [dependencies installed](https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md#install-dependencies).

```
$ yarn
```

### Local Development

```
$ yarn start
```

This command starts a local development server and opens up a browser window. Most changes are reflected live without having to restart the server.

### Build

```
$ yarn build
```

This command generates static content into the `build` directory and can be served using any static contents hosting service.

### Deployment

Using SSH:

```
$ USE_SSH=true yarn deploy
```

Not using SSH:

```
$ GIT_USER=<Your GitHub username> yarn deploy
```

If you are using GitHub pages for hosting, this command is a convenient way to build the website and push to the `gh-pages` branch.

### Continuous Integration

Some common defaults for linting/formatting have been set for you. If you integrate your project with an open source Continuous Integration system (e.g. Travis CI, CircleCI), you may check for issues using the following command.

```
$ yarn ci
```

### Validating Notebooks

You can validate that notebooks build and compile TypeScript using the following command:

```bash
$ yarn validate <PATH_TO_FILE>
```



================================================
FILE: docs/core_docs/babel.config.js
================================================
/**
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 *
 * @format
 */

module.exports = {
  presets: [require.resolve("@docusaurus/core/lib/babel/preset")],
};



================================================
FILE: docs/core_docs/docusaurus.config.js
================================================
/* eslint-disable global-require,import/no-extraneous-dependencies */

// @ts-check
// Note: type annotations allow type checking and IDEs autocompletion
// eslint-disable-next-line import/no-extraneous-dependencies
const { ProvidePlugin } = require("webpack");
const path = require("path");
require("dotenv").config();

const examplesPath = path.resolve(__dirname, "..", "..", "examples", "src");
const mdxComponentsPath = path.resolve(__dirname, "docs", "mdx_components");

const baseLightCodeBlockTheme = require("prism-react-renderer/themes/vsLight");
const baseDarkCodeBlockTheme = require("prism-react-renderer/themes/vsDark");

const baseUrl = "/";

/** @type {import('@docusaurus/types').Config} */
const config = {
  title: "🦜️🔗 Langchain",
  tagline: "LangChain.js Docs",
  favicon: "img/brand/favicon.png",
  // Set the production url of your site here
  url: "https://js.langchain.com",
  // Set the /<baseUrl>/ pathname under which your site is served
  // For GitHub pages deployment, it is often '/<projectName>/'
  baseUrl,

  onBrokenLinks: "throw",
  onBrokenMarkdownLinks: "throw",

  plugins: [
    () => ({
      name: "custom-webpack-config",
      configureWebpack: () => ({
        plugins: [
          new ProvidePlugin({
            process: require.resolve("process/browser"),
          }),
        ],
        resolve: {
          fallback: {
            path: false,
            url: false,
          },
          alias: {
            "@examples": examplesPath,
            "@mdx_components": mdxComponentsPath,
            react: path.resolve("../../node_modules/react"),
          },
        },
        module: {
          rules: [
            {
              test: examplesPath,
              use: ["json-loader", "./scripts/code-block-loader.js"],
            },
            {
              test: /\.ya?ml$/,
              use: "yaml-loader",
            },
            {
              test: /\.m?js/,
              resolve: {
                fullySpecified: false,
              },
            },
          ],
        },
      }),
    }),
  ],

  presets: [
    [
      "classic",
      /** @type {import('@docusaurus/preset-classic').Options} */
      ({
        docs: {
          sidebarPath: require.resolve("./sidebars.js"),
          remarkPlugins: [
            [require("@docusaurus/remark-plugin-npm2yarn"), { sync: true }],
          ],
          async sidebarItemsGenerator({
            defaultSidebarItemsGenerator,
            ...args
          }) {
            const sidebarItems = await defaultSidebarItemsGenerator(args);
            sidebarItems.forEach((subItem) => {
              // This allows breaking long sidebar labels into multiple lines
              // by inserting a zero-width space after each slash.
              if (
                "label" in subItem &&
                subItem.label &&
                subItem.label.includes("/")
              ) {
                // eslint-disable-next-line no-param-reassign
                subItem.label = subItem.label.replace(/\//g, "/\u200B");
              }
              if (args.item.className) {
                subItem.className = args.item.className;
              }
            });
            return sidebarItems;
          },
        },
        pages: {
          remarkPlugins: [require("@docusaurus/remark-plugin-npm2yarn")],
        },
        theme: {
          customCss: require.resolve("./src/css/custom.css"),
        },
      }),
    ],
  ],

  webpack: {
    jsLoader: (isServer) => ({
      loader: require.resolve("swc-loader"),
      options: {
        jsc: {
          parser: {
            syntax: "typescript",
            tsx: true,
          },
          target: "es2017",
        },
        module: {
          type: isServer ? "commonjs" : "es6",
        },
      },
    }),
  },

  themeConfig:
    /** @type {import('@docusaurus/preset-classic').ThemeConfig} */
    ({
      announcementBar: {
        content:
          '<strong class="announcement-bar-text">Join us at <a href="https://interrupt.langchain.com/" target="_blank" rel="noopener noreferrer"> Interrupt: The Agent AI Conference by LangChain</a> on May 13 & 14 in San Francisco!</strong>',
        backgroundColor: "#d0c9fe",
      },
      prism: {
        theme: {
          ...baseLightCodeBlockTheme,
          plain: {
            ...baseLightCodeBlockTheme.plain,
            backgroundColor: "#F5F5F5",
          },
        },
        darkTheme: {
          ...baseDarkCodeBlockTheme,
          plain: {
            ...baseDarkCodeBlockTheme.plain,
            backgroundColor: "#222222",
          },
        },
      },
      image: "img/brand/theme-image.png",
      navbar: {
        logo: {
          src: "img/brand/wordmark.png",
          srcDark: "img/brand/wordmark-dark.png",
        },
        items: [
          {
            type: "docSidebar",
            position: "left",
            sidebarId: "integrations",
            label: "Integrations",
          },
          {
            href: "https://v03.api.js.langchain.com",
            label: "API Reference",
            position: "left",
          },
          {
            type: "dropdown",
            label: "More",
            position: "left",
            items: [
              {
                to: "/docs/people/",
                label: "People",
              },
              {
                to: "/docs/community",
                label: "Community",
              },
              {
                to: "/docs/troubleshooting/errors",
                label: "Error reference",
              },
              {
                to: "/docs/additional_resources/tutorials",
                label: "External guides",
              },
              {
                to: "/docs/contributing",
                label: "Contributing",
              },
            ],
          },
          {
            type: "dropdown",
            label: "v0.3",
            position: "right",
            items: [
              {
                label: "v0.3",
                href: "/docs/introduction",
              },
              {
                label: "v0.2",
                href: "https://js.langchain.com/v0.2/docs/introduction",
              },
              {
                label: "v0.1",
                href: "https://js.langchain.com/v0.1/docs/get_started/introduction",
              },
            ],
          },
          {
            type: "dropdown",
            label: "🦜🔗",
            position: "right",
            items: [
              {
                href: "https://smith.langchain.com",
                label: "LangSmith",
              },
              {
                href: "https://docs.smith.langchain.com",
                label: "LangSmith Docs",
              },
              {
                href: "https://smith.langchain.com/hub",
                label: "LangChain Hub",
              },
              {
                href: "https://github.com/langchain-ai/langserve",
                label: "LangServe",
              },
              {
                href: "https://python.langchain.com/",
                label: "Python Docs",
              },
            ],
          },
          {
            href: "https://chatjs.langchain.com",
            label: "Chat",
            position: "right",
          },
          // Please keep GitHub link to the right for consistency.
          {
            href: "https://github.com/langchain-ai/langchainjs",
            className: "header-github-link",
            position: "right",
            "aria-label": "GitHub repository",
          },
        ],
      },
      footer: {
        style: "light",
        links: [
          {
            title: "Community",
            items: [
              {
                label: "Twitter",
                href: "https://twitter.com/LangChainAI",
              },
            ],
          },
          {
            title: "GitHub",
            items: [
              {
                label: "Python",
                href: "https://github.com/langchain-ai/langchain",
              },
              {
                label: "JS/TS",
                href: "https://github.com/langchain-ai/langchainjs",
              },
            ],
          },
          {
            title: "More",
            items: [
              {
                label: "Homepage",
                href: "https://langchain.com",
              },
              {
                label: "Blog",
                href: "https://blog.langchain.dev",
              },
            ],
          },
        ],
        copyright: `Copyright © ${new Date().getFullYear()} LangChain, Inc.`,
      },
      algolia: {
        // The application ID provided by Algolia
        appId: "3EZV6U1TYC",

        // Public API key: it is safe to commit it
        // this is linked to erick@langchain.dev currently
        apiKey: "180851bbb9ba0ef6be9214849d6efeaf",

        indexName: "js-langchain-latest",

        contextualSearch: false,
      },
    }),

  scripts: [
    baseUrl + "js/google_analytics.js",
    {
      src: "https://www.googletagmanager.com/gtag/js?id=G-TVSL7JBE9Y",
      async: true,
    },
  ],

  customFields: {
    supabasePublicKey: process.env.NEXT_PUBLIC_SUPABASE_PUBLIC_KEY,
    supabaseUrl: process.env.NEXT_PUBLIC_SUPABASE_URL,
  },
};

module.exports = config;



================================================
FILE: docs/core_docs/package.json
================================================
{
  "name": "core_docs",
  "version": "0.0.0",
  "private": true,
  "scripts": {
    "docusaurus": "docusaurus",
    "start": "yarn quarto && rimraf ./docs/api && NODE_OPTIONS=--max-old-space-size=7168 docusaurus start",
    "build": "yarn clean && yarn quarto && rimraf ./build && NODE_OPTIONS=--max-old-space-size=7168 DOCUSAURUS_SSR_CONCURRENCY=4 docusaurus build",
    "build:vercel": "yarn clean && bash ./scripts/vercel_build.sh && yarn quarto:vercel && rimraf ./build && NODE_OPTIONS=--max-old-space-size=7168 DOCUSAURUS_SSR_CONCURRENCY=4 docusaurus build",
    "swizzle": "docusaurus swizzle",
    "deploy": "docusaurus deploy",
    "clear": "docusaurus clear",
    "serve": "docusaurus serve",
    "write-translations": "docusaurus write-translations",
    "write-heading-ids": "docusaurus write-heading-ids",
    "lint:eslint": "NODE_OPTIONS=--max-old-space-size=4096 eslint --cache --ext .ts,.js src/",
    "lint": "yarn lint:eslint",
    "lint:fix": "yarn lint --fix",
    "precommit": "lint-staged",
    "format": "prettier --write \"**/*.{js,jsx,ts,tsx,md,mdx}\"",
    "format:check": "prettier --check \"**/*.{js,jsx,ts,tsx,md,mdx}\"",
    "clean": "rm -rf .docusaurus/ .turbo/ .build/",
    "quarto": "quarto render docs/ && node ./scripts/quarto-build.js && python3 ./scripts/append_related_links.py ./docs",
    "quarto:vercel": "node ./scripts/quarto-build.js && python3 ./scripts/append_related_links.py ./docs",
    "gen": "yarn gen:supabase",
    "gen:supabase": "npx supabase gen types typescript --project-id 'xsqpnijvmbodcxyapnyq' --schema public > ./src/supabase.d.ts",
    "broken-links": "node ./scripts/check-broken-links.js",
    "check:broken-links": "yarn quarto && yarn broken-links",
    "check:broken-links:ci": "yarn quarto:vercel && yarn broken-links",
    "validate": "yarn notebook_validate"
  },
  "dependencies": {
    "@docusaurus/core": "2.4.3",
    "@docusaurus/preset-classic": "2.4.3",
    "@docusaurus/remark-plugin-npm2yarn": "2.4.3",
    "@docusaurus/theme-mermaid": "2.4.3",
    "@mdx-js/react": "^1.6.22",
    "@supabase/supabase-js": "^2.45.0",
    "clsx": "^1.2.1",
    "cookie": "^0.6.0",
    "isomorphic-dompurify": "^2.9.0",
    "json-loader": "^0.5.7",
    "marked": "^12.0.2",
    "process": "^0.11.10",
    "react": "^17.0.2",
    "react-dom": "^17.0.2",
    "uuid": "^10.0.0",
    "webpack": "^5.75.0"
  },
  "devDependencies": {
    "@babel/eslint-parser": "^7.18.2",
    "@langchain/langgraph": "^0.2.34",
    "@langchain/scripts": "workspace:*",
    "@microsoft/fetch-event-source": "^2.0.1",
    "@swc/core": "^1.3.62",
    "@types/cookie": "^0",
    "docusaurus-plugin-typedoc": "1.0.0-next.5",
    "dotenv": "^16.4.5",
    "eslint": "^8.19.0",
    "eslint-config-airbnb": "^19.0.4",
    "eslint-config-prettier": "^8.5.0",
    "eslint-plugin-header": "^3.1.1",
    "eslint-plugin-import": "^2.26.0",
    "eslint-plugin-jsx-a11y": "^6.6.0",
    "eslint-plugin-react": "^7.30.1",
    "eslint-plugin-react-hooks": "^4.6.0",
    "glob": "^10.3.10",
    "prettier": "^2.8.3",
    "rimraf": "^5.0.1",
    "supabase": "^1.148.6",
    "swc-loader": "^0.2.3",
    "ts-morph": "^23.0.0",
    "tsx": "^3.12.3",
    "typedoc": "^0.24.4",
    "typedoc-plugin-markdown": "next",
    "typescript": "~5.1.6",
    "yaml-loader": "^0.8.0"
  },
  "packageManager": "yarn@3.4.1",
  "browserslist": {
    "production": [
      ">0.5%",
      "not dead",
      "not op_mini all"
    ],
    "development": [
      "last 1 chrome version",
      "last 1 firefox version",
      "last 1 safari version"
    ]
  },
  "resolutions": {
    "typedoc-plugin-markdown@next": "patch:typedoc-plugin-markdown@npm%3A4.0.0-next.6#./.yarn/patches/typedoc-plugin-markdown-npm-4.0.0-next.6-96b4b47746.patch"
  },
  "engines": {
    "node": ">=18"
  }
}



================================================
FILE: docs/core_docs/sidebars.js
================================================
/**
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 *
 * @format
 */

/**
 * Creating a sidebar enables you to:
 - create an ordered group of docs
 - render a sidebar for each doc of that group
 - provide next/previous navigation

 The sidebars can be generated from the filesystem, or explicitly defined here.

 Create as many sidebars as you want.
 */

module.exports = {
  docs: [
    "introduction",
    {
      type: "category",
      link: { type: "doc", id: "tutorials/index" },
      label: "Tutorials",
      collapsible: false,
      items: [
        {
          type: "autogenerated",
          dirName: "tutorials",
          className: "hidden",
        },
      ],
    },
    {
      type: "category",
      link: { type: "doc", id: "how_to/index" },
      label: "How-to guides",
      collapsible: false,
      items: [
        {
          type: "autogenerated",
          dirName: "how_to",
          className: "hidden",
        },
      ],
    },
    {
      type: "category",
      link: { type: "doc", id: "concepts/index" },
      label: "Conceptual Guide",
      collapsible: false,
      items: [
        {
          type: "autogenerated",
          dirName: "concepts",
          className: "hidden",
        },
      ],
    },
    {
      type: "category",
      label: "Ecosystem",
      collapsed: false,
      collapsible: false,
      items: [
        {
          type: "link",
          href: "https://docs.smith.langchain.com/",
          label: "🦜🛠️ LangSmith",
        },
        {
          type: "link",
          href: "https://langchain-ai.github.io/langgraphjs/",
          label: "🦜🕸️ LangGraph.js",
        },
      ],
    },
    {
      type: "category",
      label: "Versions",
      collapsed: false,
      collapsible: false,
      items: [
        {
          type: "doc",
          id: "versions/v0_3/index",
          label: "v0.3",
        },
        {
          type: "category",
          label: "v0.2",
          items: [
            {
              type: "autogenerated",
              dirName: "versions/v0_2",
            },
          ],
        },
        {
          type: "category",
          label: "Migrating from v0.0 memory",
          link: { type: "doc", id: "versions/migrating_memory/index" },
          collapsible: false,
          collapsed: false,
          items: [
            {
              type: "autogenerated",
              dirName: "versions/migrating_memory",
              className: "hidden",
            },
          ],
        },
        "versions/release_policy",
      ],
    },
    "security",
  ],
  integrations: [
    {
      type: "category",
      label: "Providers",
      collapsible: false,
      items: [
        {
          type: "autogenerated",
          dirName: "integrations/platforms",
        },
        {
          type: "category",
          label: "More",
          collapsed: true,
          items: [
            {
              type: "autogenerated",
              dirName: "integrations/providers",
            },
          ],
          link: {
            type: "generated-index",
            slug: "integrations/providers",
          },
        },
      ],
      link: {
        type: "doc",
        id: "integrations/platforms/index",
      },
    },
    {
      type: "category",
      label: "Components",
      collapsible: false,
      items: [
        {
          type: "category",
          label: "Chat models",
          collapsible: false,
          items: [
            {
              type: "autogenerated",
              dirName: "integrations/chat",
              className: "hidden",
            },
          ],
          link: {
            type: "doc",
            id: "integrations/chat/index",
          },
        },
        {
          type: "category",
          label: "LLMs",
          collapsible: false,
          items: [
            {
              type: "autogenerated",
              dirName: "integrations/llms",
              className: "hidden",
            },
          ],
          link: {
            type: "doc",
            id: "integrations/llms/index",
          },
        },
        {
          type: "category",
          label: "Embedding models",
          collapsible: false,
          items: [
            {
              type: "autogenerated",
              dirName: "integrations/text_embedding",
              className: "hidden",
            },
          ],
          link: {
            type: "doc",
            id: "integrations/text_embedding/index",
          },
        },
        {
          type: "category",
          label: "Document loaders",
          collapsed: true,
          items: [
            {
              type: "category",
              label: "File loaders",
              collapsible: false,
              items: [
                {
                  type: "autogenerated",
                  dirName: "integrations/document_loaders/file_loaders",
                  className: "hidden",
                },
              ],
              link: {
                type: "doc",
                id: "integrations/document_loaders/file_loaders/index",
              },
            },
            {
              type: "category",
              label: "Web loaders",
              collapsible: false,
              items: [
                {
                  type: "autogenerated",
                  dirName: "integrations/document_loaders/web_loaders",
                  className: "hidden",
                },
              ],
              link: {
                type: "doc",
                id: "integrations/document_loaders/web_loaders/index",
              },
            },
          ],
          link: {
            type: "doc",
            id: "integrations/document_loaders/index",
          },
        },
        {
          type: "category",
          label: "Vector stores",
          collapsible: false,
          items: [
            {
              type: "autogenerated",
              dirName: "integrations/vectorstores",
              className: "hidden",
            },
          ],
          link: {
            type: "doc",
            id: "integrations/vectorstores/index",
          },
        },
        {
          type: "category",
          label: "Retrievers",
          collapsible: false,
          items: [
            {
              type: "autogenerated",
              dirName: "integrations/retrievers",
              className: "hidden",
            },
          ],
          link: {
            type: "doc",
            id: "integrations/retrievers/index",
          },
        },
        {
          type: "category",
          label: "Tools/Toolkits",
          collapsible: false,
          items: [
            {
              type: "autogenerated",
              dirName: "integrations/tools",
              className: "hidden",
            },
          ],
          link: {
            type: "doc",
            id: "integrations/tools/index",
          },
        },
        {
          type: "category",
          label: "Toolkits",
          collapsible: false,
          className: "hidden",
          items: [
            {
              type: "autogenerated",
              dirName: "integrations/toolkits",
              className: "hidden",
            },
          ],
          link: {
            type: "doc",
            id: "integrations/tools/index",
          },
        },
        {
          type: "category",
          label: "Key-value stores",
          collapsible: false,
          items: [
            {
              type: "autogenerated",
              dirName: "integrations/stores",
              className: "hidden",
            },
          ],
          link: {
            type: "doc",
            id: "integrations/stores/index",
          },
        },
        {
          type: "category",
          label: "Other",
          collapsed: true,
          items: [
            {
              type: "category",
              label: "Document transformers",
              collapsible: false,
              items: [
                {
                  type: "autogenerated",
                  dirName: "integrations/document_transformers",
                  className: "hidden",
                },
              ],
              link: {
                type: "generated-index",
                slug: "integrations/document_transformers",
              },
            },
            {
              type: "category",
              label: "Document rerankers",
              collapsible: false,
              items: [
                {
                  type: "autogenerated",
                  dirName: "integrations/document_compressors",
                  className: "hidden",
                },
              ],
              link: {
                type: "generated-index",
                slug: "integrations/document_compressors",
              },
            },
            {
              type: "category",
              label: "Model caches",
              collapsible: false,
              items: [
                {
                  type: "autogenerated",
                  dirName: "integrations/llm_caching",
                  className: "hidden",
                },
              ],
              link: {
                type: "doc",
                id: "integrations/llm_caching/index",
              },
            },
            {
              type: "category",
              label: "Graphs",
              collapsible: false,
              items: [
                {
                  type: "autogenerated",
                  dirName: "integrations/graphs",
                  className: "hidden",
                },
              ],
              link: {
                type: "generated-index",
                slug: "integrations/graphs",
              },
            },
            {
              type: "category",
              label: "Memory",
              collapsible: false,
              items: [
                {
                  type: "autogenerated",
                  dirName: "integrations/memory",
                  className: "hidden",
                },
              ],
              link: {
                type: "generated-index",
                slug: "integrations/memory",
              },
            },
            {
              type: "category",
              label: "Callbacks",
              collapsible: false,
              items: [
                {
                  type: "autogenerated",
                  dirName: "integrations/callbacks",
                  className: "hidden",
                },
              ],
              link: {
                type: "generated-index",
                slug: "integrations/callbacks",
              },
            },
            {
              type: "category",
              label: "Chat loaders",
              collapsible: false,
              items: [
                {
                  type: "autogenerated",
                  dirName: "integrations/chat_loaders",
                  className: "hidden",
                },
              ],
              link: {
                type: "generated-index",
                slug: "integrations/chat_loaders",
              },
            },
            {
              type: "category",
              label: "Adapters",
              collapsible: false,
              items: [
                {
                  type: "autogenerated",
                  dirName: "integrations/adapters",
                  className: "hidden",
                },
              ],
              link: {
                type: "generated-index",
                slug: "integrations/adapters",
              },
            },
          ],
        },
      ],
      link: {
        type: "generated-index",
        slug: "integrations/components",
      },
    },
  ],
  contributing: [
    {
      type: "category",
      label: "Contributing",
      items: [
        {
          type: "autogenerated",
          dirName: "contributing",
        },
      ],
    },
  ],
};



================================================
FILE: docs/core_docs/vercel.json
================================================
{
  "buildCommand": "yarn build:vercel",
  "outputDirectory": "build",
  "trailingSlash": true,
  "rewrites": [
    {
      "source": "/v0.1/:path(.*/?)*",
      "destination": "https://langchainjs-v01.vercel.app/v0.1/:path*"
    },
    {
      "source": "/v0.2/:path(.*/?)*",
      "destination": "https://langchainjs-v02.vercel.app/v0.2/:path*"
    }
  ],
  "redirects": [
    {
      "source": "/docs/how_to/callbacks_backgrounding(/?)",
      "destination": "/docs/how_to/callbacks_serverless/"
    },
    {
      "source": "/docs/get_started/introduction(/?)",
      "destination": "/docs/introduction/"
    },
    {
      "source": "/docs(/?)",
      "destination": "/docs/introduction/"
    },
    {
      "source": "/docs/get_started/introduction(/?)",
      "destination": "/docs/introduction/"
    },
    {
      "source": "/docs/how_to/tool_calls_multi_modal(/?)",
      "destination": "/docs/how_to/multimodal_inputs/"
    },
    {
      "source": "/docs/langgraph(/?)",
      "destination": "https://langchain-ai.github.io/langgraphjs/"
    },
    {
      "source": "/docs/langsmith(/?)",
      "destination": "https://docs.smith.langchain.com/"
    },
    {
      "source": "/docs/integrations/chat/chrome_ai(/?)",
      "destination": "/docs/integrations/llms/chrome_ai/"
    },
    {
      "source": "/docs/integrations/retrievers/vectorstore(/?)",
      "destination": "/docs/how_to/vectorstore_retriever/"
    },
    {
      "source": "/docs/integrations/chat_memory(/?)",
      "destination": "/docs/integrations/memory/"
    },
    {
      "source": "/docs/integrations/chat_memory/:path(.*/?)*",
      "destination": "/docs/integrations/memory/:path*"
    },
    {
      "source": "/docs/integrations/llms/togetherai(/?)",
      "destination": "/docs/integrations/llms/together/"
    },
    {
      "source": "/docs/tutorials/query_analysis(/?)",
      "destination": "/docs/tutorials/rag#query-analysis/"
    },
    {
      "source": "/docs/tutorials/local_rag(/?)",
      "destination": "/docs/tutorials/rag/"
    },
    {
      "source": "/docs/tutorials/pdf_qa(/?)",
      "destination": "/docs/tutorials/retrievers/"
    },
    {
      "source": "/docs/tutorials/agents(/?)",
      "destination": "https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/"
    },
    {
      "source": "/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT(/?)",
      "destination": "https://langchain-ai.github.io/langgraphjs/troubleshooting/errors/GRAPH_RECURSION_LIMIT/"
    },
    {
      "source": "/docs/troubleshooting/errors/INVALID_CONCURRENT_GRAPH_UPDATE(/?)",
      "destination": "https://langchain-ai.github.io/langgraphjs/troubleshooting/errors/INVALID_CONCURRENT_GRAPH_UPDATE/"
    },
    {
      "source": "/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE(/?)",
      "destination": "https://langchain-ai.github.io/langgraphjs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE/"
    },
    {
      "source": "/docs/troubleshooting/errors/MULTIPLE_SUBGRAPHS(/?)",
      "destination": "https://langchain-ai.github.io/langgraphjs/troubleshooting/errors/MULTIPLE_SUBGRAPHS/"
    },
    {
      "source": "/docs/integrations/llms/watsonx_ai(/?)",
      "destination": "https://js.langchain.com/docs/integrations/llms/ibm/"
    },
    {
      "source": "/docs/modules/model_io/prompts/quick_start/",
      "destination": "/docs/concepts/prompt_templates"
    },
    {
      "source": "/docs/modules/model_io/prompts(/?)",
      "destination": "/docs/concepts/prompt_templates"
    },
    {
      "source": "/docs/guides/expression_language/cookbook(/?)",
      "destination": "/docs/how_to/sequence"
    },
    {
      "source": "/docs/modules/model_io/models(/?)",
      "destination": "/docs/integrations/chat/"
    }
  ]
}



================================================
FILE: docs/core_docs/.eslintrc.js
================================================
/**
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 *
 * @format
 */

const OFF = 0;
const WARNING = 1;
const ERROR = 2;

module.exports = {
  root: true,
  env: {
    browser: true,
    commonjs: true,
    jest: true,
    node: true,
  },
  parser: "@babel/eslint-parser",
  parserOptions: {
    allowImportExportEverywhere: true,
  },
  extends: ["airbnb", "prettier"],
  plugins: ["react-hooks", "header"],
  ignorePatterns: [
    "build",
    "docs/api",
    "node_modules",
    "docs/_static",
    "static",
  ],
  rules: {
    // Ignore certain webpack alias because it can't be resolved
    "import/no-unresolved": [
      ERROR,
      { ignore: ["^@theme", "^@docusaurus", "^@generated"] },
    ],
    "import/extensions": OFF,
    "react/jsx-filename-extension": OFF,
    "react-hooks/rules-of-hooks": ERROR,
    "react/jsx-props-no-spreading": OFF,
    "react/prop-types": OFF, // PropTypes aren't used much these days.
    "react/function-component-definition": [
      WARNING,
      {
        namedComponents: "function-declaration",
        unnamedComponents: "arrow-function",
      },
    ],
  },
};



================================================
FILE: docs/core_docs/.gitignore
================================================
# Dependencies
/node_modules

# Production
/build

# Generated files
.docusaurus
.cache-loader
docs/api
src/supabase.d.ts

# Misc
.DS_Store
.env.local
.env.development.local
.env.test.local
.env.production.local

npm-debug.log*
yarn-debug.log*
yarn-error.log*
/scripts/tmp

# ESLint
.eslintcache

.yarn/*
!.yarn/patches
!.yarn/plugins
!.yarn/releases
!.yarn/sdks
!.yarn/versions

/.quarto/
# AUTO_GENERATED_DOCS
docs/tutorials/summarization.md
docs/tutorials/summarization.mdx
docs/tutorials/sql_qa.md
docs/tutorials/sql_qa.mdx
docs/tutorials/retrievers.md
docs/tutorials/retrievers.mdx
docs/tutorials/rag.md
docs/tutorials/rag.mdx
docs/tutorials/qa_chat_history.md
docs/tutorials/qa_chat_history.mdx
docs/tutorials/llm_chain.md
docs/tutorials/llm_chain.mdx
docs/tutorials/graph.md
docs/tutorials/graph.mdx
docs/tutorials/extraction.md
docs/tutorials/extraction.mdx
docs/tutorials/classification.md
docs/tutorials/classification.mdx
docs/tutorials/chatbot.md
docs/tutorials/chatbot.mdx
docs/how_to/trim_messages.md
docs/how_to/trim_messages.mdx
docs/how_to/tools_prompting.md
docs/how_to/tools_prompting.mdx
docs/how_to/tools_few_shot.md
docs/how_to/tools_few_shot.mdx
docs/how_to/tools_error.md
docs/how_to/tools_error.mdx
docs/how_to/tools_builtin.md
docs/how_to/tools_builtin.mdx
docs/how_to/tool_streaming.md
docs/how_to/tool_streaming.mdx
docs/how_to/tool_stream_events.md
docs/how_to/tool_stream_events.mdx
docs/how_to/tool_runtime.md
docs/how_to/tool_runtime.mdx
docs/how_to/tool_results_pass_to_model.md
docs/how_to/tool_results_pass_to_model.mdx
docs/how_to/tool_configure.md
docs/how_to/tool_configure.mdx
docs/how_to/tool_choice.md
docs/how_to/tool_choice.mdx
docs/how_to/tool_calls_multimodal.md
docs/how_to/tool_calls_multimodal.mdx
docs/how_to/tool_calling_parallel.md
docs/how_to/tool_calling_parallel.mdx
docs/how_to/tool_calling.md
docs/how_to/tool_calling.mdx
docs/how_to/tool_artifacts.md
docs/how_to/tool_artifacts.mdx
docs/how_to/structured_output.md
docs/how_to/structured_output.mdx
docs/how_to/streaming.md
docs/how_to/streaming.mdx
docs/how_to/split_by_token.md
docs/how_to/split_by_token.mdx
docs/how_to/sequence.md
docs/how_to/sequence.mdx
docs/how_to/self_query.md
docs/how_to/self_query.mdx
docs/how_to/recursive_text_splitter.md
docs/how_to/recursive_text_splitter.mdx
docs/how_to/query_no_queries.md
docs/how_to/query_no_queries.mdx
docs/how_to/query_multiple_retrievers.md
docs/how_to/query_multiple_retrievers.mdx
docs/how_to/query_multiple_queries.md
docs/how_to/query_multiple_queries.mdx
docs/how_to/query_high_cardinality.md
docs/how_to/query_high_cardinality.mdx
docs/how_to/query_few_shot.md
docs/how_to/query_few_shot.mdx
docs/how_to/query_constructing_filters.md
docs/how_to/query_constructing_filters.mdx
docs/how_to/qa_streaming.md
docs/how_to/qa_streaming.mdx
docs/how_to/qa_sources.md
docs/how_to/qa_sources.mdx
docs/how_to/qa_per_user.md
docs/how_to/qa_per_user.mdx
docs/how_to/qa_citations.md
docs/how_to/qa_citations.mdx
docs/how_to/qa_chat_history_how_to.md
docs/how_to/qa_chat_history_how_to.mdx
docs/how_to/prompts_composition.md
docs/how_to/prompts_composition.mdx
docs/how_to/passthrough.md
docs/how_to/passthrough.mdx
docs/how_to/output_parser_xml.md
docs/how_to/output_parser_xml.mdx
docs/how_to/output_parser_structured.md
docs/how_to/output_parser_structured.mdx
docs/how_to/output_parser_json.md
docs/how_to/output_parser_json.mdx
docs/how_to/output_parser_fixing.md
docs/how_to/output_parser_fixing.mdx
docs/how_to/multiple_queries.md
docs/how_to/multiple_queries.mdx
docs/how_to/multimodal_prompts.md
docs/how_to/multimodal_prompts.mdx
docs/how_to/multimodal_inputs.md
docs/how_to/multimodal_inputs.mdx
docs/how_to/migrate_agent.md
docs/how_to/migrate_agent.mdx
docs/how_to/message_history.md
docs/how_to/message_history.mdx
docs/how_to/merge_message_runs.md
docs/how_to/merge_message_runs.mdx
docs/how_to/logprobs.md
docs/how_to/logprobs.mdx
docs/how_to/lcel_cheatsheet.md
docs/how_to/lcel_cheatsheet.mdx
docs/how_to/graph_semantic.md
docs/how_to/graph_semantic.mdx
docs/how_to/graph_prompting.md
docs/how_to/graph_prompting.mdx
docs/how_to/graph_mapping.md
docs/how_to/graph_mapping.mdx
docs/how_to/graph_constructing.md
docs/how_to/graph_constructing.mdx
docs/how_to/functions.md
docs/how_to/functions.mdx
docs/how_to/filter_messages.md
docs/how_to/filter_messages.mdx
docs/how_to/few_shot_examples_chat.md
docs/how_to/few_shot_examples_chat.mdx
docs/how_to/few_shot_examples.md
docs/how_to/few_shot_examples.mdx
docs/how_to/extraction_parse.md
docs/how_to/extraction_parse.mdx
docs/how_to/extraction_long_text.md
docs/how_to/extraction_long_text.mdx
docs/how_to/extraction_examples.md
docs/how_to/extraction_examples.mdx
docs/how_to/example_selectors_langsmith.md
docs/how_to/example_selectors_langsmith.mdx
docs/how_to/example_selectors.md
docs/how_to/example_selectors.mdx
docs/how_to/document_loader_markdown.md
docs/how_to/document_loader_markdown.mdx
docs/how_to/document_loader_html.md
docs/how_to/document_loader_html.mdx
docs/how_to/custom_tools.md
docs/how_to/custom_tools.mdx
docs/how_to/custom_llm.md
docs/how_to/custom_llm.mdx
docs/how_to/custom_chat.md
docs/how_to/custom_chat.mdx
docs/how_to/custom_callbacks.md
docs/how_to/custom_callbacks.mdx
docs/how_to/convert_runnable_to_tool.md
docs/how_to/convert_runnable_to_tool.mdx
docs/how_to/code_splitter.md
docs/how_to/code_splitter.mdx
docs/how_to/chatbots_tools.md
docs/how_to/chatbots_tools.mdx
docs/how_to/chatbots_retrieval.md
docs/how_to/chatbots_retrieval.mdx
docs/how_to/chatbots_memory.md
docs/how_to/chatbots_memory.mdx
docs/how_to/chat_streaming.md
docs/how_to/chat_streaming.mdx
docs/how_to/character_text_splitter.md
docs/how_to/character_text_splitter.mdx
docs/how_to/cancel_execution.md
docs/how_to/cancel_execution.mdx
docs/how_to/callbacks_serverless.md
docs/how_to/callbacks_serverless.mdx
docs/how_to/callbacks_runtime.md
docs/how_to/callbacks_runtime.mdx
docs/how_to/callbacks_custom_events.md
docs/how_to/callbacks_custom_events.mdx
docs/how_to/callbacks_constructor.md
docs/how_to/callbacks_constructor.mdx
docs/how_to/callbacks_attach.md
docs/how_to/callbacks_attach.mdx
docs/how_to/binding.md
docs/how_to/binding.mdx
docs/how_to/assign.md
docs/how_to/assign.mdx
docs/how_to/agent_executor.md
docs/how_to/agent_executor.mdx
docs/concepts/t.md
docs/concepts/t.mdx
docs/versions/migrating_memory/conversation_summary_memory.md
docs/versions/migrating_memory/conversation_summary_memory.mdx
docs/versions/migrating_memory/conversation_buffer_window_memory.md
docs/versions/migrating_memory/conversation_buffer_window_memory.mdx
docs/versions/migrating_memory/chat_history.md
docs/versions/migrating_memory/chat_history.mdx
docs/troubleshooting/errors/INVALID_TOOL_RESULTS.md
docs/troubleshooting/errors/INVALID_TOOL_RESULTS.mdx
docs/integrations/toolkits/vectorstore.md
docs/integrations/toolkits/vectorstore.mdx
docs/integrations/toolkits/sql.md
docs/integrations/toolkits/sql.mdx
docs/integrations/toolkits/openapi.md
docs/integrations/toolkits/openapi.mdx
docs/integrations/vectorstores/weaviate.md
docs/integrations/vectorstores/weaviate.mdx
docs/integrations/vectorstores/upstash.md
docs/integrations/vectorstores/upstash.mdx
docs/integrations/vectorstores/supabase.md
docs/integrations/vectorstores/supabase.mdx
docs/integrations/vectorstores/redis.md
docs/integrations/vectorstores/redis.mdx
docs/integrations/vectorstores/qdrant.md
docs/integrations/vectorstores/qdrant.mdx
docs/integrations/vectorstores/pinecone.md
docs/integrations/vectorstores/pinecone.mdx
docs/integrations/vectorstores/pgvector.md
docs/integrations/vectorstores/pgvector.mdx
docs/integrations/vectorstores/mongodb_atlas.md
docs/integrations/vectorstores/mongodb_atlas.mdx
docs/integrations/vectorstores/memory.md
docs/integrations/vectorstores/memory.mdx
docs/integrations/vectorstores/mariadb.md
docs/integrations/vectorstores/mariadb.mdx
docs/integrations/vectorstores/hnswlib.md
docs/integrations/vectorstores/hnswlib.mdx
docs/integrations/vectorstores/faiss.md
docs/integrations/vectorstores/faiss.mdx
docs/integrations/vectorstores/elasticsearch.md
docs/integrations/vectorstores/elasticsearch.mdx
docs/integrations/vectorstores/chroma.md
docs/integrations/vectorstores/chroma.mdx
docs/integrations/stores/in_memory.md
docs/integrations/stores/in_memory.mdx
docs/integrations/stores/file_system.md
docs/integrations/stores/file_system.mdx
docs/integrations/text_embedding/togetherai.md
docs/integrations/text_embedding/togetherai.mdx
docs/integrations/text_embedding/pinecone.md
docs/integrations/text_embedding/pinecone.mdx
docs/integrations/text_embedding/openai.md
docs/integrations/text_embedding/openai.mdx
docs/integrations/text_embedding/ollama.md
docs/integrations/text_embedding/ollama.mdx
docs/integrations/text_embedding/mistralai.md
docs/integrations/text_embedding/mistralai.mdx
docs/integrations/text_embedding/ibm.md
docs/integrations/text_embedding/ibm.mdx
docs/integrations/text_embedding/google_vertex_ai.md
docs/integrations/text_embedding/google_vertex_ai.mdx
docs/integrations/text_embedding/google_generativeai.md
docs/integrations/text_embedding/google_generativeai.mdx
docs/integrations/text_embedding/fireworks.md
docs/integrations/text_embedding/fireworks.mdx
docs/integrations/text_embedding/cohere.md
docs/integrations/text_embedding/cohere.mdx
docs/integrations/text_embedding/cloudflare_ai.md
docs/integrations/text_embedding/cloudflare_ai.mdx
docs/integrations/text_embedding/bytedance_doubao.md
docs/integrations/text_embedding/bytedance_doubao.mdx
docs/integrations/text_embedding/bedrock.md
docs/integrations/text_embedding/bedrock.mdx
docs/integrations/text_embedding/azure_openai.md
docs/integrations/text_embedding/azure_openai.mdx
docs/integrations/retrievers/tavily.md
docs/integrations/retrievers/tavily.mdx
docs/integrations/retrievers/kendra-retriever.md
docs/integrations/retrievers/kendra-retriever.mdx
docs/integrations/retrievers/exa.md
docs/integrations/retrievers/exa.mdx
docs/integrations/retrievers/bm25.md
docs/integrations/retrievers/bm25.mdx
docs/integrations/retrievers/bedrock-knowledge-bases.md
docs/integrations/retrievers/bedrock-knowledge-bases.mdx
docs/integrations/llms/together.md
docs/integrations/llms/together.mdx
docs/integrations/llms/openai.md
docs/integrations/llms/openai.mdx
docs/integrations/llms/ollama.md
docs/integrations/llms/ollama.mdx
docs/integrations/llms/mistral.md
docs/integrations/llms/mistral.mdx
docs/integrations/llms/ibm.md
docs/integrations/llms/ibm.mdx
docs/integrations/llms/google_vertex_ai.md
docs/integrations/llms/google_vertex_ai.mdx
docs/integrations/llms/fireworks.md
docs/integrations/llms/fireworks.mdx
docs/integrations/llms/cohere.md
docs/integrations/llms/cohere.mdx
docs/integrations/llms/cloudflare_workersai.md
docs/integrations/llms/cloudflare_workersai.mdx
docs/integrations/llms/bedrock.md
docs/integrations/llms/bedrock.mdx
docs/integrations/llms/azure.md
docs/integrations/llms/azure.mdx
docs/integrations/llms/arcjet.md
docs/integrations/llms/arcjet.mdx
docs/integrations/document_compressors/ibm.md
docs/integrations/document_compressors/ibm.mdx
docs/integrations/chat/xai.md
docs/integrations/chat/xai.mdx
docs/integrations/chat/togetherai.md
docs/integrations/chat/togetherai.mdx
docs/integrations/chat/openai.md
docs/integrations/chat/openai.mdx
docs/integrations/chat/ollama.md
docs/integrations/chat/ollama.mdx
docs/integrations/chat/novita.md
docs/integrations/chat/novita.mdx
docs/integrations/chat/mistral.md
docs/integrations/chat/mistral.mdx
docs/integrations/chat/ibm.md
docs/integrations/chat/ibm.mdx
docs/integrations/chat/groq.md
docs/integrations/chat/groq.mdx
docs/integrations/chat/google_vertex_ai.md
docs/integrations/chat/google_vertex_ai.mdx
docs/integrations/chat/google_generativeai.md
docs/integrations/chat/google_generativeai.mdx
docs/integrations/chat/fireworks.md
docs/integrations/chat/fireworks.mdx
docs/integrations/chat/cohere.md
docs/integrations/chat/cohere.mdx
docs/integrations/chat/cloudflare_workersai.md
docs/integrations/chat/cloudflare_workersai.mdx
docs/integrations/chat/cerebras.md
docs/integrations/chat/cerebras.mdx
docs/integrations/chat/bedrock_converse.md
docs/integrations/chat/bedrock_converse.mdx
docs/integrations/chat/bedrock.md
docs/integrations/chat/bedrock.mdx
docs/integrations/chat/azure.md
docs/integrations/chat/azure.mdx
docs/integrations/chat/arcjet.md
docs/integrations/chat/arcjet.mdx
docs/integrations/chat/anthropic.md
docs/integrations/chat/anthropic.mdx
docs/integrations/tools/tavily_search.md
docs/integrations/tools/tavily_search.mdx
docs/integrations/tools/serpapi.md
docs/integrations/tools/serpapi.mdx
docs/integrations/tools/google_scholar.md
docs/integrations/tools/google_scholar.mdx
docs/integrations/tools/exa_search.md
docs/integrations/tools/exa_search.mdx
docs/integrations/tools/duckduckgo_search.md
docs/integrations/tools/duckduckgo_search.mdx
docs/integrations/retrievers/self_query/weaviate.md
docs/integrations/retrievers/self_query/weaviate.mdx
docs/integrations/retrievers/self_query/vectara.md
docs/integrations/retrievers/self_query/vectara.mdx
docs/integrations/retrievers/self_query/supabase.md
docs/integrations/retrievers/self_query/supabase.mdx
docs/integrations/retrievers/self_query/qdrant.md
docs/integrations/retrievers/self_query/qdrant.mdx
docs/integrations/retrievers/self_query/pinecone.md
docs/integrations/retrievers/self_query/pinecone.mdx
docs/integrations/retrievers/self_query/memory.md
docs/integrations/retrievers/self_query/memory.mdx
docs/integrations/retrievers/self_query/hnswlib.md
docs/integrations/retrievers/self_query/hnswlib.mdx
docs/integrations/retrievers/self_query/chroma.md
docs/integrations/retrievers/self_query/chroma.mdx
docs/integrations/document_loaders/file_loaders/unstructured.md
docs/integrations/document_loaders/file_loaders/unstructured.mdx
docs/integrations/document_loaders/file_loaders/text.md
docs/integrations/document_loaders/file_loaders/text.mdx
docs/integrations/document_loaders/file_loaders/pdf.md
docs/integrations/document_loaders/file_loaders/pdf.mdx
docs/integrations/document_loaders/file_loaders/directory.md
docs/integrations/document_loaders/file_loaders/directory.mdx
docs/integrations/document_loaders/file_loaders/csv.md
docs/integrations/document_loaders/file_loaders/csv.mdx
docs/integrations/document_loaders/web_loaders/web_puppeteer.md
docs/integrations/document_loaders/web_loaders/web_puppeteer.mdx
docs/integrations/document_loaders/web_loaders/web_cheerio.md
docs/integrations/document_loaders/web_loaders/web_cheerio.mdx
docs/integrations/document_loaders/web_loaders/recursive_url_loader.md
docs/integrations/document_loaders/web_loaders/recursive_url_loader.mdx
docs/integrations/document_loaders/web_loaders/pdf.md
docs/integrations/document_loaders/web_loaders/pdf.mdx
docs/integrations/document_loaders/web_loaders/langsmith.md
docs/integrations/document_loaders/web_loaders/langsmith.mdx
docs/integrations/document_loaders/web_loaders/firecrawl.md
docs/integrations/document_loaders/web_loaders/firecrawl.mdx


================================================
FILE: docs/core_docs/.prettierignore
================================================
node_modules
build
.docusaurus
docs/api
docs/_static
static



================================================
FILE: docs/core_docs/data/ls_few_shot_example_dataset.json
================================================
[
  {
    "id": "646668fd-f11a-4379-9a29-4b109a2d5b2c",
    "created_at": "2024-07-08T22:29:20.545604+00:00",
    "modified_at": "2024-08-15T18:02:12.047567+00:00",
    "name": "#6466 @ multiverse-math-examples-for-few-shot",
    "dataset_id": "d64fd4c5-5642-40fe-85d1-d757467415b6",
    "source_run_id": "0d1e8e6e-35d4-4fcc-a467-86df53b6d1db",
    "metadata": {
      "dataset_split": [
        "base"
      ]
    },
    "inputs": {
      "input": "-(2 + 1/1)",
      "system": false
    },
    "outputs": {
      "output": [
        {
          "id": "be323bd4-13cd-4b46-ba68-28a5ed967d72",
          "type": "system",
          "content": "You are requested to solve math questions in an alternate mathematical universe. The operations have been altered to yield different results than expected. Do not guess the answer or rely on your  innate knowledge of math. Use the provided tools to answer the question. While associativity and commutativity apply, distributivity does not. Answer the question using the fewest possible tools. Please return a non-empty answer that includes the numeric response on its own without any clarifications.",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "bd54fd59-521b-45b7-b2e9-08acf3332f5f",
          "type": "human",
          "content": "-(2 + 1/1)",
          "example": false,
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-0e266463-50cd-4c1a-b27b-b59ad1ec6159-0",
          "type": "ai",
          "content": [
            {
              "text": "Here is how to calculate -(2 + 1/1) using the provided tools:",
              "type": "text"
            },
            {
              "id": "toolu_01SNLptzRgnhwTDmxHx4STSe",
              "name": "divide",
              "type": "tool_use",
              "input": {
                "a": 1,
                "b": 1
              }
            }
          ],
          "example": false,
          "tool_calls": [
            {
              "id": "toolu_01SNLptzRgnhwTDmxHx4STSe",
              "args": {
                "a": 1,
                "b": 1
              },
              "name": "divide"
            }
          ],
          "usage_metadata": {
            "input_tokens": 892,
            "total_tokens": 981,
            "output_tokens": 89
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01VvfJ3GkFHJZ69ZS7WygFrd",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 892,
              "output_tokens": 89
            },
            "stop_reason": "tool_use",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "95b3e1f7-e72f-497b-a4c9-54b846440628",
          "name": "divide",
          "type": "tool",
          "content": "0.5",
          "tool_call_id": "toolu_01SNLptzRgnhwTDmxHx4STSe",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-24405a09-ad67-46c2-b7c3-1c6ae2c1adfb-0",
          "type": "ai",
          "content": [
            {
              "id": "toolu_01CdkUVCzwXNyYgkpktpqrtZ",
              "name": "add",
              "type": "tool_use",
              "input": {
                "a": 2,
                "b": 0.5
              }
            }
          ],
          "example": false,
          "tool_calls": [
            {
              "id": "toolu_01CdkUVCzwXNyYgkpktpqrtZ",
              "args": {
                "a": 2,
                "b": 0.5
              },
              "name": "add"
            }
          ],
          "usage_metadata": {
            "input_tokens": 995,
            "total_tokens": 1066,
            "output_tokens": 71
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_011Cbd6wmPNL5jiuyC8jiDmr",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 995,
              "output_tokens": 71
            },
            "stop_reason": "tool_use",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "873fdae7-359b-4ef1-82db-b7c322fdd207",
          "name": "add",
          "type": "tool",
          "content": "3.7",
          "tool_call_id": "toolu_01CdkUVCzwXNyYgkpktpqrtZ",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-78284a0e-ac49-4c6a-8bba-d29d9458235e-0",
          "type": "ai",
          "content": [
            {
              "id": "toolu_01SP9dMetiCT7jsj9gN6pXpW",
              "name": "negate",
              "type": "tool_use",
              "input": {
                "a": 3.7
              }
            }
          ],
          "example": false,
          "tool_calls": [
            {
              "id": "toolu_01SP9dMetiCT7jsj9gN6pXpW",
              "args": {
                "a": 3.7
              },
              "name": "negate"
            }
          ],
          "usage_metadata": {
            "input_tokens": 1080,
            "total_tokens": 1135,
            "output_tokens": 55
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01X4vR4Mn5hFt3AxnrR5YNr6",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 1080,
              "output_tokens": 55
            },
            "stop_reason": "tool_use",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "e154f0cb-fa05-4020-abb9-080f0cd92d79",
          "name": "negate",
          "type": "tool",
          "content": "3.7",
          "tool_call_id": "toolu_01SP9dMetiCT7jsj9gN6pXpW",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-5f40b61b-485a-40ae-93ae-a61bbd800f09-0",
          "type": "ai",
          "content": "So the result of -(2 + 1/1) in this alternate universe is 3.7.",
          "example": false,
          "tool_calls": [],
          "usage_metadata": {
            "input_tokens": 1149,
            "total_tokens": 1177,
            "output_tokens": 28
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01TfCqdEytXBEoUQ68JSMkVj",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 1149,
              "output_tokens": 28
            },
            "stop_reason": "end_turn",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "90cbb099-abf3-48f9-b5a9-a36324e82fd4",
          "type": "human",
          "content": "You got the correct answer of 3.7, but your response included additional text. Please return the correct numerical answer of 3.7 to the user.",
          "example": false,
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-c8521068-27d7-4ba6-a1a3-51e27ef1add1-0",
          "type": "ai",
          "content": "3.7",
          "example": false,
          "tool_calls": [],
          "usage_metadata": {
            "input_tokens": 1214,
            "total_tokens": 1221,
            "output_tokens": 7
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01VWCSzRm46ruZEqZJL9nwf6",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 1214,
              "output_tokens": 7
            },
            "stop_reason": "end_turn",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        }
      ]
    }
  },
  {
    "id": "c9c1f569-d456-4246-9956-7cbd9fa1643f",
    "created_at": "2024-07-08T23:30:12.484721+00:00",
    "modified_at": "2024-08-15T18:01:54.639132+00:00",
    "name": "#c9c1 @ multiverse-math-examples-for-few-shot",
    "dataset_id": "d64fd4c5-5642-40fe-85d1-d757467415b6",
    "source_run_id": "d70c2f5c-c67f-4df5-a171-44fbaaef8973",
    "metadata": {
      "dataset_split": [
        "base"
      ]
    },
    "inputs": {
      "input": "evaluate negate(-131,778)",
      "system": false
    },
    "outputs": {
      "output": [
        {
          "id": "9c6222d9-c1f5-4134-91bb-585ab9d8c76a",
          "type": "system",
          "content": "You are requested to solve math questions in an alternate mathematical universe. The operations have been altered to yield different results than expected. Do not guess the answer or rely on your  innate knowledge of math. Use the provided tools to answer the question. While associativity and commutativity apply, distributivity does not. Answer the question using the fewest possible tools. Your final answer should include the numeric response on its own without any clarifications. Please return a non-empty answer.",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "f6770a24-520e-4007-8445-3fedae7111fb",
          "type": "human",
          "content": "evaluate negate(-131,778)",
          "example": false,
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-863b1744-5b04-4e8e-9f24-69ba6103da96-0",
          "type": "ai",
          "content": [
            {
              "id": "toolu_01Wof73Bby8miDefBHxtkkqT",
              "name": "negate",
              "type": "tool_use",
              "input": {
                "a": -131778
              }
            }
          ],
          "example": false,
          "tool_calls": [
            {
              "id": "toolu_01Wof73Bby8miDefBHxtkkqT",
              "args": {
                "a": -131778
              },
              "name": "negate"
            }
          ],
          "usage_metadata": {
            "input_tokens": 894,
            "total_tokens": 948,
            "output_tokens": 54
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01UrTuLYZJwFhPiYSADHWc2D",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 894,
              "output_tokens": 54
            },
            "stop_reason": "tool_use",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "21f5bc1b-975a-406d-903b-eefc374d5497",
          "name": "negate",
          "type": "tool",
          "content": "-131778.0",
          "tool_call_id": "toolu_01Wof73Bby8miDefBHxtkkqT",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-8d33b16a-9507-4b83-82ba-5840f85664c1-0",
          "type": "ai",
          "content": "Therefore, the answer is 131778.0",
          "example": false,
          "tool_calls": [],
          "usage_metadata": {
            "input_tokens": 965,
            "total_tokens": 979,
            "output_tokens": 14
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01TPHCoxNXqTtWXMZddWdrbP",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 965,
              "output_tokens": 14
            },
            "stop_reason": "end_turn",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "88fec757-c12a-4cfa-aa2d-2d2ec27fac2e",
          "type": "human",
          "content": "Please use the output of the negate tool correctly, and return a numerical answer only to the user.",
          "example": false,
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-88b2ebbd-c687-435a-a7d0-58e44d5bd13e-0",
          "type": "ai",
          "content": [
            {
              "id": "toolu_01JRRLgMWpDjNJ7djQodX8BR",
              "name": "negate",
              "type": "tool_use",
              "input": {
                "a": -131778
              }
            }
          ],
          "example": false,
          "tool_calls": [
            {
              "id": "toolu_01JRRLgMWpDjNJ7djQodX8BR",
              "args": {
                "a": -131778
              },
              "name": "negate"
            }
          ],
          "usage_metadata": {
            "input_tokens": 1003,
            "total_tokens": 1057,
            "output_tokens": 54
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01NUGNnVXywV29Cx5o93VkHd",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 1003,
              "output_tokens": 54
            },
            "stop_reason": "tool_use",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "c30e47d9-845d-43ea-9596-15d53c754fd5",
          "name": "negate",
          "type": "tool",
          "content": "-131778.0",
          "tool_call_id": "toolu_01JRRLgMWpDjNJ7djQodX8BR",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-7fdc3c94-27c4-42e4-808a-a4b74fb24265-0",
          "type": "ai",
          "content": "131778.0",
          "example": false,
          "tool_calls": [],
          "usage_metadata": {
            "input_tokens": 1074,
            "total_tokens": 1082,
            "output_tokens": 8
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01CL6Qer4ErNKbUWVhPDqNQ6",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 1074,
              "output_tokens": 8
            },
            "stop_reason": "end_turn",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "9da0f050-7b86-4e97-904f-78e374620928",
          "type": "human",
          "content": "The tool is outputting -131778, but you are returning 131778. Please fix this error and return the correct tool output to the user.",
          "example": false,
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-349122b2-5cd1-414b-a3e7-0c0586d6e78e-0",
          "type": "ai",
          "content": "-131778.0",
          "example": false,
          "tool_calls": [],
          "usage_metadata": {
            "input_tokens": 1116,
            "total_tokens": 1124,
            "output_tokens": 8
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_018jMmGf3qfaNZptZb5vWUSw",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 1116,
              "output_tokens": 8
            },
            "stop_reason": "end_turn",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        }
      ]
    }
  },
  {
    "id": "33243dc4-a558-4ee8-943d-38ba42c5562b",
    "created_at": "2024-07-09T18:29:40.803329+00:00",
    "modified_at": "2024-08-15T18:01:42.238491+00:00",
    "name": "#3324 @ multiverse-math-examples-for-few-shot",
    "dataset_id": "d64fd4c5-5642-40fe-85d1-d757467415b6",
    "source_run_id": "96448823-4e13-48c4-97c2-87302334d5dc",
    "metadata": {
      "dataset_split": [
        "base"
      ]
    },
    "inputs": {
      "input": "If I earn 5% monthly interest, and I start with 10 dollars how much money will I have at the end of the year?",
      "system": false
    },
    "outputs": {
      "output": [
        {
          "id": "552c2e8e-8389-4673-9139-c32876717786",
          "type": "system",
          "content": "You are requested to solve math questions in an alternate mathematical universe. The operations have been altered to yield different results than expected. Do not guess the answer or rely on your innate knowledge of math. Use the provided tools to answer the question. While associativity and commutativity apply, distributivity does not. Answer the question using the fewest possible tools. Your final answer should include the numeric response on its own without any clarifications. That means if the answer is 5, you should return \"5\" and not \"The answer is 5\" or \"So the answer to your question is 5\".",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "a7f41758-ca33-4121-8d7f-7fc396314e04",
          "type": "human",
          "content": "If I earn 5% monthly interest, and I start with 10 dollars how much money will I have at the end of the year?",
          "example": false,
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-2e8cb241-792a-41e2-84c5-e8e71d7580b5-0",
          "type": "ai",
          "content": [
            {
              "text": "Okay, let's break this down step-by-step using the provided tools:",
              "type": "text"
            },
            {
              "id": "toolu_01AcUN6M8mAA6dpstTstQAKb",
              "name": "multiply",
              "type": "tool_use",
              "input": {
                "a": 10,
                "b": 1.05
              }
            }
          ],
          "example": false,
          "tool_calls": [
            {
              "id": "toolu_01AcUN6M8mAA6dpstTstQAKb",
              "args": {
                "a": 10,
                "b": 1.05
              },
              "name": "multiply"
            }
          ],
          "usage_metadata": {
            "input_tokens": 943,
            "total_tokens": 1031,
            "output_tokens": 88
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_017RUX7BLpxFxNSW3kYmTeAs",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 943,
              "output_tokens": 88
            },
            "stop_reason": "tool_use",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "b60c0094-fb6d-44db-9ab0-24bc6fdb4cd7",
          "name": "multiply",
          "type": "tool",
          "content": "11.55",
          "tool_call_id": "toolu_01AcUN6M8mAA6dpstTstQAKb",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-f50fef08-89bb-4f6f-909c-8d25c5260d0c-0",
          "type": "ai",
          "content": [
            {
              "text": "This calculates the amount after 1 month with 5% interest on the initial $10.\n\nTo calculate for the full year, we can repeatedly apply this operation 12 times:",
              "type": "text"
            },
            {
              "id": "toolu_01VQMnv3A6YE1BJMRuL9j1vW",
              "name": "power",
              "type": "tool_use",
              "input": {
                "a": 1.05,
                "b": 12
              }
            }
          ],
          "example": false,
          "tool_calls": [
            {
              "id": "toolu_01VQMnv3A6YE1BJMRuL9j1vW",
              "args": {
                "a": 1.05,
                "b": 12
              },
              "name": "power"
            }
          ],
          "usage_metadata": {
            "input_tokens": 1046,
            "total_tokens": 1158,
            "output_tokens": 112
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01QXCFsM79ZEjxZibHv2Y1CF",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 1046,
              "output_tokens": 112
            },
            "stop_reason": "tool_use",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "5ed0dbe5-3dd1-4104-84bb-fe487038e971",
          "name": "power",
          "type": "tool",
          "content": "1.9799315994393987",
          "tool_call_id": "toolu_01VQMnv3A6YE1BJMRuL9j1vW",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-b423e2c7-8dd2-4e10-a6ca-ff25bfd1a0ec-0",
          "type": "ai",
          "content": [
            {
              "text": "Then multiply the initial $10 by this result:",
              "type": "text"
            },
            {
              "id": "toolu_01GR5C9CLsAhvStCGNZqcAbF",
              "name": "multiply",
              "type": "tool_use",
              "input": {
                "a": 10,
                "b": 1.9799315994393987
              }
            }
          ],
          "example": false,
          "tool_calls": [
            {
              "id": "toolu_01GR5C9CLsAhvStCGNZqcAbF",
              "args": {
                "a": 10,
                "b": 1.9799315994393987
              },
              "name": "multiply"
            }
          ],
          "usage_metadata": {
            "input_tokens": 1177,
            "total_tokens": 1265,
            "output_tokens": 88
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_016WrvMUw8i9rLMnaVKdjJY6",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 1177,
              "output_tokens": 88
            },
            "stop_reason": "tool_use",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "25be6748-1e78-4818-8b0b-20d3cb532570",
          "name": "multiply",
          "type": "tool",
          "content": "21.779247593833386",
          "tool_call_id": "toolu_01GR5C9CLsAhvStCGNZqcAbF",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-7691499f-0a67-4894-9ab4-61bc6d398f6f-0",
          "type": "ai",
          "content": "So with an initial $10 and 5% monthly interest compounded monthly, the amount at the end of 1 year in this alternate universe is 21.779247593833386.",
          "example": false,
          "tool_calls": [],
          "usage_metadata": {
            "input_tokens": 1283,
            "total_tokens": 1328,
            "output_tokens": 45
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01JCK4R6nwNjdiFVt5hV14Wj",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 1283,
              "output_tokens": 45
            },
            "stop_reason": "end_turn",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "b7ea9655-b8ec-4be1-a3f2-c6bd6f3f188b",
          "type": "human",
          "content": "You found the correct answer of 21.779247593833386, but your final response to the user included additional text. Please only return the correct answer.",
          "example": false,
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-22f8c361-9e93-412f-97f6-9918396bc7a1-0",
          "type": "ai",
          "content": "21.779247593833386",
          "example": false,
          "tool_calls": [],
          "usage_metadata": {
            "input_tokens": 1364,
            "total_tokens": 1375,
            "output_tokens": 11
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01Puvngg6iTza8JT1FjkPbGJ",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 1364,
              "output_tokens": 11
            },
            "stop_reason": "end_turn",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        }
      ]
    }
  },
  {
    "id": "034358b8-1f29-437b-b756-b69a98fe84a7",
    "created_at": "2024-07-09T19:01:51.89186+00:00",
    "modified_at": "2024-08-15T18:01:29.784596+00:00",
    "name": "#0343 @ multiverse-math-examples-for-few-shot",
    "dataset_id": "d64fd4c5-5642-40fe-85d1-d757467415b6",
    "source_run_id": "e53ce0b6-e3c0-4710-be31-d7110c1dd20d",
    "metadata": {
      "dataset_split": [
        "base"
      ]
    },
    "inputs": {
      "input": "What is the product of the odd numbers between 0-10?",
      "system": false
    },
    "outputs": {
      "output": [
        {
          "id": "5fb109a9-7779-47d2-a93b-30753afd8044",
          "type": "system",
          "content": "You are requested to solve math questions in an alternate mathematical universe. The operations have been altered to yield different results than expected. Do not guess the answer or rely on your innate knowledge of math. Use the provided tools to answer the question. While associativity and commutativity apply, distributivity does not. Answer the question using the fewest possible tools. Your final answer should include the numeric response on its own without any clarifications. That means if the answer is 5, you should return \"5\" and not \"The answer is 5\" or \"So the answer to your question is 5\".",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "c2c64324-10c1-483b-bbe4-40cdbd080161",
          "type": "human",
          "content": "What is the product of the odd numbers between 0-10?",
          "example": false,
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-e9a337d1-046f-449b-8aa0-7e26c37430cd-0",
          "type": "ai",
          "content": [
            {
              "text": "Here is how to calculate the product of the odd numbers between 0 and 10 using the provided tools:",
              "type": "text"
            },
            {
              "id": "toolu_01KNcT7ZQXCfNZgbbbBZQcFi",
              "name": "multiply",
              "type": "tool_use",
              "input": {
                "a": 1,
                "b": 3
              }
            }
          ],
          "example": false,
          "tool_calls": [
            {
              "id": "toolu_01KNcT7ZQXCfNZgbbbBZQcFi",
              "args": {
                "a": 1,
                "b": 3
              },
              "name": "multiply"
            }
          ],
          "usage_metadata": {
            "input_tokens": 927,
            "total_tokens": 1020,
            "output_tokens": 93
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01F7ZV92AdCE7H8JxT2qYe9m",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 927,
              "output_tokens": 93
            },
            "stop_reason": "tool_use",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "81bf003b-3462-4c83-a48b-e8a898bc0c25",
          "name": "multiply",
          "type": "tool",
          "content": "3.3",
          "tool_call_id": "toolu_01KNcT7ZQXCfNZgbbbBZQcFi",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-4a4c9725-66cc-4791-938c-ba5dda946958-0",
          "type": "ai",
          "content": [
            {
              "id": "toolu_01SwKfTekPgfMcDPPZVmpeMp",
              "name": "multiply",
              "type": "tool_use",
              "input": {
                "a": 3.3,
                "b": 5
              }
            }
          ],
          "example": false,
          "tool_calls": [
            {
              "id": "toolu_01SwKfTekPgfMcDPPZVmpeMp",
              "args": {
                "a": 3.3,
                "b": 5
              },
              "name": "multiply"
            }
          ],
          "usage_metadata": {
            "input_tokens": 1034,
            "total_tokens": 1105,
            "output_tokens": 71
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01X2MqYR5G7DmAxkA7tkbJdK",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 1034,
              "output_tokens": 71
            },
            "stop_reason": "tool_use",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "c303cb96-5884-4dbb-80e4-52bfb2d8be3d",
          "name": "multiply",
          "type": "tool",
          "content": "18.15",
          "tool_call_id": "toolu_01SwKfTekPgfMcDPPZVmpeMp",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-27ff6d22-a454-493c-b6d6-69c30dbf9737-0",
          "type": "ai",
          "content": [
            {
              "id": "toolu_01QjKrxNqkAmetaxLgsr7Bo5",
              "name": "multiply",
              "type": "tool_use",
              "input": {
                "a": 18.15,
                "b": 7
              }
            }
          ],
          "example": false,
          "tool_calls": [
            {
              "id": "toolu_01QjKrxNqkAmetaxLgsr7Bo5",
              "args": {
                "a": 18.15,
                "b": 7
              },
              "name": "multiply"
            }
          ],
          "usage_metadata": {
            "input_tokens": 1119,
            "total_tokens": 1190,
            "output_tokens": 71
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_014yB3FDWkRMrJXbbHoY4wxy",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 1119,
              "output_tokens": 71
            },
            "stop_reason": "tool_use",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "4f89ea15-0be0-4a76-b48d-01549ab0930c",
          "name": "multiply",
          "type": "tool",
          "content": "139.755",
          "tool_call_id": "toolu_01QjKrxNqkAmetaxLgsr7Bo5",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-4827dd2f-b40b-4e1d-a372-4a146881321f-0",
          "type": "ai",
          "content": [
            {
              "id": "toolu_01N9S9eDnznaxu2ZbVvPYdes",
              "name": "multiply",
              "type": "tool_use",
              "input": {
                "a": 139.755,
                "b": 9
              }
            }
          ],
          "example": false,
          "tool_calls": [
            {
              "id": "toolu_01N9S9eDnznaxu2ZbVvPYdes",
              "args": {
                "a": 139.755,
                "b": 9
              },
              "name": "multiply"
            }
          ],
          "usage_metadata": {
            "input_tokens": 1204,
            "total_tokens": 1275,
            "output_tokens": 71
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_018G5j4ahPH1ymCCBKd9J5zA",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 1204,
              "output_tokens": 71
            },
            "stop_reason": "tool_use",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "75a0f193-f3fd-42c0-903c-a38f92469dc8",
          "name": "multiply",
          "type": "tool",
          "content": "1383.5745",
          "tool_call_id": "toolu_01N9S9eDnznaxu2ZbVvPYdes",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-643cbd84-57d7-40be-9c3e-c490896b3e0d-0",
          "type": "ai",
          "content": "So the product of the odd numbers between 0 and 10 is 1383.5745",
          "example": false,
          "tool_calls": [],
          "usage_metadata": {
            "input_tokens": 1291,
            "total_tokens": 1317,
            "output_tokens": 26
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01VvwTB4oMXKyCsr1f274qf9",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 1291,
              "output_tokens": 26
            },
            "stop_reason": "end_turn",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "8857a93d-6bc9-4689-8be7-5e630a029cb2",
          "type": "human",
          "content": "You found the correct answer, but your final response to the user included additional text. Please only return the correct answer.",
          "example": false,
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-4f500072-2e25-4cf5-b53c-231b744da550-0",
          "type": "ai",
          "content": "1383.5745",
          "example": false,
          "tool_calls": [],
          "usage_metadata": {
            "input_tokens": 1344,
            "total_tokens": 1353,
            "output_tokens": 9
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01H19ivooTk6dXCEwF54txmt",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 1344,
              "output_tokens": 9
            },
            "stop_reason": "end_turn",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        }
      ]
    }
  },
  {
    "id": "415dd80f-c6fe-4350-aec3-ca35fbb8bafa",
    "created_at": "2024-07-09T19:37:14.641556+00:00",
    "modified_at": "2024-08-15T18:01:14.492779+00:00",
    "name": "#415d @ multiverse-math-examples-for-few-shot",
    "dataset_id": "d64fd4c5-5642-40fe-85d1-d757467415b6",
    "source_run_id": "3e207d90-6880-4496-ba67-093b41d2bf3d",
    "metadata": {
      "dataset_split": [
        "base"
      ]
    },
    "inputs": {
      "input": "evaluate the negation of -100",
      "system": false
    },
    "outputs": {
      "output": [
        {
          "id": "43d1f620-1f59-46f5-b99b-96efc5bfebd8",
          "type": "system",
          "content": "You are requested to solve math questions in an alternate mathematical universe. The operations have been altered to yield different results than expected. Do not guess the answer or rely on your innate knowledge of math. Use the provided tools to answer the question. While associativity and commutativity apply, distributivity does not. Answer the question using the fewest possible tools. Your final answer should include the numeric response on its own without any clarifications. That means if the answer is 5, you should return \"5\" and not \"The answer is 5\" or \"So the answer to your question is 5\".",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "df7b244e-98f8-48ea-8e43-cdd7b283bcd3",
          "type": "human",
          "content": "evaluate the negation of -100",
          "example": false,
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-f2dcb8ec-45ff-4920-af49-b5a7bfa6a3cc-0",
          "type": "ai",
          "content": [
            {
              "id": "toolu_01HTpq4cYNUac6F7omUc2Wz3",
              "name": "negate",
              "type": "tool_use",
              "input": {
                "a": -100
              }
            }
          ],
          "example": false,
          "tool_calls": [
            {
              "id": "toolu_01HTpq4cYNUac6F7omUc2Wz3",
              "args": {
                "a": -100
              },
              "name": "negate"
            }
          ],
          "usage_metadata": {
            "input_tokens": 920,
            "total_tokens": 973,
            "output_tokens": 53
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01YUXV8e7RLcYf7FLTStX68D",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 920,
              "output_tokens": 53
            },
            "stop_reason": "tool_use",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "492a7f3f-0464-4312-9cfb-7671943e4065",
          "name": "negate",
          "type": "tool",
          "content": "-100.0",
          "tool_call_id": "toolu_01HTpq4cYNUac6F7omUc2Wz3",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-88edae55-abb3-4d2c-b096-a85e45ee04b6-0",
          "type": "ai",
          "content": "So the answer is 100.",
          "example": false,
          "tool_calls": [],
          "usage_metadata": {
            "input_tokens": 989,
            "total_tokens": 1000,
            "output_tokens": 11
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01VPw9da9nFRby75Y937YMnE",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 989,
              "output_tokens": 11
            },
            "stop_reason": "end_turn",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "964f806e-44a9-46d5-9805-8f367fe7f141",
          "type": "human",
          "content": "100 is incorrect. Please refer to the output of your tool call.",
          "example": false,
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-e6c73250-c5b2-4805-83af-0b7cea1aba2b-0",
          "type": "ai",
          "content": [
            {
              "text": "You're right, my previous answer was incorrect. Let me re-evaluate using the tool output:",
              "type": "text"
            },
            {
              "id": "toolu_01XsJQboYghGDygQpPjJkeRq",
              "name": "negate",
              "type": "tool_use",
              "input": {
                "a": -100
              }
            }
          ],
          "example": false,
          "tool_calls": [
            {
              "id": "toolu_01XsJQboYghGDygQpPjJkeRq",
              "args": {
                "a": -100
              },
              "name": "negate"
            }
          ],
          "usage_metadata": {
            "input_tokens": 1019,
            "total_tokens": 1093,
            "output_tokens": 74
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01287oV1ftb1R2JJKoDjyWvA",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 1019,
              "output_tokens": 74
            },
            "stop_reason": "tool_use",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "fbf42cde-f004-41a7-99f1-5641acd8a083",
          "name": "negate",
          "type": "tool",
          "content": "-100.0",
          "tool_call_id": "toolu_01XsJQboYghGDygQpPjJkeRq",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-a6bcb848-6966-4bbf-919a-ded95e79dd91-0",
          "type": "ai",
          "content": "The answer is -100.0",
          "example": false,
          "tool_calls": [],
          "usage_metadata": {
            "input_tokens": 1108,
            "total_tokens": 1119,
            "output_tokens": 11
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_016bFgPZvrVEcN6RDyEbVtzz",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 1108,
              "output_tokens": 11
            },
            "stop_reason": "end_turn",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "63c94d84-815d-4b56-8c3a-ecc947869822",
          "type": "human",
          "content": "You have the correct numerical answer but are returning additional text. Please only respond with the numerical answer.",
          "example": false,
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-eda696da-ad0f-4454-b0fc-00bccb2cbee6-0",
          "type": "ai",
          "content": "-100.0",
          "example": false,
          "tool_calls": [],
          "usage_metadata": {
            "input_tokens": 1142,
            "total_tokens": 1149,
            "output_tokens": 7
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_019zQeQSyTfNaGfYKqMtVYbP",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 1142,
              "output_tokens": 7
            },
            "stop_reason": "end_turn",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        }
      ]
    }
  },
  {
    "id": "23b6ef25-b5fa-4399-9682-b08d05ae9688",
    "created_at": "2024-07-09T19:21:07.142926+00:00",
    "modified_at": "2024-07-09T19:21:07.142926+00:00",
    "name": "#23b6 @ multiverse-math-examples-for-few-shot",
    "dataset_id": "d64fd4c5-5642-40fe-85d1-d757467415b6",
    "source_run_id": "2ce994c9-cc39-4d8b-a145-7f23fd03b945",
    "metadata": {
      "dataset_split": [
        "base"
      ]
    },
    "inputs": {
      "input": "if one gazoink is 4 badoinks, each of which is 6 foos, each of wich is 3 bars - how many bars in 3 gazoinks?",
      "system": false
    },
    "outputs": {
      "output": [
        {
          "id": "cbe7ed83-86e1-4e46-89de-6646f8b55cef",
          "type": "system",
          "content": "You are requested to solve math questions in an alternate mathematical universe. The operations have been altered to yield different results than expected. Do not guess the answer or rely on your  innate knowledge of math. Use the provided tools to answer the question. While associativity and commutativity apply, distributivity does not. Answer the question using the fewest possible tools. Only include the numeric response without any clarifications.",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "04946246-09a8-4465-be95-037efd7dae55",
          "type": "human",
          "content": "if one gazoink is 4 badoinks, each of which is 6 foos, each of wich is 3 bars - how many bars in 3 gazoinks?",
          "example": false,
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-d6f0954e-b21b-4ea8-ad98-0ee64cfc824e-0",
          "type": "ai",
          "content": [
            {
              "id": "toolu_016RjRHSEyDZRqKhGrb8uvjJ",
              "name": "multiply",
              "type": "tool_use",
              "input": {
                "a": 3,
                "b": 4
              }
            }
          ],
          "example": false,
          "tool_calls": [
            {
              "id": "toolu_016RjRHSEyDZRqKhGrb8uvjJ",
              "args": {
                "a": 3,
                "b": 4
              },
              "name": "multiply"
            }
          ],
          "usage_metadata": {
            "input_tokens": 916,
            "total_tokens": 984,
            "output_tokens": 68
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01MBWxgouUBzomwTvXhomGVq",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 916,
              "output_tokens": 68
            },
            "stop_reason": "tool_use",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "3d4c72c4-f009-48ce-b739-1d3f28ee4803",
          "name": "multiply",
          "type": "tool",
          "content": "13.2",
          "tool_call_id": "toolu_016RjRHSEyDZRqKhGrb8uvjJ",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-26dd7e83-f5fb-4c70-8ba1-271300ffeb25-0",
          "type": "ai",
          "content": [
            {
              "id": "toolu_01PqvszxiuXrVJ9bwgTWaH3q",
              "name": "multiply",
              "type": "tool_use",
              "input": {
                "a": 13.2,
                "b": 6
              }
            }
          ],
          "example": false,
          "tool_calls": [
            {
              "id": "toolu_01PqvszxiuXrVJ9bwgTWaH3q",
              "args": {
                "a": 13.2,
                "b": 6
              },
              "name": "multiply"
            }
          ],
          "usage_metadata": {
            "input_tokens": 999,
            "total_tokens": 1070,
            "output_tokens": 71
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01VTFvtCxtR3rN58hCmjt2oH",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 999,
              "output_tokens": 71
            },
            "stop_reason": "tool_use",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "ca4e0317-7b3a-4638-933c-1efd98bc4fda",
          "name": "multiply",
          "type": "tool",
          "content": "87.12",
          "tool_call_id": "toolu_01PqvszxiuXrVJ9bwgTWaH3q",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-007794ac-3590-4b9e-b678-008f02e40042-0",
          "type": "ai",
          "content": [
            {
              "id": "toolu_01LU3RqRUXZRLRoJ2AZNmPed",
              "name": "multiply",
              "type": "tool_use",
              "input": {
                "a": 87.12,
                "b": 3
              }
            }
          ],
          "example": false,
          "tool_calls": [
            {
              "id": "toolu_01LU3RqRUXZRLRoJ2AZNmPed",
              "args": {
                "a": 87.12,
                "b": 3
              },
              "name": "multiply"
            }
          ],
          "usage_metadata": {
            "input_tokens": 1084,
            "total_tokens": 1155,
            "output_tokens": 71
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_017BEkSqmTsmtJaTxAzfRMEh",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 1084,
              "output_tokens": 71
            },
            "stop_reason": "tool_use",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "7f58c121-6f21-4c7b-ba38-aa820e274ff8",
          "name": "multiply",
          "type": "tool",
          "content": "287.496",
          "tool_call_id": "toolu_01LU3RqRUXZRLRoJ2AZNmPed",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-51e35afb-7ec6-4738-93e2-92f80b5c9377-0",
          "type": "ai",
          "content": "287.496",
          "example": false,
          "tool_calls": [],
          "usage_metadata": {
            "input_tokens": 1169,
            "total_tokens": 1176,
            "output_tokens": 7
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01Tx9kSNapSg8aUbWZXiS1NL",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 1169,
              "output_tokens": 7
            },
            "stop_reason": "end_turn",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        }
      ]
    }
  },
  {
    "id": "f3a5a482-910b-402a-8b49-31dbd38dba69",
    "created_at": "2024-07-09T18:31:22.954376+00:00",
    "modified_at": "2024-07-09T18:31:22.954376+00:00",
    "name": "#f3a5 @ multiverse-math-examples-for-few-shot",
    "dataset_id": "d64fd4c5-5642-40fe-85d1-d757467415b6",
    "source_run_id": "e313510f-5155-48fb-a1af-8ec998cd7dbd",
    "metadata": {
      "dataset_split": [
        "base"
      ]
    },
    "inputs": {
      "input": "What is the value of tangent of 15 degrees?",
      "system": false
    },
    "outputs": {
      "output": [
        {
          "id": "e3f864e9-10e4-402e-9a2a-04680152146e",
          "type": "system",
          "content": "You are requested to solve math questions in an alternate mathematical universe. The operations have been altered to yield different results than expected. Do not guess the answer or rely on your innate knowledge of math. Use the provided tools to answer the question. While associativity and commutativity apply, distributivity does not. Answer the question using the fewest possible tools. Your final answer should include the numeric response on its own without any clarifications. That means if the answer is 5, you should return \"5\" and not \"The answer is 5\" or \"So the answer to your question is 5\".",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "88c4febe-5445-4f91-937e-caa8760cec19",
          "type": "human",
          "content": "What is the value of tangent of 15 degrees?",
          "example": false,
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-9a440c86-f7f9-4219-b6bc-a35d64bbfc38-0",
          "type": "ai",
          "content": [
            {
              "text": "Here is how to calculate the tangent of 15 degrees using the provided tools:",
              "type": "text"
            },
            {
              "id": "toolu_01M9AvVXTPvFWLiKni8Sh1T4",
              "name": "pi",
              "type": "tool_use",
              "input": {}
            }
          ],
          "example": false,
          "tool_calls": [
            {
              "id": "toolu_01M9AvVXTPvFWLiKni8Sh1T4",
              "args": {},
              "name": "pi"
            }
          ],
          "usage_metadata": {
            "input_tokens": 926,
            "total_tokens": 979,
            "output_tokens": 53
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01Uk5q5uxL5Y5YGt5BQMwVEU",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 926,
              "output_tokens": 53
            },
            "stop_reason": "tool_use",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "39667772-2baf-4e9e-97ae-961e3670a474",
          "name": "pi",
          "type": "tool",
          "content": "2.718281828459045",
          "tool_call_id": "toolu_01M9AvVXTPvFWLiKni8Sh1T4",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-940b9125-591b-41f3-bbd8-6b474953bbd0-0",
          "type": "ai",
          "content": [
            {
              "id": "toolu_01JGvtXdtQtLmk7LwERkqDKH",
              "name": "divide",
              "type": "tool_use",
              "input": {
                "a": 15,
                "b": 180
              }
            }
          ],
          "example": false,
          "tool_calls": [
            {
              "id": "toolu_01JGvtXdtQtLmk7LwERkqDKH",
              "args": {
                "a": 15,
                "b": 180
              },
              "name": "divide"
            }
          ],
          "usage_metadata": {
            "input_tokens": 997,
            "total_tokens": 1066,
            "output_tokens": 69
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01HwYrEuUKLeZ9WHF8k4XpEm",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 997,
              "output_tokens": 69
            },
            "stop_reason": "tool_use",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "003a2745-230c-4e1a-80b3-62aca06d40ae",
          "name": "divide",
          "type": "tool",
          "content": "0.041666666666666664",
          "tool_call_id": "toolu_01JGvtXdtQtLmk7LwERkqDKH",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-e3881f88-26ef-4710-b79d-ec87cedc9b3f-0",
          "type": "ai",
          "content": [
            {
              "id": "toolu_012jdaKjUq4Up8EJF7PtbRnU",
              "name": "multiply",
              "type": "tool_use",
              "input": {
                "a": 0.041666666666666664,
                "b": 2.718281828459045
              }
            }
          ],
          "example": false,
          "tool_calls": [
            {
              "id": "toolu_012jdaKjUq4Up8EJF7PtbRnU",
              "args": {
                "a": 0.041666666666666664,
                "b": 2.718281828459045
              },
              "name": "multiply"
            }
          ],
          "usage_metadata": {
            "input_tokens": 1085,
            "total_tokens": 1167,
            "output_tokens": 82
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_014gPQ75pqPgox2gDgB5kGiW",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 1085,
              "output_tokens": 82
            },
            "stop_reason": "tool_use",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "13c27c72-71d6-4763-843c-9f8ac1fed43f",
          "name": "multiply",
          "type": "tool",
          "content": "0.12458791713770624",
          "tool_call_id": "toolu_012jdaKjUq4Up8EJF7PtbRnU",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-913fb3f0-1642-489f-b6af-486ddfb7e599-0",
          "type": "ai",
          "content": [
            {
              "id": "toolu_013GcHeUTjjcPm8zyrUb8y7K",
              "name": "sin",
              "type": "tool_use",
              "input": {
                "radians": 0.12458791713770624
              }
            }
          ],
          "example": false,
          "tool_calls": [
            {
              "id": "toolu_013GcHeUTjjcPm8zyrUb8y7K",
              "args": {
                "radians": 0.12458791713770624
              },
              "name": "sin"
            }
          ],
          "usage_metadata": {
            "input_tokens": 1186,
            "total_tokens": 1246,
            "output_tokens": 60
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01JagZM5K2BZy3gqbVqNvyfh",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 1186,
              "output_tokens": 60
            },
            "stop_reason": "tool_use",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "80035591-053a-41c7-a780-6ef2accb55bd",
          "name": "sin",
          "type": "tool",
          "content": "0.9922489593051885",
          "tool_call_id": "toolu_013GcHeUTjjcPm8zyrUb8y7K",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-e8187305-6596-43ba-b53e-fdc3c4bc949c-0",
          "type": "ai",
          "content": [
            {
              "id": "toolu_01H8VQ4CY79YWEv7UbNCq6PU",
              "name": "cos",
              "type": "tool_use",
              "input": {
                "radians": 0.12458791713770624
              }
            }
          ],
          "example": false,
          "tool_calls": [
            {
              "id": "toolu_01H8VQ4CY79YWEv7UbNCq6PU",
              "args": {
                "radians": 0.12458791713770624
              },
              "name": "cos"
            }
          ],
          "usage_metadata": {
            "input_tokens": 1265,
            "total_tokens": 1325,
            "output_tokens": 60
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01BxSVNvzVcLqgw8w9r2TdCy",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 1265,
              "output_tokens": 60
            },
            "stop_reason": "tool_use",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "d0bd5b2c-5f3c-453c-ba68-6905a07d1505",
          "name": "cos",
          "type": "tool",
          "content": "0.12426585515647588",
          "tool_call_id": "toolu_01H8VQ4CY79YWEv7UbNCq6PU",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-e1452ef5-35c0-4f6e-a73f-838aa5a1d618-0",
          "type": "ai",
          "content": [
            {
              "id": "toolu_01RAiD2JXsJXj3AHcodeYyoU",
              "name": "divide",
              "type": "tool_use",
              "input": {
                "a": 0.9922489593051885,
                "b": 0.12426585515647588
              }
            }
          ],
          "example": false,
          "tool_calls": [
            {
              "id": "toolu_01RAiD2JXsJXj3AHcodeYyoU",
              "args": {
                "a": 0.9922489593051885,
                "b": 0.12426585515647588
              },
              "name": "divide"
            }
          ],
          "usage_metadata": {
            "input_tokens": 1344,
            "total_tokens": 1427,
            "output_tokens": 83
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01TWTmVU2N9QSCokuFqZsTsH",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 1344,
              "output_tokens": 83
            },
            "stop_reason": "tool_use",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "7c5b39e3-b824-4fb0-8522-4e218cc595ca",
          "name": "divide",
          "type": "tool",
          "content": "3.9924440951850615",
          "tool_call_id": "toolu_01RAiD2JXsJXj3AHcodeYyoU",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-a3c4b10f-4235-43d8-8d0c-d722d287c99e-0",
          "type": "ai",
          "content": "3.9924440951850615",
          "example": false,
          "tool_calls": [],
          "usage_metadata": {
            "input_tokens": 1446,
            "total_tokens": 1458,
            "output_tokens": 12
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01VzZDqJPcWgwq8tGe8ktHjN",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 1446,
              "output_tokens": 12
            },
            "stop_reason": "end_turn",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        }
      ]
    }
  },
  {
    "id": "a738cf19-6e5e-43d8-88b0-51a9f2f7bb32",
    "created_at": "2024-07-09T18:00:05.951776+00:00",
    "modified_at": "2024-07-09T18:00:05.951776+00:00",
    "name": "#a738 @ multiverse-math-examples-for-few-shot",
    "dataset_id": "d64fd4c5-5642-40fe-85d1-d757467415b6",
    "source_run_id": "08825d00-6d61-4ce9-adae-dd9181dac5dc",
    "metadata": {
      "dataset_split": [
        "base"
      ]
    },
    "inputs": {
      "input": "For the first 5 days of the week I ran 45 minutes each day, and on the weekend I did one 100 minute run. How many minutes did I spend running this week?",
      "system": false
    },
    "outputs": {
      "output": [
        {
          "id": "39cbdd34-e80d-4cee-b64d-a08b332960de",
          "type": "system",
          "content": "You are requested to solve math questions in an alternate mathematical universe. The operations have been altered to yield different results than expected. Do not guess the answer or rely on your  innate knowledge of math. Use the provided tools to answer the question. While associativity and commutativity apply, distributivity does not. Answer the question using the fewest possible tools. Your final answer should include the numeric response on its own without any clarifications. Please return a non-empty answer.",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "88e60ba2-4976-40ab-9cc1-83017f33c36d",
          "type": "human",
          "content": "For the first 5 days of the week I ran 45 minutes each day, and on the weekend I did one 100 minute run. How many minutes did I spend running this week?",
          "example": false,
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-26185a0d-97a3-4e5f-baaf-c0c5cdb8624f-0",
          "type": "ai",
          "content": [
            {
              "id": "toolu_01XNBmZD1wHJJZQJ6GddFnxZ",
              "name": "multiply",
              "type": "tool_use",
              "input": {
                "a": 5,
                "b": 45
              }
            }
          ],
          "example": false,
          "tool_calls": [
            {
              "id": "toolu_01XNBmZD1wHJJZQJ6GddFnxZ",
              "args": {
                "a": 5,
                "b": 45
              },
              "name": "multiply"
            }
          ],
          "usage_metadata": {
            "input_tokens": 928,
            "total_tokens": 996,
            "output_tokens": 68
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01RwjPkwEam2UnV8r6mP4fVd",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 928,
              "output_tokens": 68
            },
            "stop_reason": "tool_use",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "c9769517-674a-434f-9364-abfb267cd83c",
          "name": "multiply",
          "type": "tool",
          "content": "247.5",
          "tool_call_id": "toolu_01XNBmZD1wHJJZQJ6GddFnxZ",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-b7870fa1-5427-4199-b270-59d6150da667-0",
          "type": "ai",
          "content": [
            {
              "id": "toolu_01BS72aUeWQzbkzJjQwrkx7A",
              "name": "add",
              "type": "tool_use",
              "input": {
                "a": 247.5,
                "b": 100
              }
            }
          ],
          "example": false,
          "tool_calls": [
            {
              "id": "toolu_01BS72aUeWQzbkzJjQwrkx7A",
              "args": {
                "a": 247.5,
                "b": 100
              },
              "name": "add"
            }
          ],
          "usage_metadata": {
            "input_tokens": 1011,
            "total_tokens": 1082,
            "output_tokens": 71
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_015h6Re3HMYfDoLJ9QVZftaY",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 1011,
              "output_tokens": 71
            },
            "stop_reason": "tool_use",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "250f500d-bbd5-4191-8e2a-f77435841778",
          "name": "add",
          "type": "tool",
          "content": "348.7",
          "tool_call_id": "toolu_01BS72aUeWQzbkzJjQwrkx7A",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-fa7b9bb3-533c-44b8-9e34-35af1286c0a6-0",
          "type": "ai",
          "content": "348.7",
          "example": false,
          "tool_calls": [],
          "usage_metadata": {
            "input_tokens": 1096,
            "total_tokens": 1103,
            "output_tokens": 7
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_0145tzF34B7pZBrDCv6FtzWs",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 1096,
              "output_tokens": 7
            },
            "stop_reason": "end_turn",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        }
      ]
    }
  },
  {
    "id": "5a6536b6-55ad-45ee-b3e1-ba5f0a89fc03",
    "created_at": "2024-07-08T23:26:34.541129+00:00",
    "modified_at": "2024-07-08T23:26:34.541129+00:00",
    "name": "#5a65 @ multiverse-math-examples-for-few-shot",
    "dataset_id": "d64fd4c5-5642-40fe-85d1-d757467415b6",
    "source_run_id": "6e4eb059-d73d-4b8b-9b04-4a809e374139",
    "metadata": {
      "dataset_split": [
        "base"
      ]
    },
    "inputs": {
      "input": "Negate the value of 2 subtracted from 5",
      "system": false
    },
    "outputs": {
      "output": [
        {
          "id": "c01e7746-a71b-4d7c-af5e-9523646ba964",
          "type": "system",
          "content": "You are requested to solve math questions in an alternate mathematical universe. The operations have been altered to yield different results than expected. Do not guess the answer or rely on your  innate knowledge of math. Use the provided tools to answer the question. While associativity and commutativity apply, distributivity does not. Answer the question using the fewest possible tools. Your final answer should include the numeric response on its own without any clarifications. Please return a non-empty answer.",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "48344924-81ec-4987-80d4-c94902e81852",
          "type": "human",
          "content": "Negate the value of 2 subtracted from 5",
          "example": false,
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-f3dd2c24-8ae5-4143-8ad3-3075ba432c17-0",
          "type": "ai",
          "content": [
            {
              "id": "toolu_01Qvk84UL1MMQC7UMLXjDEoL",
              "name": "subtract",
              "type": "tool_use",
              "input": {
                "a": 5,
                "b": 2
              }
            }
          ],
          "example": false,
          "tool_calls": [
            {
              "id": "toolu_01Qvk84UL1MMQC7UMLXjDEoL",
              "args": {
                "a": 5,
                "b": 2
              },
              "name": "subtract"
            }
          ],
          "usage_metadata": {
            "input_tokens": 900,
            "total_tokens": 968,
            "output_tokens": 68
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01HA93WmfUr1pGKGwh25pNvs",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 900,
              "output_tokens": 68
            },
            "stop_reason": "tool_use",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "6cbd94cc-083d-4756-bb63-703c68af36cd",
          "name": "subtract",
          "type": "tool",
          "content": "0.0",
          "tool_call_id": "toolu_01Qvk84UL1MMQC7UMLXjDEoL",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-97a2b647-4141-4285-b1ec-e30ced74fbc5-0",
          "type": "ai",
          "content": [
            {
              "id": "toolu_01J6Nw9Nej1QUtvPchRvBoks",
              "name": "negate",
              "type": "tool_use",
              "input": {
                "a": 0
              }
            }
          ],
          "example": false,
          "tool_calls": [
            {
              "id": "toolu_01J6Nw9Nej1QUtvPchRvBoks",
              "args": {
                "a": 0
              },
              "name": "negate"
            }
          ],
          "usage_metadata": {
            "input_tokens": 983,
            "total_tokens": 1038,
            "output_tokens": 55
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_016G7JgaK9BEbDZtr7Tpyx2C",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 983,
              "output_tokens": 55
            },
            "stop_reason": "tool_use",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        },
        {
          "id": "064bd7ef-d6e1-4ae1-b7cd-5d879391701a",
          "name": "negate",
          "type": "tool",
          "content": "0.0",
          "tool_call_id": "toolu_01J6Nw9Nej1QUtvPchRvBoks",
          "additional_kwargs": {},
          "response_metadata": {}
        },
        {
          "id": "run-5982b347-0e73-4e91-9cf4-65a03b918b65-0",
          "type": "ai",
          "content": "0.0",
          "example": false,
          "tool_calls": [],
          "usage_metadata": {
            "input_tokens": 1052,
            "total_tokens": 1059,
            "output_tokens": 7
          },
          "additional_kwargs": {},
          "response_metadata": {
            "id": "msg_01QbDwVuMjk4gRzZjv3xUqeU",
            "model": "claude-3-sonnet-20240229",
            "usage": {
              "input_tokens": 1052,
              "output_tokens": 7
            },
            "stop_reason": "end_turn",
            "stop_sequence": null
          },
          "invalid_tool_calls": []
        }
      ]
    }
  }
]


================================================
FILE: docs/core_docs/data/people.yml
================================================
maintainers:
- login: nfcampos
  count: 343
  avatarUrl: https://avatars.githubusercontent.com/u/56902?u=fdb30e802c68bc338dd9c0820f713e4fdac75db7&v=4
  twitterUsername: nfcampos
  url: https://github.com/nfcampos
- login: bracesproul
  count: 764
  avatarUrl: https://avatars.githubusercontent.com/u/46789226?u=83f467441c4b542b900fe2bb8fe45e26bf918da0&v=4
  twitterUsername: bracesproul
  url: https://github.com/bracesproul
- login: dqbd
  count: 45
  avatarUrl: https://avatars.githubusercontent.com/u/1443449?u=fe32372ae8f497065ef0a1c54194d9dff36fb81d&v=4
  twitterUsername: __dqbd
  url: https://github.com/dqbd
- login: jacoblee93
  count: 1226
  avatarUrl: https://avatars.githubusercontent.com/u/6952323?u=d785f9406c5a78ebd75922567b2693fb643c3bb0&v=4
  twitterUsername: hacubu
  url: https://github.com/jacoblee93
- login: hwchase17
  count: 73
  avatarUrl: https://avatars.githubusercontent.com/u/11986836?u=f4c4f21a82b2af6c9f91e1f1d99ea40062f7a101&v=4
  twitterUsername: null
  url: https://github.com/hwchase17
top_recent_contributors:
- login: dl102306
  count: 2.594246611122073
  avatarUrl: https://avatars.githubusercontent.com/u/2979960?v=4
  twitterUsername: null
  url: https://github.com/dl102306
- login: Anirudh31415926535
  count: 2.378611865453971
  avatarUrl: https://avatars.githubusercontent.com/u/171019460?v=4
  twitterUsername: null
  url: https://github.com/Anirudh31415926535
- login: dependabot
  count: 2.118005716372584
  avatarUrl: https://avatars.githubusercontent.com/in/29110?v=4
  twitterUsername: null
  url: https://github.com/apps/dependabot
- login: chentschel
  count: 1.8419619282928634
  avatarUrl: https://avatars.githubusercontent.com/u/319227?u=db20ce1d424f10d7760665ab693791ebc580131a&v=4
  twitterUsername: chentschel_
  url: https://github.com/chentschel
- login: sinedied
  count: 1.5981076527116855
  avatarUrl: https://avatars.githubusercontent.com/u/593151?u=08557bbdd96221813b8aec932dd7de895ac040ea&v=4
  twitterUsername: sinedied
  url: https://github.com/sinedied
- login: miloradvojnovic
  count: 1.5406950154046446
  avatarUrl: https://avatars.githubusercontent.com/u/11798350?u=a9b931a1a3319787bef5e2d16e1fdec0888cdad5&v=4
  twitterUsername: null
  url: https://github.com/miloradvojnovic
- login: anthonychu
  count: 1.469340114219017
  avatarUrl: https://avatars.githubusercontent.com/u/3982077?u=8bbebac42cb84a25c629f83f212b2d099ffa3964&v=4
  twitterUsername: nthonyChu
  url: https://github.com/anthonychu
- login: josemussa
  count: 1.4646097752078864
  avatarUrl: https://avatars.githubusercontent.com/u/4422500?u=d676ede0cec8ee5df6879ebf9d8b72d51ea1eb7f&v=4
  twitterUsername: null
  url: https://github.com/josemussa
- login: ovuruska
  count: 1.2073722297602894
  avatarUrl: https://avatars.githubusercontent.com/u/75265893?u=7f11152d07f1719da22084388c09b5fc64ab6c89&v=4
  twitterUsername: VuruskanerOguz
  url: https://github.com/ovuruska
- login: tofuliang
  count: 1.192831678125796
  avatarUrl: https://avatars.githubusercontent.com/u/1814685?v=4
  twitterUsername: null
  url: https://github.com/tofuliang
- login: AvaterClasher
  count: 1.0900805104055902
  avatarUrl: https://avatars.githubusercontent.com/u/116944847?u=102a870b3efed7f30f0a57123391a293eb6f5b08&v=4
  twitterUsername: Avater004
  url: https://github.com/AvaterClasher
- login: jl4nz
  count: 1.071155682903534
  avatarUrl: https://avatars.githubusercontent.com/u/94814971?u=266358610eeb54c3393dc127718dd6a997fdbf52&v=4
  twitterUsername: jlanzarotti
  url: https://github.com/jl4nz
- login: volodymyr-memsql
  count: 1.0661641541038527
  avatarUrl: https://avatars.githubusercontent.com/u/57520563?v=4
  twitterUsername: null
  url: https://github.com/volodymyr-memsql
top_contributors:
- login: afirstenberg
  count: 22.268597738308145
  avatarUrl: https://avatars.githubusercontent.com/u/3507578?v=4
  twitterUsername: null
  url: https://github.com/afirstenberg
- login: ppramesi
  count: 20.877822550346437
  avatarUrl: https://avatars.githubusercontent.com/u/6775031?v=4
  twitterUsername: null
  url: https://github.com/ppramesi
- login: jacobrosenthal
  count: 14.267494040569296
  avatarUrl: https://avatars.githubusercontent.com/u/455796?v=4
  twitterUsername: null
  url: https://github.com/jacobrosenthal
- login: sullivan-sean
  count: 11.942497805641993
  avatarUrl: https://avatars.githubusercontent.com/u/22581534?u=8f88473db2f929a965b6371733efda28e3fa1948&v=4
  twitterUsername: null
  url: https://github.com/sullivan-sean
- login: sinedied
  count: 10.521352610424566
  avatarUrl: https://avatars.githubusercontent.com/u/593151?u=08557bbdd96221813b8aec932dd7de895ac040ea&v=4
  twitterUsername: sinedied
  url: https://github.com/sinedied
- login: tomasonjo
  count: 8.253700280332291
  avatarUrl: https://avatars.githubusercontent.com/u/19948365?v=4
  twitterUsername: tb_tomaz
  url: https://github.com/tomasonjo
- login: skarard
  count: 7.547501901594376
  avatarUrl: https://avatars.githubusercontent.com/u/602085?u=f8a9736cfa9fe8875d19861b0276e24de8f3d0a0&v=4
  twitterUsername: skarard
  url: https://github.com/skarard
- login: chasemcdo
  count: 6.830368980796448
  avatarUrl: https://avatars.githubusercontent.com/u/74692158?u=9c25a170d24cc30f10eafc4d44a38067cdf5eed8&v=4
  twitterUsername: null
  url: https://github.com/chasemcdo
- login: MaximeThoonsen
  count: 6.725032283736919
  avatarUrl: https://avatars.githubusercontent.com/u/4814551?u=efb35c6a7dc1ce99dfa8ac8f0f1314cdb4fddfe1&v=4
  twitterUsername: maxthoon
  url: https://github.com/MaximeThoonsen
- login: easwee
  count: 6.61248803513189
  avatarUrl: https://avatars.githubusercontent.com/u/2518825?u=a24026bc5ed35688174b1a36f3c29eda594d38d7&v=4
  twitterUsername: easwee
  url: https://github.com/easwee
- login: mieslep
  count: 6.565459295836945
  avatarUrl: https://avatars.githubusercontent.com/u/5420540?u=8f038c002fbce42427999eb715dc9f868cef1c84&v=4
  twitterUsername: null
  url: https://github.com/mieslep
- login: ysnows
  count: 5.946853120965178
  avatarUrl: https://avatars.githubusercontent.com/u/11255869?u=b0b519b6565c43d01795ba092521c8677f30134c&v=4
  twitterUsername: enconvo_ai
  url: https://github.com/ysnows
- login: tyumentsev4
  count: 5.937394473842732
  avatarUrl: https://avatars.githubusercontent.com/u/56769451?u=088102b6160822bc68c25a2a5df170080d0b16a2&v=4
  twitterUsername: null
  url: https://github.com/tyumentsev4
- login: nickscamara
  count: 5.789517811503743
  avatarUrl: https://avatars.githubusercontent.com/u/20311743?u=29bf2391ae34297a12a88d813731b0bdf289e4a5&v=4
  twitterUsername: nickscamara_
  url: https://github.com/nickscamara
- login: nigel-daniels
  count: 5.7386038052115005
  avatarUrl: https://avatars.githubusercontent.com/u/4641452?v=4
  twitterUsername: null
  url: https://github.com/nigel-daniels
- login: MJDeligan
  count: 5.577508609177005
  avatarUrl: https://avatars.githubusercontent.com/u/48515433?v=4
  twitterUsername: null
  url: https://github.com/MJDeligan
- login: jeasonnow
  count: 5.487271244993293
  avatarUrl: https://avatars.githubusercontent.com/u/16950207?u=ab2d0d4f1574398ac842e6bb3c2ba020ab7711eb&v=4
  twitterUsername: null
  url: https://github.com/jeasonnow
- login: malandis
  count: 4.955686027081205
  avatarUrl: https://avatars.githubusercontent.com/u/3690240?v=4
  twitterUsername: mlonml
  url: https://github.com/malandis
- login: danielchalef
  count: 4.393048187010669
  avatarUrl: https://avatars.githubusercontent.com/u/131175?u=332fe36f12d9ffe9e4414dc776b381fe801a9c53&v=4
  twitterUsername: null
  url: https://github.com/danielchalef
- login: Swimburger
  count: 4.30247126533074
  avatarUrl: https://avatars.githubusercontent.com/u/3382717?u=5a84a173b0e80effc9161502c0848bf06c84bde9&v=4
  twitterUsername: RealSwimburger
  url: https://github.com/Swimburger
- login: Anush008
  count: 4.11180681500838
  avatarUrl: https://avatars.githubusercontent.com/u/46051506?u=026f5f140e8b7ba4744bf971f9ebdea9ebab67ca&v=4
  twitterUsername: null
  url: https://github.com/Anush008
- login: mfortman11
  count: 4.008715657220755
  avatarUrl: https://avatars.githubusercontent.com/u/6100513?u=c758a02fc05dc36315fcfadfccd6208883436cb8&v=4
  twitterUsername: null
  url: https://github.com/mfortman11
- login: kwkr
  count: 3.876826551715185
  avatarUrl: https://avatars.githubusercontent.com/u/20127759?v=4
  twitterUsername: zukerpie
  url: https://github.com/kwkr
- login: sarangan12
  count: 3.814606725038421
  avatarUrl: https://avatars.githubusercontent.com/u/602456?u=d39962c60b0ac5fea4e97cb67433a42c736c3c5b&v=4
  twitterUsername: null
  url: https://github.com/sarangan12
- login: fahreddinozcan
  count: 3.6832424135364583
  avatarUrl: https://avatars.githubusercontent.com/u/88107904?v=4
  twitterUsername: null
  url: https://github.com/fahreddinozcan
- login: ewfian
  count: 3.6525618490035825
  avatarUrl: https://avatars.githubusercontent.com/u/12423122?u=681de0c470e9b349963ee935ddfd6b2e097e7181&v=4
  twitterUsername: null
  url: https://github.com/ewfian
- login: jl4nz
  count: 3.322486583856189
  avatarUrl: https://avatars.githubusercontent.com/u/94814971?u=266358610eeb54c3393dc127718dd6a997fdbf52&v=4
  twitterUsername: jlanzarotti
  url: https://github.com/jl4nz
- login: volodymyr-memsql
  count: 3.2697631770291293
  avatarUrl: https://avatars.githubusercontent.com/u/57520563?v=4
  twitterUsername: null
  url: https://github.com/volodymyr-memsql
- login: jasondotparse
  count: 3.2458389391317124
  avatarUrl: https://avatars.githubusercontent.com/u/13938372?u=0e3f80aa515c41b7d9084b73d761cad378ebdc7a&v=4
  twitterUsername: null
  url: https://github.com/jasondotparse
- login: mishushakov
  count: 3.025937666523869
  avatarUrl: https://avatars.githubusercontent.com/u/10400064?u=52b50611d587317f397a96f898753099d65931f1&v=4
  twitterUsername: mishushakov
  url: https://github.com/mishushakov
- login: kristianfreeman
  count: 2.910863456801872
  avatarUrl: https://avatars.githubusercontent.com/u/922353?u=212a67ff65d67d39e41c3cb58cd7a7b8b2f89f3e&v=4
  twitterUsername: null
  url: https://github.com/kristianfreeman
- login: neebdev
  count: 2.91018110784209
  avatarUrl: https://avatars.githubusercontent.com/u/94310799?u=b6f604bc6c3a6380f0b83025ca94e2e22179ac2a&v=4
  twitterUsername: null
  url: https://github.com/neebdev
- login: tsg
  count: 2.9018860000514564
  avatarUrl: https://avatars.githubusercontent.com/u/101817?u=39f31ff29d2589046148c6ed1c1c923982d86b1a&v=4
  twitterUsername: tudor_g
  url: https://github.com/tsg
- login: lokesh-couchbase
  count: 2.8986026968579655
  avatarUrl: https://avatars.githubusercontent.com/u/113521973?v=4
  twitterUsername: null
  url: https://github.com/lokesh-couchbase
- login: nicoloboschi
  count: 2.8676287561914933
  avatarUrl: https://avatars.githubusercontent.com/u/23314389?u=2014e20e246530fa89bd902fe703b6f9e6ecf833&v=4
  twitterUsername: nicoloboschi
  url: https://github.com/nicoloboschi
- login: zackproser
  count: 2.822841682739776
  avatarUrl: https://avatars.githubusercontent.com/u/1769996?u=67913e5af19c6ea2df87f33db0ddd2b6cb805eb5&v=4
  twitterUsername: zackproser
  url: https://github.com/zackproser
- login: justindra
  count: 2.8213473329884717
  avatarUrl: https://avatars.githubusercontent.com/u/4289486?v=4
  twitterUsername: justindra_
  url: https://github.com/justindra
- login: vincelwt
  count: 2.653038382181122
  avatarUrl: https://avatars.githubusercontent.com/u/5092466?u=713f9947e4315b6f0ef62ec5cccd978133006783&v=4
  twitterUsername: vincelwt
  url: https://github.com/vincelwt
- login: cwoolum
  count: 2.6393729641077233
  avatarUrl: https://avatars.githubusercontent.com/u/942415?u=8210ef711d1666ec234db9a0c4a9b32fd9f36593&v=4
  twitterUsername: chriswoolum
  url: https://github.com/cwoolum
- login: sunner
  count: 2.619509428240187
  avatarUrl: https://avatars.githubusercontent.com/u/255413?v=4
  twitterUsername: null
  url: https://github.com/sunner
- login: dl102306
  count: 2.594246611122073
  avatarUrl: https://avatars.githubusercontent.com/u/2979960?v=4
  twitterUsername: null
  url: https://github.com/dl102306
- login: rahilvora
  count: 2.5339652606929115
  avatarUrl: https://avatars.githubusercontent.com/u/5127548?u=0cd74312c28da39646785409fb0a37a9b3d3420a&v=4
  twitterUsername: null
  url: https://github.com/rahilvora
- login: lukywong
  count: 2.519864307838807
  avatarUrl: https://avatars.githubusercontent.com/u/1433871?v=4
  twitterUsername: null
  url: https://github.com/lukywong
- login: mayooear
  count: 2.5039401650282382
  avatarUrl: https://avatars.githubusercontent.com/u/107035552?u=708ca9b002559f6175803a80a1e47f3e84ba91e2&v=4
  twitterUsername: mayowaoshin
  url: https://github.com/mayooear
- login: chitalian
  count: 2.460732921934039
  avatarUrl: https://avatars.githubusercontent.com/u/26822232?u=accedd106a5e9d8335cb631c1bfe84b8cc494083&v=4
  twitterUsername: justinstorre
  url: https://github.com/chitalian
- login: paaatrrrick
  count: 2.4007245461518325
  avatarUrl: https://avatars.githubusercontent.com/u/88113528?u=23275c7b8928a38b34195358ea9f4d057fe1e171&v=4
  twitterUsername: null
  url: https://github.com/paaatrrrick
- login: alexleventer
  count: 2.3963442958008176
  avatarUrl: https://avatars.githubusercontent.com/u/3254549?u=794d178a761379e162a1092c556e98a9ec5c2410&v=4
  twitterUsername: null
  url: https://github.com/alexleventer
- login: Anirudh31415926535
  count: 2.378611865453971
  avatarUrl: https://avatars.githubusercontent.com/u/171019460?v=4
  twitterUsername: null
  url: https://github.com/Anirudh31415926535
- login: 3eif
  count: 2.323209691414097
  avatarUrl: https://avatars.githubusercontent.com/u/29833473?u=37b8f7a25883ee98bc6b6bd6029c6d5479724e2f&v=4
  twitterUsername: sabziz
  url: https://github.com/3eif
- login: BitVoyagerMan
  count: 2.2958954742635953
  avatarUrl: https://avatars.githubusercontent.com/u/121993229?u=717ed7012c040d5bf3a8ff1fd695a6a4f1ff0626&v=4
  twitterUsername: null
  url: https://github.com/BitVoyagerMan
- login: xixixao
  count: 2.2876291681217342
  avatarUrl: https://avatars.githubusercontent.com/u/1473433?u=c4bf1cf9f8699c8647894cd226c0bf9124bdad58&v=4
  twitterUsername: null
  url: https://github.com/xixixao
- login: ovuruska
  count: 2.286474945226641
  avatarUrl: https://avatars.githubusercontent.com/u/75265893?u=7f11152d07f1719da22084388c09b5fc64ab6c89&v=4
  twitterUsername: VuruskanerOguz
  url: https://github.com/ovuruska
- login: jo32
  count: 2.2851005981511694
  avatarUrl: https://avatars.githubusercontent.com/u/501632?u=a714d65c000d8f489f9fc2363f9a372b0dba05e3&v=4
  twitterUsername: null
  url: https://github.com/jo32
- login: RohitMidha23
  count: 2.232630526458589
  avatarUrl: https://avatars.githubusercontent.com/u/38888530?u=5c4b99eff970e551e5b756f270aa5234bc666316&v=4
  twitterUsername: null
  url: https://github.com/RohitMidha23
- login: karol-f
  count: 2.2305403257775134
  avatarUrl: https://avatars.githubusercontent.com/u/893082?u=0cda88d40a24ee696580f2e62f5569f49117cf40&v=4
  twitterUsername: null
  url: https://github.com/karol-f
- login: konstantinov-raft
  count: 2.223080433170316
  avatarUrl: https://avatars.githubusercontent.com/u/105433902?v=4
  twitterUsername: null
  url: https://github.com/konstantinov-raft
- login: jameshfisher
  count: 2.143515802348958
  avatarUrl: https://avatars.githubusercontent.com/u/166966?u=b78059abca798fbce8c9da4f6ddfb72ea03b20bb&v=4
  twitterUsername: MrJamesFisher
  url: https://github.com/jameshfisher
- login: the-powerpointer
  count: 2.1273610063772668
  avatarUrl: https://avatars.githubusercontent.com/u/134403026?u=ddd77b62b35c5497ae3d846f8917bdd81e5ef19e&v=4
  twitterUsername: null
  url: https://github.com/the-powerpointer
- login: davidfant
  count: 2.123250038617256
  avatarUrl: https://avatars.githubusercontent.com/u/17096641?u=9b935c68c077d53642c1b4aff62f04d08e2ffac7&v=4
  twitterUsername: null
  url: https://github.com/davidfant
- login: dependabot
  count: 2.118005716372584
  avatarUrl: https://avatars.githubusercontent.com/in/29110?v=4
  twitterUsername: null
  url: https://github.com/apps/dependabot
- login: MthwRobinson
  count: 2.106176213349417
  avatarUrl: https://avatars.githubusercontent.com/u/1635179?u=0631cb84ca580089198114f94d9c27efe730220e&v=4
  twitterUsername: null
  url: https://github.com/MthwRobinson
- login: SimonPrammer
  count: 2.036351588156493
  avatarUrl: https://avatars.githubusercontent.com/u/44960995?u=a513117a60e9f1aa09247ec916018ee272897169&v=4
  twitterUsername: null
  url: https://github.com/SimonPrammer
- login: munkhorgil
  count: 1.989209476894206
  avatarUrl: https://avatars.githubusercontent.com/u/978987?u=eff77a6f7bc4edbace4929731638d4727923013f&v=4
  twitterUsername: null
  url: https://github.com/munkhorgil
- login: alx13
  count: 1.9438496864726376
  avatarUrl: https://avatars.githubusercontent.com/u/1572864?v=4
  twitterUsername: null
  url: https://github.com/alx13
- login: castroCrea
  count: 1.9413681649706493
  avatarUrl: https://avatars.githubusercontent.com/u/20707343?u=25e872c764bd31b71148f2dec896f64be5e034ff&v=4
  twitterUsername: Pao_Cto
  url: https://github.com/castroCrea
- login: samheutmaker
  count: 1.8909830007390982
  avatarUrl: https://avatars.githubusercontent.com/u/1767032?u=a50f2b3b339eb965b9c812977aa10d64202e2e95&v=4
  twitterUsername: 0xSamHogan
  url: https://github.com/samheutmaker
- login: archie-swif
  count: 1.8879520198010826
  avatarUrl: https://avatars.githubusercontent.com/u/2158707?u=8a0aeee45e93ba575321804a7b709bf8897941de&v=4
  twitterUsername: null
  url: https://github.com/archie-swif
- login: valdo99
  count: 1.8815427075304894
  avatarUrl: https://avatars.githubusercontent.com/u/41517614?u=ba37c9a21db3068953ae50d90c1cd07c3dec3abd&v=4
  twitterUsername: valdozzz1
  url: https://github.com/valdo99
- login: chentschel
  count: 1.8419619282928634
  avatarUrl: https://avatars.githubusercontent.com/u/319227?u=db20ce1d424f10d7760665ab693791ebc580131a&v=4
  twitterUsername: chentschel_
  url: https://github.com/chentschel
- login: gmpetrov
  count: 1.8103159579166737
  avatarUrl: https://avatars.githubusercontent.com/u/4693180?u=8cf781d9099d6e2f2d2caf7612a5c2811ba13ef8&v=4
  twitterUsername: georges_petrov
  url: https://github.com/gmpetrov
- login: mattzcarey
  count: 1.8103097806232766
  avatarUrl: https://avatars.githubusercontent.com/u/77928207?u=fc8febe2a4b67384046eb4041b325bb34665d59c&v=4
  twitterUsername: mattzcarey
  url: https://github.com/mattzcarey
- login: albertpurnama
  count: 1.8068671958039872
  avatarUrl: https://avatars.githubusercontent.com/u/14824254?u=b3acdfc46d3d26d44f66a7312b102172c7ff9722&v=4
  twitterUsername: albertpurnama
  url: https://github.com/albertpurnama
- login: CahidArda
  count: 1.8064422121491948
  avatarUrl: https://avatars.githubusercontent.com/u/57228345?v=4
  twitterUsername: null
  url: https://github.com/CahidArda
- login: yroc92
  count: 1.7662681234109807
  avatarUrl: https://avatars.githubusercontent.com/u/17517541?u=7405432fa828c094e130e8193be3cae04ac96d11&v=4
  twitterUsername: null
  url: https://github.com/yroc92
- login: Basti-an
  count: 1.7547477174673676
  avatarUrl: https://avatars.githubusercontent.com/u/42387209?u=43ac44545861ce4adec99f973aeea3e6cf9a1bc0&v=4
  twitterUsername: null
  url: https://github.com/Basti-an
- login: CarlosZiegler
  count: 1.743607324352666
  avatarUrl: https://avatars.githubusercontent.com/u/38855507?u=65c19ae772581fb7367f646ed90be44311e60e70&v=4
  twitterUsername: carlosziegler
  url: https://github.com/CarlosZiegler
- login: iloveitaly
  count: 1.7083954601863427
  avatarUrl: https://avatars.githubusercontent.com/u/150855?v=4
  twitterUsername: mike_bianco
  url: https://github.com/iloveitaly
- login: dilling
  count: 1.6352429296591733
  avatarUrl: https://avatars.githubusercontent.com/u/5846912?v=4
  twitterUsername: null
  url: https://github.com/dilling
- login: anselm94
  count: 1.6311660935283947
  avatarUrl: https://avatars.githubusercontent.com/u/9033201?u=e5f657c3a1657c089d7cb88121e544ae7212e6f1&v=4
  twitterUsername: MerbinJAnselm
  url: https://github.com/anselm94
- login: aixgeek
  count: 1.5850862534183767
  avatarUrl: https://avatars.githubusercontent.com/u/9697715?u=d139c5568375c2472ac6142325e6856cd766d88d&v=4
  twitterUsername: geekxai
  url: https://github.com/aixgeek
- login: gramliu
  count: 1.5791775791775793
  avatarUrl: https://avatars.githubusercontent.com/u/24856195?u=9f55337506cdcac3146772c56b4634e6b46a5e46&v=4
  twitterUsername: gramliu
  url: https://github.com/gramliu
- login: jeffchuber
  count: 1.564589469083851
  avatarUrl: https://avatars.githubusercontent.com/u/891664?u=722172a0061f68ab22819fa88a354ec973f70a63&v=4
  twitterUsername: null
  url: https://github.com/jeffchuber
- login: ywkim
  count: 1.5593229675944416
  avatarUrl: https://avatars.githubusercontent.com/u/588581?u=df702e5b817a56476cb0cd8e7587b9be844d2850&v=4
  twitterUsername: ywkim
  url: https://github.com/ywkim
- login: jirimoravcik
  count: 1.5492762061520144
  avatarUrl: https://avatars.githubusercontent.com/u/951187?u=e80c215810058f57145042d12360d463e3a53443&v=4
  twitterUsername: null
  url: https://github.com/jirimoravcik
- login: miloradvojnovic
  count: 1.5406950154046446
  avatarUrl: https://avatars.githubusercontent.com/u/11798350?u=a9b931a1a3319787bef5e2d16e1fdec0888cdad5&v=4
  twitterUsername: null
  url: https://github.com/miloradvojnovic
- login: janvi-kalra
  count: 1.53306342780027
  avatarUrl: https://avatars.githubusercontent.com/u/119091286?u=ed9e9d72bbf9964b80f81e5ba8d1d5b2f860c23f&v=4
  twitterUsername: janvikalra_
  url: https://github.com/janvi-kalra
- login: yuku
  count: 1.5249478139076982
  avatarUrl: https://avatars.githubusercontent.com/u/96157?v=4
  twitterUsername: yuku_t
  url: https://github.com/yuku
- login: conroywhitney
  count: 1.514880034611978
  avatarUrl: https://avatars.githubusercontent.com/u/249891?u=36703ce68261be59109622877012be08fbc090da&v=4
  twitterUsername: conroywhitney
  url: https://github.com/conroywhitney
- login: seuha516
  count: 1.511875544931005
  avatarUrl: https://avatars.githubusercontent.com/u/79067549?u=de7a2688cb44010afafd055d707f3463585494df&v=4
  twitterUsername: null
  url: https://github.com/seuha516
- login: Czechh
  count: 1.4922336407937202
  avatarUrl: https://avatars.githubusercontent.com/u/4779936?u=ab072503433effc18c071b31adda307988877d5e&v=4
  twitterUsername: null
  url: https://github.com/Czechh
- login: adam101
  count: 1.4865913414423242
  avatarUrl: https://avatars.githubusercontent.com/u/1535782?v=4
  twitterUsername: null
  url: https://github.com/adam101
- login: OlegIvaniv
  count: 1.485600148643627
  avatarUrl: https://avatars.githubusercontent.com/u/12657221?v=4
  twitterUsername: null
  url: https://github.com/OlegIvaniv
- login: jaclar
  count: 1.484066966625106
  avatarUrl: https://avatars.githubusercontent.com/u/362704?u=52d868cc75c793fa895ef7035ae45516bd915e84&v=4
  twitterUsername: jaclar
  url: https://github.com/jaclar
- login: TeCHiScy
  count: 1.4707811568276683
  avatarUrl: https://avatars.githubusercontent.com/u/741195?u=e5937011ef84ff8a4b4b62ac1926a291c04f5d8b&v=4
  twitterUsername: null
  url: https://github.com/TeCHiScy
- login: anthonychu
  count: 1.469340114219017
  avatarUrl: https://avatars.githubusercontent.com/u/3982077?u=8bbebac42cb84a25c629f83f212b2d099ffa3964&v=4
  twitterUsername: nthonyChu
  url: https://github.com/anthonychu
- login: josemussa
  count: 1.4646097752078864
  avatarUrl: https://avatars.githubusercontent.com/u/4422500?u=d676ede0cec8ee5df6879ebf9d8b72d51ea1eb7f&v=4
  twitterUsername: null
  url: https://github.com/josemussa
- login: ivoneijr
  count: 1.462283158877586
  avatarUrl: https://avatars.githubusercontent.com/u/6401435?u=96c11b6333636bd784ffbff72998591f3b3f087b&v=4
  twitterUsername: null
  url: https://github.com/ivoneijr
- login: tonisives
  count: 1.4433959992368206
  avatarUrl: https://avatars.githubusercontent.com/u/1083534?v=4
  twitterUsername: tonisives
  url: https://github.com/tonisives
- login: Njuelle
  count: 1.4376108924163842
  avatarUrl: https://avatars.githubusercontent.com/u/3192870?u=e126aae39f36565450ebc854b35c6e890b705e71&v=4
  twitterUsername: null
  url: https://github.com/Njuelle
- login: Roland0511
  count: 1.4309446898279385
  avatarUrl: https://avatars.githubusercontent.com/u/588050?u=3c91917389117ee84843d961252ab7a2b9097e0e&v=4
  twitterUsername: lizhou_zhu
  url: https://github.com/Roland0511
- login: SebastjanPrachovskij
  count: 1.4270208511853757
  avatarUrl: https://avatars.githubusercontent.com/u/86522260?u=66898c89771c7b8ff38958e9fb9563a1cf7f8004&v=4
  twitterUsername: null
  url: https://github.com/SebastjanPrachovskij
- login: cinqisap
  count: 1.425069723409155
  avatarUrl: https://avatars.githubusercontent.com/u/158295355?v=4
  twitterUsername: null
  url: https://github.com/cinqisap
- login: dylanintech
  count: 1.4248741912293315
  avatarUrl: https://avatars.githubusercontent.com/u/86082012?u=6516bbf39c5af198123d8ed2e35fff5d200f4d2e&v=4
  twitterUsername: dxlantxch
  url: https://github.com/dylanintech
- login: andrewnguonly
  count: 1.4221207958881887
  avatarUrl: https://avatars.githubusercontent.com/u/7654246?u=b8599019655adaada3cdc3c3006798df42c44494&v=4
  twitterUsername: andrewnguonly
  url: https://github.com/andrewnguonly
- login: clemenspeters
  count: 1.4063956990937188
  avatarUrl: https://avatars.githubusercontent.com/u/13015002?u=059c556d90a2e5639dee42123077d51223c190f0&v=4
  twitterUsername: PetersClemens
  url: https://github.com/clemenspeters
- login: ShaunBaker
  count: 1.4047837363443452
  avatarUrl: https://avatars.githubusercontent.com/u/1176557?u=c2e8ecfb45b736fc4d3bbfe182e26936bd519fd3&v=4
  twitterUsername: null
  url: https://github.com/ShaunBaker
- login: machulav
  count: 1.404147024663524
  avatarUrl: https://avatars.githubusercontent.com/u/2857712?u=6809bef8bf07c46b39cd2fcd6027ed86e76372cd&v=4
  twitterUsername: null
  url: https://github.com/machulav
- login: dersia
  count: 1.3856560415122312
  avatarUrl: https://avatars.githubusercontent.com/u/1537958?u=5da46ca1cd93c6fed927c612fc454ba51d0a36b1&v=4
  twitterUsername: null
  url: https://github.com/dersia
- login: joshsny
  count: 1.3783891982385268
  avatarUrl: https://avatars.githubusercontent.com/u/7135900?u=109e43c5e906a8ecc1a2d465c4457f5cf29328a5&v=4
  twitterUsername: joshsny
  url: https://github.com/joshsny
- login: eactisgrosso
  count: 1.372139099542256
  avatarUrl: https://avatars.githubusercontent.com/u/2279003?u=d122874eedb211359d4bf0119877d74ea7d5bcab&v=4
  twitterUsername: null
  url: https://github.com/eactisgrosso
- login: frankolson
  count: 1.3622589531680442
  avatarUrl: https://avatars.githubusercontent.com/u/6773706?u=738775762205a07fd7de297297c99f781e957c58&v=4
  twitterUsername: thinkolson
  url: https://github.com/frankolson
- login: uthmanmoh
  count: 1.3561536743354925
  avatarUrl: https://avatars.githubusercontent.com/u/83053931?u=5c715d2d4f6786fa749276de8eced710be8bfa99&v=4
  twitterUsername: null
  url: https://github.com/uthmanmoh
- login: Jordan-Gilliam
  count: 1.3528138528138527
  avatarUrl: https://avatars.githubusercontent.com/u/25993686?u=319a6ed2119197d4d11301614a104ae686f9fc70&v=4
  twitterUsername: nolansym
  url: https://github.com/Jordan-Gilliam
- login: winor30
  count: 1.3481015442826345
  avatarUrl: https://avatars.githubusercontent.com/u/12413150?u=691a5e076bdd8c9e9fd637a41496b29e11b0c82f&v=4
  twitterUsername: winor30
  url: https://github.com/winor30
- login: willemmulder
  count: 1.333493258645407
  avatarUrl: https://avatars.githubusercontent.com/u/70933?u=206fafc72fd14b4291cb29269c5e1cc8081d043b&v=4
  twitterUsername: willemmulder
  url: https://github.com/willemmulder
- login: mhart
  count: 1.314023891343479
  avatarUrl: https://avatars.githubusercontent.com/u/367936?v=4
  twitterUsername: hichaelmart
  url: https://github.com/mhart
- login: mvaker
  count: 1.3064713064713065
  avatarUrl: https://avatars.githubusercontent.com/u/5671913?u=2e237cb1dd51f9d0dd01f0deb80003163641fc49&v=4
  twitterUsername: null
  url: https://github.com/mvaker
- login: vitaly-ps
  count: 1.3063658201784487
  avatarUrl: https://avatars.githubusercontent.com/u/141448200?u=a3902a9c11399c916f1af2bf0ead901e7afe1a67&v=4
  twitterUsername: null
  url: https://github.com/vitaly-ps
- login: cbh123
  count: 1.3061284427145958
  avatarUrl: https://avatars.githubusercontent.com/u/14149230?u=ca710ca2a64391470163ddef6b5ea7633ab26872&v=4
  twitterUsername: charliebholtz
  url: https://github.com/cbh123
- login: Neverland3124
  count: 1.301994301994302
  avatarUrl: https://avatars.githubusercontent.com/u/52025513?u=865e861a1abb0d78be587f685d28fe8a00aee8fe&v=4
  twitterUsername: null
  url: https://github.com/Neverland3124
- login: jasonnathan
  count: 1.2936835460554628
  avatarUrl: https://avatars.githubusercontent.com/u/780157?u=006e0deda897eb1a4abcc459adcd7242dcbe8fee&v=4
  twitterUsername: jason_nathan
  url: https://github.com/jasonnathan
- login: Maanethdesilva
  count: 1.2846497764530551
  avatarUrl: https://avatars.githubusercontent.com/u/94875583?v=4
  twitterUsername: null
  url: https://github.com/Maanethdesilva
- login: fuleinist
  count: 1.2845103789266226
  avatarUrl: https://avatars.githubusercontent.com/u/1163738?v=4
  twitterUsername: null
  url: https://github.com/fuleinist
- login: kwadhwa18
  count: 1.2830645771822242
  avatarUrl: https://avatars.githubusercontent.com/u/6015244?u=a127081404b8dc16ac0e84a869dfff4ac82bbab2&v=4
  twitterUsername: null
  url: https://github.com/kwadhwa18
- login: sousousore1
  count: 1.274799599198397
  avatarUrl: https://avatars.githubusercontent.com/u/624438?v=4
  twitterUsername: null
  url: https://github.com/sousousore1
- login: seth-25
  count: 1.274023659050397
  avatarUrl: https://avatars.githubusercontent.com/u/49222652?u=203c2bef6cbb77668a289b8272aea4fb654558d5&v=4
  twitterUsername: null
  url: https://github.com/seth-25
- login: tomi-mercado
  count: 1.2671966816453508
  avatarUrl: https://avatars.githubusercontent.com/u/60221771?u=f8c1214535e402b0ff5c3428bfe98b586b517106&v=4
  twitterUsername: tomsito_ts
  url: https://github.com/tomi-mercado
- login: JHeidinga
  count: 1.2602823818293432
  avatarUrl: https://avatars.githubusercontent.com/u/1702015?u=fa33fb709707e2429f10fbb824abead61628d50c&v=4
  twitterUsername: null
  url: https://github.com/JHeidinga
- login: niklas-lohmann
  count: 1.2581453634085213
  avatarUrl: https://avatars.githubusercontent.com/u/68230177?v=4
  twitterUsername: null
  url: https://github.com/niklas-lohmann
- login: Durisvk
  count: 1.2562437105479827
  avatarUrl: https://avatars.githubusercontent.com/u/8467003?u=f07b8c070eaed3ad8972be4f4ca91afb1ae6e2c0&v=4
  twitterUsername: null
  url: https://github.com/Durisvk
- login: BjoernRave
  count: 1.2556762493521911
  avatarUrl: https://avatars.githubusercontent.com/u/36173920?u=c3acae11221a037c16254e2187555ea6259d89c3&v=4
  twitterUsername: bjoern_rave
  url: https://github.com/BjoernRave
- login: LordMsz
  count: 1.2519987699876998
  avatarUrl: https://avatars.githubusercontent.com/u/33070601?u=ddc6c16156f6397198692c547324e51f94c70ca7&v=4
  twitterUsername: null
  url: https://github.com/LordMsz
- login: tanyaasharma
  count: 1.2499106738092052
  avatarUrl: https://avatars.githubusercontent.com/u/140478067?v=4
  twitterUsername: null
  url: https://github.com/tanyaasharma
- login: crazyurus
  count: 1.2498577975251404
  avatarUrl: https://avatars.githubusercontent.com/u/2209055?u=b39f7e70f137ff3d1785d261cb15067f0d91ae05&v=4
  twitterUsername: null
  url: https://github.com/crazyurus
- login: qalqi
  count: 1.247286079182631
  avatarUrl: https://avatars.githubusercontent.com/u/1781048?u=837879a7e62c6b3736dc39a31ff42873bee2c532&v=4
  twitterUsername: null
  url: https://github.com/qalqi
- login: katarinasupe
  count: 1.246442719702482
  avatarUrl: https://avatars.githubusercontent.com/u/61758502?u=20cdcb0bae81b9eb330c94f7cfae462327785219&v=4
  twitterUsername: supe_katarina
  url: https://github.com/katarinasupe
- login: paul-paliychuk
  count: 1.245805528608245
  avatarUrl: https://avatars.githubusercontent.com/u/26054637?u=edd1e4f54e91b549f2edb525d43210f4f04d7367&v=4
  twitterUsername: null
  url: https://github.com/paul-paliychuk
- login: andrewlei
  count: 1.2386414411182212
  avatarUrl: https://avatars.githubusercontent.com/u/1158058?v=4
  twitterUsername: null
  url: https://github.com/andrewlei
- login: floomby
  count: 1.2297357814599192
  avatarUrl: https://avatars.githubusercontent.com/u/3113021?v=4
  twitterUsername: null
  url: https://github.com/floomby
- login: milanjrodd
  count: 1.2234407887448766
  avatarUrl: https://avatars.githubusercontent.com/u/121220673?u=55636f26ea48e77e0372008089ff2c38691eaa0a&v=4
  twitterUsername: null
  url: https://github.com/milanjrodd
- login: NickMandylas
  count: 1.2216829174004553
  avatarUrl: https://avatars.githubusercontent.com/u/19514618?u=95f8c29ed06696260722c2c6aa7bac3a1136d7a2&v=4
  twitterUsername: nicknetau
  url: https://github.com/NickMandylas
- login: DravenCat
  count: 1.2097773313242928
  avatarUrl: https://avatars.githubusercontent.com/u/55412122?v=4
  twitterUsername: null
  url: https://github.com/DravenCat
- login: Alireza29675
  count: 1.2074392093367425
  avatarUrl: https://avatars.githubusercontent.com/u/2771377?u=65ec71f9860ac2610e1cb5028173f67713a174d7&v=4
  twitterUsername: alireza29675
  url: https://github.com/Alireza29675
- login: zhengxs2018
  count: 1.2073887489504618
  avatarUrl: https://avatars.githubusercontent.com/u/7506913?u=42c32ca59ae2e44532cd45027e5b62d2712cf2a2&v=4
  twitterUsername: null
  url: https://github.com/zhengxs2018
- login: tofuliang
  count: 1.192831678125796
  avatarUrl: https://avatars.githubusercontent.com/u/1814685?v=4
  twitterUsername: null
  url: https://github.com/tofuliang
- login: cmtoomey
  count: 1.1890213611525087
  avatarUrl: https://avatars.githubusercontent.com/u/12201602?u=ea5cbb8d158980f6050dd41ae41b7f72e0a47337&v=4
  twitterUsername: Sock1tToomey
  url: https://github.com/cmtoomey
- login: igorshapiro
  count: 1.186488541327251
  avatarUrl: https://avatars.githubusercontent.com/u/1085209?u=16b60724316a7ed8e8b52af576c121215461922a&v=4
  twitterUsername: null
  url: https://github.com/igorshapiro
- login: ezynda3
  count: 1.1749026654687031
  avatarUrl: https://avatars.githubusercontent.com/u/5308871?v=4
  twitterUsername: what_the_func
  url: https://github.com/ezynda3
- login: more-by-more
  count: 1.1723569578526019
  avatarUrl: https://avatars.githubusercontent.com/u/67614844?u=d3d818efb3e3e2ddda589d6157f853922a460f5b&v=4
  twitterUsername: more__studio
  url: https://github.com/more-by-more
- login: noble-varghese
  count: 1.169995826128436
  avatarUrl: https://avatars.githubusercontent.com/u/109506617?u=c1d2a1813c51bff89bfa85d533633ed4c201ba2e&v=4
  twitterUsername: null
  url: https://github.com/noble-varghese
- login: SananR
  count: 1.1688741721854305
  avatarUrl: https://avatars.githubusercontent.com/u/14956384?u=538ff9bf09497059b312067333f68eba75594802&v=4
  twitterUsername: null
  url: https://github.com/SananR
- login: fraserxu
  count: 1.1641274004747006
  avatarUrl: https://avatars.githubusercontent.com/u/1183541?v=4
  twitterUsername: fraserxu
  url: https://github.com/fraserxu
- login: ashvardanian
  count: 1.1635835095137421
  avatarUrl: https://avatars.githubusercontent.com/u/1983160?u=536f2558c6ac33b74a6d89520dcb27ba46954070&v=4
  twitterUsername: ashvardanian
  url: https://github.com/ashvardanian
- login: adeelehsan
  count: 1.163265306122449
  avatarUrl: https://avatars.githubusercontent.com/u/8156837?u=99cacfbd962ff58885bdf68e5fc640fc0d3cb87c&v=4
  twitterUsername: null
  url: https://github.com/adeelehsan
- login: henriquegdantas
  count: 1.1626857749469215
  avatarUrl: https://avatars.githubusercontent.com/u/12974790?u=80d76f256a7854da6ae441b6ee078119877398e7&v=4
  twitterUsername: null
  url: https://github.com/henriquegdantas
- login: evad1n
  count: 1.1613760938410924
  avatarUrl: https://avatars.githubusercontent.com/u/50718218?u=ee35784971ef8dcdfdb25cfe0a8284ca48724938&v=4
  twitterUsername: null
  url: https://github.com/evad1n
- login: benjibc
  count: 1.1581632653061225
  avatarUrl: https://avatars.githubusercontent.com/u/1585539?u=654a21985c875f78a20eda7e4884e8d64de86fba&v=4
  twitterUsername: null
  url: https://github.com/benjibc
- login: P-E-B
  count: 1.1575327745540511
  avatarUrl: https://avatars.githubusercontent.com/u/38215315?u=3985b6a3ecb0e8338c5912ea9e20787152d0ad7a&v=4
  twitterUsername: null
  url: https://github.com/P-E-B
- login: omikader
  count: 1.152142614957631
  avatarUrl: https://avatars.githubusercontent.com/u/16735699?u=29fc7c7c777c3cabc22449b68bbb01fe2fa0b574&v=4
  twitterUsername: null
  url: https://github.com/omikader
- login: jasongill
  count: 1.1428571428571428
  avatarUrl: https://avatars.githubusercontent.com/u/241711?v=4
  twitterUsername: null
  url: https://github.com/jasongill
- login: Luisotee
  count: 1.1353383458646618
  avatarUrl: https://avatars.githubusercontent.com/u/50471205?u=059d6ab166e5a32c496ff50ef6e3fb0ca04a50ad&v=4
  twitterUsername: null
  url: https://github.com/Luisotee
- login: puigde
  count: 1.125363627638318
  avatarUrl: https://avatars.githubusercontent.com/u/83642160?u=7e76b13b7484e4601bea47dc6e238c89d453a24d&v=4
  twitterUsername: polpuigdemont
  url: https://github.com/puigde
- login: Adrastopoulos
  count: 1.118279569892473
  avatarUrl: https://avatars.githubusercontent.com/u/76796897?u=0bd50d301b4c7025f29396af44c8e1829eff1db6&v=4
  twitterUsername: null
  url: https://github.com/Adrastopoulos
- login: chase-crumbaugh
  count: 1.1127079590720925
  avatarUrl: https://avatars.githubusercontent.com/u/90289500?u=0129550ecfbb4a92922fff7a406566a47a23dfb0&v=4
  twitterUsername: null
  url: https://github.com/chase-crumbaugh
- login: Zeneos
  count: 1.1119838542518954
  avatarUrl: https://avatars.githubusercontent.com/u/95008961?v=4
  twitterUsername: null
  url: https://github.com/Zeneos
- login: joseanu
  count: 1.1101000909918106
  avatarUrl: https://avatars.githubusercontent.com/u/2730127?u=9fe1d593bd63c7f116b9c46e9cbd359a2e4304f0&v=4
  twitterUsername: jantonioulloa
  url: https://github.com/joseanu
- login: JackFener
  count: 1.108656849620705
  avatarUrl: https://avatars.githubusercontent.com/u/20380671?u=b51d10b71850203e6360655fa59cc679c5a498e6&v=4
  twitterUsername: null
  url: https://github.com/JackFener
- login: swyxio
  count: 1.1079251294305057
  avatarUrl: https://avatars.githubusercontent.com/u/6764957?u=97ad815028595b73b06ee4b0510e66bbe391228d&v=4
  twitterUsername: swyx
  url: https://github.com/swyxio
- login: pczekaj
  count: 1.1072005633673014
  avatarUrl: https://avatars.githubusercontent.com/u/1460539?u=24c2db4a29757f608a54a062340a466cad843825&v=4
  twitterUsername: null
  url: https://github.com/pczekaj
- login: devinburnette
  count: 1.1044585013472967
  avatarUrl: https://avatars.githubusercontent.com/u/13012689?u=7b68c67ea1bbc272c35be7c0bcf1c66a04554179&v=4
  twitterUsername: null
  url: https://github.com/devinburnette
- login: ananis25
  count: 1.1039797561536693
  avatarUrl: https://avatars.githubusercontent.com/u/16446513?u=5026326ed39bfee8325c30cdbd24ac20519d21b8&v=4
  twitterUsername: ananis25
  url: https://github.com/ananis25
- login: joaopcm
  count: 1.0963667731359579
  avatarUrl: https://avatars.githubusercontent.com/u/58827242?u=3e03812a1074f2ce888b751c48e78a849c7e0aff&v=4
  twitterUsername: jopcmelo
  url: https://github.com/joaopcm
- login: SalehHindi
  count: 1.0950556467797847
  avatarUrl: https://avatars.githubusercontent.com/u/15721377?u=37fadd6a7bf9dfa63ceb866bda23ca44a7b2c0c2&v=4
  twitterUsername: SalehOfTomorrow
  url: https://github.com/SalehHindi
- login: AvaterClasher
  count: 1.0900805104055902
  avatarUrl: https://avatars.githubusercontent.com/u/116944847?u=102a870b3efed7f30f0a57123391a293eb6f5b08&v=4
  twitterUsername: Avater004
  url: https://github.com/AvaterClasher
- login: JamsheedMistri
  count: 1.0876276783657168
  avatarUrl: https://avatars.githubusercontent.com/u/13024750?u=6ae631199ec7c0bb34eb8d56200023cdd94720d3&v=4
  twitterUsername: null
  url: https://github.com/JamsheedMistri
- login: cmanou
  count: 1.0871305418719213
  avatarUrl: https://avatars.githubusercontent.com/u/683160?u=e9050e4341c2c9d46b035ea17ea94234634e1b2c&v=4
  twitterUsername: null
  url: https://github.com/cmanou
- login: micahriggan
  count: 1.0791195549507453
  avatarUrl: https://avatars.githubusercontent.com/u/3626473?u=508e8c831d8eb804e95985d5191a08c761544fad&v=4
  twitterUsername: null
  url: https://github.com/micahriggan
- login: w00ing
  count: 1.075050709939148
  avatarUrl: https://avatars.githubusercontent.com/u/29723695?u=563d4a628c9af35f827f476e38635310f1cec114&v=4
  twitterUsername: wooing0306
  url: https://github.com/w00ing
- login: madmed88
  count: 1.0614216701173222
  avatarUrl: https://avatars.githubusercontent.com/u/1579388?u=62ca1bfe7c271b5fd1d77abc470aa5e535b1ed83&v=4
  twitterUsername: null
  url: https://github.com/madmed88
- login: ardsh
  count: 1.0609951845906902
  avatarUrl: https://avatars.githubusercontent.com/u/23664687?u=158ef7e156a7881b8647ece63683aca2c28f132e&v=4
  twitterUsername: null
  url: https://github.com/ardsh
- login: JoeABCDEF
  count: 1.0464925848421436
  avatarUrl: https://avatars.githubusercontent.com/u/39638510?u=f5fac0a3578572817b37a6dfc00adacb705ec7d0&v=4
  twitterUsername: null
  url: https://github.com/JoeABCDEF
- login: saul-jb
  count: 1.04251968503937
  avatarUrl: https://avatars.githubusercontent.com/u/2025187?v=4
  twitterUsername: null
  url: https://github.com/saul-jb
- login: JTCorrin
  count: 1.0404043172236763
  avatarUrl: https://avatars.githubusercontent.com/u/73115680?v=4
  twitterUsername: null
  url: https://github.com/JTCorrin
- login: zandko
  count: 1.026159026159026
  avatarUrl: https://avatars.githubusercontent.com/u/37948383?u=04ccf6e060b27e39c931c2608381351cf236a28f&v=4
  twitterUsername: null
  url: https://github.com/zandko
- login: federicoestevez
  count: 1.0229885057471264
  avatarUrl: https://avatars.githubusercontent.com/u/10424147?v=4
  twitterUsername: null
  url: https://github.com/federicoestevez
- login: martinseanhunt
  count: 1.0161725067385445
  avatarUrl: https://avatars.githubusercontent.com/u/65744?u=ddac1e773828d8058a40bca680cf549e955f69ae&v=4
  twitterUsername: null
  url: https://github.com/martinseanhunt
- login: functorism
  count: 1.0136089225416964
  avatarUrl: https://avatars.githubusercontent.com/u/17207277?u=4df9bc30a55b4da4b3d6fd20a2956afd722bde24&v=4
  twitterUsername: null
  url: https://github.com/functorism
- login: erictt
  count: 1.0129149331680978
  avatarUrl: https://avatars.githubusercontent.com/u/9592198?u=567fa49c73e824525d33eefd836ece16ab9964c8&v=4
  twitterUsername: null
  url: https://github.com/erictt
- login: WilliamEspegren
  count: 1.0091400083090984
  avatarUrl: https://avatars.githubusercontent.com/u/131612909?v=4
  twitterUsername: WilliamEspegren
  url: https://github.com/WilliamEspegren
- login: lesters
  count: 1.00853889943074
  avatarUrl: https://avatars.githubusercontent.com/u/5798036?u=4eba31d63c3818d17fb8f9aa923599ac63ebfea8&v=4
  twitterUsername: null
  url: https://github.com/lesters
- login: my8bit
  count: 1.0073145245559039
  avatarUrl: https://avatars.githubusercontent.com/u/782268?u=d83da3e6269d53a828bbeb6d661049a1ed185cb0&v=4
  twitterUsername: null
  url: https://github.com/my8bit
- login: erhant
  count: 1.0040241448692153
  avatarUrl: https://avatars.githubusercontent.com/u/16037166?u=9d056a2f5059684620e22aa4d880e38183309b51&v=4
  twitterUsername: 0xerhant
  url: https://github.com/erhant
top_reviewers:
- login: afirstenberg
  count: 23
  avatarUrl: https://avatars.githubusercontent.com/u/3507578?v=4
  twitterUsername: null
  url: https://github.com/afirstenberg
- login: sullivan-sean
  count: 8
  avatarUrl: https://avatars.githubusercontent.com/u/22581534?u=8f88473db2f929a965b6371733efda28e3fa1948&v=4
  twitterUsername: null
  url: https://github.com/sullivan-sean
- login: tomasonjo
  count: 8
  avatarUrl: https://avatars.githubusercontent.com/u/19948365?v=4
  twitterUsername: tb_tomaz
  url: https://github.com/tomasonjo
- login: ppramesi
  count: 7
  avatarUrl: https://avatars.githubusercontent.com/u/6775031?v=4
  twitterUsername: null
  url: https://github.com/ppramesi
- login: jacobrosenthal
  count: 6
  avatarUrl: https://avatars.githubusercontent.com/u/455796?v=4
  twitterUsername: null
  url: https://github.com/jacobrosenthal
- login: sinedied
  count: 6
  avatarUrl: https://avatars.githubusercontent.com/u/593151?u=08557bbdd96221813b8aec932dd7de895ac040ea&v=4
  twitterUsername: sinedied
  url: https://github.com/sinedied
- login: mieslep
  count: 5
  avatarUrl: https://avatars.githubusercontent.com/u/5420540?u=8f038c002fbce42427999eb715dc9f868cef1c84&v=4
  twitterUsername: null
  url: https://github.com/mieslep



================================================
FILE: docs/core_docs/docs/community.mdx
================================================
# Community navigator

Hi! Thanks for being here. We're lucky to have a community of so many passionate developers building with LangChain–we have so much to teach and learn from each other. Community members contribute code, host meetups, write blog posts, amplify each other's work, become each other's customers and collaborators, and so much more.

Whether you're new to LangChain, looking to go deeper, or just want to get more exposure to the world of building with LLMs, this page can point you in the right direction.

- **🦜 Contribute to LangChain**

- **🌍 Meetups, Events, and Hackathons**

- **📣 Help Us Amplify Your Work**

- **💬 Stay in the loop**

# 🦜 Contribute to LangChain

LangChain is the product of over 5,000+ contributions by 1,500+ contributors, and there is **still** so much to do together. Here are some ways to get involved:

- **[Open a pull request](https://github.com/langchain-ai/langchainjs/issues):** we'd appreciate all forms of contributions–new features, infrastructure improvements, better documentation, bug fixes, etc. If you have an improvement or an idea, we'd love to work on it with you.
- **[Read our contributor guidelines:](https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md)** We ask contributors to follow a ["fork and pull request"](https://docs.github.com/en/get-started/quickstart/contributing-to-projects) workflow, run a few local checks for formatting, linting, and testing before submitting, and follow certain documentation and testing conventions.
- **Become an expert:** our experts help the community by answering product questions in Discord. If that's a role you'd like to play, we'd be so grateful! (And we have some special experts-only goodies/perks we can tell you more about). Send us an email to introduce yourself at hello@langchain.dev and we'll take it from there!
- **Integrate with LangChain:** if your product integrates with LangChain–or aspires to–we want to help make sure the experience is as smooth as possible for you and end users. Send us an email at hello@langchain.dev and tell us what you're working on.
  - **Become an Integration Maintainer:** Partner with our team to ensure your integration stays up-to-date and talk directly with users (and answer their inquiries) in our Discord. Introduce yourself at hello@langchain.dev if you'd like to explore this role.

# 🌍 Meetups, Events, and Hackathons

One of our favorite things about working in AI is how much enthusiasm there is for building together. We want to help make that as easy and impactful for you as possible!

- **Find a meetup, hackathon, or webinar:** you can find the one for you on on our [global events calendar](https://mirror-feeling-d80.notion.site/0bc81da76a184297b86ca8fc782ee9a3?v=0d80342540df465396546976a50cfb3f).
  - **Submit an event to our calendar:** email us at events@langchain.dev with a link to your event page! We can also help you spread the word with our local communities.
- **Host a meetup:** If you want to bring a group of builders together, we want to help! We can publicize your event on our event calendar/Twitter, share with our local communities in Discord, send swag, or potentially hook you up with a sponsor. Email us at events@langchain.dev to tell us about your event!
- **Become a meetup sponsor:** we often hear from groups of builders that want to get together, but are blocked or limited on some dimension (space to host, budget for snacks, prizes to distribute, etc.). If you'd like to help, send us an email to events@langchain.dev we can share more about how it works!
- **Speak at an event:** meetup hosts are always looking for great speakers, presenters, and panelists. If you'd like to do that at an event, send us an email to hello@langchain.dev with more information about yourself, what you want to talk about, and what city you're based in and we'll try to match you with an upcoming event!
- **Tell us about your LLM community:** If you host or participate in a community that would welcome support from LangChain and/or our team, send us an email at hello@langchain.dev and let us know how we can help.

# 📣 Help Us Amplify Your Work

If you're working on something you're proud of, and think the LangChain community would benefit from knowing about it, we want to help you show it off.

- **Post about your work and mention us:** we love hanging out on Twitter to see what people in the space are talking about and working on. If you tag [@langchainai](https://twitter.com/LangChainAI), we'll almost certainly see it and can show you some love.
- **Publish something on our blog:** if you're writing about your experience building with LangChain, we'd love to post (or crosspost) it on our blog! E-mail hello@langchain.dev with a draft of your post! Or even an idea for something you want to write about.
- **Get your product onto our [integrations hub](https://integrations.langchain.com/):** Many developers take advantage of our seamless integrations with other products, and come to our integrations hub to find out who those are. If you want to get your product up there, tell us about it (and how it works with LangChain) at hello@langchain.dev.

# ☀️ Stay in the loop

Here's where our team hangs out, talks shop, spotlights cool work, and shares what we're up to. We'd love to see you there too.

- **[Twitter](https://twitter.com/LangChainAI):** we post about what we're working on and what cool things we're seeing in the space. If you tag @langchainai in your post, we'll almost certainly see it, and can snow you some love!
- **[GitHub](https://github.com/langchain-ai/langchainjs):** open pull requests, contribute to a discussion, and/or contribute
- **[Subscribe to our bi-weekly Release Notes](https://6w1pwbss0py.typeform.com/to/KjZB1auB):** a twice/month email roundup of the coolest things going on in our orbit



================================================
FILE: docs/core_docs/docs/introduction.mdx
================================================
---
sidebar_position: 0
---

# Introduction

**LangChain** is a framework for developing applications powered by large language models (LLMs).

LangChain simplifies every stage of the LLM application lifecycle:

- **Development**: Build your applications using LangChain's open-source [building blocks](/docs/concepts/lcel), [components](/docs/concepts), and [third-party integrations](/docs/integrations/platforms/).
  Use [LangGraph.js](/docs/concepts/architecture#langgraph) to build stateful agents with first-class streaming and human-in-the-loop support.
- **Productionization**: Use [LangSmith](https://docs.smith.langchain.com/) to inspect, monitor and evaluate your chains, so that you can continuously optimize and deploy with confidence.
- **Deployment**: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Cloud](https://langchain-ai.github.io/langgraph/cloud/).

import ThemedImage from "@theme/ThemedImage";
import useBaseUrl from "@docusaurus/useBaseUrl";

<ThemedImage
  alt="Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers."
  sources={{
    light: useBaseUrl("/svg/langchain_stack_062024.svg"),
    dark: useBaseUrl("/svg/langchain_stack_062024_dark.svg"),
  }}
  title="LangChain Framework Overview"
  style={{ width: "100%" }}
/>

Concretely, the framework consists of the following open-source libraries:

- **`@langchain/core`**: Base abstractions and LangChain Expression Language.
- **`@langchain/community`**: Third party integrations.
  - Partner packages (e.g. **`@langchain/openai`**, **`@langchain/anthropic`**, etc.): Some integrations have been further split into their own lightweight packages that only depend on **`@langchain/core`**.
- **`langchain`**: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.
- **[LangGraph.js](https://langchain-ai.github.io/langgraphjs/)**: Build robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.
- **[LangSmith](https://docs.smith.langchain.com)**: A developer platform that lets you debug, test, evaluate, and monitor LLM applications.

:::note

These docs focus on the JavaScript LangChain library. [Head here](https://python.langchain.com) for docs on the Python LangChain library.

:::

## [Tutorials](/docs/tutorials)

If you're looking to build something specific or are more of a hands-on learner, check out our [tutorials](/docs/tutorials).
This is the best place to get started.

These are the best ones to get started with:

- [Build a Simple LLM Application](/docs/tutorials/llm_chain)
- [Build a Chatbot](/docs/tutorials/chatbot)
- [Build an Agent](https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/)
- [LangGraph.js quickstart](https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/)

Explore the full list of LangChain tutorials [here](/docs/tutorials), and check out other [LangGraph tutorials here](https://langchain-ai.github.io/langgraphjs/tutorials/).

## [How-To Guides](/docs/how_to/)

[Here](/docs/how_to/) you'll find short answers to “How do I….?” types of questions.
These how-to guides don't cover topics in depth - you'll find that material in the [Tutorials](/docs/tutorials) and the [API Reference](https://api.js.langchain.com).
However, these guides will help you quickly accomplish common tasks.

Check out [LangGraph-specific how-tos here](https://langchain-ai.github.io/langgraphjs/how-tos/).

## [Conceptual Guide](/docs/concepts)

Introductions to all the key parts of LangChain you'll need to know! [Here](/docs/concepts) you'll find high level explanations of all LangChain concepts.

For a deeper dive into LangGraph concepts, check out [this page](https://langchain-ai.github.io/langgraph/concepts/).

## [API reference](https://api.js.langchain.com)

Head to the reference section for full documentation of all classes and methods in the LangChain JavaScript packages.

## Ecosystem

### [🦜🛠️ LangSmith](https://docs.smith.langchain.com)

Trace and evaluate your language model applications and intelligent agents to help you move from prototype to production.

### [🦜🕸️ LangGraph](https://langchain-ai.github.io/langgraphjs/)

Build stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.

## Additional resources

### [Security](/docs/security)

Read up on our [Security](/docs/security) best practices to make sure you're developing safely with LangChain.

### [Integrations](/docs/integrations/platforms/)

LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of [integrations](/docs/integrations/platforms/).

### [Contributing](/docs/contributing)

Check out the developer's guide for guidelines on contributing and help getting your dev environment set up.



================================================
FILE: docs/core_docs/docs/packages.mdx
================================================
# 📕 Package Versioning

As of now, LangChain has an ad hoc release process: releases are cut with high frequency by
a maintainer and published to [PyPI](https://pypi.org/).
The different packages are versioned slightly differently.

## `@langchain/core`

`@langchain/core` is currently on version `0.1.x`.

As `@langchain/core` contains the base abstractions and runtime for the whole LangChain ecosystem, we will communicate any breaking changes with advance notice and version bumps. The exception for this is anything marked with the `beta` decorator (you can see this in the API reference and will see warnings when using such functionality). The reason for beta features is that given the rate of change of the field, being able to move quickly is still a priority.

Minor version increases will occur for:

- Breaking changes for any public interfaces marked as `beta`.

Patch version increases will occur for:

- Bug fixes
- New features
- Any changes to private interfaces
- Any changes to `beta` features

## `langchain`

`langchain` is currently on version `0.1.x`

Minor version increases will occur for:

- Breaking changes for any public interfaces NOT marked as `beta`.

Patch version increases will occur for:

- Bug fixes
- New features
- Any changes to private interfaces
- Any changes to `beta` features

We are working on the `langchain` v0.2 release, which will have some breaking changes to legacy Chains and Agents.
Additionally, we will remove `@langchain/community` as a dependency and stop re-exporting integrations that have been moved to `@langchain/community`.

## `@langchain/community`

`@langchain/community` is currently on version `0.0.x`

All changes will be accompanied by a patch version increase.

## Partner Packages

Partner packages are versioned independently.



================================================
FILE: docs/core_docs/docs/people.mdx
================================================
---
hide_table_of_contents: true
---

import People from "@theme/People";

# People

There are some incredible humans from all over the world who have been instrumental in helping the LangChain community flourish 🌐!

This page highlights a few of those folks who have dedicated their time to the open-source repo in the form of direct contributions and reviews.

## Top reviewers

As LangChain has grown, the amount of surface area that maintainers cover has grown as well.

Thank you to the following folks who have gone above and beyond in reviewing incoming PRs 🙏!

<People type="top_reviewers"></People>

## Top recent contributors

The list below contains contributors who have had the most PRs merged in the last three months, weighted (imperfectly) by impact.

Thank you all so much for your time and efforts in making LangChain better ❤️!

<People type="top_recent_contributors" count="20"></People>

## Core maintainers

Hello there 👋!

We're LangChain's core maintainers. If you've spent time in the community, you've probably crossed paths
with at least one of us already.

<People type="maintainers"></People>

## Top all-time contributors

And finally, this is an all-time list of all-stars who have made significant contributions to the framework 🌟:

<People type="top_contributors"></People>

We're so thankful for your support!

And one more thank you to [@tiangolo](https://github.com/tiangolo) for inspiration via FastAPI's [excellent people page](https://fastapi.tiangolo.com/fastapi-people).



================================================
FILE: docs/core_docs/docs/security.md
================================================
# Security

LangChain has a large ecosystem of integrations with various external resources like local and remote file systems, APIs and databases. These integrations allow developers to create versatile applications that combine the power of LLMs with the ability to access, interact with and manipulate external resources.

## Best Practices

When building such applications developers should remember to follow good security practices:

- [**Limit Permissions**](https://en.wikipedia.org/wiki/Principle_of_least_privilege): Scope permissions specifically to the application's need. Granting broad or excessive permissions can introduce significant security vulnerabilities. To avoid such vulnerabilities, consider using read-only credentials, disallowing access to sensitive resources, using sandboxing techniques (such as running inside a container), etc. as appropriate for your application.
- **Anticipate Potential Misuse**: Just as humans can err, so can Large Language Models (LLMs). Always assume that any system access or credentials may be used in any way allowed by the permissions they are assigned. For example, if a pair of database credentials allows deleting data, it’s safest to assume that any LLM able to use those credentials may in fact delete data.
- [**Defense in Depth**](<https://en.wikipedia.org/wiki/Defense_in_depth_(computing)>): No security technique is perfect. Fine-tuning and good chain design can reduce, but not eliminate, the odds that a Large Language Model (LLM) may make a mistake. It’s best to combine multiple layered security approaches rather than relying on any single layer of defense to ensure security. For example: use both read-only permissions and sandboxing to ensure that LLMs are only able to access data that is explicitly meant for them to use.

Risks of not doing so include, but are not limited to:

- Data corruption or loss.
- Unauthorized access to confidential information.
- Compromised performance or availability of critical resources.

Example scenarios with mitigation strategies:

- A user may ask an agent with access to the file system to delete files that should not be deleted or read the content of files that contain sensitive information. To mitigate, limit the agent to only use a specific directory and only allow it to read or write files that are safe to read or write. Consider further sandboxing the agent by running it in a container.
- A user may ask an agent with write access to an external API to write malicious data to the API, or delete data from that API. To mitigate, give the agent read-only API keys, or limit it to only use endpoints that are already resistant to such misuse.
- A user may ask an agent with access to a database to drop a table or mutate the schema. To mitigate, scope the credentials to only the tables that the agent needs to access and consider issuing READ-ONLY credentials.

If you're building applications that access external resources like file systems, APIs
or databases, consider speaking with your company's security team to determine how to best
design and secure your applications.

## Reporting a Vulnerability

Please report security vulnerabilities by email to security@langchain.dev. This will ensure the issue is promptly triaged and acted upon as needed.



================================================
FILE: docs/core_docs/docs/_static/css/custom.css
================================================
pre {
  white-space: break-spaces;
}

@media (min-width: 1200px) {
  .container,
  .container-lg,
  .container-md,
  .container-sm,
  .container-xl {
    max-width: 2560px !important;
  }
}

#my-component-root *, #headlessui-portal-root * {
  z-index: 10000;
}

.content-container p {
    margin: revert;
}


================================================
FILE: docs/core_docs/docs/additional_resources/tutorials.mdx
================================================
# External guides

Below are links to external tutorials and courses on LangChain.js. For other written guides on common use cases for LangChain.js, check out the [tutorials](/docs/tutorials/) and [how to](/docs/how_to/) sections.

---

## Deeplearning.ai

We've partnered with [Deeplearning.ai](https://deeplearning.ai) and [Andrew Ng](https://en.wikipedia.org/wiki/Andrew_Ng)
on a LangChain.js short course.

It covers LCEL and other building blocks you can combine to build more complex chains, as well as fundamentals around
loading data for retrieval augmented generation (RAG). Try it for free below:

- [Build LLM Apps with LangChain.js](https://www.deeplearning.ai/short-courses/build-llm-apps-with-langchain-js)

## Scrimba interactive guides

[Scrimba](https://scrimba.com) is a code-learning platform that allows you to interactively edit and run
code while watching a video walkthrough.

We've partnered with Scrimba on course materials (called "scrims") that teach the fundamentals of building with LangChain.js -
check them out below, and check back for more as they become available!

### Learn LangChain.js

- [Learn LangChain.js on Scrimba](https://scrimba.com/learn/langchain)

An full end-to-end course that walks through how to build a chatbot that can answer questions about a provided document. A great
introduction to LangChain and a great first project for learning how to use LangChain Expression Language primitives to perform retrieval!

### LangChain Expression Language (LCEL)

- [The basics (PromptTemplate + LLM)](https://v2.scrimba.com/s05iemh)
- [Adding an output parser](https://scrimba.com/scrim/co6ae44248eacc1abd87ae3dc)
- [Attaching function calls to a model](https://scrimba.com/scrim/cof5449f5bc972f8c90be6a82)
- [Composing multiple chains](https://scrimba.com/scrim/co14344c29595bfb29c41f12a)
- [Retrieval chains](https://scrimba.com/scrim/co0e040d09941b4000244db46)
- [Conversational retrieval chains ("Chat with Docs")](https://scrimba.com/scrim/co3ed4a9eb4c6c6d0361a507c)

### Deeper dives

- [Setting up a new `PromptTemplate`](https://scrimba.com/scrim/cbGwRwuV)
- [Setting up `ChatOpenAI` parameters](https://scrimba.com/scrim/cEgbBBUw)
- [Attaching stop sequences](https://scrimba.com/scrim/co9704e389428fe2193eb955c)

## Neo4j GraphAcademy

[Neo4j](https://neo4j.com) has put together a hands-on, practical course that shows how to build a movie-recommending chatbot in Next.js.
It covers retrieval-augmented generation (RAG), tracking history, and more. Check it out below:

- [Build a Neo4j-backed Chatbot with TypeScript](https://graphacademy.neo4j.com/courses/llm-chatbot-typescript/?ref=langchainjs)

## LangChain.js x AI SDK

How to use LangChain.js with AI SDK and React Server Components.

- [Streaming agentic data to the client](https://github.com/langchain-ai/langchain-nextjs-template/blob/main/app/ai_sdk/agent/README.md)
- [Streaming tool responses to the client](https://github.com/langchain-ai/langchain-nextjs-template/blob/main/app/ai_sdk/tools/README.md)

---



================================================
FILE: docs/core_docs/docs/concepts/agents.mdx
================================================
# Agents

By themselves, language models can't take actions - they just output text. Agents are systems that take a high-level task and use an LLM as a reasoning engine to decide what actions to take and execute those actions.

[LangGraph](/docs/concepts/architecture#langgraph) is an extension of LangChain specifically aimed at creating highly controllable and customizable agents. We recommend that you use LangGraph for building agents.

Please see the following resources for more information:

- LangGraph docs on [common agent architectures](https://langchain-ai.github.io/langgraphjs/concepts/agentic_concepts/)
- [Pre-built agents in LangGraph](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph_prebuilt.createReactAgent.html)

## Legacy agent concept: AgentExecutor

LangChain previously introduced the `AgentExecutor` as a runtime for agents.
While it served as an excellent starting point, its limitations became apparent when dealing with more sophisticated and customized agents.
As a result, we're gradually phasing out `AgentExecutor` in favor of more flexible solutions in LangGraph.

### Transitioning from AgentExecutor to langgraph

If you're currently using `AgentExecutor`, don't worry! We've prepared resources to help you:

1. For those who still need to use `AgentExecutor`, we offer a comprehensive guide on [how to use AgentExecutor](/docs/how_to/agent_executor).

2. However, we strongly recommend transitioning to LangGraph for improved flexibility and control. To facilitate this transition, we've created a detailed [migration guide](/docs/how_to/migrate_agent) to help you move from `AgentExecutor` to LangGraph seamlessly.



================================================
FILE: docs/core_docs/docs/concepts/architecture.mdx
================================================
import ThemedImage from "@theme/ThemedImage";
import useBaseUrl from "@docusaurus/useBaseUrl";

# Architecture

LangChain is a framework that consists of a number of packages.

<ThemedImage
  alt="Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers."
  sources={{
    light: useBaseUrl("/svg/langchain_stack_062024.svg"),
    dark: useBaseUrl("/svg/langchain_stack_062024_dark.svg"),
  }}
  title="LangChain Framework Overview"
  style={{ width: "100%" }}
/>

## @langchain/core

This package contains base abstractions for different components and ways to compose them together.
The interfaces for core components like chat models, vector stores, tools and more are defined here.
No third-party integrations are defined here.
The dependencies are very lightweight.

## langchain

The main `langchain` package contains chains and retrieval strategies that make up an application's cognitive architecture.
These are NOT third-party integrations.
All chains, agents, and retrieval strategies here are NOT specific to any one integration, but rather generic across all integrations.

## Integration packages

Popular integrations have their own packages (e.g. `@langchain/openai`, `@langchain/anthropic`, etc) so that they can be properly versioned and appropriately lightweight.

For more information see:

- A list [integrations packages](/docs/integrations/platforms/)
- The [API Reference](https://api.js.langchain.com/) where you can find detailed information about each of the integration package.

## @langchain/community

This package contains third-party integrations that are maintained by the LangChain community.
Key integration packages are separated out (see above).
This contains integrations for various components (chat models, vector stores, tools, etc).
All dependencies in this package are optional to keep the package as lightweight as possible.

## @langchain/langgraph

`@langchain/langgraph` is an orchestration framework aimed at building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.

LangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom flows.

:::info [Further reading]

- See our LangGraph overview [here](https://langchain-ai.github.io/langgraphjs/concepts/high_level/#core-principles).
- See our LangGraph Academy Course [here](https://academy.langchain.com/courses/intro-to-langgraph).

:::

## LangSmith

A developer platform that lets you debug, test, evaluate, and monitor LLM applications.

For more information, see the [LangSmith documentation](https://docs.smith.langchain.com)



================================================
FILE: docs/core_docs/docs/concepts/callbacks.mdx
================================================
# Callbacks

:::note Prerequisites

- [Runnable interface](/docs/concepts/runnables)

:::

LangChain provides a callback system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.

You can subscribe to these events by using the `callbacks` argument available throughout the API. This argument is list of handler objects, which are expected to implement one or more of the methods described below in more detail.

## Callback events

| Event            | Event Trigger                               | Associated Method      |
| ---------------- | ------------------------------------------- | ---------------------- |
| Chat model start | When a chat model starts                    | `handleChatModelStart` |
| LLM start        | When a llm starts                           | `handleLlmStart`       |
| LLM new token    | When an llm OR chat model emits a new token | `handleLlmNewToken`    |
| LLM ends         | When an llm OR chat model ends              | `handleLlmEnd`         |
| LLM errors       | When an llm OR chat model errors            | `handleLlmError`       |
| Chain start      | When a chain starts running                 | `handleChainStart`     |
| Chain end        | When a chain ends                           | `handleChainEnd`       |
| Chain error      | When a chain errors                         | `handleChainError`     |
| Tool start       | When a tool starts running                  | `handleToolStart`      |
| Tool end         | When a tool ends                            | `handleToolEnd`        |
| Tool error       | When a tool errors                          | `handleToolError`      |
| Retriever start  | When a retriever starts                     | `handleRetrieverStart` |
| Retriever end    | When a retriever ends                       | `handleRetrieverEnd`   |
| Retriever error  | When a retriever errors                     | `handleRetrieverError` |

## Callback handlers

- Callback handlers implement the [BaseCallbackHandler](https://api.js.langchain.com/classes/_langchain_core.callbacks_base.BaseCallbackHandler.html) interface.

During run-time LangChain configures an appropriate callback manager (e.g., [CallbackManager](https://api.js.langchain.com/classes/_langchain_core.callbacks_manager.BaseCallbackManager.html)) which will be responsible for calling the appropriate method on each "registered" callback handler when the event is triggered.

## Passing callbacks

The `callbacks` property is available on most objects throughout the API (Models, Tools, Agents, etc.) in two different places:

- **Request time callbacks**: Passed at the time of the request in addition to the input data.
  Available on all standard `Runnable` objects. These callbacks are INHERITED by all children
  of the object they are defined on. For example, `await chain.invoke({ number: 25 }, { callbacks: [handler] })`.
- **Constructor callbacks**: `const chain = new TheNameOfSomeChain({ callbacks: [handler] })`. These callbacks
  are passed as arguments to the constructor of the object. The callbacks are scoped
  only to the object they are defined on, and are **not** inherited by any children of the object.

:::warning

Constructor callbacks are scoped only to the object they are defined on. They are **not** inherited by children
of the object.

:::

If you're creating a custom chain or runnable, you need to remember to propagate request time
callbacks to any child objects.

For specifics on how to use callbacks, see the [relevant how-to guides here](/docs/how_to/#callbacks).



================================================
FILE: docs/core_docs/docs/concepts/chat_history.mdx
================================================
# Chat history

:::info Prerequisites

- [Messages](/docs/concepts/messages)
- [Chat models](/docs/concepts/chat_models)
- [Tool calling](/docs/concepts/tool_calling)

:::

Chat history is a record of the conversation between the user and the chat model. It is used to maintain context and state throughout the conversation. The chat history is sequence of [messages](/docs/concepts/messages), each of which is associated with a specific [role](/docs/concepts/messages#role), such as "user", "assistant", "system", or "tool".

## Conversation patterns

![Conversation patterns](/img/conversation_patterns.png)

Most conversations start with a **system message** that sets the context for the conversation. This is followed by a **user message** containing the user's input, and then an **assistant message** containing the model's response.

The **assistant** may respond directly to the user or if configured with tools request that a [tool](/docs/concepts/tool_calling) be invoked to perform a specific task.

So a full conversation often involves a combination of two patterns of alternating messages:

1. The **user** and the **assistant** representing a back-and-forth conversation.
2. The **assistant** and **tool messages** representing an ["agentic" workflow](/docs/concepts/agents) where the assistant is invoking tools to perform specific tasks.

## Managing chat history

Since chat models have a maximum limit on input size, it's important to manage chat history and trim it as needed to avoid exceeding the [context window](/docs/concepts/chat_models#context_window).

While processing chat history, it's essential to preserve a correct conversation structure.

Key guidelines for managing chat history:

- The conversation should follow one of these structures:
  - The first message is either a "user" message or a "system" message, followed by a "user" and then an "assistant" message.
  - The last message should be either a "user" message or a "tool" message containing the result of a tool call.
- When using [tool calling](/docs/concepts/tool_calling), a "tool" message should only follow an "assistant" message that requested the tool invocation.

:::tip

Understanding correct conversation structure is essential for being able to properly implement
[memory](https://langchain-ai.github.io/langgraphjs/concepts/memory/) in chat models.

:::

## Related resources

- [How to trim messages](/docs/how_to/trim_messages/)
- [Memory guide](https://langchain-ai.github.io/langgraphjs/concepts/memory/) for information on implementing short-term and long-term memory in chat models using [LangGraph](https://langchain-ai.github.io/langgraphjs/).



================================================
FILE: docs/core_docs/docs/concepts/chat_models.mdx
================================================
# Chat models

## Overview

Large Language Models (LLMs) are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, question answering, and more, without needing task-specific tuning for every scenario.

Modern LLMs are typically accessed through a chat model interface that takes a list of [messages](/docs/concepts/messages) as input and returns a [message](/docs/concepts/messages) as output.

The newest generation of chat models offer additional capabilities:

- [Tool calling](/docs/concepts/tool_calling): Many popular chat models offer a native [tool calling](/docs/concepts/tool_calling) API. This API allows developers to build rich applications that enable AI to interact with external services, APIs, and databases. Tool calling can also be used to extract structured information from unstructured data and perform various other tasks.
- [Structured output](/docs/concepts/structured_outputs): A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.
- [Multimodality](/docs/concepts/multimodality): The ability to work with data other than text; for example, images, audio, and video.

## Features

LangChain provides a consistent interface for working with chat models from different providers while offering additional features for monitoring, debugging, and optimizing the performance of applications that use LLMs.

- Integrations with many chat model providers (e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq). Please see [chat model integrations](/docs/integrations/chat/) for an up-to-date list of supported models.
- Use either LangChain's [messages](/docs/concepts/messages) format or OpenAI format.
- Standard [tool calling API](/docs/concepts/tool_calling): standard interface for binding tools to models, accessing tool call requests made by models, and sending tool results back to the model.
- Standard API for [structuring outputs](/docs/concepts/structured_outputs) via the `withStructuredOutput` method.
- Integration with [LangSmith](https://docs.smith.langchain.com) for monitoring and debugging production-grade applications based on LLMs.
- Additional features like standardized [token usage](/docs/concepts/messages#token_usage), [rate limiting](#rate-limiting), [caching](#cache) and more.

## Integrations

LangChain has many chat model integrations that allow you to use a wide variety of models from different providers.

These integrations are one of two types:

1. **Official models**: These are models that are officially supported by LangChain and/or model provider. You can find these models in the `@langchain/<provider>` packages.
2. **Community models**: There are models that are mostly contributed and supported by the community. You can find these models in the `@langchain/community` package.

LangChain chat models are named with a convention that prefixes "Chat" to their class names (e.g., `ChatOllama`, `ChatAnthropic`, `ChatOpenAI`, etc.).

Please review the [chat model integrations](/docs/integrations/chat/) for a list of supported models.

:::note
Models that do **not** include the prefix "Chat" in their name or include "LLM" as a suffix in their name typically refer to older models that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output.
:::

## Interface

LangChain chat models implement the [BaseChatModel](https://api.js.langchain.com/classes/_langchain_core.language_models_chat_models.BaseChatModel.html) interface. Because BaseChatModel also implements the [Runnable Interface](/docs/concepts/runnables), chat models support a [standard streaming interface](/docs/concepts/streaming), optimized [batching](/docs/concepts/runnables#batch), and more. Please see the [Runnable Interface](/docs/concepts/runnables) for more details.

Many of the key methods of chat models operate on [messages](/docs/concepts/messages) as input and return messages as output.

Chat models offer a standard set of parameters that can be used to configure the model. These parameters are typically used to control the behavior of the model, such as the temperature of the output, the maximum number of tokens in the response, and the maximum time to wait for a response. Please see the [standard parameters](#standard-parameters) section for more details.

:::note
In documentation, we will often use the terms "LLM" and "Chat Model" interchangeably. This is because most modern LLMs are exposed to users via a chat model interface.

However, LangChain also has implementations of older LLMs that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output. These models are typically named without the "Chat" prefix (e.g., `Ollama`, `Anthropic`, `OpenAI`, etc.).
These models implement the [BaseLLM](https://api.js.langchain.com/classes/_langchain_core.language_models_llms.BaseLLM.html) interface and may be named with the "LLM" suffix (e.g., `OpenAILLM`, etc.). Generally, users should not use these models.
:::

### Key methods

The key methods of a chat model are:

1. **invoke**: The primary method for interacting with a chat model. It takes a list of [messages](/docs/concepts/messages) as input and returns a list of messages as output.
2. **stream**: A method that allows you to stream the output of a chat model as it is generated.
3. **batch**: A method that allows you to batch multiple requests to a chat model together for more efficient processing.
4. **bindTools**: A method that allows you to bind a tool to a chat model for use in the model's execution context.
5. **withStructuredOutput**: A wrapper around the `invoke` method for models that natively support [structured output](/docs/concepts/structured_outputs).

Other important methods can be found in the [BaseChatModel API Reference](https://api.js.langchain.com/classes/_langchain_core.language_models_chat_models.BaseChatModel.html).

### Inputs and outputs

Modern LLMs are typically accessed through a chat model interface that takes [messages](/docs/concepts/messages) as input and returns [messages](/docs/concepts/messages) as output. Messages are typically associated with a role (e.g., "system", "human", "assistant") and one or more content blocks that contain text or potentially multimodal data (e.g., images, audio, video).

LangChain supports two message formats to interact with chat models:

1. **LangChain Message Format**: LangChain's own message format, which is used by default and is used internally by LangChain.
2. **OpenAI's Message Format**: OpenAI's message format.

### Standard parameters

Many chat models have standardized parameters that can be used to configure the model:

| Parameter     | Description                                                                                                                                                                         |
| ------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `model`       | The name or identifier of the specific AI model you want to use (e.g., `"gpt-3.5-turbo"` or `"gpt-4"`).                                                                             |
| `temperature` | Controls the randomness of the model's output. A higher value (e.g., 1.0) makes responses more creative, while a lower value (e.g., 0.1) makes them more deterministic and focused. |
| `timeout`     | The maximum time (in seconds) to wait for a response from the model before canceling the request. Ensures the request doesn’t hang indefinitely.                                    |
| `maxTokens`   | Limits the total number of tokens (words and punctuation) in the response. This controls how long the output can be.                                                                |
| `stop`        | Specifies stop sequences that indicate when the model should stop generating tokens. For example, you might use specific strings to signal the end of a response.                   |
| `maxRetries`  | The maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.                                             |
| `apiKey`      | The API key required for authenticating with the model provider. This is usually issued when you sign up for access to the model.                                                   |
| `baseUrl`     | The URL of the API endpoint where requests are sent. This is typically provided by the model's provider and is necessary for directing your requests.                               |

Some important things to note:

- Standard parameters only apply to model providers that expose parameters with the intended functionality. For example, some providers do not expose a configuration for maximum output tokens, so max_tokens can't be supported on these.
- Standard params are currently only enforced on integrations that have their own integration packages (e.g. `@langchain/openai`, `@langchain/anthropic`, etc.), they're not enforced on models in `@langchain/community`.

ChatModels also accept other parameters that are specific to that integration. To find all the parameters supported by a ChatModel head to the [API reference](https://api.js.langchain.com/) for that model.

## Tool calling

Chat models can call [tools](/docs/concepts/tools) to perform tasks such as fetching data from a database, making API requests, or running custom code. Please
see the [tool calling](/docs/concepts/tool_calling) guide for more information.

## Structured outputs

Chat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely
useful for information extraction tasks. Please read more about
the technique in the [structured outputs](/docs/concepts/structured_outputs) guide.

## Multimodality

Large Language Models (LLMs) are not limited to processing text. They can also be used to process other types of data, such as images, audio, and video. This is known as [multimodality](/docs/concepts/multimodality).

Currently, only some LLMs support multimodal inputs, and almost none support multimodal outputs. Please consult the specific model documentation for details.

## Context window

A chat model's context window refers to the maximum size of the input sequence the model can process at one time. While the context windows of modern LLMs are quite large, they still present a limitation that developers must keep in mind when working with chat models.

If the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the context window determines how much information the model can "remember" throughout a conversation. Developers often need to manage the input within the context window to maintain a coherent dialogue without exceeding the limit. For more details on handling memory in conversations, refer to the [memory](https://langchain-ai.github.io/langgraphjs/concepts/memory/).

The size of the input is measured in [tokens](/docs/concepts/tokens) which are the unit of processing that the model uses.

## Advanced topics

### Caching

Chat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of requests made to the model provider. In practice, caching chat model responses is a complex problem and should be approached with caution.

The reason is that getting a cache hit is unlikely after the first or second interaction in a conversation if relying on caching the **exact** inputs into the model. For example, how likely do you think that multiple conversations start with the exact same message? What about the exact same three messages?

An alternative approach is to use semantic caching, where you cache responses based on the meaning of the input rather than the exact input itself. This can be effective in some situations, but not in others.

A semantic cache introduces a dependency on another model on the critical path of your application (e.g., the semantic cache may rely on an [embedding model](/docs/concepts/embedding_models) to convert text to a vector representation), and it's not guaranteed to capture the meaning of the input accurately.

However, there might be situations where caching chat model responses is beneficial. For example, if you have a chat model that is used to answer frequently asked questions, caching responses can help reduce the load on the model provider and improve response times.

Please see the [how to cache chat model responses](/docs/how_to/#chat-model-caching) guide for more details.

## Related resources

- How-to guides on using chat models: [how-to guides](/docs/how_to/#chat-models).
- List of supported chat models: [chat model integrations](/docs/integrations/chat/).

### Conceptual guides

- [Messages](/docs/concepts/messages)
- [Tool calling](/docs/concepts/tool_calling)
- [Multimodality](/docs/concepts/multimodality)
- [Structured outputs](/docs/concepts/structured_outputs)
- [Tokens](/docs/concepts/tokens)



================================================
FILE: docs/core_docs/docs/concepts/document_loaders.mdx
================================================
# Document loaders

<span data-heading-keywords="document loader,document loaders"></span>

:::info[Prerequisites]

- [Document loaders API reference](/docs/how_to/#document-loaders)

:::

Document loaders are designed to load document objects. LangChain has hundreds of integrations with various data sources to load data from: Slack, Notion, Google Drive, etc.

## Integrations

You can find available integrations on the [Document loaders integrations page](/docs/integrations/document_loaders/).

## Interface

Documents loaders implement the [BaseLoader interface](https://api.js.langchain.com/classes/_langchain_core.document_loaders_base.BaseDocumentLoader.html).

Each DocumentLoader has its own specific parameters, but they can all be invoked in the same way with the `.load` method or `.lazy_load`.

Here's a simple example:

```typescript
import { CSVLoader } from "@langchain/community/document_loaders/fs/csv";

const loader = new CSVLoader(
  ...  // <-- Integration specific parameters here
);
const data = await loader.load();
```

## Related resources

Please see the following resources for more information:

- [How-to guides for document loaders](/docs/how_to/#document-loaders)
- [Document API reference](https://api.js.langchain.com/classes/_langchain_core.documents.Document.html)
- [Document loaders integrations](/docs/integrations/document_loaders/)



================================================
FILE: docs/core_docs/docs/concepts/embedding_models.mdx
================================================
# Embedding models

<span data-heading-keywords="embedding,embeddings"></span>

:::info[Prerequisites]

- [Documents](https://api.js.langchain.com/classes/_langchain_core.documents.Document.html)

:::

:::info[Note]

This conceptual overview focuses on text-based embedding models.

Embedding models can also be [multimodal](/docs/concepts/multimodality) though such models are not currently supported by LangChain.

:::

Imagine being able to capture the essence of any text - a tweet, document, or book - in a single, compact representation.
This is the power of embedding models, which lie at the heart of many retrieval systems.
Embedding models transform human language into a format that machines can understand and compare with speed and accuracy.
These models take text as input and produce a fixed-length array of numbers, a numerical fingerprint of the text's semantic meaning.
Embeddings allow search system to find relevant documents not just based on keyword matches, but on semantic understanding.

## Key concepts

![Conceptual Overview](/img/embeddings_concept.png)

(1) **Embed text as a vector**: Embeddings transform text into a numerical vector representation.

(2) **Measure similarity**: Embedding vectors can be comparing using simple mathematical operations.

## Embedding

### Historical context

The landscape of embedding models has evolved significantly over the years.
A pivotal moment came in 2018 when Google introduced [BERT (Bidirectional Encoder Representations from Transformers)](https://www.nvidia.com/en-us/glossary/bert/).
BERT applied transformer models to embed text as a simple vector representation, which lead to unprecedented performance across various NLP tasks.
However, BERT wasn't optimized for generating sentence embeddings efficiently.
This limitation spurred the creation of [SBERT (Sentence-BERT)](https://www.sbert.net/examples/training/sts/README.html), which adapted the BERT architecture to generate semantically rich sentence embeddings, easily comparable via similarity metrics like cosine similarity, dramatically reduced the computational overhead for tasks like finding similar sentences.
Today, the embedding model ecosystem is diverse, with numerous providers offering their own implementations.
To navigate this variety, researchers and practitioners often turn to benchmarks like the Massive Text Embedding Benchmark (MTEB) [here](https://huggingface.co/blog/mteb) for objective comparisons.

:::info[Further reading]

- See the [seminal BERT paper](https://arxiv.org/abs/1810.04805).
- See Cameron Wolfe's [excellent review](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2) of embedding models.
- See the [Massive Text Embedding Benchmark (MTEB)](https://huggingface.co/blog/mteb) leaderboard for a comprehensive overview of embedding models.

:::

### Interface

LangChain provides a universal interface for working with them, providing standard methods for common operations.
This common interface simplifies interaction with various embedding providers through two central methods:

- `embedDocuments`: For embedding multiple texts (documents)
- `embedQuery`: For embedding a single text (query)

This distinction is important, as some providers employ different embedding strategies for documents (which are to be searched) versus queries (the search input itself).
To illustrate, here's a practical example using LangChain's `.embedDocuments` method to embed a list of strings:

```typescript
import { OpenAIEmbeddings } from "@langchain/openai";
const embeddingsModel = new OpenAIEmbeddings();
const embeddings = await embeddingsModel.embedDocuments([
  "Hi there!",
  "Oh, hello!",
  "What's your name?",
  "My friends call me World",
  "Hello World!",
]);

console.log(`(${embeddings.length}, ${embeddings[0].length})`);
// (5, 1536)
```

For convenience, you can also use the `embedQuery` method to embed a single text:

```typescript
const queryEmbedding = await embeddingsModel.embedQuery(
  "What is the meaning of life?"
);
```

:::info[Further reading]

- See the full list of [LangChain embedding model integrations](/docs/integrations/text_embedding/).
- See these [how-to guides](/docs/how_to/embed_text) for working with embedding models.

:::

### Integrations

LangChain offers many embedding model integrations which you can find [on the embedding models](/docs/integrations/text_embedding/) integrations page.

## Measure similarity

Each embedding is essentially a set of coordinates, often in a high-dimensional space.
In this space, the position of each point (embedding) reflects the meaning of its corresponding text.
Just as similar words might be close to each other in a thesaurus, similar concepts end up close to each other in this embedding space.
This allows for intuitive comparisons between different pieces of text.
By reducing text to these numerical representations, we can use simple mathematical operations to quickly measure how alike two pieces of text are, regardless of their original length or structure.
Some common similarity metrics include:

- **Cosine Similarity**: Measures the cosine of the angle between two vectors.
- **Euclidean Distance**: Measures the straight-line distance between two points.
- **Dot Product**: Measures the projection of one vector onto another.

The choice of similarity metric should be chosen based on the model.
As an example, [OpenAI suggests cosine similarity for their embeddings](https://platform.openai.com/docs/guides/embeddings/which-distance-function-should-i-use), which can be easily implemented:

```typescript
function cosineSimilarity(vec1: number[], vec2: number[]): number {
  const dotProduct = vec1.reduce((sum, val, i) => sum + val * vec2[i], 0);
  const norm1 = Math.sqrt(vec1.reduce((sum, val) => sum + val * val, 0));
  const norm2 = Math.sqrt(vec2.reduce((sum, val) => sum + val * val, 0));
  return dotProduct / (norm1 * norm2);
}

const similarity = cosineSimilarity(queryResult, documentResult);
console.log("Cosine Similarity:", similarity);
```

:::info[Further reading]

- See Simon Willison's [nice blog post and video](https://simonwillison.net/2023/Oct/23/embeddings/) on embeddings and similarity metrics.
- See [this documentation](https://developers.google.com/machine-learning/clustering/dnn-clustering/supervised-similarity) from Google on similarity metrics to consider with embeddings.
- See Pinecone's [blog post](https://www.pinecone.io/learn/vector-similarity/) on similarity metrics.
- See OpenAI's [FAQ](https://platform.openai.com/docs/guides/embeddings/faq) on what similarity metric to use with OpenAI embeddings.

:::



================================================
FILE: docs/core_docs/docs/concepts/evaluation.mdx
================================================
# Evaluation

<span data-heading-keywords="evaluation,evaluate"></span>

Evaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.
It involves testing the model's responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose.
This process is vital for building reliable applications.

![](/img/langsmith_evaluate.png)

[LangSmith](https://docs.smith.langchain.com/) helps with this process in a few ways:

- It makes it easier to create and curate datasets via its tracing and annotation features
- It provides an evaluation framework that helps you define metrics and run your app against your dataset
- It allows you to track results over time and automatically run your evaluators on a schedule or as part of CI/Code

To learn more, check out [this LangSmith guide](https://docs.smith.langchain.com/concepts/evaluation).



================================================
FILE: docs/core_docs/docs/concepts/example_selectors.mdx
================================================
# Example selectors

:::note Prerequisites

- [Chat models](/docs/concepts/chat_models/)
- [Few-shot prompting](/docs/concepts/few_shot_prompting/)

:::

## Overview

One common prompting technique for achieving better performance is to include examples as part of the prompt. This is known as [few-shot prompting](/docs/concepts/few_shot_prompting).

This gives the [language model](/docs/concepts/chat_models/) concrete examples of how it should behave.
Sometimes these examples are hardcoded into the prompt, but for more advanced situations it may be nice to dynamically select them.

**Example Selectors** are classes responsible for selecting and then formatting examples into prompts.

> See the [API reference for more information.](https://v03.api.js.langchain.com/classes/_langchain_core.example_selectors.BaseExampleSelector.html)

## Related resources

- [Example selector how-to guides](/docs/how_to/#example-selectors)



================================================
FILE: docs/core_docs/docs/concepts/few_shot_prompting.mdx
================================================
# Few-shot prompting

:::note Prerequisites

- [Chat models](/docs/concepts/chat_models/)

:::

## Overview

One of the most effective ways to improve model performance is to give a model examples of
what you want it to do. The technique of adding example inputs and expected outputs
to a model prompt is known as "few-shot prompting". The technique is based on the
[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) paper.
There are a few things to think about when doing few-shot prompting:

1. How are examples generated?
2. How many examples are in each prompt?
3. How are examples selected at runtime?
4. How are examples formatted in the prompt?

Here are the considerations for each.

## 1. Generating examples

The first and most important step of few-shot prompting is coming up with a good dataset of examples. Good examples should be relevant at runtime, clear, informative, and provide information that was not already known to the model.

At a high-level, the basic ways to generate examples are:

- Manual: a person/people generates examples they think are useful.
- Better model: a better (presumably more expensive/slower) model's responses are used as examples for a worse (presumably cheaper/faster) model.
- User feedback: users (or labelers) leave feedback on interactions with the application and examples are generated based on that feedback (for example, all interactions with positive feedback could be turned into examples).
- LLM feedback: same as user feedback but the process is automated by having models evaluate themselves.

Which approach is best depends on your task. For tasks where a small number core principles need to be understood really well, it can be valuable hand-craft a few really good examples.
For tasks where the space of correct behaviors is broader and more nuanced, it can be useful to generate many examples in a more automated fashion so that there's a higher likelihood of there being some highly relevant examples for any runtime input.

**Single-turn v.s. multi-turn examples**

Another dimension to think about when generating examples is what the example is actually showing.

The simplest types of examples just have a user input and an expected model output. These are single-turn examples.

One more complex type if example is where the example is an entire conversation, usually in which a model initially responds incorrectly and a user then tells the model how to correct its answer.
This is called a multi-turn example. Multi-turn examples can be useful for more nuanced tasks where its useful to show common errors and spell out exactly why they're wrong and what should be done instead.

## 2. Number of examples

Once we have a dataset of examples, we need to think about how many examples should be in each prompt.
The key tradeoff is that more examples generally improve performance, but larger prompts increase costs and latency.
And beyond some threshold having too many examples can start to confuse the model.
Finding the right number of examples is highly dependent on the model, the task, the quality of the examples, and your cost and latency constraints.
Anecdotally, the better the model is the fewer examples it needs to perform well and the more quickly you hit steeply diminishing returns on adding more examples.
But, the best/only way to reliably answer this question is to run some experiments with different numbers of examples.

## 3. Selecting examples

Assuming we are not adding our entire example dataset into each prompt, we need to have a way of selecting examples from our dataset based on a given input. We can do this:

- Randomly
- By (semantic or keyword-based) similarity of the inputs
- Based on some other constraints, like token size

LangChain has a number of [`ExampleSelectors`](/docs/concepts/example_selectors) which make it easy to use any of these techniques.

Generally, selecting by semantic similarity leads to the best model performance. But how important this is is again model and task specific, and is something worth experimenting with.

## 4. Formatting examples

Most state-of-the-art models these days are chat models, so we'll focus on formatting examples for those. Our basic options are to insert the examples:

- In the system prompt as a string
- As their own messages

If we insert our examples into the system prompt as a string, we'll need to make sure it's clear to the model where each example begins and which parts are the input versus output. Different models respond better to different syntaxes, like [ChatML](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/chat-markup-language), XML, TypeScript, etc.

If we insert our examples as messages, where each example is represented as a sequence of Human, AI messages, we might want to also assign [names](/docs/concepts/messages) to our messages like `"example_user"` and `"example_assistant"` to make it clear that these messages correspond to different actors than the latest input message.

**Formatting tool call examples**

One area where formatting examples as messages can be tricky is when our example outputs have tool calls. This is because different models have different constraints on what types of message sequences are allowed when any tool calls are generated.

- Some models require that any `AIMessage` with tool calls be immediately followed by `ToolMessage`s for every tool call,
- Some models additionally require that any `ToolMessage`s be immediately followed by an `AIMessage` before the next `HumanMessage`,
- Some models require that tools are passed in to the model if there are any tool calls / `ToolMessage`s in the chat history.

These requirements are model-specific and should be checked for the model you are using. If your model requires `ToolMessage`s after tool calls and/or `AIMessage`s after `ToolMessage`s and your examples only include expected tool calls and not the actual tool outputs, you can try adding dummy `ToolMessage`s / `AIMessage`s to the end of each example with generic contents to satisfy the API constraints.
In these cases it's especially worth experimenting with inserting your examples as strings versus messages, as having dummy messages can adversely affect certain models.

You can see a case study of how Anthropic and OpenAI respond to different few-shot prompting techniques on two different tool calling benchmarks [here](https://blog.langchain.dev/few-shot-prompting-to-improve-tool-calling-performance/).



================================================
FILE: docs/core_docs/docs/concepts/index.mdx
================================================
# Conceptual guide

This guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.

We recommend that you go through at least one of the [Tutorials](/docs/tutorials) before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.

The conceptual guide does not cover step-by-step instructions or specific implementation examples — those are found in the [How-to guides](/docs/how_to/) and [Tutorials](/docs/tutorials). For detailed reference material, please see the [API reference](https://api.js.langchain.com/).

## High level

- **[Why LangChain?](/docs/concepts/why_langchain)**: Overview of the value that LangChain provides.
- **[Architecture](/docs/concepts/architecture)**: How packages are organized in the LangChain ecosystem.

## Concepts

- **[Chat models](/docs/concepts/chat_models)**: LLMs exposed via a chat API that process sequences of messages as input and output a message.
- **[Messages](/docs/concepts/messages)**: The unit of communication in chat models, used to represent model input and output.
- **[Chat history](/docs/concepts/chat_history)**: A conversation represented as a sequence of messages, alternating between user messages and model responses.
- **[Tools](/docs/concepts/tools)**: A function with an associated schema defining the function's name, description, and the arguments it accepts.
- **[Tool calling](/docs/concepts/tool_calling)**: A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message.
- **[Structured output](/docs/concepts/structured_outputs)**: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.
- **[Memory](https://langchain-ai.github.io/langgraphjs/concepts/memory/)**: Information about a conversation that is persisted so that it can be used in future conversations.
- **[Multimodality](/docs/concepts/multimodality)**: The ability to work with data that comes in different forms, such as text, audio, images, and video.
- **[Runnable interface](/docs/concepts/runnables)**: The base abstraction that many LangChain components and the LangChain Expression Language are built on.
- **[Streaming](/docs/concepts/streaming)**: LangChain streaming APIs for surfacing results as they are generated.
- **[LangChain Expression Language (LCEL)](/docs/concepts/lcel)**: A syntax for orchestrating LangChain components. Most useful for simpler applications.
- **[Document loaders](/docs/concepts/document_loaders)**: Load a source as a list of documents.
- **[Retrieval](/docs/concepts/retrieval)**: Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query.
- **[Text splitters](/docs/concepts/text_splitters)**: Split long text into smaller chunks that can be individually indexed to enable granular retrieval.
- **[Embedding models](/docs/concepts/embedding_models)**: Models that represent data such as text or images in a vector space.
- **[Vector stores](/docs/concepts/vectorstores)**: Storage of and efficient search over vectors and associated metadata.
- **[Retriever](/docs/concepts/retrievers)**: A component that returns relevant documents from a knowledge base in response to a query.
- **[Retrieval Augmented Generation (RAG)](/docs/concepts/rag)**: A technique that enhances language models by combining them with external knowledge bases.
- **[Agents](/docs/concepts/agents)**: Use a [language model](/docs/concepts/chat_models) to choose a sequence of actions to take. Agents can interact with external resources via [tool](/docs/concepts/tools).
- **[Prompt templates](/docs/concepts/prompt_templates)**: Component for factoring out the static parts of a model "prompt" (usually a sequence of messages). Useful for serializing, versioning, and reusing these static parts.
- **[Output parsers](/docs/concepts/output_parsers)**: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily useful prior to the general availability of [tool calling](/docs/concepts/tool_calling) and [structured outputs](/docs/concepts/structured_outputs).
- **[Few-shot prompting](/docs/concepts/few_shot_prompting)**: A technique for improving model performance by providing a few examples of the task to perform in the prompt.
- **[Example selectors](/docs/concepts/example_selectors)**: Used to select the most relevant examples from a dataset based on a given input. Example selectors are used in few-shot prompting to select examples for a prompt.
- **[Callbacks](/docs/concepts/callbacks)**: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more.
- **[Tracing](/docs/concepts/tracing)**: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications.
- **[Evaluation](/docs/concepts/evaluation)**: The process of assessing the performance and effectiveness of AI applications. This involves testing the model's responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications.

## Glossary

- **[AIMessageChunk](/docs/concepts/messages#aimessagechunk)**: A partial response from an AI message. Used when streaming responses from a chat model.
- **[AIMessage](/docs/concepts/messages#aimessage)**: Represents a complete response from an AI model.
- **[StructuredTool](https://api.js.langchain.com/classes/_langchain_core.tools.StructuredTool.html)**: The base class for all tools in LangChain.
- **[batch](/docs/concepts/runnables)**: Use to execute a runnable with batch inputs a Runnable.
- **[bindTools](/docs/concepts/tool_calling/#tool-binding)**: Allows models to interact with tools.
- **[Caching](/docs/concepts/chat_models#caching)**: Storing results to avoid redundant calls to a chat model.
- **[Context window](/docs/concepts/chat_models#context-window)**: The maximum size of input a chat model can process.
- **[Conversation patterns](/docs/concepts/chat_history#conversation-patterns)**: Common patterns in chat interactions.
- **[Document](https://api.js.langchain.com/classes/_langchain_core.documents.Document.html)**: LangChain's representation of a document.
- **[Embedding models](/docs/concepts/embedding_models)**: Models that generate vector embeddings for various data types.
- **[HumanMessage](/docs/concepts/messages#humanmessage)**: Represents a message from a human user.
- **[input and output types](/docs/concepts/runnables#input-and-output-types)**: Types used for input and output in Runnables.
- **[Integration packages](/docs/concepts/architecture#integration-packages)**: Third-party packages that integrate with LangChain.
- **[invoke](/docs/concepts/runnables)**: A standard method to invoke a Runnable.
- **[JSON mode](/docs/concepts/structured_outputs#json-mode)**: Returning responses in JSON format.
- **[@langchain/community](/docs/concepts/architecture#langchaincommunity)**: Community-driven components for LangChain.
- **[@langchain/core](/docs/concepts/architecture#langchaincore)**: Core langchain package. Includes base interfaces and in-memory implementations.
- **[langchain](/docs/concepts/architecture#langchain)**: A package for higher level components (e.g., some pre-built chains).
- **[@langchain/langgraph](/docs/concepts/architecture#langchainlanggraph)**: Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows.
- **[Managing chat history](/docs/concepts/chat_history#managing-chat-history)**: Techniques to maintain and manage the chat history.
- **[OpenAI format](/docs/concepts/messages#openai-format)**: OpenAI's message format for chat models.
- **[Propagation of RunnableConfig](/docs/concepts/runnables#propagation-of-runnableconfig)**: Propagating configuration through Runnables.
- **[RemoveMessage](/docs/concepts/messages#removemessage)**: An abstraction used to remove a message from chat history, used primarily in LangGraph.
- **[role](/docs/concepts/messages#role)**: Represents the role (e.g., user, assistant) of a chat message.
- **[RunnableConfig](/docs/concepts/runnables#runnableconfig)**: Use to pass run time information to Runnables (e.g., `runName`, `runId`, `tags`, `metadata`, `maxConcurrency`, `recursionLimit`, `configurable`).
- **[Standard parameters for chat models](/docs/concepts/chat_models#standard-parameters)**: Parameters such as API key, `temperature`, and `maxTokens`,
- **[stream](/docs/concepts/streaming)**: Use to stream output from a Runnable or a graph.
- **[Tokenization](/docs/concepts/tokens#how-tokens-work-in-language-models)**: The process of converting data into tokens and vice versa.
- **[Tokens](/docs/concepts/tokens)**: The basic unit that a language model reads, processes, and generates under the hood.
- **[Tool artifacts](/docs/concepts/tools#tool-artifacts)**: Add artifacts to the output of a tool that will not be sent to the model, but will be available for downstream processing.
- **[Tool binding](/docs/concepts/tool_calling#tool-binding)**: Binding tools to models.
- **[`tool`](/docs/concepts/tools)**: Function for creating tools in LangChain.
- **[Toolkits](/docs/concepts/tools#toolkits)**: A collection of tools that can be used together.
- **[ToolMessage](/docs/concepts/messages#toolmessage)**: Represents a message that contains the results of a tool execution.
- **[Vector stores](/docs/concepts/vectorstores)**: Datastores specialized for storing and efficiently searching vector embeddings.
- **[withStructuredOutput](/docs/concepts/structured_outputs/#structured-output-method)**: A helper method for chat models that natively support [tool calling](/docs/concepts/tool_calling) to get structured output matching a given schema specified via Zod, JSON schema or a function.

import RedirectAnchors from "@theme/RedirectAnchors";

<RedirectAnchors />



================================================
FILE: docs/core_docs/docs/concepts/key_value_stores.mdx
================================================
# Key-value stores

## Overview

LangChain provides a key-value store interface for storing and retrieving data.

LangChain includes a [`BaseStore`](https://api.js.langchain.com/classes/_langchain_core.stores.BaseStore.html) interface,
which allows for storage of arbitrary data. However, LangChain components that require KV-storage accept a
more specific `BaseStore<string, Uint8Array>` instance that stores binary data (referred to as a `ByteStore`), and internally take care of
encoding and decoding data for their specific needs.

This means that as a user, you only need to think about one type of store rather than different ones for different types of data.

## Usage

The key-value store interface in LangChain is used primarily for:

1. Caching [embeddings](/docs/concepts/embedding_models) via [CachedBackedEmbeddings](https://api.js.langchain.com/classes/langchain.embeddings_cache_backed.CacheBackedEmbeddings.html) to avoid recomputing embeddings for repeated queries or when re-indexing content.

2. As a simple [Document](https://api.js.langchain.com/classes/_langchain_core.documents.Document.html) persistence layer in some retrievers.

Please see these how-to guides for more information:

- [How to cache embeddings guide](/docs/how_to/caching_embeddings/).
- [How to retriever using multiple vectors per document](/docs/how_to/custom_retriever/).

## Interface

All [`BaseStore`](https://api.js.langchain.com/classes/_langchain_core.stores.BaseStore.html)s support the following interface. Note that the interface allows for modifying **multiple** key-value pairs at once:

- `mget(keys: string[]): Promise<(Uint8Array | undefined)[]>`: get the contents of multiple keys, returning `undefined` if the key does not exist
- `mset(keyValuePairs: [string, Uint8Array][]): Promise<void>`: set the contents of multiple keys
- `mdelete(keys: string[]): Promise<void>`: delete multiple keys
- `yieldKeys(prefix?: string): AsyncIterator<string>`: yield all keys in the store, optionally filtering by a prefix

## Integrations

Please reference the [stores integration page](/docs/integrations/stores/) for a list of available key-value store integrations.



================================================
FILE: docs/core_docs/docs/concepts/lcel.mdx
================================================
# LangChain Expression Language (LCEL)

:::info Prerequisites

- [Runnable Interface](/docs/concepts/runnables)

:::

The **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https://en.wikipedia.org/wiki/Declarative_programming) approach to building new [Runnables](/docs/concepts/runnables) from existing Runnables.

This means that you describe what you want to happen, rather than how you want it to happen, allowing LangChain to optimize the run-time execution of the chains.

We often refer to a `Runnable` created using LCEL as a "chain". It's important to remember that a "chain" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).

:::note

- The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.
- Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.
- A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://api.js.langchain.com/modules/_langchain_core.runnables.html). Many of these Runnables are useful when composing custom "chains" in LangChain using LCEL.

:::

## Benefits of LCEL

LangChain optimizes the run-time execution of chains built with LCEL in a number of ways:

- **Optimize parallel execution**: Run Runnables in parallel using [RunnableParallel](#RunnableParallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/concepts/runnables#batch). Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.
- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a [chat model](/docs/concepts/chat_models) or [llm](/docs/concepts/text_llms) comes out).

Other benefits include:

- [**Seamless LangSmith tracing**](https://docs.smith.langchain.com)
  As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.
  With LCEL, **all** steps are automatically logged to [LangSmith](https://docs.smith.langchain.com/) for maximum observability and debuggability.
- **Standard API**: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.
- [**Deployable with LangServe**](/docs/concepts/architecture#langserve): Chains built with LCEL can be deployed using for production use.

## Should I use LCEL?

LCEL is an [orchestration solution](<https://en.wikipedia.org/wiki/Orchestration_(computing)>) -- it allows LangChain to handle run-time execution of chains in an optimized way.

While we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of [LangGraph](/docs/concepts/architecture#langgraph).

In LangGraph, users define graphs that specify the flow of the application. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.

Here are some guidelines:

- If you are making a single LLM call, you don't need LCEL; instead call the underlying [chat model](/docs/concepts/chat_models) directly.
- If you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you're taking advantage of the LCEL benefits.
- If you're building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use [LangGraph](/docs/concepts/architecture#langgraph) instead. Remember that you can always use LCEL within individual nodes in LangGraph.

## Composition Primitives

`LCEL` chains are built by composing existing `Runnables` together. The two main composition primitives are [RunnableSequence](https://api.js.langchain.com/classes/_langchain_core.runnables.RunnableSequence.html) and [RunnableParallel](https://api.js.langchain.com/classes/_langchain_core.runnables.RunnableParallel.html).

Many other composition primitives (e.g., [RunnableAssign](https://api.js.langchain.com/classes/_langchain_core.runnables.RunnableAssign.html)) can be thought of as variations of these two primitives.

:::note
You can find a list of all composition primitives in the [LangChain Core API Reference](https://api.js.langchain.com/modules/_langchain_core.runnables.html).
:::

### RunnableSequence

`RunnableSequence` is a composition primitive that allows you "chain" multiple runnables sequentially, with the output of one runnable serving as the input to the next.

```typescript
import { RunnableSequence } from "@langchain/core/runnables";
const chain = new RunnableSequence({
  first: runnable1,
  // Optional, use if you have more than two runnables
  // middle: [...],
  last: runnable2,
});
```

Invoking the `chain` with some input:

```typescript
const finalOutput = await chain.invoke(someInput);
```

corresponds to the following:

```typescript
const output1 = await runnable1.invoke(someInput);
const finalOutput = await runnable2.invoke(output1);
```

:::note
`runnable1` and `runnable2` are placeholders for any `Runnable` that you want to chain together.
:::

### RunnableParallel

`RunnableParallel` is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.

```typescript
import { RunnableParallel } from "@langchain/core/runnables";
const chain = new RunnableParallel({
  key1: runnable1,
  key2: runnable2,
});
```

Invoking the `chain` with some input:

```typescript
const finalOutput = await chain.invoke(someInput);
```

Will yield a `finalOutput` object with the same keys as the input object, but with the values replaced by the output of the corresponding runnable.

```typescript
{
  key1: await runnable1.invoke(someInput),
  key2: await runnable2.invoke(someInput),
}
```

Recall, that the runnables are executed in parallel, so while the result is the same as
object comprehension shown above, the execution time is much faster.

## Composition Syntax

The usage of `RunnableSequence` and `RunnableParallel` is so common that we created a shorthand syntax for using them. This helps
to make the code more readable and concise.

### The `pipe` method.

You can `pipe` runnables together using the `.pipe(runnable)` method.

```typescript
const chain = runnable1.pipe(runnable2);
```

is Equivalent to:

```typescript
const chain = new RunnableSequence({
  first: runnable1,
  last: runnable2,
});
```

#### RunnableLambda functions

You can define generic TypeScript functions are runnables through the `RunnableLambda` class.

```typescript
const someFunc = RunnableLambda.from((input) => {
  return input;
});

const chain = someFunc.pipe(runnable1);
```

## Legacy chains

LCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as `LLMChain` and
`ConversationalRetrievalChain`. Many of these legacy chains hide important details like prompts, and as a wider variety
of viable models emerge, customization has become more and more important.

For guides on how to do specific tasks with LCEL, check out [the relevant how-to guides](/docs/how_to/#langchain-expression-language-lcel).



================================================
FILE: docs/core_docs/docs/concepts/messages.mdx
================================================
# Messages

:::info Prerequisites

- [Chat Models](/docs/concepts/chat_models)

:::

## Overview

Messages are the unit of communication in [chat models](/docs/concepts/chat_models). They are used to represent the input and output of a chat model, as well as any additional context or metadata that may be associated with a conversation.

Each message has a **role** (e.g., "user", "assistant"), **content** (e.g., text, multimodal data), and additional metadata that can vary depending on the chat model provider.

LangChain provides a unified message format that can be used across chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider.

## What inside a message?

A message typically consists of the following pieces of information:

- **Role**: The role of the message (e.g., "user", "assistant").
- **Content**: The content of the message (e.g., text, multimodal data).
- Additional metadata: id, name, [token usage](/docs/concepts/tokens) and other model-specific metadata.

### Role

Roles are used to distinguish between different types of messages in a conversation and help the chat model understand how to respond to a given sequence of messages.

| **Role**              | **Description**                                                                                                                                                                                                 |
| --------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **system**            | Used to tell the chat model how to behave and provide additional context. Not supported by all chat model providers.                                                                                            |
| **user**              | Represents input from a user interacting with the model, usually in the form of text or other interactive input.                                                                                                |
| **assistant**         | Represents a response from the model, which can include text or a request to invoke tools.                                                                                                                      |
| **tool**              | A message used to pass the results of a tool invocation back to the model after external data or processing has been retrieved. Used with chat models that support [tool calling](/docs/concepts/tool_calling). |
| **function** (legacy) | This is a legacy role, corresponding to OpenAI's legacy function-calling API. **tool** role should be used instead.                                                                                             |

### Content

The content of a message text or an array of objects representing [multimodal data](/docs/concepts/multimodality) (e.g., images, audio, video). The exact format of the content can vary between different chat model providers.

Currently, most chat models support text as the primary content type, with some models also supporting multimodal data. However, support for multimodal data is still limited across most chat model providers.

For more information see:

- [HumanMessage](#humanmessage) -- for content in the input from the user.
- [AIMessage](#aimessage) -- for content in the response from the model.
- [Multimodality](/docs/concepts/multimodality) -- for more information on multimodal content.

### Other Message Data

Depending on the chat model provider, messages can include other data such as:

- **ID**: An optional unique identifier for the message.
- **Name**: An optional `name` property which allows differentiate between different entities/speakers with the same role. Not all models support this!
- **Metadata**: Additional information about the message, such as timestamps, token usage, etc.
- **Tool Calls**: A request made by the model to call one or more tools> See [tool calling](/docs/concepts/tool_calling) for more information.

## Conversation Structure

The sequence of messages into a chat model should follow a specific structure to ensure that the chat model can generate a valid response.

For example, a typical conversation structure might look like this:

1. **User Message**: "Hello, how are you?"
2. **Assistant Message**: "I'm doing well, thank you for asking."
3. **User Message**: "Can you tell me a joke?"
4. **Assistant Message**: "Sure! Why did the scarecrow win an award? Because he was outstanding in his field!"

Please read the [chat history](/docs/concepts/chat_history) guide for more information on managing chat history and ensuring that the conversation structure is correct.

## LangChain Messages

LangChain provides a unified message format that can be used across all chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider.

LangChain messages are classes that subclass from a [BaseMessage](https://api.js.langchain.com/classes/_langchain_core.messages.BaseMessage.html).

The five main message types are:

- [SystemMessage](#systemmessage): corresponds to **system** role
- [HumanMessage](#humanmessage): corresponds to **user** role
- [AIMessage](#aimessage): corresponds to **assistant** role
- [AIMessageChunk](#aimessagechunk): corresponds to **assistant** role, used for [streaming](/docs/concepts/streaming) responses
- [ToolMessage](#toolmessage): corresponds to **tool** role

Other important messages include:

- [RemoveMessage](#removemessage) -- does not correspond to any role. This is an abstraction, mostly used in [LangGraph](/docs/concepts/architecture#langgraph) to manage chat history.
- **Legacy** [FunctionMessage](#legacy-functionmessage): corresponds to the **function** role in OpenAI's **legacy** function-calling API.

You can find more information about **messages** in the [API Reference](https://api.js.langchain.com/modules/_langchain_core.messages.html).

### SystemMessage

A `SystemMessage` is used to prime the behavior of the AI model and provide additional context, such as instructing the model to adopt a specific persona or setting the tone of the conversation (e.g., "This is a conversation about cooking").

Different chat providers may support system message in one of the following ways:

- **Through a "system" message role**: In this case, a system message is included as part of the message sequence with the role explicitly set as "system."
- **Through a separate API parameter for system instructions**: Instead of being included as a message, system instructions are passed via a dedicated API parameter.
- **No support for system messages**: Some models do not support system messages at all.

Most major chat model providers support system instructions via either a chat message or a separate API parameter. LangChain will automatically adapt based on the provider’s capabilities. If the provider supports a separate API parameter for system instructions, LangChain will extract the content of a system message and pass it through that parameter.

If no system message is supported by the provider, in most cases LangChain will attempt to incorporate the system message's content into a HumanMessage or raise an exception if that is not possible. However, this behavior is not yet consistently enforced across all implementations, and if using a less popular implementation of a chat model (e.g., an implementation from the `@langchain/community` package) it is recommended to check the specific documentation for that model.

### HumanMessage

The `HumanMessage` corresponds to the **"user"** role. A human message represents input from a user interacting with the model.

#### Text Content

Most chat models expect the user input to be in the form of text.

```typescript
import { HumanMessage } from "@langchain/core/messages";

await model.invoke([new HumanMessage("Hello, how are you?")]);
```

:::tip
When invoking a chat model with a string as input, LangChain will automatically convert the string into a `HumanMessage` object. This is mostly useful for quick testing.

```typescript
await model.invoke("Hello, how are you?");
```

:::

#### Multi-modal Content

Some chat models accept multimodal inputs, such as images, audio, video, or files like PDFs.

Please see the [multimodality](/docs/concepts/multimodality) guide for more information.

### AIMessage

`AIMessage` is used to represent a message with the role **"assistant"**. This is the response from the model, which can include text or a request to invoke tools. It could also include other media types like images, audio, or video -- though this is still uncommon at the moment.

```typescript
import { HumanMessage } from "@langchain/core/messages";

const aiMessage = await model.invoke([new HumanMessage("Tell me a joke")]);
console.log(aiMessage);
```

```text
AIMessage({
  content: "Why did the chicken cross the road?\n\nTo get to the other side!",
  tool_calls: [],
  response_metadata: { ... },
  usage_metadata: { ... },
})
```

An `AIMessage` has the following attributes. The attributes which are **standardized** are the ones that LangChain attempts to standardize across different chat model providers. **raw** fields are specific to the model provider and may vary.

| Attribute            | Standardized/Raw | Description                                                                                                                                                                                      |
| -------------------- | :--------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `content`            | Raw              | Usually a string, but can be a list of content blocks. See [content](#content) for details.                                                                                                      |
| `tool_calls`         | Standardized     | Tool calls associated with the message. See [tool calling](/docs/concepts/tool_calling) for details.                                                                                             |
| `invalid_tool_calls` | Standardized     | Tool calls with parsing errors associated with the message. See [tool calling](/docs/concepts/tool_calling) for details.                                                                         |
| `usage_metadata`     | Standardized     | Usage metadata for a message, such as [token counts](/docs/concepts/tokens). See [Usage Metadata API Reference](https://api.js.langchain.com/types/_langchain_core.messages.UsageMetadata.html). |
| `id`                 | Standardized     | An optional unique identifier for the message, ideally provided by the provider/model that created the message.                                                                                  |
| `response_metadata`  | Raw              | Response metadata, e.g., response headers, logprobs, token counts.                                                                                                                               |

#### content

The **content** property of an `AIMessage` represents the response generated by the chat model.

The content is either:

- **text** -- the norm for virtually all chat models.
- A **array of objects** -- Each object represents a content block and is associated with a `type`.
  - Used by Anthropic for surfacing agent thought process when doing [tool calling](/docs/concepts/tool_calling).
  - Used by OpenAI for audio outputs. Please see [multi-modal content](/docs/concepts/multimodality) for more information.

:::important
The **content** property is **not** standardized across different chat model providers, mostly because there are
still few examples to generalize from.
:::

### AIMessageChunk

It is common to [stream](/docs/concepts/streaming) responses for the chat model as they are being generated, so the user can see the response in real-time instead of waiting for the entire response to be generated before displaying it.

It is returned from the `stream`, and `streamEvents` methods of the chat model.

For example,

```typescript
for await (const chunk of model.stream([
  new HumanMessage("what color is the sky?"),
])) {
  console.log(chunk);
}
```

`AIMessageChunk` follows nearly the same structure as `AIMessage`, but uses a different [ToolCallChunk](https://api.js.langchain.com/types/_langchain_core.messages_tool.ToolCallChunk.html)
to be able to stream tool calling in a standardized manner.

#### Aggregating

`<Type>MessageChunks` have a `concat` method you can use, or you can import it. This is useful when you want to display the final response to the user.

```typescript
const aiMessage = chunk1.concat(chunk2).concat(chunk3).concat(...);
```

or

```typescript
import { concat } from "@langchain/core/utils/stream";
const aiMessage = concat(chunk1, chunk2);
```

### ToolMessage

This represents a message with role "tool", which contains the result of [calling a tool](/docs/concepts/tool_calling). In addition to `role` and `content`, this message has:

- a `tool_call_id` field which conveys the id of the call to the tool that was called to produce this result.
- an `artifact` field which can be used to pass along arbitrary artifacts of the tool execution which are useful to track but which should not be sent to the model.

Please see [tool calling](/docs/concepts/tool_calling) for more information.

### RemoveMessage

This is a special message type that does not correspond to any roles. It is used
for managing chat history in [LangGraph](/docs/concepts/architecture#langgraph).

Please see the following for more information on how to use the `RemoveMessage`:

- [Memory conceptual guide](https://langchain-ai.github.io/langgraphjs/concepts/memory/)
- [How to delete messages](https://langchain-ai.github.io/langgraphjs/how-tos/delete-messages/)

### (Legacy) FunctionMessage

This is a legacy message type, corresponding to OpenAI's legacy function-calling API. `ToolMessage` should be used instead to correspond to the updated tool-calling API.

## OpenAI Format

### Inputs

Chat models also accept OpenAI's format as **inputs** to chat models:

```typescript
await chatModel.invoke([
  {
    role: "user",
    content: "Hello, how are you?",
  },
  {
    role: "assistant",
    content: "I'm doing well, thank you for asking.",
  },
  {
    role: "user",
    content: "Can you tell me a joke?",
  },
]);
```

### Outputs

At the moment, the output of the model will be in terms of LangChain messages, so you will need to convert the output to the OpenAI format if you
need OpenAI format for the output as well.



================================================
FILE: docs/core_docs/docs/concepts/multimodality.mdx
================================================
# Multimodality

## Overview

**Multimodality** refers to the ability to work with data that comes in different forms, such as text, audio, images, and video. Multimodality can appear in various components, allowing models and systems to handle and process a mix of these data types seamlessly.

- **Chat Models**: These could, in theory, accept and generate multimodal inputs and outputs, handling a variety of data types like text, images, audio, and video.
- **Embedding Models**: Embedding Models can represent multimodal content, embedding various forms of data—such as text, images, and audio—into vector spaces.
- **Vector Stores**: Vector stores could search over embeddings that represent multimodal data, enabling retrieval across different types of information.

## Multimodality in chat models

:::info Pre-requisites

- [Chat models](/docs/concepts/chat_models)
- [Messages](/docs/concepts/messages)

:::

Multimodal support is still relatively new and less common, model providers have not yet standardized on the "best" way to define the API. As such, LangChain's multimodal abstractions are lightweight and flexible, designed to accommodate different model providers' APIs and interaction patterns, but are **not** standardized across models.

### How to use multimodal models

- Use the [chat model integration table](/docs/integrations/chat/) to identify which models support multimodality.
- Reference the [relevant how-to guides](/docs/how_to/#multimodal) for specific examples of how to use multimodal models.

### What kind of multimodality is supported?

#### Inputs

Some models can accept multimodal inputs, such as images, audio, video, or files. The types of multimodal inputs supported depend on the model provider. For instance, [Google's Gemini](/docs/integrations/chat/google_generativeai/) supports documents like PDFs as inputs.

Most chat models that support **multimodal inputs** also accept those values in OpenAI's content blocks format. So far this is restricted to image inputs. For models like Gemini which support video and other bytes input, the APIs also support the native, model-specific representations.

The gist of passing multimodal inputs to a chat model is to use content blocks that specify a type and corresponding data. For example, to pass an image to a chat model:

```typescript
import { HumanMessage } from "@langchain/core/messages";

const message = new HumanMessage({
  content: [
    { type: "text", text: "describe the weather in this image" },
    { type: "image_url", image_url: { url: image_url } },
  ],
});
const response = await model.invoke([message]);
```

:::caution
The exact format of the content blocks may vary depending on the model provider. Please refer to the chat model's
integration documentation for the correct format. Find the integration in the [chat model integration table](/docs/integrations/chat/).
:::

#### Outputs

Virtually no popular chat models support multimodal outputs at the time of writing (October 2024).

The only exception is OpenAI's chat model ([gpt-4o-audio-preview](/docs/integrations/chat/openai/)), which can generate audio outputs.

Multimodal outputs will appear as part of the [AIMessage](/docs/concepts/messages/#aimessage) response object.

Please see the [ChatOpenAI](/docs/integrations/chat/openai/) for more information on how to use multimodal outputs.

#### Tools

Currently, no chat model is designed to work **directly** with multimodal data in a [tool call request](/docs/concepts/tool_calling) or [ToolMessage](/docs/concepts/tool_calling) result.

However, a chat model can easily interact with multimodal data by invoking tools with references (e.g., a URL) to the multimodal data, rather than the data itself. For example, any model capable of [tool calling](/docs/concepts/tool_calling) can be equipped with tools to download and process images, audio, or video.

## Multimodality in embedding models

:::info Prerequisites

- [Embedding Models](/docs/concepts/embedding_models)

:::

**Embeddings** are vector representations of data used for tasks like similarity search and retrieval.

The current [embedding interface](https://api.js.langchain.com/classes/_langchain_core.embeddings.Embeddings.html) used in LangChain is optimized entirely for text-based data, and will **not** work with multimodal data.

As use cases involving multimodal search and retrieval tasks become more common, we expect to expand the embedding interface to accommodate other data types like images, audio, and video.

## Multimodality in vector stores

:::info Prerequisites

- [Vector stores](/docs/concepts/vectorstores)

:::

Vector stores are databases for storing and retrieving embeddings, which are typically used in search and retrieval tasks. Similar to embeddings, vector stores are currently optimized for text-based data.

As use cases involving multimodal search and retrieval tasks become more common, we expect to expand the vector store interface to accommodate other data types like images, audio, and video.



================================================
FILE: docs/core_docs/docs/concepts/output_parsers.mdx
================================================
# Output parsers

<span data-heading-keywords="output parser"></span>

:::note

The information here refers to parsers that take a text output from a model try to parse it into a more structured representation.
More and more models are supporting function (or tool) calling, which handles this automatically.
It is recommended to use function/tool calling rather than output parsing.
See documentation for that [here](/docs/concepts/tool_calling).

:::

Output parsers are responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks.
Useful when you are using LLMs to generate structured data, or to normalize output from chat models and LLMs.

LangChain has lots of different types of output parsers. This is a list of output parsers LangChain supports. The table below has various pieces of information:

- **Name**: The name of the output parser
- **Supports Streaming**: Whether the output parser supports streaming.
- **Has Format Instructions**: Whether the output parser has format instructions. This is generally available except when (a) the desired schema is not specified in the prompt but rather in other parameters (like OpenAI function calling), or (b) when the OutputParser wraps another OutputParser.
- **Calls LLM**: Whether this output parser itself calls an LLM. This is usually only done by output parsers that attempt to correct misformatted output.
- **Input Type**: Expected input type. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need a message with specific kwargs.
- **Output Type**: The output type of the object returned by the parser.
- **Description**: Our commentary on this output parser and when to use it.

| Name                                                                                                          | Supports Streaming | Has Format Instructions | Calls LLM | Input Type            | Output Type              | Description                                                                                                                                                                                                   |
| ------------------------------------------------------------------------------------------------------------- | ------------------ | ----------------------- | --------- | --------------------- | ------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [JSON](https://api.js.langchain.com/classes/_langchain_core.output_parsers.JsonOutputParser.html)             | ✅                 | ✅                      |           | `string` \| `Message` | JSON object              | Returns a JSON object as specified. Probably the most reliable output parser for getting structured data that does NOT use function calling.                                                                  |
| [XML](https://api.js.langchain.com/classes/_langchain_core.output_parsers.XMLOutputParser.html)               | ✅                 | ✅                      |           | `string` \| `Message` | `object`                 | Returns a object of tags. Use when XML output is needed. Use with models that are good at writing XML (like Anthropic's).                                                                                     |
| [CSV](https://api.js.langchain.com/classes/langchain.output_parsers.CommaSeparatedListOutputParser.html)      | ✅                 | ✅                      |           | `string` \| `Message` | `Array<string>`          | Returns a list of comma separated values.                                                                                                                                                                     |
| [OutputFixing](https://api.js.langchain.com/classes/langchain.output_parsers.OutputFixingParser.html)         |                    |                         | ✅        | `string` \| `Message` |                          | Wraps another output parser. If that output parser errors, then this will pass the error message and the bad output to an LLM and ask it to fix the output.                                                   |
| [Datetime](https://api.js.langchain.com/classes/langchain.output_parsers.DatetimeOutputParser.html)           |                    | ✅                      |           | `string` \| `Message` | `Date`                   | Parses response into a datetime string.                                                                                                                                                                       |
| [Structured](https://api.js.langchain.com/classes/_langchain_core.output_parsers.StructuredOutputParser.html) |                    | ✅                      |           | `string` \| `Message` | `Record<string, string>` | An output parser that returns structured information. It is less powerful than other output parsers since it only allows for fields to be strings. This can be useful when you are working with smaller LLMs. |

For specifics on how to use output parsers, see the [relevant how-to guides here](/docs/how_to/#output-parsers).



================================================
FILE: docs/core_docs/docs/concepts/prompt_templates.mdx
================================================
# Prompt Templates

Prompt templates help to translate user input and parameters into instructions for a language model.
This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.

Prompt Templates take as input an object, where each key represents a variable in the prompt template to fill in.

Prompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a string or a list of messages.
The reason this PromptValue exists is to make it easy to switch between strings and messages.

There are a few different types of prompt templates:

## String PromptTemplates

These prompt templates are used to format a single string, and generally are used for simpler inputs.
For example, a common way to construct and use a PromptTemplate is as follows:

```typescript
import { PromptTemplate } from "@langchain/core/prompts";

const promptTemplate = PromptTemplate.fromTemplate(
  "Tell me a joke about {topic}"
);

await promptTemplate.invoke({ topic: "cats" });
```

```text
StringPromptValue {
  value: 'Tell me a joke about cats'
}
```

## ChatPromptTemplates

These prompt templates are used to format a list of messages. These "templates" consist of a list of templates themselves.
For example, a common way to construct and use a ChatPromptTemplate is as follows:

```typescript
import { ChatPromptTemplate } from "@langchain/core/prompts";

const promptTemplate = ChatPromptTemplate.fromMessages([
  ["system", "You are a helpful assistant"],
  ["user", "Tell me a joke about {topic}"],
]);

await promptTemplate.invoke({ topic: "cats" });
```

```text
ChatPromptValue {
  messages: [
    SystemMessage {
      "content": "You are a helpful assistant",
      "additional_kwargs": {},
      "response_metadata": {}
    },
    HumanMessage {
      "content": "Tell me a joke about cats",
      "additional_kwargs": {},
      "response_metadata": {}
    }
  ]
}
```

In the above example, this ChatPromptTemplate will construct two messages when called.
The first is a system message, that has no variables to format.
The second is a HumanMessage, and will be formatted by the `topic` variable the user passes in.

## MessagesPlaceholder

<span data-heading-keywords="messagesplaceholder"></span>

This prompt template is responsible for adding a list of messages in a particular place.
In the above ChatPromptTemplate, we saw how we could format two messages, each one a string.
But what if we wanted the user to pass in a list of messages that we would slot into a particular spot?
This is how you use MessagesPlaceholder.

```typescript
import {
  ChatPromptTemplate,
  MessagesPlaceholder,
} from "@langchain/core/prompts";
import { HumanMessage } from "@langchain/core/messages";

const promptTemplate = ChatPromptTemplate.fromMessages([
  ["system", "You are a helpful assistant"],
  new MessagesPlaceholder("msgs"),
]);

await promptTemplate.invoke({ msgs: [new HumanMessage("hi!")] });
```

```text
ChatPromptValue {
  messages: [
    SystemMessage {
      "content": "You are a helpful assistant",
      "additional_kwargs": {},
      "response_metadata": {}
    },
    HumanMessage {
      "content": "hi!",
      "additional_kwargs": {},
      "response_metadata": {}
    }
  ]
}
```

This will produce a list of two messages, the first one being a system message, and the second one being the HumanMessage we passed in.
If we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5 passed in).
This is useful for letting a list of messages be slotted into a particular spot.

An alternative way to accomplish the same thing without using the `MessagesPlaceholder` class explicitly is:

```typescript
const promptTemplate = ChatPromptTemplate.fromMessages([
  ["system", "You are a helpful assistant"],
  // highlight-next-line
  ["placeholder", "{msgs}"], // <-- This is the changed part
]);
```

For specifics on how to use prompt templates, see the [relevant how-to guides here](/docs/how_to/#prompt-templates).



================================================
FILE: docs/core_docs/docs/concepts/rag.mdx
================================================
# Retrieval augmented generation (rag)

:::info[Prerequisites]

- [Retrieval](/docs/concepts/retrieval/)

:::

## Overview

Retrieval Augmented Generation (RAG) is a powerful technique that enhances [language models](/docs/concepts/chat_models/) by combining them with external knowledge bases.
RAG addresses [a key limitation of models](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise): models rely on fixed training datasets, which can lead to outdated or incomplete information.
When given a query, RAG systems first search a knowledge base for relevant information.
The system then incorporates this retrieved information into the model's prompt.
The model uses the provided context to generate a response to the query.
By bridging the gap between vast language models and dynamic, targeted information retrieval, RAG is a powerful technique for building more capable and reliable AI systems.

## Key concepts

![Conceptual Overview](/img/rag_concepts.png)

(1) **Retrieval system**: Retrieve relevant information from a knowledge base.

(2) **Adding external knowledge**: Pass retrieved information to a model.

## Retrieval system

Model's have internal knowledge that is often fixed, or at least not updated frequently due to the high cost of training.
This limits their ability to answer questions about current events, or to provide specific domain knowledge.
To address this, there are various knowledge injection techniques like [fine-tuning](https://hamel.dev/blog/posts/fine_tuning_valuable.html) or continued pre-training.
Both are [costly](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise) and often [poorly suited](https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts) for factual retrieval.
Using a retrieval system offers several advantages:

- **Up-to-date information**: RAG can access and utilize the latest data, keeping responses current.
- **Domain-specific expertise**: With domain-specific knowledge bases, RAG can provide answers in specific domains.
- **Reduced hallucination**: Grounding responses in retrieved facts helps minimize false or invented information.
- **Cost-effective knowledge integration**: RAG offers a more efficient alternative to expensive model fine-tuning.

:::info[Further reading]

See our conceptual guide on [retrieval](/docs/concepts/retrieval/).

:::

## Adding external knowledge

With a retrieval system in place, we need to pass knowledge from this system to the model.
A RAG pipeline typically achieves this following these steps:

- Receive an input query.
- Use the retrieval system to search for relevant information based on the query.
- Incorporate the retrieved information into the prompt sent to the LLM.
- Generate a response that leverages the retrieved context.

As an example, here's a simple RAG workflow that passes information from a [retriever](/docs/concepts/retrievers/) to a [chat model](/docs/concepts/chat_models/):

```typescript
import { ChatOpenAI } from "@langchain/openai";

// Define a system prompt that tells the model how to use the retrieved context
const systemPrompt = `You are an assistant for question-answering tasks.
Use the following pieces of retrieved context to answer the question.
If you don't know the answer, just say that you don't know.
Use three sentences maximum and keep the answer concise.
Context: {context}:`;

// Define a question
const question =
  "What are the main components of an LLM-powered autonomous agent system?";

// Retrieve relevant documents
const docs = await retriever.invoke(question);

// Combine the documents into a single string
const docsText = docs.map((d) => d.pageContent).join("");

// Populate the system prompt with the retrieved context
const systemPromptFmt = systemPrompt.replace("{context}", docsText);

// Create a model
const model = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0,
});

// Generate a response
const questions = await model.invoke([
  {
    role: "system",
    content: systemPromptFmt,
  },
  {
    role: "user",
    content: question,
  },
]);
```

:::info[Further reading]

RAG a deep area with many possible optimization and design choices:

- See [this excellent blog](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval?utm_source=profile&utm_medium=reader2) from Cameron Wolfe for a comprehensive overview and history of RAG.
- See our [RAG how-to guides](/docs/how_to/#qa-with-rag).
- See our RAG [tutorials](/docs/tutorials/rag).
- See our RAG from Scratch course, with [code](https://github.com/langchain-ai/rag-from-scratch) and [video playlist](https://www.youtube.com/playlist?list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x).
- Also, see our RAG from Scratch course [on Freecodecamp](https://youtu.be/sVcwVQRHIc8?feature=shared).

:::



================================================
FILE: docs/core_docs/docs/concepts/retrieval.mdx
================================================
# Retrieval

:::info[Prerequisites]

- [Retrievers](/docs/concepts/retrievers/)
- [Vector stores](/docs/concepts/vectorstores/)
- [Embeddings](/docs/concepts/embedding_models/)
- [Text splitters](/docs/concepts/text_splitters/)

:::

:::danger[Security]

Some of the concepts reviewed here utilize models to generate queries (e.g., for SQL or graph databases).
There are inherent risks in doing this.
Make sure that your database connection permissions are scoped as narrowly as possible for your application's needs.
This will mitigate, though not eliminate, the risks of building a model-driven system capable of querying databases.
For more on general security best practices, see our [security guide](/docs/security/).

:::

## Overview

Retrieval systems are fundamental to many AI applications, efficiently identifying relevant information from large datasets.
These systems accommodate various data formats:

- Unstructured text (e.g., documents) is often stored in vector stores or lexical search indexes.
- Structured data is typically housed in relational or graph databases with defined schemas.

Despite this diversity in data formats, modern AI applications increasingly aim to make all types of data accessible through natural language interfaces.
Models play a crucial role in this process by translating natural language queries into formats compatible with the underlying search index or database.
This translation enables more intuitive and flexible interactions with complex data structures.

## Key concepts

![Retrieval](/img/retrieval_concept.png)

(1) **Query analysis**: A process where models transform or construct search queries to optimize retrieval.

(2) **Information retrieval**: Search queries are used to fetch information from various retrieval systems.

## Query analysis

While users typically prefer to interact with retrieval systems using natural language, retrieval systems can specific query syntax or benefit from particular keywords.
Query analysis serves as a bridge between raw user input and optimized search queries. Some common applications of query analysis include:

1. **Query Re-writing**: Queries can be re-written or expanded to improve semantic or lexical searches.
2. **Query Construction**: Search indexes may require structured queries (e.g., SQL for databases).

Query analysis employs models to transform or construct optimized search queries from raw user input.

### Query re-writing

Retrieval systems should ideally handle a wide spectrum of user inputs, from simple and poorly worded queries to complex, multi-faceted questions.
To achieve this versatility, a popular approach is to use models to transform raw user queries into more effective search queries.
This transformation can range from simple keyword extraction to sophisticated query expansion and reformulation.
Here are some key benefits of using models for query analysis in unstructured data retrieval:

1. **Query Clarification**: Models can rephrase ambiguous or poorly worded queries for clarity.
2. **Semantic Understanding**: They can capture the intent behind a query, going beyond literal keyword matching.
3. **Query Expansion**: Models can generate related terms or concepts to broaden the search scope.
4. **Complex Query Handling**: They can break down multi-part questions into simpler sub-queries.

Various techniques have been developed to leverage models for query re-writing, including:

| Name                                                                                                      | When to use                                                                     | Description                                                                                                                                                                                                                                                                            |
| --------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Decomposition](https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb) | When a question can be broken down into smaller subproblems.                    | Decompose a question into a set of subproblems / questions, which can either be solved sequentially (use the answer from first + retrieval to answer the second) or in parallel (consolidate each answer into final answer).                                                           |
| [Step-back](https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb)     | When a higher-level conceptual understanding is required.                       | First prompt the LLM to ask a generic step-back question about higher-level concepts or principles, and retrieve relevant facts about them. Use this grounding to help answer the user question. [Paper](https://arxiv.org/pdf/2310.06117).                                            |
| [HyDE](https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb)          | If you have challenges retrieving relevant documents using the raw user inputs. | Use an LLM to convert questions into hypothetical documents that answer the question. Use the embedded hypothetical documents to retrieve real documents with the premise that doc-doc similarity search can produce more relevant matches. [Paper](https://arxiv.org/abs/2212.10496). |

As an example, query decomposition can simply be accomplished using prompting and a structured output that enforces a list of sub-questions.
These can then be run sequentially or in parallel on a downstream retrieval system.

```typescript
import { z } from "zod";
import { ChatOpenAI } from "@langchain/openai";
import { SystemMessage, HumanMessage } from "@langchain/core/messages";

// Define a zod object for the structured output
const Questions = z.object({
  questions: z
    .array(z.string())
    .describe("A list of sub-questions related to the input query."),
});

// Create an instance of the model and enforce the output structure
const model = new ChatOpenAI({ modelName: "gpt-4", temperature: 0 });
const structuredModel = model.withStructuredOutput(Questions);

// Define the system prompt
const system = `You are a helpful assistant that generates multiple sub-questions related to an input question.
The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation.`;

// Pass the question to the model
const question =
  "What are the main components of an LLM-powered autonomous agent system?";
const questions = await structuredModel.invoke([
  new SystemMessage(system),
  new HumanMessage(question),
]);
```

:::tip

See our RAG from Scratch videos for a few different specific approaches:

- [Multi-query](https://youtu.be/JChPi0CRnDY?feature=shared)
- [Decomposition](https://youtu.be/h0OPWlEOank?feature=shared)
- [Step-back](https://youtu.be/xn1jEjRyJ2U?feature=shared)
- [HyDE](https://youtu.be/SaDzIVkYqyY?feature=shared)

:::

### Query construction

Query analysis also can focus on translating natural language queries into specialized query languages or filters.
This translation is crucial for effectively interacting with various types of databases that house structured or semi-structured data.

1. **Structured Data examples**: For relational and graph databases, Domain-Specific Languages (DSLs) are used to query data.

   - **Text-to-SQL**: [Converts natural language to SQL](https://paperswithcode.com/task/text-to-sql) for relational databases.
   - **Text-to-Cypher**: [Converts natural language to Cypher](https://neo4j.com/labs/neodash/2.4/user-guide/extensions/natural-language-queries/) for graph databases.

2. **Semi-structured Data examples**: For vectorstores, queries can combine semantic search with metadata filtering.
   - **Natural Language to Metadata Filters**: Converts user queries into [appropriate metadata filters](https://docs.pinecone.io/guides/data/filter-with-metadata).

These approaches leverage models to bridge the gap between user intent and the specific query requirements of different data storage systems. Here are some popular techniques:

| Name                                     | When to Use                                                                                                                          | Description                                                                                                                                                                                                                                          |
| ---------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Self Query](/docs/how_to/self_query/)   | If users are asking questions that are better answered by fetching documents based on metadata rather than similarity with the text. | This uses an LLM to transform user input into two things: (1) a string to look up semantically, (2) a metadata filter to go along with it. This is useful because oftentimes questions are about the METADATA of documents (not the content itself). |
| [Text to SQL](/docs/tutorials/sql_qa/)   | If users are asking questions that require information housed in a relational database, accessible via SQL.                          | This uses an LLM to transform user input into a SQL query.                                                                                                                                                                                           |
| [Text-to-Cypher](/docs/tutorials/graph/) | If users are asking questions that require information housed in a graph database, accessible via Cypher.                            | This uses an LLM to transform user input into a Cypher query.                                                                                                                                                                                        |

As an example, here is how to use the `SelfQueryRetriever` to convert natural language queries into metadata filters.

```typescript
import { SelfQueryRetriever } from "langchain/retrievers/self_query";
import { AttributeInfo } from "langchain/chains/query_constructor";
import { ChatOpenAI } from "@langchain/openai";

const attributeInfo: AttributeInfo[] = schemaForMetadata;
const documentContents = "Brief summary of a movie";
const llm = new ChatOpenAI({ temperature: 0 });
const retriever = SelfQueryRetriever.fromLLM({
  llm,
  vectorStore,
  documentContents,
  attributeInfo,
});
```

:::info[Further reading]

- See our tutorials on [text-to-SQL](/docs/tutorials/sql_qa/), [text-to-Cypher](/docs/tutorials/graph/), and [query analysis for metadata filters](/docs/tutorials/rag#query-analysis).
- See our [blog post overview](https://blog.langchain.dev/query-construction/).
- See our RAG from Scratch video on [query construction](https://youtu.be/kl6NwWYxvbM?feature=shared).

:::

## Information retrieval

### Common retrieval systems

#### Lexical search indexes

Many search engines are based upon matching words in a query to the words in each document.
This approach is called lexical retrieval, using search [algorithms that are typically based upon word frequencies](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2).
The intution is simple: a word appears frequently both in the user’s query and a particular document, then this document might be a good match.

The particular data structure used to implement this is often an [_inverted index_](https://www.geeksforgeeks.org/inverted-index/).
This types of index contains a list of words and a mapping of each word to a list of locations at which it occurs in various documents.
Using this data structure, it is possible to efficiently match the words in search queries to the documents in which they appear.
[BM25](https://en.wikipedia.org/wiki/Okapi_BM25#:~:text=BM25%20is%20a%20bag%2Dof,slightly%20different%20components%20and%20parameters.) and [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) are [two popular lexical search algorithms](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2).

:::info[Further reading]

- See the [BM25](/docs/integrations/retrievers/bm25/) retriever integration.

:::

#### Vector indexes

Vector indexes are an alternative way to index and store unstructured data.
See our conceptual guide on [vectorstores](/docs/concepts/vectorstores/) for a detailed overview.  
In short, rather than using word frequencies, vectorstores use an [embedding model](/docs/concepts/embedding_models/) to compress documents into high-dimensional vector representation.
This allows for efficient similarity search over embedding vectors using simple mathematical operations like cosine similarity.

:::info[Further reading]

- See our [how-to guide](/docs/how_to/vectorstore_retriever/) for more details on working with vectorstores.
- See our [list of vectorstore integrations](/docs/integrations/vectorstores/).
- See Cameron Wolfe's [blog post](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2) on the basics of vector search.

:::

#### Relational databases

Relational databases are a fundamental type of structured data storage used in many applications.
They organize data into tables with predefined schemas, where each table represents an entity or relationship.
Data is stored in rows (records) and columns (attributes), allowing for efficient querying and manipulation through SQL (Structured Query Language).
Relational databases excel at maintaining data integrity, supporting complex queries, and handling relationships between different data entities.

:::info[Further reading]

- See our [tutorial](/docs/tutorials/sql_qa/) for working with SQL databases.

:::

#### Graph databases

Graph databases are a specialized type of database designed to store and manage highly interconnected data.
Unlike traditional relational databases, graph databases use a flexible structure consisting of nodes (entities), edges (relationships), and properties.
This structure allows for efficient representation and querying of complex, interconnected data.
Graph databases store data in a graph structure, with nodes, edges, and properties.
They are particularly useful for storing and querying complex relationships between data points, such as social networks, supply-chain management, fraud detection, and recommendation services

:::info[Further reading]

- See our [tutorial](/docs/tutorials/graph/) for working with graph databases.
- See Neo4j's [starter kit for LangChain](https://neo4j.com/developer-blog/langchain-neo4j-starter-kit/).

:::

### Retriever

LangChain provides a unified interface for interacting with various retrieval systems through the [retriever](/docs/concepts/retrievers/) concept. The interface is straightforward:

1. Input: A query (string)
2. Output: A list of documents (standardized LangChain [Document](https://api.js.langchain.com/classes/_langchain_core.documents.Document.html) objects)

You can create a retriever using any of the retrieval systems mentioned earlier. The query analysis techniques we discussed are particularly useful here, as they enable natural language interfaces for databases that typically require structured query languages.
For example, you can build a retriever for a SQL database using text-to-SQL conversion. This allows a natural language query (string) to be transformed into a SQL query behind the scenes.
Regardless of the underlying retrieval system, all retrievers in LangChain share a common interface. You can use them with the simple `invoke` method:

```typescript
const docs = await retriever.invoke(query);
```

:::info[Further reading]

- See our [conceptual guide on retrievers](/docs/concepts/retrievers/).
- See our [how-to guide](/docs/how_to/#retrievers) on working with retrievers.

:::



================================================
FILE: docs/core_docs/docs/concepts/retrievers.mdx
================================================
# Retrievers

<span data-heading-keywords="retriever,retrievers"></span>

:::info[Prerequisites]

- [Vector stores](/docs/concepts/vectorstores/)
- [Embeddings](/docs/concepts/embedding_models/)
- [Text splitters](/docs/concepts/text_splitters/)

:::

## Overview

Many different types of retrieval systems exist, including vectorstores, graph databases, and relational databases.
With the rise on popularity of large language models, retrieval systems have become an important component in AI application (e.g., [RAG](/docs/concepts/rag/)).
Because of their importance and variability, LangChain provides a uniform interface for interacting with different types of retrieval systems.
The LangChain [retriever](/docs/concepts/retrievers/) interface is straightforward:

1. Input: A query (string)
2. Output: A list of documents (standardized LangChain [Document](https://api.js.langchain.com/classes/_langchain_core.documents.Document.html) objects)

## Key concept

![Retriever](/img/retriever_concept.png)

All retrievers implement a simple interface for retrieving documents using natural language queries.

## Interface

The only requirement for a retriever is the ability to accepts a query and return documents.
In particular, [LangChain's retriever class](https://api.js.langchain.com/classes/_langchain_core.retrievers.BaseRetriever.html) only requires that the `_getRelevantDocuments` method is implemented, which takes a `query: string` and returns a list of [Document](https://api.js.langchain.com/classes/_langchain_core.documents.Document.html) objects that are most relevant to the query.
The underlying logic used to get relevant documents is specified by the retriever and can be whatever is most useful for the application.

A LangChain retriever is a [runnable](/docs/how_to/lcel_cheatsheet/), which is a standard interface is for LangChain components.
This means that it has a few common methods, including `invoke`, that are used to interact with it. A retriever can be invoked with a query:

```typescript
const docs = await retriever.invoke(query);
```

Retrievers return a list of [Document](https://api.js.langchain.com/classes/_langchain_core.documents.Document.html) objects, which have two attributes:

- `pageContent`: The content of this document. Currently is a string.
- `metadata`: Arbitrary metadata associated with this document (e.g., document id, file name, source, etc).

:::info[Further reading]

- See our [how-to guide](/docs/how_to/custom_retriever/) on building your own custom retriever.

:::

## Common types

Despite the flexibility of the retriever interface, a few common types of retrieval systems are frequently used.

### Search apis

It's important to note that retrievers don't need to actually _store_ documents.
For example, we can be built retrievers on top of search APIs that simply return search results!

### Relational or graph database

Retrievers can be built on top of relational or graph databases.
In these cases, [query analysis](/docs/concepts/retrieval/) techniques to construct a structured query from natural language is critical.
For example, you can build a retriever for a SQL database using text-to-SQL conversion. This allows a natural language query (string) retriever to be transformed into a SQL query behind the scenes.

:::info[Further reading]

- See our [tutorial](/docs/tutorials/sql_qa/) for context on how to build a retreiver using a SQL database and text-to-SQL.
- See our [tutorial](/docs/tutorials/graph/) for context on how to build a retreiver using a graph database and text-to-Cypher.

:::

### Lexical search

As discussed in our conceptual review of [retrieval](/docs/concepts/retrieval/), many search engines are based upon matching words in a query to the words in each document.
[BM25](https://en.wikipedia.org/wiki/Okapi_BM25#:~:text=BM25%20is%20a%20bag%2Dof,slightly%20different%20components%20and%20parameters.) and [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) are [two popular lexical search algorithms](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2).
LangChain has retrievers for many popular lexical search algorithms / engines.

:::info[Further reading]

- See the [BM25](/docs/integrations/retrievers/bm25/) retriever integration.

:::

### Vector store

[Vector stores](/docs/concepts/vectorstores/) are a powerful and efficient way to index and retrieve unstructured data.
An vectorstore can be used as a retriever by calling the `asRetriever()` method.

```typescript
const vectorstore = new MyVectorStore();
const retriever = vectorstore.asRetriever();
```

## Advanced retrieval patterns

### Ensemble

Because the retriever interface is so simple, returning a list of `Document` objects given a search query, it is possible to combine multiple retrievers using ensembling.
This is particularly useful when you have multiple retrievers that are good at finding different types of relevant documents.
It is easy to create an [ensemble retriever](/docs/how_to/ensemble_retriever/) that combines multiple retrievers with linear weighted scores:

```typescript
// Initialize the ensemble retriever
const ensembleRetriever = new EnsembleRetriever({
  retrievers: [bm25Retriever, vectorStoreRetriever],
  weights: [0.5, 0.5],
});
```

When ensembling, how do we combine search results from many retrievers?
This motivates the concept of re-ranking, which takes the output of multiple retrievers and combines them using a more sophisticated algorithm such as [Reciprocal Rank Fusion (RRF)](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf).

### Source document retention

Many retrievers utilize some kind of index to make documents easily searchable.
The process of indexing can include a transformation step (e.g., vectorstores often use document splitting).
Whatever transformation is used, can be very useful to retain a link between the _transformed document_ and the original, giving the retriever the ability to return the _original_ document.

![Retrieval with full docs](/img/retriever_full_docs.png)

This is particularly useful in AI applications, because it ensures no loss in document context for the model.
For example, you may use small chunk size for indexing documents in a vectorstore.
If you return _only_ the chunks as the retrieval result, then the model will have lost the original document context for the chunks.

LangChain has two different retrievers that can be used to address this challenge.
The [Multi-Vector](/docs/how_to/multi_vector/) retriever allows the user to use any document transformation (e.g., use an LLM to write a summary of the document) for indexing while retaining linkage to the source document.
The [ParentDocument](/docs/how_to/parent_document_retriever/) retriever links document chunks from a text-splitter transformation for indexing while retaining linkage to the source document.

| Name                                                      | Index Type                    | Uses an LLM               | When to Use                                                                                                                             | Description                                                                                                                                                                                                              |
| --------------------------------------------------------- | ----------------------------- | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [ParentDocument](/docs/how_to/parent_document_retriever/) | Vector store + Document Store | No                        | If your pages have lots of smaller pieces of distinct information that are best indexed by themselves, but best retrieved all together. | This involves indexing multiple chunks for each document. Then you find the chunks that are most similar in embedding space, but you retrieve the whole parent document and return that (rather than individual chunks). |
| [Multi Vector](/docs/how_to/multi_vector/)                | Vector store + Document Store | Sometimes during indexing | If you are able to extract information from documents that you think is more relevant to index than the text itself.                    | This involves creating multiple vectors for each document. Each vector could be created in a myriad of ways - examples include summaries of the text and hypothetical questions.                                         |

:::info[Further reading]

- See our [how-to guide](/docs/how_to/parent_document_retriever/) on using the ParentDocument retriever.
- See our [how-to guide](/docs/how_to/multi_vector/) on using the MultiVector retriever.
- See our RAG from Scratch video on the [multi vector retriever](https://youtu.be/gTCU9I6QqCE?feature=shared).

:::



================================================
FILE: docs/core_docs/docs/concepts/runnables.mdx
================================================
# Runnable interface

The Runnable interface is foundational for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [output parsers](/docs/concepts/output_parsers), [retrievers](/docs/concepts/retrievers), [compiled LangGraph graphs](https://langchain-ai.github.io/langgraphjs/concepts/low_level/#compiling-your-graph) and more.

This guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.

:::info Related Resources

- The ["Runnable" Interface API Reference](https://api.js.langchain.com/classes/_langchain_core.runnables.Runnable.html) provides a detailed overview of the Runnable interface and its methods.
- A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://api.js.langchain.com/modules/_langchain_core.runnables.html). Many of these Runnables are useful when composing custom "chains" in LangChain using the [LangChain Expression Language (LCEL)](/docs/concepts/lcel).

:::

## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

- [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input is transformed into an output.
- [Batched](/docs/how_to/lcel_cheatsheet/#batch-a-runnable/): Multiple inputs are efficiently transformed into outputs.
- [Streamed](/docs/how_to/lcel_cheatsheet/#stream-a-runnable): Outputs are streamed as they are produced.
- Inspected: Schematic information about Runnable's input, output, and configuration can be accessed.
- Composed: Multiple Runnables can be composed to work together using [the LangChain Expression Language (LCEL)](/docs/concepts/lcel) to create complex pipelines.

Please review the [LCEL Cheatsheet](/docs/how_to/lcel_cheatsheet) for some common patterns that involve the Runnable interface and LCEL expressions.

### Optimized parallel execution (batch)

LangChain Runnables offer a built-in `batch` API that allow you to process multiple inputs in parallel.

Using this method can significantly improve performance when needing to process multiple independent inputs, as the
processing can be done in parallel instead of sequentially.

The batching method is:

- `batch`: Process multiple inputs in parallel, returning results in the same order as the inputs.

The default implementation of `batch` executed the `invoke` method in parallel.

Some Runnables may provide their own implementations of `batch` that are optimized for their specific use case (e.g.,
rely on a `batch` API provided by a model provider).

:::tip
When processing a large number of inputs using `batch`, users may want to control the maximum number of parallel calls. This can be done by setting the `maxConcurrency` attribute in the `RunnableConfig` object. See the [RunnableConfig](/docs/concepts/runnables#RunnableConfig) for more information.
:::

## Streaming apis

<span data-heading-keywords="streaming-api"></span>

Streaming is critical in making applications based on LLMs feel responsive to end-users.

Runnables expose the following three streaming APIs:

1. [`stream`](https://api.js.langchain.com/classes/_langchain_core.runnables.Runnable.html#stream): yields the output a Runnable as it is generated.
2. [`streamEvents`](https://v03.api.js.langchain.com/classes/_langchain_core.runnables.Runnable.html#streamEvents): a more advanced streaming API that allows streaming intermediate steps and final output
3. **legacy** `streamLog`: a legacy streaming API that streams intermediate steps and final output

Please refer to the [Streaming Conceptual Guide](/docs/concepts/streaming) for more details on how to stream in LangChain.

## Input and output types

Every `Runnable` is characterized by an input and output type. These input and output types can be any TypeScript object, and are defined by the Runnable itself.

Runnable methods that result in the execution of the Runnable (e.g., `invoke`, `batch`, `stream`, `streamEvents`) work with these input and output types.

- `invoke`: Accepts an input and returns an output.
- `batch`: Accepts a list of inputs and returns a list of outputs.
- `stream`: Accepts an input and returns a generator that yields outputs.

The **input type** and **output type** vary by component:

| Component    | Input Type                                           | Output Type           |
| ------------ | ---------------------------------------------------- | --------------------- |
| Prompt       | `object`                                             | `PromptValue`         |
| ChatModel    | a `string`, list of chat messages or a `PromptValue` | `ChatMessage`         |
| LLM          | a `string`, list of chat messages or a `PromptValue` | `string`              |
| OutputParser | the output of an LLM or ChatModel                    | Depends on the parser |
| Retriever    | a `string`                                           | List of `Document`s   |
| Tool         | a `string` or `object`, depending on the tool        | Depends on the tool   |

Please refer to the individual component documentation for more information on the input and output types and how to use them.

## RunnableConfig

Any of the methods that are used to execute the runnable (e.g., `invoke`, `batch`, `stream`, `streamEvents`) accept a second argument called
`RunnableConfig` ([API Reference](https://api.js.langchain.com/interfaces/_langchain_core.runnables.RunnableConfig.html)). This argument is an object that contains configuration for the Runnable that will be used
at run time during the execution of the runnable.

A `RunnableConfig` can have any of the following properties defined:

| Attribute        | Description                                                                                |
| ---------------- | ------------------------------------------------------------------------------------------ |
| `runName`        | Name used for the given Runnable (not inherited).                                          |
| `runId`          | Unique identifier for this call. sub-calls will get their own unique run ids.              |
| `tags`           | Tags for this call and any sub-calls.                                                      |
| `metadata`       | Metadata for this call and any sub-calls.                                                  |
| `callbacks`      | Callbacks for this call and any sub-calls.                                                 |
| `maxConcurrency` | Maximum number of parallel calls to make (e.g., used by batch).                            |
| `recursionLimit` | Maximum number of times a call can recurse (e.g., used by Runnables that return Runnables) |
| `configurable`   | Runtime values for configurable attributes of the Runnable.                                |

Passing `config` to the `invoke` method is done like so:

```typescript
await someRunnable.invoke(someInput, {
  runName: "myRun",
  tags: ["tag1", "tag2"],
  metadata: { key: "value" },
});
```

### Propagation of RunnableConfig

Many `Runnables` are composed of other Runnables, and it is important that the `RunnableConfig` is propagated to all sub-calls made by the Runnable. This allows providing run time configuration values to the parent Runnable that are inherited by all sub-calls.

If this were not the case, it would be impossible to set and propagate [callbacks](/docs/concepts/callbacks) or other configuration values like `tags` and `metadata` which
are expected to be inherited by all sub-calls.

There are two main patterns by which new `Runnables` are created:

1. Declaratively using [LangChain Expression Language (LCEL)](/docs/concepts/lcel):

   ```typescript
   const chain = prompt.pipe(chatModel).pipe(outputParser);
   ```

2. Using a [custom Runnable](#custom-runnables) (e.g., `RunnableLambda`) or using the `tool` function:

   ```typescript
   const foo = (input) => {
     // Note that .invoke() is used directly here
     // highlight-next-line
     return barRunnable.invoke(input);
   };
   const fooRunnable = RunnableLambda.from(foo);
   ```

LangChain will try to propagate `RunnableConfig` automatically for both of the patterns.

Propagating the `RunnableConfig` manually is done like so:

```typescript
// Note the config argument
// highlight-next-line
const foo = (input, config) => {
  return barRunnable.invoke(input, config);
};
const fooRunnable = RunnableLambda.from(foo);
```

### Setting custom run name, tags, and metadata

The `runName`, `tags`, and `metadata` attributes of the `RunnableConfig` object can be used to set custom values for the run name, tags, and metadata for a given Runnable.

The `runName` is a string that can be used to set a custom name for the run. This name will be used in logs and other places to identify the run. It is not inherited by sub-calls.

The `tags` and `metadata` attributes are arrays and objects, respectively, that can be used to set custom tags and metadata for the run. These values are inherited by sub-calls.

Using these attributes can be useful for tracking and debugging runs, as they will be surfaced in [LangSmith](https://docs.smith.langchain.com/) as trace attributes that you can
filter and search on.

The attributes will also be propagated to [callbacks](/docs/concepts/callbacks), and will appear in streaming APIs like [streamEvents](/docs/concepts/streaming) as part of each event in the stream.

:::note Related

- [How-to trace with LangChain](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain)

:::

### Setting run id

:::note
This is an advanced feature that is unnecessary for most users.
:::

You may need to set a custom `runId` for a given run, in case you want
to reference it later or correlate it with other systems.

The `runId` MUST be a valid UUID string and **unique** for each run. It is used to identify
the parent run, sub-class will get their own unique run ids automatically.

To set a custom `runId`, you can pass it as a key-value pair in the `config` object when invoking the Runnable:

```typescript
import { v4 as uuidv4 } from "uuid";

const runId = uuidv4();

await someRunnable.invoke(someInput, {
  runId,
});

// Do something with the runId
```

### Setting recursion limit

:::note
This is an advanced feature that is unnecessary for most users.
:::

Some Runnables may return other Runnables, which can lead to infinite recursion if not handled properly. To prevent this, you can set a `recursion_limit` in the `RunnableConfig` object. This will limit the number of times a Runnable can recurse.

### Setting max concurrency

If using the `batch` methods, you can set the `maxConcurrency` attribute in the `RunnableConfig` object to control the maximum number of parallel calls to make. This can be useful when you want to limit the number of parallel calls to prevent overloading a server or API.

### Setting configurable

The `configurable` field is used to pass runtime values for configurable attributes of the Runnable.

It is used frequently in [LangGraph](/docs/concepts/architecture#langgraph) with
[LangGraph Persistence](https://langchain-ai.github.io/langgraphjs/concepts/persistence/)
and [memory](https://langchain-ai.github.io/langgraphjs/concepts/memory/).

It is used for a similar purpose in [RunnableWithMessageHistory](https://api.js.langchain.com/classes/_langchain_core.runnables.RunnableWithMessageHistory.html) to specify
a `session_id` to keep track of conversation history.

### Setting callbacks

Use this option to configure [callbacks](/docs/concepts/callbacks) for the runnable at
runtime. The callbacks will be passed to all sub-calls made by the runnable.

```typescript
await someRunnable.invoke(someInput, {
  callbacks: [SomeCallbackHandler(), AnotherCallbackHandler()],
});
```

Please read the [Callbacks Conceptual Guide](/docs/concepts/callbacks) for more information on how to use callbacks in LangChain.

## Creating a runnable from a function

You may need to create a custom Runnable that runs arbitrary logic. This is especially
useful if using [LangChain Expression Language (LCEL)](/docs/concepts/lcel) to compose
multiple Runnables and you need to add custom processing logic in one of the steps.

There are two ways to create a custom Runnable from a function:

- `RunnableLambda`: Use this simple transformations where streaming is not required.
- `RunnableGenerator`: use this for more complex transformations when streaming is needed.

See the [How to run custom functions](/docs/how_to/functions) guide for more information on how to use `RunnableLambda` and `RunnableGenerator`.

:::important
Users should not try to subclass Runnables to create a new custom Runnable. It is
much more complex and error-prone than simply using `RunnableLambda` or `RunnableGenerator`.
:::



================================================
FILE: docs/core_docs/docs/concepts/streaming.mdx
================================================
# Streaming

:::info Prerequisites

- [Runnable Interface](/docs/concepts/runnables)
- [Chat Models](/docs/concepts/chat_models)

:::

**Streaming** is crucial for enhancing the responsiveness of applications built on [LLMs](/docs/concepts/chat_models). By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.

## Overview

Generating full responses from [LLMs](/docs/concepts/chat_models) often incurs a delay of several seconds, which becomes more noticeable in complex applications with multiple model calls. Fortunately, LLMs generate responses iteratively, allowing for intermediate results to be displayed as they are produced. By streaming these intermediate outputs, LangChain enables smoother UX in LLM-powered apps and offers built-in support for streaming at the core of its design.

In this guide, we'll discuss streaming in LLM applications and explore how LangChain's streaming APIs facilitate real-time output from various components in your application.

## What to stream in LLM applications

In applications involving LLMs, several types of data can be streamed to improve user experience by reducing perceived latency and increasing transparency. These include:

### 1. Streaming LLM outputs

The most common and critical data to stream is the output generated by the LLM itself. LLMs often take time to generate full responses, and by streaming the output in real-time, users can see partial results as they are produced. This provides immediate feedback and helps reduce the wait time for users.

### 2. Streaming pipeline or workflow progress

Beyond just streaming LLM output, it’s useful to stream progress through more complex workflows or pipelines, giving users a sense of how the application is progressing overall. This could include:

- **In LangGraph Workflows:**
  With [LangGraph](/docs/concepts/architecture#langgraph), workflows are composed of nodes and edges that represent various steps. Streaming here involves tracking changes to the **graph state** as individual **nodes** request updates. This allows for more granular monitoring of which node in the workflow is currently active, giving real-time updates about the status of the workflow as it progresses through different stages.

- **In LCEL Pipelines:**
  Streaming updates from an [LCEL](/docs/concepts/lcel) pipeline involves capturing progress from individual **sub-runnables**. For example, as different steps or components of the pipeline execute, you can stream which sub-runnable is currently running, providing real-time insight into the overall pipeline's progress.

Streaming pipeline or workflow progress is essential in providing users with a clear picture of where the application is in the execution process.

### 3. Streaming custom data

In some cases, you may need to stream **custom data** that goes beyond the information provided by the pipeline or workflow structure. This custom information is injected within a specific step in the workflow, whether that step is a tool or a LangGraph node. For example, you could stream updates about what a tool is doing in real-time or the progress through a LangGraph node. This granular data, which is emitted directly from within the step, provides more detailed insights into the execution of the workflow and is especially useful in complex processes where more visibility is needed.

## Streaming APIs

LangChain two main APIs for streaming output in real-time. These APIs are supported by any component that implements the [Runnable Interface](/docs/concepts/runnables), including [LLMs](/docs/concepts/chat_models), [compiled LangGraph graphs](https://langchain-ai.github.io/langgraphjs/concepts/low_level/), and any Runnable generated with [LCEL](/docs/concepts/lcel).

1. [`stream`](https://api.js.langchain.com/classes/_langchain_core.runnables.Runnable.html#stream): Use to stream outputs from individual Runnables (e.g., a chat model) as they are generated or stream any workflow created with LangGraph.
2. [`streamEvents`](https://api.js.langchain.com/classes/_langchain_core.runnables.Runnable.html#streamEvents): Use this API to get access to custom events and intermediate outputs from LLM applications built entirely with [LCEL](/docs/concepts/lcel). Note that this API is available, but not needed when working with LangGraph.

:::note
In addition, there is a **legacy** [streamLog](https://api.js.langchain.com/classes/_langchain_core.runnables.Runnable.html#streamLog) API. This API is not recommended for new projects it is more complex and less feature-rich than the other streaming APIs.
:::

### `stream()`

The `stream()` method returns an iterator that yields chunks of output synchronously as they are produced. You can use a `for await` loop to process each chunk in real-time. For example, when using an LLM, this allows the output to be streamed incrementally as it is generated, reducing the wait time for users.

The type of chunk yielded by the `stream()` methods depends on the component being streamed. For example, when streaming from an [LLM](/docs/concepts/chat_models) each component will be an [`AIMessageChunk`](/docs/concepts/messages#aimessagechunk); however, for other components, the chunk may be different.

The `stream()` method returns an iterator that yields these chunks as they are produced. For example,

```typescript
for await (const chunk of await component.stream(someInput)) {
  // IMPORTANT: Keep the processing of each chunk as efficient as possible.
  // While you're processing the current chunk, the upstream component is
  // waiting to produce the next one. For example, if working with LangGraph,
  // graph execution is paused while the current chunk is being processed.
  // In extreme cases, this could even result in timeouts (e.g., when llm outputs are
  // streamed from an API that has a timeout).
  console.log(chunk);
}
```

#### Usage with chat models

When using `stream()` with chat models, the output is streamed as [`AIMessageChunks`](/docs/concepts/messages#aimessagechunk) as it is generated by the LLM. This allows you to present or process the LLM's output incrementally as it's being produced, which is particularly useful in interactive applications or interfaces.

#### Usage with LangGraph

[LangGraph](/docs/concepts/architecture#langgraph) compiled graphs are [Runnables](/docs/concepts/runnables) and support the standard streaming APIs.

When using the _stream_ and methods with LangGraph, you can **one or more** [streaming mode](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph_pregel.Pregel.html#streamMode) which allow you to control the type of output that is streamed. The available streaming modes are:

- **"values"**: Emit all values of the [state](https://langchain-ai.github.io/langgraphjs/concepts/low_level/) for each step.
- **"updates"**: Emit only the node name(s) and updates that were returned by the node(s) after each step.
- **"debug"**: Emit debug events for each step.
- **"messages"**: Emit LLM [messages](/docs/concepts/messages) [token-by-token](/docs/concepts/tokens).

For more information, please see:

- [LangGraph streaming conceptual guide](https://langchain-ai.github.io/langgraphjs/concepts/streaming/) for more information on how to stream when working with LangGraph.
- [LangGraph streaming how-to guides](https://langchain-ai.github.io/langgraphjs/how-tos/#streaming) for specific examples of streaming in LangGraph.

#### Usage with LCEL

If you compose multiple Runnables using [LangChain’s Expression Language (LCEL)](/docs/concepts/lcel), the `stream()` methods will, by convention, stream the output of the last step in the chain. This allows the final processed result to be streamed incrementally. **LCEL** tries to optimize streaming latency in pipelines such that the streaming results from the last step are available as soon as possible.

### `streamEvents`

<span data-heading-keywords="streamEvents,stream_events,stream events"></span>

:::tip
Use the `streamEvents` API to access custom data and intermediate outputs from LLM applications built entirely with [LCEL](/docs/concepts/lcel).

While this API is available for use with [LangGraph](/docs/concepts/architecture#langgraph) as well, it is usually not necessary when working with LangGraph, as the `stream` methods provide comprehensive streaming capabilities for LangGraph graphs.
:::

For chains constructed using **LCEL**, the `.stream()` method only streams the output of the final step from te chain. This might be sufficient for some applications, but as you build more complex chains of several LLM calls together, you may want to use the intermediate values of the chain alongside the final output. For example, you may want to return sources alongside the final generation when building a chat-over-documents app.

There are ways to do this [using callbacks](/docs/concepts/callbacks), or by constructing your chain in such a way that it passes intermediate
values to the end with something like chained [`.assign()`](/docs/how_to/passthrough/) calls, but LangChain also includes an
`.streamEvents()` method that combines the flexibility of callbacks with the ergonomics of `.stream()`. When called, it returns an iterator
which yields [various types of events](/docs/how_to/streaming/#event-reference) that you can filter and process according
to the needs of your project.

Here's one small example that prints just events containing streamed chat model output:

```typescript
import { StringOutputParser } from "@langchain/core/output_parsers";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { ChatAnthropic } from "@langchain/anthropic";

const model = new ChatAnthropic({ model: "claude-3-sonnet-20240229" });

const prompt = ChatPromptTemplate.fromTemplate("tell me a joke about {topic}");
const parser = StringOutputParser();
const chain = prompt.pipe(model).pipe(parser);

for await (const event of await chain.streamEvents(
  { topic: "parrot" },
  { version: "v2" }
)) {
  if (event.event === "on_chat_model_stream") {
    console.log(event);
  }
}
```

You can roughly think of it as an iterator over callback events (though the format differs) - and you can use it on almost all LangChain components!

See [this guide](/docs/how_to/streaming/#using-stream-events) for more detailed information on how to use `.streamEvents()`, including a table listing available events.

## Writing custom data to the stream

To write custom data to the stream, you will need to choose one of the following methods based on the component you are working with:

1. [dispatch_events](https://api.js.langchain.com/functions/_langchain_core.callbacks_dispatch.dispatchCustomEvent.html#) can be used to write custom data that will be surfaced through the **streamEvents** API. See [how to dispatch custom callback events](/docs/how_to/callbacks_custom_events/#stream-events-api) for more information.

## "Auto-Streaming" Chat Models

LangChain simplifies streaming from [chat models](/docs/concepts/chat_models) by automatically enabling streaming mode in certain cases, even when you're not explicitly calling the streaming methods. This is particularly useful when you use the non-streaming `invoke` method but still want to stream the entire application, including intermediate results from the chat model.

### How It Works

When you call the `invoke` method on a chat model, LangChain will automatically switch to streaming mode if it detects that you are trying to stream the overall application.

Under the hood, it'll have `invoke` use the `stream` method to generate its output. The result of the invocation will be the same as far as the code that was using `invoke` is concerned; however, while the chat model is being streamed, LangChain will take care of invoking `on_llm_new_token` events in LangChain's [callback system](/docs/concepts/callbacks). These callback events
allow LangGraph `stream` and `streamEvents` to surface the chat model's output in real-time.

Example:

```typescript
const node = (state) => {
    ...
    // The code below uses the invoke method, but LangChain will
    // automatically switch to streaming mode
    // when it detects that the overall
    // application is being streamed.
    ai_message = model.invoke(state["messages"])
    ...

    for await (const chunk of await compiledGraph.stream(..., { streamMode: "messages" })) {
      // ... do something
    }
}
```

## Related Resources

Please see the following how-to guides for specific examples of streaming in LangChain:

- [LangGraph conceptual guide on streaming](https://langchain-ai.github.io/langgraphjs/concepts/streaming/)
- [LangGraph streaming how-to guides](https://langchain-ai.github.io/langgraphjs/how-tos/#streaming)
- [How to stream runnables](/docs/how_to/streaming/): This how-to guide goes over common streaming patterns with LangChain components (e.g., chat models) and with [LCEL](/docs/concepts/lcel).
- [How to stream chat models](/docs/how_to/chat_streaming/)
- [How to stream tool calls](/docs/how_to/tool_streaming/)

For writing custom data to the stream, please see the following resources:

- If using LCEL, see [how to dispatch custom callback events](/docs/how_to/callbacks_custom_events/#stream-events-api).



================================================
FILE: docs/core_docs/docs/concepts/structured_outputs.mdx
================================================
# Structured outputs

## Overview

For many applications, such as chatbots, models need to respond to users directly in natural language.
However, there are scenarios where we need models to output in a _structured format_.
For example, we might want to store the model output in a database and ensure that the output conforms to the database schema.
This need motivates the concept of structured output, where models can be instructed to respond with a particular output structure.

![Structured output](/img/structured_output.png)

## Key concepts

**(1) Schema definition:** The output structure is represented as a schema, which can be defined in several ways.
**(2) Returning structured output:** The model is given this schema, and is instructed to return output that conforms to it.

## Recommended usage

This pseudo-code illustrates the recommended workflow when using structured output.
LangChain provides a method, [`withStructuredOutput()`](/docs/how_to/structured_output/#the-.withstructuredoutput-method), that automates the process of binding the schema to the [model](/docs/concepts/chat_models/) and parsing the output.
This helper function is available for all model providers that support structured output.

```typescript
// Define schema
const schema = { foo: "bar" };
// Bind schema to model
const modelWithStructure = model.withStructuredOutput(schema);
// Invoke the model to produce structured output that matches the schema
const structuredOutput = await modelWithStructure.invoke(userInput);
```

## Schema definition

The central concept is that the output structure of model responses needs to be represented in some way.
While types of objects you can use depend on the model you're working with, there are common types of objects that are typically allowed or recommended for structured output in TypeScript.

The simplest and most common format for structured output is a Zod schema definition:

```typescript
import { z } from "zod";

const ResponseFormatter = z.object({
  answer: z.string().describe("The answer to the user's question"),
  followup_question: z
    .string()
    .describe("A followup question the user could ask"),
});
```

You can also define a JSONSchema object, which is what Zod schemas are converted to internally before being sent to the model provider:

```json
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://example.com/product.schema.json",
  "title": "ResponseFormatter",
  "type": "object",
  "properties": {
    "answer": {
      "description": "The answer to the user's question",
      "type": "string"
    },
    "followup_question": {
      "description": "A followup question the user could ask",
      "type": "string"
    }
  },
  "required": ["answer", "followup_question"]
}
```

## Returning structured output

With a schema defined, we need a way to instruct the model to use it.
While one approach is to include this schema in the prompt and _ask nicely_ for the model to use it, this is not recommended.
Several more powerful methods that utilizes native features in the model provider's API are available.

### Using tool calling

Many [model providers support](/docs/integrations/chat/) tool calling, a concept discussed in more detail in our [tool calling guide](/docs/concepts/tool_calling/).
In short, tool calling involves binding a tool to a model and, when appropriate, the model can _decide_ to call this tool and ensure its response conforms to the tool's schema.
With this in mind, the central concept is straightforward: _create a tool with our schema and bind it to the model!_
Here is an example using the `ResponseFormatter` schema defined above:

```typescript
import { ChatOpenAI } from "@langchain/openai";

const model = new ChatOpenAI({
  modelName: "gpt-4",
  temperature: 0,
});

// Create a tool with ResponseFormatter as its schema.
const responseFormatterTool = tool(async () => {}, {
  name: "responseFormatter",
  schema: ResponseFormatter,
});

// Bind the created tool to the model
const modelWithTools = model.bindTools([responseFormatterTool]);

// Invoke the model
const aiMsg = await modelWithTools.invoke(
  "What is the powerhouse of the cell?"
);
```

### JSON mode

In addition to tool calling, some model providers support a feature called `JSON mode`.
This supports JSON schema definition as input and enforces the model to produce a conforming JSON output.
You can find a table of model providers that support JSON mode [here](/docs/integrations/chat/).
Here is an example of how to use JSON mode with OpenAI:

```typescript
import { ChatOpenAI } from "@langchain/openai";

const model = new ChatOpenAI({
  model: "gpt-4",
}).bind({
  response_format: { type: "json_object" },
});

const aiMsg = await model.invoke(
  "Return a JSON object with key 'random_nums' and a value of 10 random numbers in [0-99]"
);
console.log(aiMsg.content);
// Output: {
//   "random_nums": [23, 47, 89, 15, 34, 76, 58, 3, 62, 91]
// }
```

One important point to flag: the model _still_ returns a string, which needs to be parsed into a JSON object.
This can, of course, simply use the `json` library or a JSON output parser if you need more advanced functionality.
See this [how-to guide on the JSON output parser](/docs/how_to/output_parser_json) for more details.

```typescript
import json
const jsonObject = JSON.parse(aiMsg.content)
// {'random_ints': [23, 47, 89, 15, 34, 76, 58, 3, 62, 91]}
```

## Structured output method

There are a few challenges when producing structured output with the above methods:

(1) If using tool calling, tool call arguments needs to be parsed from an object back to the original schema.

(2) In addition, the model needs to be instructed to _always_ use the tool when we want to enforce structured output, which is a provider specific setting.

(3) If using JSON mode, the output needs to be parsed into a JSON object.

With these challenges in mind, LangChain provides a helper function (`withStructuredOutput()`) to streamline the process.

![Diagram of with structured output](/img/with_structured_output.png)

This both binds the schema to the model as a tool and parses the output to the specified output schema.

```typescript
// Bind the schema to the model
const modelWithStructure = model.withStructuredOutput(ResponseFormatter);
// Invoke the model
const structuredOutput = await modelWithStructure.invoke(
  "What is the powerhouse of the cell?"
);
// Get back the object
console.log(structuredOutput);
// { answer: "The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.", followup_question: "What is the function of ATP in the cell?" }
```

:::info[Further reading]

For more details on usage, see our [how-to guide](/docs/how_to/structured_output/#the-.withstructuredoutput-method).

:::



================================================
FILE: docs/core_docs/docs/concepts/t.ipynb
================================================
# Jupyter notebook converted to Python script.

import {
  ChatPromptTemplate,
  MessagesPlaceholder,
} from "@langchain/core/prompts";
import { HumanMessage } from "@langchain/core/messages";

const promptTemplate = ChatPromptTemplate.fromMessages([
  ["system", "You are a helpful assistant"],
  new MessagesPlaceholder("msgs"),
]);

await promptTemplate.invoke({ msgs: [new HumanMessage("hi!")] });
# Output:
#   ChatPromptValue {

#     lc_serializable: true,

#     lc_kwargs: {

#       messages: [

#         SystemMessage {

#           "content": "You are a helpful assistant",

#           "additional_kwargs": {},

#           "response_metadata": {}

#         },

#         HumanMessage {

#           "content": "hi!",

#           "additional_kwargs": {},

#           "response_metadata": {}

#         }

#       ]

#     },

#     lc_namespace: [ 'langchain_core', 'prompt_values' ],

#     messages: [

#       SystemMessage {

#         "content": "You are a helpful assistant",

#         "additional_kwargs": {},

#         "response_metadata": {}

#       },

#       HumanMessage {

#         "content": "hi!",

#         "additional_kwargs": {},

#         "response_metadata": {}

#       }

#     ]

#   }




================================================
FILE: docs/core_docs/docs/concepts/text_llms.mdx
================================================
# String-in, string-out llms

:::tip
You are probably looking for the [Chat Model Concept Guide](/docs/concepts/chat_models) page for more information.
:::

LangChain has implementations for older language models that take a string as input and return a string as output. These models are typically named without the "Chat" prefix (e.g., `Ollama`, `Anthropic`, `OpenAI`, etc.), and may include the "LLM" suffix (e.g., `OpenAILLM`, etc.). These models implement the [`BaseLLM`](https://api.js.langchain.com/classes/_langchain_core.language_models_llms.BaseLLM.html) interface.

Users should be using almost exclusively the newer [Chat Models](/docs/concepts/chat_models) as most
model providers have adopted a chat like interface for interacting with language models.



================================================
FILE: docs/core_docs/docs/concepts/text_splitters.mdx
================================================
# Text splitters

<span data-heading-keywords="text splitter,text splitting"></span>

:::info[Prerequisites]

- [Documents](/docs/concepts/retrievers/#interface)
- [Tokenization](/docs/concepts/tokens)

:::

## Overview

Document splitting is often a crucial preprocessing step for many applications.
It involves breaking down large texts into smaller, manageable chunks.
This process offers several benefits, such as ensuring consistent processing of varying document lengths, overcoming input size limitations of models, and improving the quality of text representations used in retrieval systems.
There are several strategies for splitting documents, each with its own advantages.

## Key concepts

![Conceptual Overview](/img/text_splitters.png)

Text splitters split documents into smaller chunks for use in downstream applications.

## Why split documents?

There are several reasons to split documents:

- **Handling non-uniform document lengths**: Real-world document collections often contain texts of varying sizes. Splitting ensures consistent processing across all documents.
- **Overcoming model limitations**: Many embedding models and language models have maximum input size constraints. Splitting allows us to process documents that would otherwise exceed these limits.
- **Improving representation quality**: For longer documents, the quality of embeddings or other representations may degrade as they try to capture too much information. Splitting can lead to more focused and accurate representations of each section.
- **Enhancing retrieval precision**: In information retrieval systems, splitting can improve the granularity of search results, allowing for more precise matching of queries to relevant document sections.
- **Optimizing computational resources**: Working with smaller chunks of text can be more memory-efficient and allow for better parallelization of processing tasks.

Now, the next question is _how_ to split the documents into chunks! There are several strategies, each with its own advantages.

:::info[Further reading]

- See Greg Kamradt's [chunkviz](https://chunkviz.up.railway.app/) to visualize different splitting strategies discussed below.

:::

## Approaches

### Length-based

The most intuitive strategy is to split documents based on their length. This simple yet effective approach ensures that each chunk doesn't exceed a specified size limit.
Key benefits of length-based splitting:

- Straightforward implementation
- Consistent chunk sizes
- Easily adaptable to different model requirements

Types of length-based splitting:

- **Token-based**: Splits text based on the number of tokens, which is useful when working with language models.
- **Character-based**: Splits text based on the number of characters, which can be more consistent across different types of text.

Example implementation using LangChain's `CharacterTextSplitter` with character based splitting:

```typescript
import { CharacterTextSplitter } from "@langchain/textsplitters";
const textSplitter = new CharacterTextSplitter({
  chunkSize: 100,
  chunkOverlap: 0,
});
const texts = await textSplitter.splitText(document);
```

:::info[Further reading]

- See the how-to guide for [token-based](/docs/how_to/split_by_token/) splitting.
- See the how-to guide for [character-based](/docs/how_to/character_text_splitter/) splitting.

:::

### Text-structured based

Text is naturally organized into hierarchical units such as paragraphs, sentences, and words.
We can leverage this inherent structure to inform our splitting strategy, creating split that maintain natural language flow, maintain semantic coherence within split, and adapts to varying levels of text granularity.
LangChain's [`RecursiveCharacterTextSplitter`](/docs/how_to/recursive_text_splitter/) implements this concept:

- The `RecursiveCharacterTextSplitter` attempts to keep larger units (e.g., paragraphs) intact.
- If a unit exceeds the chunk size, it moves to the next level (e.g., sentences).
- This process continues down to the word level if necessary.

Here is example usage:

```typescript
import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";

const textSplitter = new RecursiveCharacterTextSplitter({
  chunkSize: 100,
  chunkOverlap: 0,
});
const texts = await textSplitter.splitText(document);
```

:::info[Further reading]

- See the how-to guide for [recursive text splitting](/docs/how_to/recursive_text_splitter/).

:::

### Document-structured based

Some documents have an inherent structure, such as HTML, Markdown, or JSON files.
In these cases, it's beneficial to split the document based on its structure, as it often naturally groups semantically related text.
Key benefits of structure-based splitting:

- Preserves the logical organization of the document
- Maintains context within each chunk
- Can be more effective for downstream tasks like retrieval or summarization

Examples of structure-based splitting:

- **Markdown**: Split based on headers (e.g., #, ##, ###)
- **HTML**: Split using tags
- **JSON**: Split by object or array elements
- **Code**: Split by functions, classes, or logical blocks

:::info[Further reading]

- See the how-to guide for [Code splitting](/docs/how_to/code_splitter/).

:::

### Semantic meaning based

Unlike the previous methods, semantic-based splitting actually considers the _content_ of the text.
While other approaches use document or text structure as proxies for semantic meaning, this method directly analyzes the text's semantics.
There are several ways to implement this, but conceptually the approach is split text when there are significant changes in text _meaning_.
As an example, we can use a sliding window approach to generate embeddings, and compare the embeddings to find significant differences:

- Start with the first few sentences and generate an embedding.
- Move to the next group of sentences and generate another embedding (e.g., using a sliding window approach).
- Compare the embeddings to find significant differences, which indicate potential "break points" between semantic sections.

This technique helps create chunks that are more semantically coherent, potentially improving the quality of downstream tasks like retrieval or summarization.

:::info[Further reading]

- See Greg Kamradt's [notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb) showcasing semantic splitting.

:::



================================================
FILE: docs/core_docs/docs/concepts/tokens.mdx
================================================
# Tokens

Modern large language models (LLMs) are typically based on a transformer architecture that processes a sequence of units known as tokens. Tokens are the fundamental elements that models use to break down input and generate output. In this section, we'll discuss what tokens are and how they are used by language models.

## What is a token?

A **token** is the basic unit that a language model reads, processes, and generates. These units can vary based on how the model provider defines them, but in general, they could represent:

- A whole word (e.g., "apple"),
- A part of a word (e.g., "app"),
- Or other linguistic components such as punctuation or spaces.

The way the model tokenizes the input depends on its **tokenizer algorithm**, which converts the input into tokens. Similarly, the model’s output comes as a stream of tokens, which is then decoded back into human-readable text.

## How tokens work in language models

The reason language models use tokens is tied to how they understand and predict language. Rather than processing characters or entire sentences directly, language models focus on **tokens**, which represent meaningful linguistic units. Here's how the process works:

1. **Input Tokenization**: When you provide a model with a prompt (e.g., "LangChain is cool!"), the tokenizer algorithm splits the text into tokens. For example, the sentence could be tokenized into parts like `["Lang", "Chain", " is", " cool", "!"]`. Note that token boundaries don’t always align with word boundaries.
   ![](/img/tokenization.png)

2. **Processing**: The transformer architecture behind these models processes tokens sequentially to predict the next token in a sentence. It does this by analyzing the relationships between tokens, capturing context and meaning from the input.
3. **Output Generation**: The model generates new tokens one by one. These output tokens are then decoded back into human-readable text.

Using tokens instead of raw characters allows the model to focus on linguistically meaningful units, which helps it capture grammar, structure, and context more effectively.

## Tokens don’t have to be text

Although tokens are most commonly used to represent text, they don’t have to be limited to textual data. Tokens can also serve as abstract representations of **multi-modal data**, such as:

- **Images**,
- **Audio**,
- **Video**,
- And other types of data.

At the time of writing, virtually no models support **multi-modal output**, and only a few models can handle **multi-modal inputs** (e.g., text combined with images or audio). However, as advancements in AI continue, we expect **multi-modality** to become much more common. This would allow models to process and generate a broader range of media, significantly expanding the scope of what tokens can represent and how models can interact with diverse types of data.

:::note
In principle, **anything that can be represented as a sequence of tokens** could be modeled in a similar way. For example, **DNA sequences**—which are composed of a series of nucleotides (A, T, C, G)—can be tokenized and modeled to capture patterns, make predictions, or generate sequences. This flexibility allows transformer-based models to handle diverse types of sequential data, further broadening their potential applications across various domains, including bioinformatics, signal processing, and other fields that involve structured or unstructured sequences.
:::

Please see the [multimodality](/docs/concepts/multimodality) section for more information on multi-modal inputs and outputs.

## Why not use characters?

Using tokens instead of individual characters makes models both more efficient and better at understanding context and grammar. Tokens represent meaningful units, like whole words or parts of words, allowing models to capture language structure more effectively than by processing raw characters. Token-level processing also reduces the number of units the model has to handle, leading to faster computation.

In contrast, character-level processing would require handling a much larger sequence of input, making it harder for the model to learn relationships and context. Tokens enable models to focus on linguistic meaning, making them more accurate and efficient in generating responses.

## How tokens correspond to text

Please see this post from [OpenAI](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them) for more details on how tokens are counted and how they correspond to text.

According to the OpenAI post, the approximate token counts for English text are as follows:

- 1 token ~= 4 chars in English
- 1 token ~= ¾ words
- 100 tokens ~= 75 words



================================================
FILE: docs/core_docs/docs/concepts/tool_calling.mdx
================================================
# Tool calling

:::info[Prerequisites]

- [Tools](/docs/concepts/tools)
- [Chat Models](/docs/concepts/chat_models)

:::

## Overview

Many AI applications interact directly with humans. In these cases, it is appropriate for models to respond in natural language.
But what about cases where we want a model to also interact _directly_ with systems, such as databases or an API?
These systems often have a particular input schema; for example, APIs frequently have a required payload structure.
This need motivates the concept of _tool calling_. You can use [tool calling](https://platform.openai.com/docs/guides/function-calling/example-use-cases) to request model responses that match a particular schema.

:::info
You will sometimes hear the term `function calling`. We use this term interchangeably with `tool calling`.
:::

![Conceptual overview of tool calling](/img/tool_calling_concept.png)

## Key concepts

**(1) Tool Creation:** Use the [tool](https://api.js.langchain.com/functions/_langchain_core.tools.tool-1.html) function to create a [tool](/docs/concepts/tools). A tool is an association between a function and its schema.
**(2) Tool Binding:** The tool needs to be connected to a model that supports tool calling. This gives the model awareness of the tool and the associated input schema required by the tool.
**(3) Tool Calling:** When appropriate, the model can decide to call a tool and ensure its response conforms to the tool's input schema.
**(4) Tool Execution:** The tool can be executed using the arguments provided by the model.

![Conceptual parts of tool calling](/img/tool_calling_components.png)

## Recommended usage

This pseudo-code illustrates the recommended workflow for using tool calling.
Created tools are passed to `.bindTools()` method as a list.
This model can be called, as usual. If a tool call is made, model's response will contain the tool call arguments.
The tool call arguments can be passed directly to the tool.

```typescript
// Tool creation
const tools = [myTool];
// Tool binding
const modelWithTools = model.bindTools(tools);
// Tool calling
const response = await modelWithTools.invoke(userInput);
```

## Tool creation

The recommended way to create a tool is using the `tool` function.

```typescript
import { tool } from "@langchain/core/tools";

const multiply = tool(
  ({ a, b }: { a: number; b: number }): number => {
    /**
     * Multiply a and b.
     */
    return a * b;
  },
  {
    name: "multiply",
    description: "Multiply two numbers",
    schema: z.object({
      a: z.number(),
      b: z.number(),
    }),
  }
);
```

:::info[Further reading]

- See our conceptual guide on [tools](/docs/concepts/tools/) for more details.
- See our [model integrations](/docs/integrations/chat/) that support tool calling.
- See our [how-to guide](/docs/how_to/tool_calling/) on tool calling.

:::

For tool calling that does not require a function to execute, you can also define just the tool schema:

```typescript
const multiplyTool = {
  name: "multiply",
  description: "Multiply two numbers",
  schema: z.object({
    a: z.number(),
    b: z.number(),
  }),
};
```

## Tool binding

[Many](https://platform.openai.com/docs/guides/function-calling) [model providers](https://platform.openai.com/docs/guides/function-calling) support tool calling.

:::tip
See our [model integration page](/docs/integrations/chat/) for a list of providers that support tool calling.
:::

The central concept to understand is that LangChain provides a standardized interface for connecting tools to models.
The `.bindTools()` method can be used to specify which tools are available for a model to call.

```typescript
const modelWithTools = model.bindTools([toolsList]);
```

As a specific example, let's take a function `multiply` and bind it as a tool to a model that supports tool calling.

```typescript
const multiply = tool(
  ({ a, b }: { a: number; b: number }): number => {
    /**
     * Multiply a and b.
     *
     * @param a - first number
     * @param b - second number
     * @returns The product of a and b
     */
    return a * b;
  },
  {
    name: "multiply",
    description: "Multiply two numbers",
    schema: z.object({
      a: z.number(),
      b: z.number(),
    }),
  }
);

const llmWithTools = toolCallingModel.bindTools([multiply]);
```

## Tool calling

![Diagram of a tool call by a model](/img/tool_call_example.png)

A key principle of tool calling is that the model decides when to use a tool based on the input's relevance. The model doesn't always need to call a tool.
For example, given an unrelated input, the model would not call the tool:

```typescript
const result = await llmWithTools.invoke("Hello world!");
```

The result would be an `AIMessage` containing the model's response in natural language (e.g., "Hello!").
However, if we pass an input _relevant to the tool_, the model should choose to call it:

```typescript
const result = await llmWithTools.invoke("What is 2 multiplied by 3?");
```

As before, the output `result` will be an `AIMessage`.
But, if the tool was called, `result` will have a `tool_calls` attribute.
This attribute includes everything needed to execute the tool, including the tool name and input arguments:

```
result.tool_calls
{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'xxx', 'type': 'tool_call'}
```

For more details on usage, see our [how-to guides](/docs/how_to/#tools)!

## Tool execution

[Tools](/docs/concepts/tools/) implement the [Runnable](/docs/concepts/runnables/) interface, which means that they can be invoked (e.g., `tool.invoke(args)`) directly.

[LangGraph](https://langchain-ai.github.io/langgraphjs/) offers pre-built components (e.g., [`ToolNode`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph_prebuilt.ToolNode.html)) that will often invoke the tool in behalf of the user.

:::info[Further reading]

- See our [how-to guide](/docs/how_to/tool_calling/) on tool calling.
- See the [LangGraph documentation on using ToolNode](https://langchain-ai.github.io/langgraphjs/how-tos/tool-calling/).

:::

## Best practices

When designing [tools](/docs/concepts/tools/) to be used by a model, it is important to keep in mind that:

- Models that have explicit [tool-calling APIs](/docs/concepts/tool_calling) will be better at tool calling than non-fine-tuned models.
- Models will perform better if the tools have well-chosen names and descriptions.
- Simple, narrowly scoped tools are easier for models to use than complex tools.
- Asking the model to select from a large list of tools poses challenges for the model.



================================================
FILE: docs/core_docs/docs/concepts/tools.mdx
================================================
# Tools

:::info Prerequisites

- [Chat models](/docs/concepts/chat_models/)

:::

## Overview

The **tool** abstraction in LangChain associates a TypeScript **function** with a **schema** that defines the function's **name**, **description** and **input**.

**Tools** can be passed to [chat models](/docs/concepts/chat_models) that support [tool calling](/docs/concepts/tool_calling) allowing the model to request the execution of a specific function with specific inputs.

## Key concepts

- Tools are a way to encapsulate a function and its schema in a way that can be passed to a chat model.
- Create tools using the [tool](https://api.js.langchain.com/functions/_langchain_core.tools.tool-1.html) function, which simplifies the process of tool creation, supporting the following:
  - Defining tools that return **artifacts** (e.g. images, etc.)
  - Hiding input arguments from the schema (and hence from the model) using **injected tool arguments**.

## Tool interface

The tool interface is defined in the [`StructuredTool`](https://api.js.langchain.com/classes/_langchain_core.tools.StructuredTool.html) class which is a subclass of the [Runnable Interface](/docs/concepts/runnables).

The key attributes that correspond to the tool's **schema**:

- **name**: The name of the tool.
- **description**: A description of what the tool does.
- **args**: Property that returns the JSON schema for the tool's arguments.

The key methods to execute the function associated with the **tool**:

- **invoke**: Invokes the tool with the given arguments.

## Create tools using the `tool` function

The recommended way to create tools is using the [tool](https://api.js.langchain.com/functions/_langchain_core.tools.tool-1.html) function. This function is designed to simplify the process of tool creation and should be used in most cases.

```typescript
import { tool } from "@langchain/core/tools";
import { z } from "zod";

const multiply = tool(
  ({ a, b }: { a: number; b: number }): number => {
    /**
     * Multiply two numbers.
     */
    return a * b;
  },
  {
    name: "multiply",
    description: "Multiply two numbers",
    schema: z.object({
      a: z.number(),
      b: z.number(),
    }),
  }
);
```

For more details on how to create tools, see the [how to create custom tools](/docs/how_to/custom_tools/) guide.

:::note
LangChain has a few other ways to create tools; e.g., by sub-classing the [`StructuredTool`](https://api.js.langchain.com/classes/_langchain_core.tools.StructuredTool.html) class or by using `StructuredTool`. These methods are shown in the [how to create custom tools guide](/docs/how_to/custom_tools/), but
we generally recommend using the `tool` function for most cases.
:::

## Use the tool directly

Once you have defined a tool, you can use it directly by calling the function. For example, to use the `multiply` tool defined above:

```typescript
await multiply.invoke({ a: 2, b: 3 });
```

### Inspect

You can also inspect the tool's schema and other properties:

```typescript
console.log(multiply.name); // multiply
console.log(multiply.description); // Multiply two numbers.
```

:::note
If you're using pre-built LangChain or LangGraph components like [createReactAgent](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph_prebuilt.createReactAgent.html),you might not need to interact with tools directly. However, understanding how to use them can be valuable for debugging and testing. Additionally, when building custom LangGraph workflows, you may find it necessary to work with tools directly.
:::

## Configuring the schema

The `tool` function offers additional options to configure the schema of the tool (e.g., modify name, description
or parse the function's doc-string to infer the schema).

Please see the [API reference for tool](https://api.js.langchain.com/functions/_langchain_core.tools.tool-1.html) for more details and review the [how to create custom tools](/docs/how_to/custom_tools/) guide for examples.

## Tool artifacts

**Tools** are utilities that can be called by a model, and whose outputs are designed to be fed back to a model. Sometimes, however, there are artifacts of a tool's execution that we want to make accessible to downstream components in our chain or agent, but that we don't want to expose to the model itself. For example if a tool returns a custom object, a dataframe or an image, we may want to pass some metadata about this output to the model without passing the actual output to the model. At the same time, we may want to be able to access this full output elsewhere, for example in downstream tools.

```typescript
const someTool = tool(({ ... }) => {
    // do something
}, {
  // ... tool schema args
  // Set the returnType to "content_and_artifact"
  responseFormat: "content_and_artifact"
});
```

See [how to return artifacts from tools](/docs/how_to/tool_artifacts/) for more details.

### RunnableConfig

You can use the `RunnableConfig` object to pass custom run time values to tools.

If you need to access the [RunnableConfig](/docs/concepts/runnables/#RunnableConfig) object from within a tool. This can be done by using the `RunnableConfig` in the tool's function signature.

```typescript
import { RunnableConfig } from "@langchain/core/runnables";

const someTool = tool(
    async (args: any, config: RunnableConfig): Promise<[string, any]> => {
        /**
         * Tool that does something.
         */
    },
    {
        name: "some_tool",
        description: "Tool that does something",
        schema: z.object({ ... }),
        returnType: "content_and_artifact"
    }
);


await someTool.invoke(..., { configurable: { value: "some_value" } });
```

The `config` will not be part of the tool's schema and will be injected at runtime with appropriate values.

## Best practices

When designing tools to be used by models, keep the following in mind:

- Tools that are well-named, correctly-documented and properly type-hinted are easier for models to use.
- Design simple and narrowly scoped tools, as they are easier for models to use correctly.
- Use chat models that support [tool-calling](/docs/concepts/tool_calling) APIs to take advantage of tools.

## Toolkits

<span data-heading-keywords="toolkit,toolkits"></span>

LangChain has a concept of **toolkits**. This a very thin abstraction that groups tools together that
are designed to be used together for specific tasks.

### Interface

All Toolkits expose a `getTools` method which returns a list of tools. You can therefore do:

```typescript
// Initialize a toolkit
const toolkit = new ExampleTookit(...)

// Get list of tools
const tools = toolkit.getTools()
```

## Related resources

See the following resources for more information:

- [API Reference for `tool`](https://api.js.langchain.com/functions/_langchain_core.tools.tool-1.html)
- [How to create custom tools](/docs/how_to/custom_tools/)
- [How to pass run time values to tools](/docs/how_to/tool_runtime/)
- [All LangChain tool how-to guides](https://docs.langchain.com/docs/how_to/#tools)
- [Additional how-to guides that show usage with LangGraph](https://langchain-ai.github.io/langgraphjs/how-tos/tool-calling/)
- Tool integrations, see the [tool integration docs](https://docs.langchain.com/docs/integrations/tools/).



================================================
FILE: docs/core_docs/docs/concepts/tracing.mdx
================================================
# Tracing

<span data-heading-keywords="trace,tracing"></span>

A trace is essentially a series of steps that your application takes to go from input to output.
Traces contain individual steps called `runs`. These can be individual calls from a model, retriever,
tool, or sub-chains.
Tracing gives you observability inside your chains and agents, and is vital in diagnosing issues.

For a deeper dive, check out [this LangSmith conceptual guide](https://docs.smith.langchain.com/concepts/tracing).



================================================
FILE: docs/core_docs/docs/concepts/vectorstores.mdx
================================================
# Vector stores

<span data-heading-keywords="vector,vectorstore,vectorstores,vector store,vector stores"></span>

:::info[Prerequisites]

- [Embeddings](/docs/concepts/embedding_models/)
- [Text splitters](/docs/concepts/text_splitters/)

:::
:::info[Note]

This conceptual overview focuses on text-based indexing and retrieval for simplicity.
However, embedding models can be [multi-modal](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings)
and vector stores can be used to store and retrieve a variety of data types beyond text.
:::

## Overview

Vector stores are specialized data stores that enable indexing and retrieving information based on vector representations.

These vectors, called [embeddings](/docs/concepts/embedding_models/), capture the semantic meaning of data that has been embedded.

Vector stores are frequently used to search over unstructured data, such as text, images, and audio, to retrieve relevant information based on semantic similarity rather than exact keyword matches.

![Vector stores](/img/vectorstores.png)

## Integrations

LangChain has a large number of vectorstore integrations, allowing users to easily switch between different vectorstore implementations.

Please see the [full list of LangChain vectorstore integrations](/docs/integrations/vectorstores/).

## Interface

LangChain provides a standard interface for working with vector stores, allowing users to easily switch between different vectorstore implementations.

The interface consists of basic methods for writing, deleting and searching for documents in the vector store.

The key methods are:

- `addDocuments`: Add a list of texts to the vector store.
- `deleteDocuments` / `delete`: Delete a list of documents from the vector store.
- `similaritySearch`: Search for similar documents to a given query.

## Initialization

Most vectors in LangChain accept an embedding model as an argument when initializing the vector store.

We will use LangChain's [MemoryVectorStore](https://api.js.langchain.com/classes/langchain.vectorstores_memory.MemoryVectorStore.html) implementation to illustrate the API.

```typescript
import { MemoryVectorStore } from "langchain/vectorstores/memory";
// Initialize with an embedding model
const vectorStore = new MemoryVectorStore(new SomeEmbeddingModel());
```

## Adding documents

To add documents, use the `addDocuments` method.

This API works with a list of [Document](https://api.js.langchain.com/classes/_langchain_core.documents.Document.html) objects.
`Document` objects all have `pageContent` and `metadata` attributes, making them a universal way to store unstructured text and associated metadata.

```typescript
import { Document } from "@langchain/core/documents";

const document1 = new Document(
    pageContent: "I had chocalate chip pancakes and scrambled eggs for breakfast this morning.",
    metadata: { source: "tweet" },
)

const document2 = new Document(
    pageContent: "The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.",
    metadata: { source: "news" },
)

const documents = [document1, document2]

await vectorStore.addDocuments(documents)
```

You should usually provide IDs for the documents you add to the vector store, so
that instead of adding the same document multiple times, you can update the existing document.

```typescript
await vectorStore.addDocuments(documents, { ids: ["doc1", "doc2"] });
```

## Delete

To delete documents, use the `deleteDocuments` method which takes a list of document IDs to delete.

```typescript
await vectorStore.deleteDocuments(["doc1"]);
```

or the `delete` method:

```typescript
await vectorStore.deleteDocuments({ ids: ["doc1"] });
```

## Search

Vector stores embed and store the documents that added.
If we pass in a query, the vectorstore will embed the query, perform a similarity search over the embedded documents, and return the most similar ones.
This captures two important concepts: first, there needs to be a way to measure the similarity between the query and _any_ [embedded](/docs/concepts/embedding_models/) document.
Second, there needs to be an algorithm to efficiently perform this similarity search across _all_ embedded documents.

### Similarity metrics

A critical advantage of embeddings vectors is they can be compared using many simple mathematical operations:

- **Cosine Similarity**: Measures the cosine of the angle between two vectors.
- **Euclidean Distance**: Measures the straight-line distance between two points.
- **Dot Product**: Measures the projection of one vector onto another.

The choice of similarity metric can sometimes be selected when initializing the vectorstore. Please refer
to the documentation of the specific vectorstore you are using to see what similarity metrics are supported.

:::info[Further reading]

- See [this documentation](https://developers.google.com/machine-learning/clustering/dnn-clustering/supervised-similarity) from Google on similarity metrics to consider with embeddings.
- See Pinecone's [blog post](https://www.pinecone.io/learn/vector-similarity/) on similarity metrics.
- See OpenAI's [FAQ](https://platform.openai.com/docs/guides/embeddings/faq) on what similarity metric to use with OpenAI embeddings.

:::

### Similarity search

Given a similarity metric to measure the distance between the embedded query and any embedded document, we need an algorithm to efficiently search over _all_ the embedded documents to find the most similar ones.
There are various ways to do this. As an example, many vectorstores implement [HNSW (Hierarchical Navigable Small World)](https://www.pinecone.io/learn/series/faiss/hnsw/), a graph-based index structure that allows for efficient similarity search.
Regardless of the search algorithm used under the hood, the LangChain vectorstore interface has a `similaritySearch` method for all integrations.
This will take the search query, create an embedding, find similar documents, and return them as a list of [Documents](https://api.js.langchain.com/classes/_langchain_core.documents.Document.html).

```typescript
const query = "my query";
const docs = await vectorstore.similaritySearch(query);
```

Many vectorstores support search parameters to be passed with the `similaritySearch` method. See the documentation for the specific vectorstore you are using to see what parameters are supported.
As an example [Pinecone](https://api.js.langchain.com/classes/_langchain_pinecone.PineconeStore.html#similaritySearch) several parameters that are important general concepts:
Many vectorstores support [the `k`](/docs/integrations/vectorstores/pinecone/#query-directly), which controls the number of Documents to return, and `filter`, which allows for filtering documents by metadata.

- `query (string) – Text to look up documents similar to.`
- `k (number) – Number of Documents to return. Defaults to 4.`
- `filter (Record<string, any> | undefined) – Object of argument(s) to filter on metadata`

:::info[Further reading]

- See the [how-to guide](/docs/how_to/vectorstores/) for more details on how to use the `similaritySearch` method.
- See the [integrations page](/docs/integrations/vectorstores/) for more details on arguments that can be passed in to the `similaritySearch` method for specific vectorstores.

:::

### Metadata filtering

While vectorstore implement a search algorithm to efficiently search over _all_ the embedded documents to find the most similar ones, many also support filtering on metadata.
This allows structured filters to reduce the size of the similarity search space. These two concepts work well together:

1. **Semantic search**: Query the unstructured data directly, often using via embedding or keyword similarity.
2. **Metadata search**: Apply structured query to the metadata, filtering specific documents.

Vector store support for metadata filtering is typically dependent on the underlying vector store implementation.

Here is example usage with [Pinecone](/docs/integrations/vectorstores/pinecone/#query-directly), showing that we filter for all documents that have the metadata key `source` with value `tweet`.

```typescript
await vectorstore.similaritySearch(
  "LangChain provides abstractions to make working with LLMs easy",
  2,
  {
    // The arguments of this field are provider specific.
    filter: { source: "tweet" },
  }
);
```

:::info[Further reading]

- See Pinecone's [documentation](https://docs.pinecone.io/guides/data/filter-with-metadata) on filtering with metadata.
- See the [list of LangChain vectorstore integrations](/docs/integrations/retrievers/self_query/) that support metadata filtering.

:::

## Advanced search and retrieval techniques

While algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.
For example, maximal marginal relevance is a re-ranking algorithm used to diversify search results, which is applied after the initial similarity search to ensure a more diverse set of results.

| Name                                                                                                              | When to use                               | Description                                                                                           |
| ----------------------------------------------------------------------------------------------------------------- | ----------------------------------------- | ----------------------------------------------------------------------------------------------------- |
| [Maximal Marginal Relevance (MMR)](/docs/integrations/vectorstores/pinecone/#maximal-marginal-relevance-searches) | When needing to diversify search results. | MMR attempts to diversify the results of a search to avoid returning similar and redundant documents. |



================================================
FILE: docs/core_docs/docs/concepts/why_langchain.mdx
================================================
# Why LangChain?

The goal of the `langchain` package and LangChain the company is to make it as easy possible for developers to build applications that reason.
While LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem.
This page will talk about the LangChain ecosystem as a whole.
Most of the components within in the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best.

## Features

There are several primary needs that LangChain aims to address:

1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components](/docs/integrations/vectorstores/) for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.
   This diversity can make it challenging for developers to switch between providers or combine components when building applications.
   LangChain exposes a standard interface for key components, making it easy to switch between providers.

2. **Orchestration:** As applications become more complex, combining multiple components and models, there's [a growing need to efficiently connect these elements into control flows](https://lilianweng.github.io/posts/2023-06-23-agent/) that can [accomplish diverse tasks](https://www.sequoiacap.com/article/generative-ais-act-o1/).
   [Orchestration](<https://en.wikipedia.org/wiki/Orchestration_(computing)>) is crucial for building such applications.

3. **Observability and evaluation:** As applications become more complex, it becomes increasingly difficult to understand what is happening within them.
   Furthermore, the pace of development can become rate-limited by the [paradox of choice](https://en.wikipedia.org/wiki/Paradox_of_choice):
   for example, developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost.
   [Observability](https://en.wikipedia.org/wiki/Observability) and evaluations can help developers monitor their applications and rapidly answer these types of questions with confidence.

## Standardized component interfaces

LangChain provides common interfaces for components that are central to many AI applications.
As an example, all [chat models](/docs/concepts/chat_models/) implement the [BaseChatModel](https://api.js.langchain.com/classes/_langchain_core.language_models_chat_models.BaseChatModel.html) interface.
This provides a standard way to interact with chat models, supporting important but often provider-specific features like [tool calling](/docs/concepts/tool_calling/) and [structured outputs](/docs/concepts/structured_outputs/).

### Example: chat models

Many [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical features for many applications (e.g., [agents](https://langchain-ai.github.io/langgraphjs/concepts/agentic_concepts/)), that allows a developer to request model responses that match a particular schema.
The APIs for each provider differ.
LangChain's [chat model](/docs/concepts/chat_models/) interface provides a common way to bind [tools](/docs/concepts/tools) to a model in order to support [tool calling](/docs/concepts/tool_calling/):

```typescript
// Tool creation
const tools = [myTool];
// Tool binding
const modelWithTools = model.bindTools(tools);
```

Similarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/) is an extremely common use case.
Providers support different approaches for this, including [JSON mode or tool calling](https://platform.openai.com/docs/guides/structured-outputs), with different APIs.
LangChain's [chat model](/docs/concepts/chat_models/) interface provides a common way to produce structured outputs using the `withStructuredOutput()` method:

```typescript
// Define tool as a Zod schema
const schema = z.object({ ... });
// Bind schema to model
const modelWithStructure = model.withStructuredOutput(schema)
```

### Example: retrievers

In the context of [RAG](/docs/concepts/rag/) and LLM application components, LangChain's [retriever](/docs/concepts/retrievers/) interface provides a standard way to connect to many different types of data services or databases (e.g., [vector stores](/docs/concepts/vectorstores) or databases).
The underlying implementation of the retriever depends on the type of data store or database you are connecting to, but all retrievers implement the [runnable interface](/docs/concepts/runnables/), meaning they can be invoked in a common manner.

```typescript
const documents = await myRetriever.invoke("What is the meaning of life?");
```

```text
[
   Document({
      pageContent: "The meaning of life is 42.",
      metadata: { ... },
   }),
   Document({
      pageContent: "The meaning of life is to use LangChain.",
      metadata: { ... },
   }),
   ...
]
```

## Orchestration

While standardization for individual components is useful, we've increasingly seen that developers want to _combine_ components into more complex applications.
This motivates the need for [orchestration](<https://en.wikipedia.org/wiki/Orchestration_(computing)>).
There are several common characteristics of LLM applications that this orchestration layer should support:

- **Complex control flow:** The application requires complex patterns such as cycles (e.g., a loop that reiterates until a condition is met).
- **[Persistence](https://langchain-ai.github.io/langgraphjs/concepts/persistence/):** The application needs to maintain [short-term and / or long-term memory](https://langchain-ai.github.io/langgraphjs/concepts/memory/).
- **[Human-in-the-loop](https://langchain-ai.github.io/langgraphjs/concepts/human_in_the_loop/):** The application needs human interaction, e.g., pausing, reviewing, editing, approving certain steps.

The recommended way to do orchestration for these complex applications is [LangGraph](https://langchain-ai.github.io/langgraphjs/concepts/high_level/).
LangGraph is a library that gives developers a high degree of control by expressing the flow of the application as a set of nodes and edges.
LangGraph comes with built-in support for [persistence](https://langchain-ai.github.io/langgraphjs/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraphjs/concepts/human_in_the_loop/), [memory](https://langchain-ai.github.io/langgraphjs/concepts/memory/), and other features.
It's particularly well suited for building [agents](https://langchain-ai.github.io/langgraphjs/concepts/agentic_concepts/) or [multi-agent](https://langchain-ai.github.io/langgraphjs/concepts/multi_agent/) applications.
Importantly, individual LangChain components can be used within LangGraph nodes, but you can also use LangGraph **without** using LangChain components.

:::info[Further reading]

Have a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build complex applications.

:::

## Observability and evaluation

The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice.
Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost.
High quality tracing and evaluations can help you rapidly answer these types of questions with confidence.
[LangSmith](https://docs.smith.langchain.com/) is our platform that supports observability and evaluation for AI applications.
See our conceptual guides on [evaluations](https://docs.smith.langchain.com/concepts/evaluation) and [tracing](https://docs.smith.langchain.com/concepts/tracing) for more details.

:::info[Further reading]

See our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.

:::

## Conclusion

LangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:

- **Ease of swapping providers:** It allows you to swap out different component providers without having to change the underlying code.
- **Advanced features:** It provides common methods for more advanced features, such as [streaming](/docs/concepts/runnables/#streaming) and [tool calling](/docs/concepts/tool_calling/).

[LangGraph](https://langchain-ai.github.io/langgraphjs/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like including [persistence](https://langchain-ai.github.io/langgraphjs/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraphjs/concepts/human_in_the_loop/), or [memory](https://langchain-ai.github.io/langgraphjs/concepts/memory/).

[LangSmith](https://docs.smith.langchain.com/) makes it possible to iterate with confidence on your applications, by providing LLM-specific observability and framework for testing and evaluating your application.



================================================
FILE: docs/core_docs/docs/contributing/code.mdx
================================================
---
sidebar_position: 1
---

# Contribute Code

To contribute to this project, please follow the ["fork and pull request"](https://docs.github.com/en/get-started/quickstart/contributing-to-projects) workflow.
Please do not try to push directly to this repo unless you are a maintainer.

Please follow the checked-in pull request template when opening pull requests. Note related issues and tag relevant
maintainers.

Pull requests cannot land without passing the formatting, linting, and testing checks first. See [Testing](#testing) and
[Formatting and Linting](#formatting-and-linting) for how to run these checks locally.

It's essential that we maintain great documentation and testing. If you:

- Fix a bug
  - Add a relevant unit or integration test when possible. These live in `**/tests/*.test.ts` and `**/tests/*.int.test.ts/`.
- Make an improvement
  - Update any affected example notebooks and documentation. These live in `docs`.
  - Update unit and integration tests when relevant.
- Add a feature
  - Add a demo notebook/MDX file in `docs/core_docs/docs`.
  - Add unit and integration tests.

We are a small, progress-oriented team. If there's something you'd like to add or change, opening a pull request is the
best way to get our attention.

## 🚀 Quick Start

This quick start guide explains how to run the repository locally.
For a [development container](https://containers.dev/), see the [.devcontainer folder](https://github.com/langchain-ai/langchainjs/tree/main/.devcontainer).

### 🏭 Release process

As of now, LangChain has an ad hoc release process: releases are cut with high frequency by
a developer and published to [npm](https://www.npmjs.com/package/langchain).

LangChain follows the [semver](https://semver.org/) versioning standard. However, as pre-1.0 software,
even patch releases may contain [non-backwards-compatible changes](https://semver.org/#spec-item-4).

If your contribution has made its way into a release, we will want to give you credit on Twitter (only if you want though)!
If you have a Twitter account you would like us to mention, please let us know in the PR or in another manner.

#### Integration releases

The release script can be executed only while on a fresh `main` branch, with no un-committed changes, from the package root. If working from a fork of the repository, make sure to sync the forked `main` branch with the upstream `main` branch first.

You can invoke the script by calling `yarn release`. If new dependencies have been added to the integration package, install them first (i.e. run `yarn`, then `yarn release`).

There are three parameters which can be passed to this script, one required and two optional.

- **Required**: `<workspace name>`. eg: `@langchain/core` The name of the package to release. Can be found in the `name` value of the package's `package.json`
- **Optional**: `--bump-deps` eg `--bump-deps` Will find all packages in the repo which depend on this workspace and checkout a new branch, update the dep version, run yarn install, commit & push to new branch. Generally, this is not necessary.
- **Optional**: `--tag <tag>` eg `--tag beta` Add a tag to the NPM release. Useful if you want to push a release candidate.

This script automatically bumps the package version, creates a new release branch with the changes, pushes the branch to GitHub, uses `release-it` to automatically release to NPM, and more depending on the flags passed.

Halfway through this script, you'll be prompted to enter an NPM OTP (typically from an authenticator app). This value is not stored anywhere and is only used to authenticate the NPM release.

> **Note** Unless releasing `langchain`, `no` should be answered to all prompts following `Publish @langchain/<package> to npm?`. Then, the change should be manually committed with the following commit message: `<package>[patch]: Release <new version>`. E.g.: `groq[patch]: Release 0.0.1`.

Docker must be running if releasing one of `langchain`, `@langchain/core` or `@langchain/community`. These packages run LangChain's export tests, which run inside docker containers.

Full example: `yarn release @langchain/core`.

### 🛠️ Tooling

This project uses the following tools, which are worth getting familiar
with if you plan to contribute:

- **[yarn](https://yarnpkg.com/) (v3.4.1)** - dependency management
- **[eslint](https://eslint.org/)** - enforcing standard lint rules
- **[prettier](https://prettier.io/)** - enforcing standard code formatting
- **[jest](https://jestjs.io/)** - testing code
- **[TypeDoc](https://typedoc.org/)** - reference doc generation from
  comments
- **[Docusaurus](https://docusaurus.io/)** - static site generation for documentation

## 🚀 Quick Start

Clone this repo, then cd into it:

```bash
cd langchainjs
```

Next, try running the following common tasks:

## ✅ Common Tasks

Our goal is to make it as easy as possible for you to contribute to this project.
All of the below commands should be run from within a workspace directory (e.g. `langchain`, `libs/langchain-community`) unless otherwise noted.

```bash
cd langchain
```

Or, if you are working on a community integration:

```bash
cd libs/langchain-community
```

### Setup

**Prerequisite**: Node version 18+ is required. Please check node version `node -v` and update it if required.

To get started, you will need to install the dependencies for the project. To do so, run:

```bash
yarn
```

Then, you will need to switch directories into `langchain-core` and build core by running:

```bash
cd ../langchain-core
yarn
yarn build
```

### Linting

We use [eslint](https://eslint.org/) to enforce standard lint rules.
To run the linter, run:

```bash
yarn lint
```

### Formatting

We use [prettier](https://prettier.io) to enforce code formatting style.
To run the formatter, run:

```bash
yarn format
```

To just check for formatting differences, without fixing them, run:

```bash
yarn format:check
```

### Testing

In general, tests should be added within a `tests/` folder alongside the modules they
are testing.

**Unit tests** cover modular logic that does not require calls to outside APIs.

If you add new logic, please add a unit test.
Unit tests should be called `*.test.ts`.

To run only unit tests, run:

```bash
yarn test
```

#### Running a single test

To run a single test, run the following from within a workspace:

```bash
yarn test:single /path/to/yourtest.test.ts
```

This is useful for developing individual features.

**Integration tests** cover logic that requires making calls to outside APIs (often integration with other services).

If you add support for a new external API, please add a new integration test.
Integration tests should be called `*.int.test.ts`.

Note that most integration tests require credentials or other setup. You will likely need to set up a `langchain/.env` or `libs/langchain-community/.env` file
like the example [here](https://github.com/langchain-ai/langchainjs/blob/main/langchain/.env.example).

We generally recommend only running integration tests with `yarn test:single`, but if you want to run all integration tests, run:

```bash
yarn test:integration
```

### Building

To build the project, run:

```bash
yarn build
```

### Adding an Entrypoint

LangChain exposes multiple subpaths the user can import from, e.g.

```typescript
import { OpenAI } from "langchain/llms/openai";
```

We call these subpaths "entrypoints". In general, you should create a new entrypoint if you are adding a new integration with a 3rd party library. If you're adding self-contained functionality without any external dependencies, you can add it to an existing entrypoint.

In order to declare a new entrypoint that users can import from, you
should edit the `langchain/langchain.config.js` or `libs/langchain-community/langchain.config.js` file. To add an
entrypoint `tools` that imports from `tools/index.ts` you'd add
the following to the `entrypoints` key inside the `config` variable:

```typescript
// ...
entrypoints: {
  // ...
  tools: "tools/index",
},
// ...
```

If you're adding a new integration which requires installing a third party dependency, you must add the entrypoint to the `requiresOptionalDependency` array, also located inside `langchain/langchain.config.js` or `libs/langchain-community/langchain.config.js`.

```typescript
// ...
requiresOptionalDependency: [
  // ...
  "tools/index",
],
// ...
```

This will make sure the entrypoint is included in the published package,
and in generated documentation.

## Documentation

### Contribute Documentation

#### Install dependencies

##### Note: you only need to follow these steps if you are building the docs site locally.

1. [Quarto](https://quarto.org/) - package that converts Jupyter notebooks (`.ipynb` files) into `.mdx` files for serving in Docusaurus.
2. `yarn build --filter=core_docs` - It's as simple as that! (or you can simply run `yarn build` from `docs/core_docs/`)

All notebooks are converted to `.md` files and automatically gitignored. If you would like to create a non notebook doc, it must be a `.mdx` file.

### Writing Notebooks

When adding new dependencies inside the notebook you must update the import map inside `deno.json` in the root of the LangChain repo.

This is required because the notebooks use the Deno runtime, and Deno formats imports differently than Node.js.

Example:

```typescript
// Import in Node:
import { z } from "zod";
// Import in Deno:
import { z } from "npm:/zod";
```

See examples inside `deno.json` for more details.

Docs are largely autogenerated by [TypeDoc](https://typedoc.org/) from the code.

For that reason, we ask that you add good documentation to all classes and methods.

Similar to linting, we recognize documentation can be annoying. If you do not want to do it, please contact a project maintainer, and they can help you with it. We do not want this to be a blocker for good code getting contributed.

Documentation and the skeleton lives under the `docs/` folder. Example code is imported from under the `examples/` folder.

### Running examples

If you add a new major piece of functionality, it is helpful to add an
example to showcase how to use it. Most of our users find examples to be the
most helpful kind of documentation.

Examples can be added in the `examples/src` directory, e.g.
`examples/src/path/to/example`. This
example can then be invoked with `yarn example path/to/example` at the top
level of the repo.

To run examples that require an environment variable, you'll need to add a `.env` file under `examples/.env`.

### Build Documentation Locally

To generate and view the documentation locally, change to the project root and run `yarn` to ensure dependencies get installed
in both the `docs/` and `examples/` workspaces:

```bash
cd ..
yarn
```

Then run:

```bash
yarn docs
```

## Advanced

**Environment tests** test whether LangChain works across different JS environments, including Node.js (both ESM and CJS), Edge environments (eg. Cloudflare Workers), and browsers (using Webpack).

To run the environment tests with Docker, run the following command from the project root:

```bash
yarn test:exports:docker
```



================================================
FILE: docs/core_docs/docs/contributing/faq.mdx
================================================
---
sidebar_position: 6
sidebar_label: FAQ
---

# Frequently Asked Questions

## Pull Requests (PRs)

### How do I allow maintainers to edit my PR?

When you submit a pull request, there may be additional changes
necessary before merging it. Oftentimes, it is more efficient for the
maintainers to make these changes themselves before merging, rather than asking you
to do so in code review.

By default, most pull requests will have a
`✅ Maintainers are allowed to edit this pull request.`
badge in the right-hand sidebar.

If you do not see this badge, you may have this setting off for the fork you are
pull-requesting from. See [this Github docs page](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/allowing-changes-to-a-pull-request-branch-created-from-a-fork)
for more information.

Notably, Github doesn't allow this setting to be enabled for forks in **organizations** ([issue](https://github.com/orgs/community/discussions/5634)).
If you are working in an organization, we recommend submitting your PR from a personal
fork in order to enable this setting.



================================================
FILE: docs/core_docs/docs/contributing/index.mdx
================================================
---
sidebar_position: 0
---

# Welcome Contributors

Hi there! Thank you for even being interested in contributing to LangChain.
As an open-source project in a rapidly developing field, we are extremely open to contributions, whether they involve new features, improved infrastructure, better documentation, or bug fixes.

## 🗺️ Guidelines

### 👩‍💻 Ways to contribute

There are many ways to contribute to LangChain. Here are some common ways people contribute:

- [**Documentation**](/docs/contributing/documentation/style_guide): Help improve our docs, including this one!
- [**Code**](./code.mdx): Help us write code, fix bugs, or improve our infrastructure.
- [**Integrations**](./integrations.mdx): Help us integrate with your favorite vendors and tools.
- [**Discussions**](https://github.com/langchain-ai/langchainjs/discussions): Help answer usage questions and discuss issues with users.

### 🚩 GitHub Issues

Our [issues](https://github.com/langchain-ai/langchainjs/issues) page is kept up to date with bugs, improvements, and feature requests.

There is a taxonomy of labels to help with sorting and discovery of issues of interest. Please use these to help organize issues.

If you start working on an issue, please assign it to yourself.

If you are adding an issue, please try to keep it focused on a single, modular bug/improvement/feature.
If two issues are related, or blocking, please link them rather than combining them.

We will try to keep these issues as up-to-date as possible, though
with the rapid rate of development in this field some may get out of date.
If you notice this happening, please let us know.

### 💭 GitHub Discussions

We have a [discussions](https://github.com/langchain-ai/langchainjs/discussions) page where users can ask usage questions, discuss design decisions, and propose new features.

If you are able to help answer questions, please do so! This will allow the maintainers to spend more time focused on development and bug fixing.

### 🙋 Getting Help

Our goal is to have the simplest developer setup possible. Should you experience any difficulty getting setup, please
contact a maintainer! Not only do we want to help get you unblocked, but we also want to make sure that the process is
smooth for future contributors.

In a similar vein, we do enforce certain linting, formatting, and documentation standards in the codebase.
If you are finding these difficult (or even just annoying) to work with, feel free to contact a maintainer for help -
we do not want these to get in the way of getting good code into the codebase.

# 🌟 Recognition

If your contribution has made its way into a release, we will want to give you credit on Twitter (only if you want though)!
If you have a Twitter account you would like us to mention, please let us know in the PR or through another means.



================================================
FILE: docs/core_docs/docs/contributing/integrations.mdx
================================================
---
sidebar_position: 5
---

# Contribute Integrations

To begin, make sure you have all the dependencies outlined in guide on [Contributing Code](/docs/contributing/code/).

There are a few different places you can contribute integrations for LangChain:

- **Community**: For lighter-weight integrations that are primarily maintained by LangChain and the Open Source Community.
- **Partner Packages**: For independent packages that are co-maintained by LangChain and a partner.

For the most part, new integrations should be added to the Community package. Partner packages require more maintenance as separate packages, so please confirm with the LangChain team before creating a new partner package.

In the following sections, we'll walk through how to contribute to each of these packages from a fake company, `Parrot Link AI`.

## Community package

The `@langchain/community` package is in `libs/langchain-community` and contains most integrations.

It can be installed with e.g. `npm install @langchain/community`, and exported members can be imported with code like

```ts
import { ChatParrotLink } from "@langchain/community/chat_models/parrot_link";
import { ParrotLinkLLM } from "@langchain/community/llms/parrot_link";
import { ParrotLinkVectorStore } from "@langchain/community/vectorstores/parrot_link";
```

The `@langchain/community` package relies on manually-installed dependent packages, so you will see errors
if you try to import a package that is not installed. In our fake example, if you tried to import `ParrotLinkLLM` without installing `parrot-link-sdk`, you would see an error telling you that the package failed to import.

Let's say we wanted to implement a chat model for Parrot Link AI. We would create a new file in `libs/langchain-community/src/chat_models/parrot_link.ts` with something like the following code:

```ts
import {
  SimpleChatModel,
} from "@langchain/core/language_models/chat_models";

export class ChatParrotLink extends SimpleChatModel {

  ...
```

Tests are colocated in the `src/` directory, so you could write them in files like the below:

- Unit tests: `libs/langchain-community/src/chat_models/tests/parrot_link.test.ts`
- Integration tests: `libs/langchain-community/src/chat_models/tests/parrot_link.int.test.ts`

Unit tests should not have any external API calls or require any environment variables.

You should add documentation to:

- `docs/core_docs/docs/integrations/chat/parrot_link.mdx`

## Partner package in LangChain repo

Partner packages can be hosted in the `LangChain` monorepo.

Partner packages in the `LangChain` repo should be placed under `libs/langchain-{partner}`

A package is
installed by users with `npm install @langchain/{partner}`, and the package members
can be imported with code like:

```ts
import { X } from "@langchain/{partner}";
```

### Set up a new package

To set up a new partner package, you can use [`create-langchain-integration`](https://github.com/langchain-ai/langchainjs/blob/main/libs/create-langchain-integration/),
a utility that will automatically scaffold a repo with support for both ESM + CJS entrypoints. You can run it like this within the `libs/` folder:

```bash
cd libs/
npx create-langchain-integration
```

Then, follow the prompts to name your package.
The default package will include stubs for a Chat Model, an LLM, and/or a Vector Store. You should delete any of the files you won't use and remove them from `index.ts`.

### Dependencies

If your package needs dependencies, such as your company's SDK, you can add them to your package's `package.json` file as normal:

```bash
npm install parrot-link-sdk
```

### Write Unit and Integration Tests

Some basic tests are presented in the `src/tests/` directory. You should add more tests to cover your package's functionality.

For information on running and implementing tests, see the [Testing guide](/docs/contributing/testing/).

### Write documentation

Please copy and use the appropriate template from here:

https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-scripts/src/cli/docs/templates

You should place the notebooks with examples
in the relevant `docs/core_docs/docs/integrations` directory in the monorepo root.

### (If Necessary) Deprecate community integration

Note: this is only necessary if you're migrating an existing community integration into
a partner package. If the component you're integrating is net-new to LangChain (i.e.
not already in the `community` package), you can skip this step.

Let's pretend we migrated our `ChatParrotLink` chat model from the community package to
the partner package. We would need to deprecate the old model in the community package.
We can do this using a `@deprecated` TSDoc comment.

Before our change, our chat model might look like this:

```ts
class ChatParrotLink extends SimpleChatModel {
  ...
```

After our change, it would look like this:

```ts
/** @deprecated Install and import from `@langchain/parrot-link` instead. */
class ChatParrotLink extends SimpleChatModel {
  ...
```

You should do this for _each_ component that you're migrating to the partner package.



================================================
FILE: docs/core_docs/docs/contributing/repo_structure.mdx
================================================
---
sidebar_position: 0.5
---

# Repository Structure

If you plan on contributing to LangChain code or documentation, it can be useful
to understand the high level structure of the repository.

LangChain is organized as a [monorepo](https://en.wikipedia.org/wiki/Monorepo) that contains multiple packages.

Here's the structure visualized as a tree:

```text
.
├── docs
│   ├── core_docs # Contains content for the documentation here: https://js.langchain.com/
│   ├── api_refs # Contains content for the API refs here: https://api.js.langchain.com/
├── langchain # Main package
│   ├── src/**/tests/*.test.ts/ # Unit tests (present in each package not shown for brevity)
│   ├── src/**/tests/*.int.test.ts/ # Integration tests (present in each package not shown for brevity)
├── langchain # Base interfaces for key abstractions
├── libs # Community packages
│   ├── langchain-community # Third-party integrations
│   ├── langchain-partner-1
│   ├── langchain-partner-2
│   ├── ...
```

The root directory also contains the following files:

- `package.json`: Dependencies for building docs and linting docs.

There are other files in the root directory level, but their presence should be self-explanatory. Feel free to browse around!

## Documentation

The `/docs` directory contains the content for the documentation that is shown
at https://js.langchain.com/ and the associated API Reference https://api.js.langchain.com/

See the [documentation](/docs/contributing/documentation/style_guide) guidelines to learn how to contribute to the documentation.

## Code

The `/libs` directory contains the code for the LangChain packages.

To learn more about how to contribute code see the following guidelines:

- [Code](./code.mdx) Learn how to develop in the LangChain codebase.
- [Integrations](./integrations.mdx) to learn how to contribute to third-party integrations to langchain-community or to start a new partner package.
- [Testing](./testing.mdx) guidelines to learn how to write tests for the packages.



================================================
FILE: docs/core_docs/docs/contributing/testing.mdx
================================================
---
sidebar_position: 2
---

# Testing

In general, tests should be added within a `tests/` folder alongside the modules they
are testing.

**Unit tests** cover modular logic that does not require calls to outside APIs.

If you add new logic, please add a unit test.
Unit tests should be called `*.test.ts`.

To run only unit tests, run:

```bash
yarn test
```

### Running a single test

To run a single test, run the following from within a workspace:

```bash
yarn test:single /path/to/yourtest.test.ts
```

This is useful for developing individual features.

**Integration tests** cover logic that requires making calls to outside APIs (often integration with other services).

If you add support for a new external API, please add a new integration test.
Integration tests should be called `*.int.test.ts`.

Note that most integration tests require credentials or other setup. You will likely need to set up a `langchain/.env` or `libs/langchain-community/.env` file
like the example [here](https://github.com/langchain-ai/langchainjs/blob/main/langchain/.env.example).

We generally recommend only running integration tests with `yarn test:single`, but if you want to run all integration tests, run:

```bash
yarn test:integration
```



================================================
FILE: docs/core_docs/docs/contributing/documentation/_category_.yml
================================================
label: 'Documentation'
position: 3


================================================
FILE: docs/core_docs/docs/contributing/documentation/style_guide.mdx
================================================
---
sidebar_label: "Style guide"
---

# LangChain Documentation Style Guide

## Introduction

As LangChain continues to grow, the surface area of documentation required to cover it continues to grow too.
This page provides guidelines for anyone writing documentation for LangChain, as well as some of our philosophies around
organization and structure.

## Philosophy

LangChain's documentation aspires to follow the [Diataxis framework](https://diataxis.fr).
Under this framework, all documentation falls under one of four categories:

- **Tutorials**: Lessons that take the reader by the hand through a series of conceptual steps to complete a project.
  - An example of this is our [LCEL streaming guide](/docs/how_to/streaming).
  - Our guides on [custom components](/docs/how_to/custom_chat) is another one.
- **How-to guides**: Guides that take the reader through the steps required to solve a real-world problem.
  - The clearest examples of this are our [Use case](/docs/how_to/#use-cases) pages.
- **Reference**: Technical descriptions of the machinery and how to operate it.
  - Our [Runnable](/docs/how_to/#langchain-expression-language-lcel) pages is an example of this.
  - The [API reference pages](https://api.js.langchain.com/) are another.
- **Explanation**: Explanations that clarify and illuminate a particular topic.

Each category serves a distinct purpose and requires a specific approach to writing and structuring the content.

## Taxonomy

Keeping the above in mind, we have sorted LangChain's docs into categories. It is helpful to think in these terms
when contributing new documentation:

### Getting started

The [getting started section](/docs/introduction) includes a high-level introduction to LangChain, a quickstart that
tours LangChain's various features, and logistical instructions around installation and project setup.

It contains elements of **How-to guides** and **Explanations**.

### Use cases

[Use cases](/docs/how_to/#use-cases) are guides that are meant to show how to use LangChain to accomplish a specific task (RAG, information extraction, etc.).
The quickstarts should be good entrypoints for first-time LangChain developers who prefer to learn by getting something practical prototyped,
then taking the pieces apart retrospectively. These should mirror what LangChain is good at.

The quickstart pages here should fit the **How-to guide** category, with the other pages intended to be **Explanations** of more
in-depth concepts and strategies that accompany the main happy paths.

:::note
The below sections are listed roughly in order of increasing level of abstraction.
:::

### Expression Language

[LangChain Expression Language (LCEL)](/docs/how_to/#langchain-expression-language-lcel) is the fundamental way that most LangChain components fit together, and this section is designed to teach
developers how to use it to build with LangChain's primitives effectively.

This section should contains **Tutorials** that teach how to stream and use LCEL primitives for more abstract tasks, **Explanations** of specific behaviors,
and some **References** for how to use different methods in the Runnable interface.

### Components

The [how to section](/docs/how_to) covers concepts one level of abstraction higher than LCEL.
Abstract base classes like `BaseChatModel` and `BaseRetriever` should be covered here, as well as core implementations of these base classes,
such as `ChatPromptTemplate` and `RecursiveCharacterTextSplitter`. Customization guides belong here too.

This section should contain mostly conceptual **Tutorials**, **References**, and **Explanations** of the components they cover.

:::note
As a general rule of thumb, everything covered in the `Expression Language` and `Components` sections (with the exception of the `Composition` section of components) should
cover only components that exist in `@langchain/core`.
:::

### Integrations

The [integrations](/docs/integrations/platforms/) are specific implementations of components. These often involve third-party APIs and services.
If this is the case, as a general rule, these are maintained by the third-party partner.

This section should contain mostly **Explanations** and **References**, though the actual content here is more flexible than other sections and more at the
discretion of the third-party provider.

:::note
Concepts covered in `Integrations` should generally exist in `@langchain/community` or specific partner packages.
:::

### Tutorials and Ecosystem

The [Tutorials](/docs/tutorials) and [Ecosystem](https://docs.smith.langchain.com) sections should contain guides that address higher-level problems than the sections above.
This includes, but is not limited to, considerations around productionization and development workflows.

These should contain mostly **How-to guides**, **Explanations**, and **Tutorials**.

### API references

LangChain's API references. Should act as **References** (as the name implies) with some **Explanation**-focused content as well.

## Sample developer journey

We have set up our docs to assist a new developer to LangChain. Let's walk through the intended path:

- The developer lands on https://js.langchain.com, and reads through the introduction and the diagram.
- If they are just curious, they may be drawn to the [Quickstart](/docs/tutorials/llm_chain) to get a high-level tour of what LangChain contains.
- If they have a specific task in mind that they want to accomplish, they will be drawn to the Use-Case section. The use-case should provide a good, concrete hook that shows the value LangChain can provide them and be a good entrypoint to the framework.
- They can then move to learn more about the fundamentals of LangChain through the Expression Language sections.
- Next, they can learn about LangChain's various components and integrations.
- Finally, they can get additional knowledge through the Guides.

This is only an ideal of course - sections will inevitably reference lower or higher-level concepts that are documented in other sections.

## Guidelines

Here are some other guidelines you should think about when writing and organizing documentation.

### Linking to other sections

Because sections of the docs do not exist in a vacuum, it is important to link to other sections as often as possible
to allow a developer to learn more about an unfamiliar topic inline.

This includes linking to the API references as well as conceptual sections!

### Conciseness

In general, take a less-is-more approach. If a section with a good explanation of a concept already exists, you should link to it rather than
re-explain it, unless the concept you are documenting presents some new wrinkle.

Be concise, including in code samples.

### General style

- Use active voice and present tense whenever possible.
- Use examples and code snippets to illustrate concepts and usage.
- Use appropriate header levels (`#`, `##`, `###`, etc.) to organize the content hierarchically.
- Use bullet points and numbered lists to break down information into easily digestible chunks.
- Use tables (especially for **Reference** sections) and diagrams often to present information visually.
- Include the table of contents for longer documentation pages to help readers navigate the content, but hide it for shorter pages.



================================================
FILE: docs/core_docs/docs/how_to/agent_executor.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to use legacy LangChain Agents (AgentExecutor)

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Tools](/docs/concepts/tools)

:::

By themselves, language models can't take actions - they just output text.
Agents are systems that use an LLM as a reasoning engine to determine which actions to take and what the inputs to those actions should be.
The results of those actions can then be fed back into the agent and it determine whether more actions are needed, or whether it is okay to finish.

In this tutorial we will build an agent that can interact with multiple different tools: one being a local database, the other being a search engine. You will be able to ask this agent questions, watch it call tools, and have conversations with it.

:::{.callout-important}
This section will cover building with LangChain Agents. LangChain Agents are fine for getting started, but past a certain point you will likely want flexibility and control that they do not offer. For working with more advanced agents, we'd recommend checking out [LangGraph](https://langchain-ai.github.io/langgraphjs).
:::

## Concepts

Concepts we will cover are:
- Using [language models](/docs/concepts/chat_models), in particular their tool calling ability
- Creating a [Retriever](/docs/concepts/retrievers) to expose specific information to our agent
- Using a Search [Tool](/docs/concepts/tools) to look up things online
- [`Chat History`](/docs/concepts/chat_history), which allows a chatbot to "remember" past interactions and take them into account when responding to followup questions. 
- Debugging and tracing your application using [LangSmith](/docs/concepts/#langsmith)

## Setup

### Jupyter Notebook

This guide (and most of the other guides in the documentation) uses [Jupyter notebooks](https://jupyter.org/) and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.

This and other tutorials are perhaps most conveniently run in a Jupyter notebook. See [here](https://jupyter.org/install) for instructions on how to install.

### Installation

To install LangChain (and `cheerio` for the web loader) run:

```{=mdx}
import Npm2Yarn from '@theme/Npm2Yarn';

<Npm2Yarn>
  langchain @langchain/core cheerio
</Npm2Yarn>
```

For more details, see our [Installation guide](/docs/how_to/installation/).

### LangSmith

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.
As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.
The best way to do this is with [LangSmith](https://smith.langchain.com).

After you sign up at the link above, make sure to set your environment variables to start logging traces:

```shell
export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."

# Reduce tracing latency if you are not in a serverless environment
# export LANGCHAIN_CALLBACKS_BACKGROUND=true
```

"""

"""
## Define tools

We first need to create the tools we want to use. We will use two tools: [Tavily](/docs/integrations/tools/tavily_search) (to search online) and then a retriever over a local index we will create

### [Tavily](/docs/integrations/tools/tavily_search)

We have a built-in tool in LangChain to easily use Tavily search engine as tool.
Note that this requires an API key - they have a free tier, but if you don't have one or don't want to create one, you can always ignore this step.

Once you create your API key, you will need to export that as:

```bash
export TAVILY_API_KEY="..."
```
"""

import "cheerio"; // This is required in notebooks to use the `CheerioWebBaseLoader`
import { TavilySearchResults } from "@langchain/community/tools/tavily_search"

const search = new TavilySearchResults({
  maxResults: 2
});

await search.invoke("what is the weather in SF")
# Output:
#   [32m`[{"title":"Weather in San Francisco","url":"https://www.weatherapi.com/","content":"{'location': {'n`[39m... 1358 more characters

"""
### Retriever

We will also create a retriever over some data of our own. For a deeper explanation of each step here, see [this tutorial](/docs/tutorials/rag).
"""

import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "@langchain/openai";
import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";

const loader = new CheerioWebBaseLoader("https://docs.smith.langchain.com/overview");
const docs = await loader.load();
const splitter = new RecursiveCharacterTextSplitter(
  {
    chunkSize: 1000,
    chunkOverlap: 200
  }
);
const documents = await splitter.splitDocuments(docs);
const vectorStore = await MemoryVectorStore.fromDocuments(documents, new OpenAIEmbeddings());
const retriever = vectorStore.asRetriever();

(await retriever.invoke("how to upload a dataset"))[0];
# Output:
#   Document {

#     pageContent: [32m'description="A sample dataset in LangSmith.")client.create_examples(    inputs=[        {"postfix": '[39m... 891 more characters,

#     metadata: {

#       source: [32m"https://docs.smith.langchain.com/overview"[39m,

#       loc: { lines: { from: [33m4[39m, to: [33m4[39m } }

#     },

#     id: [90mundefined[39m

#   }

"""
Now that we have populated our index that we will do doing retrieval over, we can easily turn it into a tool (the format needed for an agent to properly use it)
"""

import { z } from "zod";
import { tool } from "@langchain/core/tools";

const retrieverTool = tool(async ({ input }, config) => {
  const docs = await retriever.invoke(input, config);
  return docs.map((doc) => doc.pageContent).join("\n\n");
}, {
  name: "langsmith_search",
  description:
    "Search for information about LangSmith. For any questions about LangSmith, you must use this tool!",
  schema: z.object({
    input: z.string()
  }),
});

"""
### Tools

Now that we have created both, we can create a list of tools that we will use downstream.
"""

const tools = [search, retrieverTool];

"""
## Using Language Models

Next, let's learn how to use a language model by to call tools. LangChain supports many different language models that you can use interchangably - select the one you want to use below!

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs openaiParams={`{ model: "gpt-4" }`} />
```
"""

"""
You can call the language model by passing in a list of messages. By default, the response is a `content` string.
"""

// @lc-docs-hide-cell

import { ChatOpenAI } from "@langchain/openai";

const model = new ChatOpenAI({ model: "gpt-4o-mini", temperature: 0 })

const response = await model.invoke([{
  role: "user",
  content: "hi!"
}]);

response.content;
# Output:
#   [32m"Hello! How can I assist you today?"[39m

"""
We can now see what it is like to enable this model to do tool calling. In order to enable that we use `.bind` to give the language model knowledge of these tools
"""

const modelWithTools = model.bindTools(tools);

"""
We can now call the model. Let's first call it with a normal message, and see how it responds. We can look at both the `content` field as well as the `tool_calls` field.
"""

const responseWithTools = await modelWithTools.invoke([{
  role: "user",
  content: "Hi!"
}])

console.log(`Content: ${responseWithTools.content}`)
console.log(`Tool calls: ${responseWithTools.tool_calls}`)
# Output:
#   Content: Hello! How can I assist you today?

#   Tool calls: 


"""
Now, let's try calling it with some input that would expect a tool to be called.
"""

const responseWithToolCalls = await modelWithTools.invoke([{
  role: "user",
  content: "What's the weather in SF?"
}])

console.log(`Content: ${responseWithToolCalls.content}`)
console.log(`Tool calls: ${JSON.stringify(responseWithToolCalls.tool_calls, null, 2)}`)
# Output:
#   Content: 

#   Tool calls: [

#     {

#       "name": "tavily_search_results_json",

#       "args": {

#         "input": "current weather in San Francisco"

#       },

#       "type": "tool_call",

#       "id": "call_gtJ5rrjXswO8EIvePrxyGQbR"

#     }

#   ]


"""
We can see that there's now no content, but there is a tool call! It wants us to call the Tavily Search tool.

This isn't calling that tool yet - it's just telling us to. In order to actually calll it, we'll want to create our agent.
"""

"""
## Create the agent

Now that we have defined the tools and the LLM, we can create the agent. We will be using a tool calling agent - for more information on this type of agent, as well as other options, see [this guide](/docs/concepts/agents/).

We can first choose the prompt we want to use to guide the agent:
"""

import { ChatPromptTemplate } from "@langchain/core/prompts";

const prompt = ChatPromptTemplate.fromMessages([
  ["system", "You are a helpful assistant"],
  ["placeholder", "{chat_history}"],
  ["human", "{input}"],
  ["placeholder", "{agent_scratchpad}"],
]);

console.log(prompt.promptMessages);
# Output:
#   [

#     SystemMessagePromptTemplate {

#       lc_serializable: true,

#       lc_kwargs: {

#         prompt: PromptTemplate {

#           lc_serializable: true,

#           lc_kwargs: {

#             inputVariables: [],

#             templateFormat: "f-string",

#             template: "You are a helpful assistant"

#           },

#           lc_runnable: true,

#           name: undefined,

#           lc_namespace: [ "langchain_core", "prompts", "prompt" ],

#           inputVariables: [],

#           outputParser: undefined,

#           partialVariables: undefined,

#           templateFormat: "f-string",

#           template: "You are a helpful assistant",

#           validateTemplate: true,

#           additionalContentFields: undefined

#         }

#       },

#       lc_runnable: true,

#       name: undefined,

#       lc_namespace: [ "langchain_core", "prompts", "chat" ],

#       inputVariables: [],

#       additionalOptions: {},

#       prompt: PromptTemplate {

#         lc_serializable: true,

#         lc_kwargs: {

#           inputVariables: [],

#           templateFormat: "f-string",

#           template: "You are a helpful assistant"

#         },

#         lc_runnable: true,

#         name: undefined,

#         lc_namespace: [ "langchain_core", "prompts", "prompt" ],

#         inputVariables: [],

#         outputParser: undefined,

#         partialVariables: undefined,

#         templateFormat: "f-string",

#         template: "You are a helpful assistant",

#         validateTemplate: true,

#         additionalContentFields: undefined

#       },

#       messageClass: undefined,

#       chatMessageClass: undefined

#     },

#     MessagesPlaceholder {

#       lc_serializable: true,

#       lc_kwargs: { variableName: "chat_history", optional: true },

#       lc_runnable: true,

#       name: undefined,

#       lc_namespace: [ "langchain_core", "prompts", "chat" ],

#       variableName: "chat_history",

#       optional: true

#     },

#     HumanMessagePromptTemplate {

#       lc_serializable: true,

#       lc_kwargs: {

#         prompt: PromptTemplate {

#           lc_serializable: true,

#           lc_kwargs: {

#             inputVariables: [Array],

#             templateFormat: "f-string",

#             template: "{input}"

#           },

#           lc_runnable: true,

#           name: undefined,

#           lc_namespace: [ "langchain_core", "prompts", "prompt" ],

#           inputVariables: [ "input" ],

#           outputParser: undefined,

#           partialVariables: undefined,

#           templateFormat: "f-string",

#           template: "{input}",

#           validateTemplate: true,

#           additionalContentFields: undefined

#         }

#       },

#       lc_runnable: true,

#       name: undefined,

#       lc_namespace: [ "langchain_core", "prompts", "chat" ],

#       inputVariables: [ "input" ],

#       additionalOptions: {},

#       prompt: PromptTemplate {

#         lc_serializable: true,

#         lc_kwargs: {

#           inputVariables: [ "input" ],

#           templateFormat: "f-string",

#           template: "{input}"

#         },

#         lc_runnable: true,

#         name: undefined,

#         lc_namespace: [ "langchain_core", "prompts", "prompt" ],

#         inputVariables: [ "input" ],

#         outputParser: undefined,

#         partialVariables: undefined,

#         templateFormat: "f-string",

#         template: "{input}",

#         validateTemplate: true,

#         additionalContentFields: undefined

#       },

#       messageClass: undefined,

#       chatMessageClass: undefined

#     },

#     MessagesPlaceholder {

#       lc_serializable: true,

#       lc_kwargs: { variableName: "agent_scratchpad", optional: true },

#       lc_runnable: true,

#       name: undefined,

#       lc_namespace: [ "langchain_core", "prompts", "chat" ],

#       variableName: "agent_scratchpad",

#       optional: true

#     }

#   ]


"""
Now, we can initalize the agent with the LLM, the prompt, and the tools. The agent is responsible for taking in input and deciding what actions to take. Crucially, the Agent does not execute those actions - that is done by the AgentExecutor (next step). For more information about how to think about these components, see our [conceptual guide](/docs/concepts/agents).

Note that we are passing in the `model`, not `modelWithTools`. That is because `createToolCallingAgent` will call `.bind` for us under the hood.
"""

import { createToolCallingAgent } from "langchain/agents";

const agent = await createToolCallingAgent({ llm: model, tools, prompt })

"""
Finally, we combine the agent (the brains) with the tools inside the AgentExecutor (which will repeatedly call the agent and execute tools).
"""

import { AgentExecutor } from "langchain/agents";

const agentExecutor = new AgentExecutor({
  agent,
  tools
})

"""
## Run the agent

We can now run the agent on a few queries! Note that for now, these are all **stateless** queries (it won't remember previous interactions).

First up, let's how it responds when there's no need to call a tool:
"""

await agentExecutor.invoke({ input: "hi!" })
# Output:
#   { input: [32m"hi!"[39m, output: [32m"Hello! How can I assist you today?"[39m }

"""
In order to see exactly what is happening under the hood (and to make sure it's not calling a tool) we can take a look at the [LangSmith trace](https://smith.langchain.com/public/b8051e80-14fd-4931-be0f-6416280bc500/r)

Let's now try it out on an example where it should be invoking the retriever
"""

await agentExecutor.invoke({ input: "how can langsmith help with testing?" })
# Output:
#   {

#     input: [32m"how can langsmith help with testing?"[39m,

#     output: [32m"LangSmith can assist with testing in several ways, particularly for applications built using large l"[39m... 1474 more characters

#   }

"""
Let's take a look at the [LangSmith trace](https://smith.langchain.com/public/35bd4f0f-aa2f-4ac2-b9a9-89ce0ca306ca/r) to make sure it's actually calling that.

Now let's try one where it needs to call the search tool:
"""

await agentExecutor.invoke({ input: "whats the weather in sf?" })
# Output:
#   {

#     input: [32m"whats the weather in sf?"[39m,

#     output: [32m"The current weather in San Francisco is as follows:\n"[39m +

#       [32m"\n"[39m +

#       [32m"- **Temperature**: 15.6°C (60.1°F)\n"[39m +

#       [32m"- **Conditio"[39m... 303 more characters

#   }

"""
We can check out the [LangSmith trace](https://smith.langchain.com/public/dfde6f46-0e7b-4dfe-813c-87d7bfb2ade5/r) to make sure it's calling the search tool effectively.
"""

"""
## Adding in memory

As mentioned earlier, this agent is stateless. This means it does not remember previous interactions. To give it memory we need to pass in previous `chat_history`.

**Note**: The input variable needs to be called `chat_history` because of the prompt we are using. If we use a different prompt, we could change the variable name.
"""

// Here we pass in an empty list of messages for chat_history because it is the first message in the chat
await agentExecutor.invoke({ input: "hi! my name is bob", chat_history: [] })
# Output:
#   {

#     input: [32m"hi! my name is bob"[39m,

#     chat_history: [],

#     output: [32m"Hello Bob! How can I assist you today?"[39m

#   }

await agentExecutor.invoke(
  {
    chat_history: [
      { role: "user", content: "hi! my name is bob" },
      { role: "assistant", content: "Hello Bob! How can I assist you today?" },
    ],
    input: "what's my name?",
  }
)
# Output:
#   {

#     chat_history: [

#       { role: [32m"user"[39m, content: [32m"hi! my name is bob"[39m },

#       {

#         role: [32m"assistant"[39m,

#         content: [32m"Hello Bob! How can I assist you today?"[39m

#       }

#     ],

#     input: [32m"what's my name?"[39m,

#     output: [32m"Your name is Bob. How can I help you today, Bob?"[39m

#   }

"""
If we want to keep track of these messages automatically, we can wrap this in a RunnableWithMessageHistory.

Because we have multiple inputs, we need to specify two things:

- `inputMessagesKey`: The input key to use to add to the conversation history.
- `historyMessagesKey`: The key to add the loaded messages into.

For more information on how to use this, see [this guide](/docs/how_to/message_history). 
"""

import { ChatMessageHistory } from "@langchain/community/stores/message/in_memory";
import { BaseChatMessageHistory } from "@langchain/core/chat_history";
import { RunnableWithMessageHistory } from "@langchain/core/runnables";

const store = {};

function getMessageHistory(sessionId: string): BaseChatMessageHistory {
  if (!(sessionId in store)) {
    store[sessionId] = new ChatMessageHistory();
  }
  return store[sessionId];
}

const agentWithChatHistory = new RunnableWithMessageHistory({
  runnable: agentExecutor,
  getMessageHistory,
  inputMessagesKey: "input",
  historyMessagesKey: "chat_history",
})

await agentWithChatHistory.invoke(
  { input: "hi! I'm bob" },
  { configurable: { sessionId: "<foo>" }},
)
# Output:
#   {

#     input: [32m"hi! I'm bob"[39m,

#     chat_history: [

#       HumanMessage {

#         "content": "hi! I'm bob",

#         "additional_kwargs": {},

#         "response_metadata": {}

#       },

#       AIMessage {

#         "content": "Hello Bob! How can I assist you today?",

#         "additional_kwargs": {},

#         "response_metadata": {},

#         "tool_calls": [],

#         "invalid_tool_calls": []

#       }

#     ],

#     output: [32m"Hello Bob! How can I assist you today?"[39m

#   }

await agentWithChatHistory.invoke(
  { input: "what's my name?" },
  { configurable: { sessionId: "<foo>" }},
)
# Output:
#   {

#     input: [32m"what's my name?"[39m,

#     chat_history: [

#       HumanMessage {

#         "content": "hi! I'm bob",

#         "additional_kwargs": {},

#         "response_metadata": {}

#       },

#       AIMessage {

#         "content": "Hello Bob! How can I assist you today?",

#         "additional_kwargs": {},

#         "response_metadata": {},

#         "tool_calls": [],

#         "invalid_tool_calls": []

#       },

#       HumanMessage {

#         "content": "what's my name?",

#         "additional_kwargs": {},

#         "response_metadata": {}

#       },

#       AIMessage {

#         "content": "Your name is Bob! How can I help you today, Bob?",

#         "additional_kwargs": {},

#         "response_metadata": {},

#         "tool_calls": [],

#         "invalid_tool_calls": []

#       }

#     ],

#     output: [32m"Your name is Bob! How can I help you today, Bob?"[39m

#   }

"""
Example LangSmith trace: https://smith.langchain.com/public/98c8d162-60ae-4493-aa9f-992d87bd0429/r
"""

"""
## Next steps

That's a wrap! In this quick start we covered how to create a simple agent. Agents are a complex topic, and there's lot to learn! 

:::{.callout-important}
This section covered building with LangChain Agents. LangChain Agents are fine for getting started, but past a certain point you will likely want flexibility and control that they do not offer. For working with more advanced agents, we'd recommend checking out [LangGraph](https://langchain-ai.github.io/langgraphjs).

You can also see [this guide to help migrate to LangGraph](/docs/how_to/migrate_agent).
:::
"""



================================================
FILE: docs/core_docs/docs/how_to/assign.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
keywords: [RunnablePassthrough, assign, LCEL]
---
"""

"""
# How to add values to a chain's state

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [LangChain Expression Language (LCEL)](/docs/concepts/lcel)
- [Chaining runnables](/docs/how_to/sequence/)
- [Calling runnables in parallel](/docs/how_to/parallel/)
- [Custom functions](/docs/how_to/functions/)
- [Passing data through](/docs/how_to/passthrough)

:::

An alternate way of [passing data through](/docs/how_to/passthrough) steps of a chain is to leave the current values of the chain state unchanged while assigning a new value under a given key. The [`RunnablePassthrough.assign()`](https://api.js.langchain.com/classes/langchain_core.runnables.RunnablePassthrough.html#assign-2) static method takes an input value and adds the extra arguments passed to the assign function.

This is useful in the common [LangChain Expression Language](/docs/concepts/lcel) pattern of additively creating a dictionary to use as input to a later step.

Here's an example:
"""

import { RunnableParallel, RunnablePassthrough } from "@langchain/core/runnables";

const runnable = RunnableParallel.from({
  extra: RunnablePassthrough.assign({
    mult: (input: { num: number }) => input.num * 3,
    modified: (input: { num: number }) => input.num + 1
  })
});

await runnable.invoke({ num: 1 });
# Output:
#   { extra: { num: [33m1[39m, mult: [33m3[39m, modified: [33m2[39m } }

"""
Let's break down what's happening here.

- The input to the chain is `{"num": 1}`. This is passed into a `RunnableParallel`, which invokes the runnables it is passed in parallel with that input.
- The value under the `extra` key is invoked. `RunnablePassthrough.assign()` keeps the original keys in the input dict (`{"num": 1}`), and assigns a new key called `mult`. The value is `lambda x: x["num"] * 3)`, which is `3`. Thus, the result is `{"num": 1, "mult": 3}`.
- `{"num": 1, "mult": 3}` is returned to the `RunnableParallel` call, and is set as the value to the key `extra`.
- At the same time, the `modified` key is called. The result is `2`, since the lambda extracts a key called `"num"` from its input and adds one.

Thus, the result is `{'extra': {'num': 1, 'mult': 3}, 'modified': 2}`.

## Streaming

One convenient feature of this method is that it allows values to pass through as soon as they are available. To show this off, we'll use `RunnablePassthrough.assign()` to immediately return source docs in a retrieval chain:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/openai @langchain/core
</Npm2Yarn>
```
"""

import { StringOutputParser } from "@langchain/core/output_parsers";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";
import { ChatOpenAI, OpenAIEmbeddings } from "@langchain/openai";
import { MemoryVectorStore } from "langchain/vectorstores/memory";

const vectorstore = await MemoryVectorStore.fromDocuments([
  { pageContent: "harrison worked at kensho", metadata: {} }
], new OpenAIEmbeddings());

const retriever = vectorstore.asRetriever();

const template = `Answer the question based only on the following context:
{context}

Question: {question}
`;

const prompt = ChatPromptTemplate.fromTemplate(template);

const model = new ChatOpenAI({ model: "gpt-4o" });

const generationChain = prompt.pipe(model).pipe(new StringOutputParser());

const retrievalChain = RunnableSequence.from([
  {
    context: retriever.pipe((docs) => docs[0].pageContent),
    question: new RunnablePassthrough()
  },
  RunnablePassthrough.assign({ output: generationChain }),
]);

const stream = await retrievalChain.stream("where did harrison work?");

for await (const chunk of stream) {
  console.log(chunk);
}
# Output:
#   { question: "where did harrison work?" }

#   { context: "harrison worked at kensho" }

#   { output: "" }

#   { output: "H" }

#   { output: "arrison" }

#   { output: " worked" }

#   { output: " at" }

#   { output: " Kens" }

#   { output: "ho" }

#   { output: "." }

#   { output: "" }


"""
We can see that the first chunk contains the original `"question"` since that is immediately available. The second chunk contains `"context"` since the retriever finishes second. Finally, the output from the `generation_chain` streams in chunks as soon as it is available.

## Next steps

Now you've learned how to pass data through your chains to help to help format the data flowing through your chains.

To learn more, see the other how-to guides on runnables in this section.
"""



================================================
FILE: docs/core_docs/docs/how_to/binding.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
keywords: [RunnableBinding, LCEL]
---
"""

"""
# How to attach runtime arguments to a Runnable

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [LangChain Expression Language (LCEL)](/docs/concepts/lcel)
- [Chaining runnables](/docs/how_to/sequence/)
- [Tool calling](/docs/how_to/tool_calling/)

:::

Sometimes we want to invoke a [`Runnable`](https://api.js.langchain.com/classes/langchain_core.runnables.Runnable.html) within a [RunnableSequence](https://api.js.langchain.com/classes/langchain_core.runnables.RunnableSequence.html) with constant arguments that are not part of the output of the preceding Runnable in the sequence, and which are not part of the user input. We can use the [`Runnable.bind()`](https://api.js.langchain.com/classes/langchain_core.runnables.Runnable.html#bind) method to set these arguments ahead of time.

## Binding stop sequences

Suppose we have a simple prompt + model chain:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/openai @langchain/core
</Npm2Yarn>
```
"""

import { StringOutputParser } from "@langchain/core/output_parsers";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { ChatOpenAI } from "@langchain/openai";

const prompt = ChatPromptTemplate.fromMessages(
    [
        [
            "system",
            "Write out the following equation using algebraic symbols then solve it. Use the format\n\nEQUATION:...\nSOLUTION:...\n\n",
        ],
        ["human", "{equation_statement}"],
    ]
)

const model = new ChatOpenAI({ temperature: 0 });

const runnable = prompt.pipe(model).pipe(new StringOutputParser());

const res = await runnable.invoke({
  equation_statement: "x raised to the third plus seven equals 12"
});

console.log(res);
# Output:
#   EQUATION: x^3 + 7 = 12

#   

#   SOLUTION: 

#   Subtract 7 from both sides:

#   x^3 = 5

#   

#   Take the cube root of both sides:

#   x = ∛5


"""
and want to call the model with certain `stop` words so that we shorten the output, which is useful in certain types of prompting techniques. While we can pass some arguments into the constructor, other runtime args use the `.bind()` method as follows:
"""

const runnableWithStop = prompt
  .pipe(model.bind({ stop: ["SOLUTION"] }))
  .pipe(new StringOutputParser());

const shorterResponse = await runnableWithStop.invoke({
  equation_statement: "x raised to the third plus seven equals 12"
});

console.log(shorterResponse);
# Output:
#   EQUATION: x^3 + 7 = 12

#   

#   


"""
What you can bind to a Runnable will depend on the extra parameters you can pass when invoking it.

## Attaching OpenAI tools

Another common use-case is tool calling. While you should generally use the [`.bind_tools()`](/docs/how_to/tool_calling/) method for tool-calling models, you can also bind provider-specific args directly if you want lower level control:
"""

const tools = [
  {
    "type": "function",
    "function": {
      "name": "get_current_weather",
      "description": "Get the current weather in a given location",
      "parameters": {
        "type": "object",
        "properties": {
          "location": {
            "type": "string",
            "description": "The city and state, e.g. San Francisco, CA",
          },
          "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
        },
        "required": ["location"],
      },
    },
  }
];

const modelWithTools = new ChatOpenAI({ model: "gpt-4o" }).bind({ tools });

await modelWithTools.invoke("What's the weather in SF, NYC and LA?")
# Output:
#   AIMessage {

#     lc_serializable: [33mtrue[39m,

#     lc_kwargs: {

#       content: [32m""[39m,

#       tool_calls: [

#         {

#           name: [32m"get_current_weather"[39m,

#           args: { location: [32m"San Francisco, CA"[39m },

#           id: [32m"call_iDKz4zU8PKBaaIT052fJkMMF"[39m

#         },

#         {

#           name: [32m"get_current_weather"[39m,

#           args: { location: [32m"New York, NY"[39m },

#           id: [32m"call_niQwZDOqO6OJTBiDBFG8FODc"[39m

#         },

#         {

#           name: [32m"get_current_weather"[39m,

#           args: { location: [32m"Los Angeles, CA"[39m },

#           id: [32m"call_zLXH2cDVQy0nAVC0ViWuEP4m"[39m

#         }

#       ],

#       invalid_tool_calls: [],

#       additional_kwargs: {

#         function_call: [90mundefined[39m,

#         tool_calls: [

#           {

#             id: [32m"call_iDKz4zU8PKBaaIT052fJkMMF"[39m,

#             type: [32m"function"[39m,

#             function: [36m[Object][39m

#           },

#           {

#             id: [32m"call_niQwZDOqO6OJTBiDBFG8FODc"[39m,

#             type: [32m"function"[39m,

#             function: [36m[Object][39m

#           },

#           {

#             id: [32m"call_zLXH2cDVQy0nAVC0ViWuEP4m"[39m,

#             type: [32m"function"[39m,

#             function: [36m[Object][39m

#           }

#         ]

#       },

#       response_metadata: {}

#     },

#     lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#     content: [32m""[39m,

#     name: [90mundefined[39m,

#     additional_kwargs: {

#       function_call: [90mundefined[39m,

#       tool_calls: [

#         {

#           id: [32m"call_iDKz4zU8PKBaaIT052fJkMMF"[39m,

#           type: [32m"function"[39m,

#           function: {

#             name: [32m"get_current_weather"[39m,

#             arguments: [32m'{"location": "San Francisco, CA"}'[39m

#           }

#         },

#         {

#           id: [32m"call_niQwZDOqO6OJTBiDBFG8FODc"[39m,

#           type: [32m"function"[39m,

#           function: {

#             name: [32m"get_current_weather"[39m,

#             arguments: [32m'{"location": "New York, NY"}'[39m

#           }

#         },

#         {

#           id: [32m"call_zLXH2cDVQy0nAVC0ViWuEP4m"[39m,

#           type: [32m"function"[39m,

#           function: {

#             name: [32m"get_current_weather"[39m,

#             arguments: [32m'{"location": "Los Angeles, CA"}'[39m

#           }

#         }

#       ]

#     },

#     response_metadata: {

#       tokenUsage: { completionTokens: [33m70[39m, promptTokens: [33m82[39m, totalTokens: [33m152[39m },

#       finish_reason: [32m"tool_calls"[39m

#     },

#     tool_calls: [

#       {

#         name: [32m"get_current_weather"[39m,

#         args: { location: [32m"San Francisco, CA"[39m },

#         id: [32m"call_iDKz4zU8PKBaaIT052fJkMMF"[39m

#       },

#       {

#         name: [32m"get_current_weather"[39m,

#         args: { location: [32m"New York, NY"[39m },

#         id: [32m"call_niQwZDOqO6OJTBiDBFG8FODc"[39m

#       },

#       {

#         name: [32m"get_current_weather"[39m,

#         args: { location: [32m"Los Angeles, CA"[39m },

#         id: [32m"call_zLXH2cDVQy0nAVC0ViWuEP4m"[39m

#       }

#     ],

#     invalid_tool_calls: []

#   }

"""
## Next steps

You now know how to bind runtime arguments to a Runnable.

Next, you might be interested in our how-to guides on [passing data through a chain](/docs/how_to/passthrough/).
"""



================================================
FILE: docs/core_docs/docs/how_to/caching_embeddings.mdx
================================================
import CodeBlock from "@theme/CodeBlock";
import InMemoryExample from "@examples/embeddings/cache_backed_in_memory.ts";
import RedisExample from "@examples/embeddings/cache_backed_redis.ts";

# How to cache embedding results

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Embeddings](/docs/concepts/embedding_models)

:::

Embeddings can be stored or temporarily cached to avoid needing to recompute them.

Caching embeddings can be done using a `CacheBackedEmbeddings` instance.

The cache backed embedder is a wrapper around an embedder that caches embeddings in a key-value store.

The text is hashed and the hash is used as the key in the cache.

The main supported way to initialized a `CacheBackedEmbeddings` is the `fromBytesStore` static method. This takes in the following parameters:

- `underlyingEmbeddings`: The embeddings model to use.
- `documentEmbeddingCache`: The cache to use for storing document embeddings.
- `namespace`: (optional, defaults to "") The namespace to use for document cache. This namespace is used to avoid collisions with other caches. For example, you could set it to the name of the embedding model used.

**Attention:** Be sure to set the namespace parameter to avoid collisions of the same text embedded using different embeddings models.

## In-memory

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

Here's a basic test example with an in memory cache. This type of cache is primarily useful for unit tests or prototyping.
Do not use this cache if you need to actually store the embeddings for an extended period of time:

<CodeBlock language="typescript">{InMemoryExample}</CodeBlock>

## Redis

Here's an example with a Redis cache.

You'll first need to install `ioredis` as a peer dependency and pass in an initialized client:

```bash npm2yarn
npm install ioredis
```

<CodeBlock language="typescript">{RedisExample}</CodeBlock>

## Next steps

You've now learned how to use caching to avoid recomputing embeddings.

Next, check out the [full tutorial on retrieval-augmented generation](/docs/tutorials/rag).



================================================
FILE: docs/core_docs/docs/how_to/callbacks_attach.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to attach callbacks to a module

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Callbacks](/docs/concepts/callbacks)
- [Chaining runnables](/docs/how_to/sequence)
- [Attach runtime arguments to a Runnable](/docs/how_to/binding)

:::

If you are composing a chain of runnables and want to reuse callbacks across multiple executions, you can attach callbacks with the [`.withConfig()`](https://api.js.langchain.com/classes/langchain_core.runnables.Runnable.html#withConfig) method. This saves you the need to pass callbacks in each time you invoke the chain.

Here's an example using LangChain's built-in [`ConsoleCallbackHandler`](https://api.js.langchain.com/classes/langchain_core.tracers_console.ConsoleCallbackHandler.html):
"""

import { ConsoleCallbackHandler } from "@langchain/core/tracers/console";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { ChatAnthropic } from "@langchain/anthropic";

const handler = new ConsoleCallbackHandler();

const prompt = ChatPromptTemplate.fromTemplate(`What is 1 + {number}?`);
const model = new ChatAnthropic({
  model: "claude-3-sonnet-20240229",
});

const chainWithCallbacks = prompt.pipe(model).withConfig({
  callbacks: [handler],
});

await chainWithCallbacks.invoke({ number: "2" });
# Output:
#   [32m[chain/start][39m [[90m[1m1:chain:RunnableSequence[22m[39m] Entering Chain run with input: {

#     "number": "2"

#   }

#   [32m[chain/start][39m [[90m1:chain:RunnableSequence > [1m2:prompt:ChatPromptTemplate[22m[39m] Entering Chain run with input: {

#     "number": "2"

#   }

#   [36m[chain/end][39m [[90m1:chain:RunnableSequence > [1m2:prompt:ChatPromptTemplate[22m[39m] [1ms] Exiting Chain run with output: {

#     "lc": 1,

#     "type": "constructor",

#     "id": [

#       "langchain_core",

#       "prompt_values",

#       "ChatPromptValue"

#     ],

#     "kwargs": {

#       "messages": [

#         {

#           "lc": 1,

#           "type": "constructor",

#           "id": [

#             "langchain_core",

#             "messages",

#             "HumanMessage"

#           ],

#           "kwargs": {

#             "content": "What is 1 + 2?",

#             "additional_kwargs": {},

#             "response_metadata": {}

#           }

#         }

#       ]

#     }

#   }

#   [32m[llm/start][39m [[90m1:chain:RunnableSequence > [1m3:llm:ChatAnthropic[22m[39m] Entering LLM run with input: {

#     "messages": [

#       [

#         {

#           "lc": 1,

#           "type": "constructor",

#           "id": [

#             "langchain_core",

#             "messages",

#             "HumanMessage"

#           ],

#           "kwargs": {

#             "content": "What is 1 + 2?",

#             "additional_kwargs": {},

#             "response_metadata": {}

#           }

#         }

#       ]

#     ]

#   }

#   [36m[llm/end][39m [[90m1:chain:RunnableSequence > [1m3:llm:ChatAnthropic[22m[39m] [797ms] Exiting LLM run with output: {

#     "generations": [

#       [

#         {

#           "text": "1 + 2 = 3",

#           "message": {

#             "lc": 1,

#             "type": "constructor",

#             "id": [

#               "langchain_core",

#               "messages",

#               "AIMessage"

#             ],

#             "kwargs": {

#               "content": "1 + 2 = 3",

#               "tool_calls": [],

#               "invalid_tool_calls": [],

#               "additional_kwargs": {

#                 "id": "msg_01WvZAqTg2hZzC4AKyeUaADs",

#                 "type": "message",

#                 "role": "assistant",

#                 "model": "claude-3-sonnet-20240229",

#                 "stop_sequence": null,

#                 "usage": {

#                   "input_tokens": 16,

#                   "output_tokens": 13

#                 },

#                 "stop_reason": "end_turn"

#               },

#               "response_metadata": {

#                 "id": "msg_01WvZAqTg2hZzC4AKyeUaADs",

#                 "model": "claude-3-sonnet-20240229",

#                 "stop_sequence": null,

#                 "usage": {

#                   "input_tokens": 16,

#                   "output_tokens": 13

#                 },

#                 "stop_reason": "end_turn"

#               }

#             }

#           }

#         }

#       ]

#     ],

#     "llmOutput": {

#       "id": "msg_01WvZAqTg2hZzC4AKyeUaADs",

#       "model": "claude-3-sonnet-20240229",

#       "stop_sequence": null,

#       "usage": {

#         "input_tokens": 16,

#         "output_tokens": 13

#       },

#       "stop_reason": "end_turn"

#     }

#   }

#   [36m[chain/end][39m [[90m[1m1:chain:RunnableSequence[22m[39m] [806ms] Exiting Chain run with output: {

#     "lc": 1,

#     "type": "constructor",

#     "id": [

#       "langchain_core",

#       "messages",

#       "AIMessage"

#     ],

#     "kwargs": {

#       "content": "1 + 2 = 3",

#       "tool_calls": [],

#       "invalid_tool_calls": [],

#       "additional_kwargs": {

#         "id": "msg_01WvZAqTg2hZzC4AKyeUaADs",

#         "type": "message",

#         "role": "assistant",

#         "model": "claude-3-sonnet-20240229",

#         "stop_sequence": null,

#         "usage": {

#           "input_tokens": 16,

#           "output_tokens": 13

#         },

#         "stop_reason": "end_turn"

#       },

#       "response_metadata": {

#         "id": "msg_01WvZAqTg2hZzC4AKyeUaADs",

#         "model": "claude-3-sonnet-20240229",

#         "stop_sequence": null,

#         "usage": {

#           "input_tokens": 16,

#           "output_tokens": 13

#         },

#         "stop_reason": "end_turn"

#       }

#     }

#   }

#   AIMessage {

#     lc_serializable: [33mtrue[39m,

#     lc_kwargs: {

#       content: [32m"1 + 2 = 3"[39m,

#       tool_calls: [],

#       invalid_tool_calls: [],

#       additional_kwargs: {

#         id: [32m"msg_01WvZAqTg2hZzC4AKyeUaADs"[39m,

#         type: [32m"message"[39m,

#         role: [32m"assistant"[39m,

#         model: [32m"claude-3-sonnet-20240229"[39m,

#         stop_sequence: [1mnull[22m,

#         usage: { input_tokens: [33m16[39m, output_tokens: [33m13[39m },

#         stop_reason: [32m"end_turn"[39m

#       },

#       response_metadata: {}

#     },

#     lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#     content: [32m"1 + 2 = 3"[39m,

#     name: [90mundefined[39m,

#     additional_kwargs: {

#       id: [32m"msg_01WvZAqTg2hZzC4AKyeUaADs"[39m,

#       type: [32m"message"[39m,

#       role: [32m"assistant"[39m,

#       model: [32m"claude-3-sonnet-20240229"[39m,

#       stop_sequence: [1mnull[22m,

#       usage: { input_tokens: [33m16[39m, output_tokens: [33m13[39m },

#       stop_reason: [32m"end_turn"[39m

#     },

#     response_metadata: {

#       id: [32m"msg_01WvZAqTg2hZzC4AKyeUaADs"[39m,

#       model: [32m"claude-3-sonnet-20240229"[39m,

#       stop_sequence: [1mnull[22m,

#       usage: { input_tokens: [33m16[39m, output_tokens: [33m13[39m },

#       stop_reason: [32m"end_turn"[39m

#     },

#     tool_calls: [],

#     invalid_tool_calls: []

#   }

"""
The bound callbacks will run for all nested module runs.

## Next steps

You've now learned how to bind callbacks to a chain.

Next, check out the other how-to guides in this section, such as how to create your own [custom callback handlers](/docs/how_to/custom_callbacks).
"""



================================================
FILE: docs/core_docs/docs/how_to/callbacks_constructor.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to pass callbacks into a module constructor

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Callbacks](/docs/concepts/callbacks)

:::

Most LangChain modules allow you to pass `callbacks` directly into the constructor. In this case, the callbacks will only be called for that instance (and any nested runs).

Here's an example using LangChain's built-in [`ConsoleCallbackHandler`](https://api.js.langchain.com/classes/langchain_core.tracers_console.ConsoleCallbackHandler.html):
"""

import { ConsoleCallbackHandler } from "@langchain/core/tracers/console";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { ChatAnthropic } from "@langchain/anthropic";

const handler = new ConsoleCallbackHandler();

const prompt = ChatPromptTemplate.fromTemplate(`What is 1 + {number}?`);
const model = new ChatAnthropic({
  model: "claude-3-sonnet-20240229",
  callbacks: [handler],
});

const chain = prompt.pipe(model);

await chain.invoke({ number: "2" });
# Output:
#   [32m[llm/start][39m [[90m[1m1:llm:ChatAnthropic[22m[39m] Entering LLM run with input: {

#     "messages": [

#       [

#         {

#           "lc": 1,

#           "type": "constructor",

#           "id": [

#             "langchain_core",

#             "messages",

#             "HumanMessage"

#           ],

#           "kwargs": {

#             "content": "What is 1 + 2?",

#             "additional_kwargs": {},

#             "response_metadata": {}

#           }

#         }

#       ]

#     ]

#   }

#   [36m[llm/end][39m [[90m[1m1:llm:ChatAnthropic[22m[39m] [1.00s] Exiting LLM run with output: {

#     "generations": [

#       [

#         {

#           "text": "1 + 2 = 3",

#           "message": {

#             "lc": 1,

#             "type": "constructor",

#             "id": [

#               "langchain_core",

#               "messages",

#               "AIMessage"

#             ],

#             "kwargs": {

#               "content": "1 + 2 = 3",

#               "tool_calls": [],

#               "invalid_tool_calls": [],

#               "additional_kwargs": {

#                 "id": "msg_011Z1cgi3gyNGxT55wnRNkXq",

#                 "type": "message",

#                 "role": "assistant",

#                 "model": "claude-3-sonnet-20240229",

#                 "stop_sequence": null,

#                 "usage": {

#                   "input_tokens": 16,

#                   "output_tokens": 13

#                 },

#                 "stop_reason": "end_turn"

#               },

#               "response_metadata": {

#                 "id": "msg_011Z1cgi3gyNGxT55wnRNkXq",

#                 "model": "claude-3-sonnet-20240229",

#                 "stop_sequence": null,

#                 "usage": {

#                   "input_tokens": 16,

#                   "output_tokens": 13

#                 },

#                 "stop_reason": "end_turn"

#               }

#             }

#           }

#         }

#       ]

#     ],

#     "llmOutput": {

#       "id": "msg_011Z1cgi3gyNGxT55wnRNkXq",

#       "model": "claude-3-sonnet-20240229",

#       "stop_sequence": null,

#       "usage": {

#         "input_tokens": 16,

#         "output_tokens": 13

#       },

#       "stop_reason": "end_turn"

#     }

#   }

#   AIMessage {

#     lc_serializable: [33mtrue[39m,

#     lc_kwargs: {

#       content: [32m"1 + 2 = 3"[39m,

#       tool_calls: [],

#       invalid_tool_calls: [],

#       additional_kwargs: {

#         id: [32m"msg_011Z1cgi3gyNGxT55wnRNkXq"[39m,

#         type: [32m"message"[39m,

#         role: [32m"assistant"[39m,

#         model: [32m"claude-3-sonnet-20240229"[39m,

#         stop_sequence: [1mnull[22m,

#         usage: { input_tokens: [33m16[39m, output_tokens: [33m13[39m },

#         stop_reason: [32m"end_turn"[39m

#       },

#       response_metadata: {}

#     },

#     lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#     content: [32m"1 + 2 = 3"[39m,

#     name: [90mundefined[39m,

#     additional_kwargs: {

#       id: [32m"msg_011Z1cgi3gyNGxT55wnRNkXq"[39m,

#       type: [32m"message"[39m,

#       role: [32m"assistant"[39m,

#       model: [32m"claude-3-sonnet-20240229"[39m,

#       stop_sequence: [1mnull[22m,

#       usage: { input_tokens: [33m16[39m, output_tokens: [33m13[39m },

#       stop_reason: [32m"end_turn"[39m

#     },

#     response_metadata: {

#       id: [32m"msg_011Z1cgi3gyNGxT55wnRNkXq"[39m,

#       model: [32m"claude-3-sonnet-20240229"[39m,

#       stop_sequence: [1mnull[22m,

#       usage: { input_tokens: [33m16[39m, output_tokens: [33m13[39m },

#       stop_reason: [32m"end_turn"[39m

#     },

#     tool_calls: [],

#     invalid_tool_calls: []

#   }

"""
You can see that we only see events from the chat model run - none from the prompt or broader chain.

## Next steps

You've now learned how to pass callbacks into a constructor.

Next, check out the other how-to guides in this section, such as how to create your own [custom callback handlers](/docs/how_to/custom_callbacks).
"""



================================================
FILE: docs/core_docs/docs/how_to/callbacks_custom_events.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to dispatch custom callback events

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Callbacks](/docs/concepts/callbacks)
- [Custom callback handlers](/docs/how_to/custom_callbacks)
- [Stream Events API](/docs/concepts/streaming#streamevents)

:::

In some situations, you may want to dipsatch a custom callback event from within a [Runnable](/docs/concepts/#runnable-interface) so it can be surfaced
in a custom callback handler or via the [Stream Events API](/docs/concepts/streaming#streamevents).

For example, if you have a long running tool with multiple steps, you can dispatch custom events between the steps and use these custom events to monitor progress.
You could also surface these custom events to an end user of your application to show them how the current task is progressing.

To dispatch a custom event you need to decide on two attributes for the event: the `name` and the `data`.

| Attribute | Type | Description                                                                                              |
|-----------|------|----------------------------------------------------------------------------------------------------------|
| name      | string  | A user defined name for the event.                                                                       |
| data      | any     | The data associated with the event. This can be anything, though we suggest making it JSON serializable. |


:::info
- Custom callback events can only be dispatched from within an existing `Runnable`.
- If using `streamEvents`, you must use `version: "v2"` to consume custom events.
- Sending or rendering custom callback events in LangSmith is not yet supported.
:::

## Stream Events API

The most useful way to consume custom events is via the [`.streamEvents()`](/docs/concepts/streaming#streamevents) method.

We can use the `dispatchCustomEvent` API to emit custom events from this method. 

```{=mdx}
:::caution Compatibility
Dispatching custom callback events requires `@langchain/core>=0.2.16`. See [this guide](/docs/how_to/installation/#installing-integration-packages) for some considerations to take when upgrading `@langchain/core`.

The default entrypoint below triggers an import and initialization of [`async_hooks`](https://nodejs.org/api/async_hooks.html) to enable automatic `RunnableConfig` passing, which is not supported in all environments. If you see import issues, you must import from `@langchain/core/callbacks/dispatch/web` and propagate the `RunnableConfig` object manually (see example below).
:::
```
"""

import { RunnableLambda } from "@langchain/core/runnables";
import { dispatchCustomEvent } from "@langchain/core/callbacks/dispatch";

const reflect = RunnableLambda.from(async (value: string) => {
  await dispatchCustomEvent("event1", { reversed: value.split("").reverse().join("") });
  await dispatchCustomEvent("event2", 5);
  return value;
});

const eventStream = await reflect.streamEvents("hello world", { version: "v2" });

for await (const event of eventStream) {
  if (event.event === "on_custom_event") {
    console.log(event);
  }
}
# Output:
#   {

#     event: 'on_custom_event',

#     run_id: '9eac217d-3a2d-4563-a91f-3bd49bee4b3d',

#     name: 'event1',

#     tags: [],

#     metadata: {},

#     data: { reversed: 'dlrow olleh' }

#   }

#   {

#     event: 'on_custom_event',

#     run_id: '9eac217d-3a2d-4563-a91f-3bd49bee4b3d',

#     name: 'event2',

#     tags: [],

#     metadata: {},

#     data: 5

#   }


"""
If you are in a web environment that does not support `async_hooks`, you must import from the web entrypoint and propagate the config manually instead:
"""

import { RunnableConfig, RunnableLambda } from "@langchain/core/runnables";
import { dispatchCustomEvent as dispatchCustomEventWeb } from "@langchain/core/callbacks/dispatch/web";

const reflect = RunnableLambda.from(async (value: string, config?: RunnableConfig) => {
  await dispatchCustomEventWeb("event1", { reversed: value.split("").reverse().join("") }, config);
  await dispatchCustomEventWeb("event2", 5, config);
  return value;
});

const eventStream = await reflect.streamEvents("hello world", { version: "v2" });

for await (const event of eventStream) {
  if (event.event === "on_custom_event") {
    console.log(event);
  }
}
# Output:
#   {

#     event: 'on_custom_event',

#     run_id: 'dee1e4f0-c5ff-4118-9391-461a0dcc4cb2',

#     name: 'event1',

#     tags: [],

#     metadata: {},

#     data: { reversed: 'dlrow olleh' }

#   }

#   {

#     event: 'on_custom_event',

#     run_id: 'dee1e4f0-c5ff-4118-9391-461a0dcc4cb2',

#     name: 'event2',

#     tags: [],

#     metadata: {},

#     data: 5

#   }


"""
## Callback Handler

Let's see how to emit custom events with `dispatchCustomEvent`.

Remember, you **must** call `dispatchCustomEvent` from within an existing `Runnable`.
"""

import { RunnableConfig, RunnableLambda } from "@langchain/core/runnables";
import { dispatchCustomEvent } from "@langchain/core/callbacks/dispatch";

const reflect = RunnableLambda.from(async (value: string) => {
  await dispatchCustomEvent("event1", { reversed: value.split("").reverse().join("") });
  await dispatchCustomEvent("event2", 5);
  return value;
});

await reflect.invoke("hello world", {
  callbacks: [{
    handleCustomEvent(eventName, data, runId) {
      console.log(eventName, data, runId);
    },
  }]
});
# Output:
#   event1 { reversed: 'dlrow olleh' } 9c3770ac-c83d-4626-9643-b5fd80eb5431

#   event2 5 9c3770ac-c83d-4626-9643-b5fd80eb5431

#   hello world


"""
## Related

You've now seen how to emit custom events from within your chains.

You can check out the more in depth guide for [stream events](/docs/how_to/streaming/#using-stream-events) for more ways to parse and receive intermediate steps from your chains.
"""



================================================
FILE: docs/core_docs/docs/how_to/callbacks_runtime.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to pass callbacks in at runtime

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Callbacks](/docs/concepts/callbacks)

:::

In many cases, it is advantageous to pass in handlers instead when running the object. When we pass through [`CallbackHandlers`](https://api.js.langchain.com/interfaces/langchain_core.callbacks_base.CallbackHandlerMethods.html) using the `callbacks` keyword arg when executing an run, those callbacks will be issued by all nested objects involved in the execution. For example, when a handler is passed through to an Agent, it will be used for all callbacks related to the agent and all the objects involved in the agent's execution, in this case, the Tools and LLM.

This prevents us from having to manually attach the handlers to each individual nested object. Here's an example using LangChain's built-in [`ConsoleCallbackHandler`](https://api.js.langchain.com/classes/langchain_core.tracers_console.ConsoleCallbackHandler.html):
"""

import { ConsoleCallbackHandler } from "@langchain/core/tracers/console";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { ChatAnthropic } from "@langchain/anthropic";

const handler = new ConsoleCallbackHandler();

const prompt = ChatPromptTemplate.fromTemplate(`What is 1 + {number}?`);
const model = new ChatAnthropic({
  model: "claude-3-sonnet-20240229",
});

const chain = prompt.pipe(model);

await chain.invoke({ number: "2" }, { callbacks: [handler] });
# Output:
#   [32m[chain/start][39m [[90m[1m1:chain:RunnableSequence[22m[39m] Entering Chain run with input: {

#     "number": "2"

#   }

#   [32m[chain/start][39m [[90m1:chain:RunnableSequence > [1m2:prompt:ChatPromptTemplate[22m[39m] Entering Chain run with input: {

#     "number": "2"

#   }

#   [36m[chain/end][39m [[90m1:chain:RunnableSequence > [1m2:prompt:ChatPromptTemplate[22m[39m] [1ms] Exiting Chain run with output: {

#     "lc": 1,

#     "type": "constructor",

#     "id": [

#       "langchain_core",

#       "prompt_values",

#       "ChatPromptValue"

#     ],

#     "kwargs": {

#       "messages": [

#         {

#           "lc": 1,

#           "type": "constructor",

#           "id": [

#             "langchain_core",

#             "messages",

#             "HumanMessage"

#           ],

#           "kwargs": {

#             "content": "What is 1 + 2?",

#             "additional_kwargs": {},

#             "response_metadata": {}

#           }

#         }

#       ]

#     }

#   }

#   [32m[llm/start][39m [[90m1:chain:RunnableSequence > [1m3:llm:ChatAnthropic[22m[39m] Entering LLM run with input: {

#     "messages": [

#       [

#         {

#           "lc": 1,

#           "type": "constructor",

#           "id": [

#             "langchain_core",

#             "messages",

#             "HumanMessage"

#           ],

#           "kwargs": {

#             "content": "What is 1 + 2?",

#             "additional_kwargs": {},

#             "response_metadata": {}

#           }

#         }

#       ]

#     ]

#   }

#   [36m[llm/end][39m [[90m1:chain:RunnableSequence > [1m3:llm:ChatAnthropic[22m[39m] [766ms] Exiting LLM run with output: {

#     "generations": [

#       [

#         {

#           "text": "1 + 2 = 3",

#           "message": {

#             "lc": 1,

#             "type": "constructor",

#             "id": [

#               "langchain_core",

#               "messages",

#               "AIMessage"

#             ],

#             "kwargs": {

#               "content": "1 + 2 = 3",

#               "tool_calls": [],

#               "invalid_tool_calls": [],

#               "additional_kwargs": {

#                 "id": "msg_01SGGkFVbUbH4fK7JS7agerD",

#                 "type": "message",

#                 "role": "assistant",

#                 "model": "claude-3-sonnet-20240229",

#                 "stop_sequence": null,

#                 "usage": {

#                   "input_tokens": 16,

#                   "output_tokens": 13

#                 },

#                 "stop_reason": "end_turn"

#               },

#               "response_metadata": {

#                 "id": "msg_01SGGkFVbUbH4fK7JS7agerD",

#                 "model": "claude-3-sonnet-20240229",

#                 "stop_sequence": null,

#                 "usage": {

#                   "input_tokens": 16,

#                   "output_tokens": 13

#                 },

#                 "stop_reason": "end_turn"

#               }

#             }

#           }

#         }

#       ]

#     ],

#     "llmOutput": {

#       "id": "msg_01SGGkFVbUbH4fK7JS7agerD",

#       "model": "claude-3-sonnet-20240229",

#       "stop_sequence": null,

#       "usage": {

#         "input_tokens": 16,

#         "output_tokens": 13

#       },

#       "stop_reason": "end_turn"

#     }

#   }

#   [36m[chain/end][39m [[90m[1m1:chain:RunnableSequence[22m[39m] [778ms] Exiting Chain run with output: {

#     "lc": 1,

#     "type": "constructor",

#     "id": [

#       "langchain_core",

#       "messages",

#       "AIMessage"

#     ],

#     "kwargs": {

#       "content": "1 + 2 = 3",

#       "tool_calls": [],

#       "invalid_tool_calls": [],

#       "additional_kwargs": {

#         "id": "msg_01SGGkFVbUbH4fK7JS7agerD",

#         "type": "message",

#         "role": "assistant",

#         "model": "claude-3-sonnet-20240229",

#         "stop_sequence": null,

#         "usage": {

#           "input_tokens": 16,

#           "output_tokens": 13

#         },

#         "stop_reason": "end_turn"

#       },

#       "response_metadata": {

#         "id": "msg_01SGGkFVbUbH4fK7JS7agerD",

#         "model": "claude-3-sonnet-20240229",

#         "stop_sequence": null,

#         "usage": {

#           "input_tokens": 16,

#           "output_tokens": 13

#         },

#         "stop_reason": "end_turn"

#       }

#     }

#   }

#   AIMessage {

#     lc_serializable: [33mtrue[39m,

#     lc_kwargs: {

#       content: [32m"1 + 2 = 3"[39m,

#       tool_calls: [],

#       invalid_tool_calls: [],

#       additional_kwargs: {

#         id: [32m"msg_01SGGkFVbUbH4fK7JS7agerD"[39m,

#         type: [32m"message"[39m,

#         role: [32m"assistant"[39m,

#         model: [32m"claude-3-sonnet-20240229"[39m,

#         stop_sequence: [1mnull[22m,

#         usage: { input_tokens: [33m16[39m, output_tokens: [33m13[39m },

#         stop_reason: [32m"end_turn"[39m

#       },

#       response_metadata: {}

#     },

#     lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#     content: [32m"1 + 2 = 3"[39m,

#     name: [90mundefined[39m,

#     additional_kwargs: {

#       id: [32m"msg_01SGGkFVbUbH4fK7JS7agerD"[39m,

#       type: [32m"message"[39m,

#       role: [32m"assistant"[39m,

#       model: [32m"claude-3-sonnet-20240229"[39m,

#       stop_sequence: [1mnull[22m,

#       usage: { input_tokens: [33m16[39m, output_tokens: [33m13[39m },

#       stop_reason: [32m"end_turn"[39m

#     },

#     response_metadata: {

#       id: [32m"msg_01SGGkFVbUbH4fK7JS7agerD"[39m,

#       model: [32m"claude-3-sonnet-20240229"[39m,

#       stop_sequence: [1mnull[22m,

#       usage: { input_tokens: [33m16[39m, output_tokens: [33m13[39m },

#       stop_reason: [32m"end_turn"[39m

#     },

#     tool_calls: [],

#     invalid_tool_calls: []

#   }

"""
If there are already existing callbacks associated with a module, these will run in addition to any passed in at runtime.

## Next steps

You've now learned how to pass callbacks at runtime.

Next, check out the other how-to guides in this section, such as how to create your own [custom callback handlers](/docs/how_to/custom_callbacks).
"""



================================================
FILE: docs/core_docs/docs/how_to/callbacks_serverless.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to await callbacks in serverless environments

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Callbacks](/docs/concepts/callbacks)

:::

As of `@langchain/core@0.3.0`, LangChain.js callbacks run in the background. This means that execution will **not** wait for the callback to either return before continuing. Prior to `0.3.0`, this behavior was the opposite.

If you are running code in [serverless environments](https://en.wikipedia.org/wiki/Serverless_computing) such as [AWS Lambda](https://aws.amazon.com/pm/lambda/) or [Cloudflare Workers](https://workers.cloudflare.com/) you should set your callbacks to be blocking to allow them time to finish or timeout.

To make callbacks blocking, set the `LANGCHAIN_CALLBACKS_BACKGROUND` environment variable to `"false"`. Alternatively, you can import the global [`awaitAllCallbacks`](https://api.js.langchain.com/functions/langchain_core.callbacks_promises.awaitAllCallbacks.html) method to ensure all callbacks finish if necessary.

To illustrate this, we'll create a [custom callback handler](/docs/how_to/custom_callbacks) that takes some time to resolve, and show the timing with and without `LANGCHAIN_CALLBACKS_BACKGROUND` set to `"false"`. Here it is without the variable set along with the `awaitAllCallbacks` global:
"""

import { RunnableLambda } from "@langchain/core/runnables";
import { awaitAllCallbacks } from "@langchain/core/callbacks/promises";

const runnable = RunnableLambda.from(() => "hello!");

const customHandler = {
  handleChainEnd: async () => {
    await new Promise((resolve) => setTimeout(resolve, 2000));
    console.log("Call finished");
  },
};

const startTime = new Date().getTime();

await runnable.invoke({ number: "2" }, { callbacks: [customHandler] });

console.log(`Elapsed time: ${new Date().getTime() - startTime}ms`);

await awaitAllCallbacks();

console.log(`Final elapsed time: ${new Date().getTime() - startTime}ms`);
# Output:
#   Elapsed time: 1ms

#   Call finished

#   Final elapsed time: 2164ms


"""
We can see that the initial `runnable.invoke()` call finishes in a short amount of time, and then roughly two seconds later, the callbacks finish.

And here it is with backgrounding off:
"""

process.env.LANGCHAIN_CALLBACKS_BACKGROUND = "false";

const startTimeBlocking = new Date().getTime();

await runnable.invoke({ number: "2" }, { callbacks: [customHandler] });

console.log(`Initial elapsed time: ${new Date().getTime() - startTimeBlocking}ms`);
# Output:
#   Call finished

#   Initial elapsed time: 2002ms


"""
This time, the initial call by itself takes two seconds because the `invoke()` call waits for the callback to return before returning.

## Next steps

You've now learned how to run callbacks in the background to reduce latency.

Next, check out the other how-to guides in this section, such as [how to create custom callback handlers](/docs/how_to/custom_callbacks).
"""



================================================
FILE: docs/core_docs/docs/how_to/cancel_execution.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to cancel execution

```{=mdx}
:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [LangChain Expression Language](/docs/concepts/lcel)
- [Chains](/docs/how_to/sequence/)
- [Streaming](/docs/how_to/streaming/)

:::
```

When building longer-running chains or [LangGraph](https://langchain-ai.github.io/langgraphjs/) agents, you may want to interrupt execution in situations such as a user leaving your app or submitting a new query.

[LangChain Expression Language (LCEL)](/docs/concepts/lcel) supports aborting runnables that are in-progress via a runtime [signal](https://developer.mozilla.org/en-US/docs/Web/API/AbortController/signal) option.

```{=mdx}
:::caution Compatibility

Built-in signal support requires `@langchain/core>=0.2.20`. Please see here for a [guide on upgrading](/docs/how_to/installation/#installing-integration-packages).

:::
```

**Note:** Individual integrations like chat models or retrievers may have missing or differing implementations for aborting execution. Signal support as described in this guide will apply in between steps of a chain.

To see how this works, construct a chain such as the one below that performs [retrieval-augmented generation](/docs/tutorials/rag). It answers questions by first searching the web using [Tavily](/docs/integrations/retrievers/tavily), then passing the results to a chat model to generate a final answer:

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs />
```
"""

// @lc-docs-hide-cell
import { ChatAnthropic } from "@langchain/anthropic";

const llm = new ChatAnthropic({
  model: "claude-3-5-sonnet-20240620",
});

import { TavilySearchAPIRetriever } from "@langchain/community/retrievers/tavily_search_api";
import type { Document } from "@langchain/core/documents";
import { StringOutputParser } from "@langchain/core/output_parsers";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";

const formatDocsAsString = (docs: Document[]) => {
  return docs.map((doc) => doc.pageContent).join("\n\n")
}

const retriever = new TavilySearchAPIRetriever({
  k: 3,
});

const prompt = ChatPromptTemplate.fromTemplate(`
Use the following context to answer questions to the best of your ability:

<context>
{context}
</context>

Question: {question}`)

const chain = RunnableSequence.from([
  {
    context: retriever.pipe(formatDocsAsString),
    question: new RunnablePassthrough(),
  },
  prompt,
  llm,
  new StringOutputParser(),
]);

"""
If you invoke it normally, you can see it returns up-to-date information:
"""

await chain.invoke("what is the current weather in SF?");
# Output:
#   Based on the provided context, the current weather in San Francisco is:

#   

#   Temperature: 17.6°C (63.7°F)

#   Condition: Sunny

#   Wind: 14.4 km/h (8.9 mph) from WSW direction

#   Humidity: 74%

#   Cloud cover: 15%

#   

#   The information indicates it's a sunny day with mild temperatures and light winds. The data appears to be from August 2, 2024, at 17:00 local time.


"""
Now, let's interrupt it early. Initialize an [`AbortController`](https://developer.mozilla.org/en-US/docs/Web/API/AbortController) and pass its `signal` property into the chain execution. To illustrate the fact that the cancellation occurs as soon as possible, set a timeout of 100ms:
"""

const controller = new AbortController();

const startTimer = console.time("timer1");

setTimeout(() => controller.abort(), 100);

try {
  await chain.invoke("what is the current weather in SF?", {
    signal: controller.signal,
  });
} catch (e) {
  console.log(e);
}

console.timeEnd("timer1");
# Output:
#   Error: Aborted

#       at EventTarget.<anonymous> (/Users/jacoblee/langchain/langchainjs/langchain-core/dist/utils/signal.cjs:19:24)

#       at [nodejs.internal.kHybridDispatch] (node:internal/event_target:825:20)

#       at EventTarget.dispatchEvent (node:internal/event_target:760:26)

#       at abortSignal (node:internal/abort_controller:370:10)

#       at AbortController.abort (node:internal/abort_controller:392:5)

#       at Timeout._onTimeout (evalmachine.<anonymous>:7:29)

#       at listOnTimeout (node:internal/timers:573:17)

#       at process.processTimers (node:internal/timers:514:7)

#   timer1: 103.204ms


"""
And you can see that execution ends after just over 100ms. Looking at [this LangSmith trace](https://smith.langchain.com/public/63c04c3b-2683-4b73-a4f7-fb12f5cb9180/r), you can see that the model is never called.

## Streaming

You can pass a `signal` when streaming too. This gives you more control over using a `break` statement within the `for await... of` loop to cancel the current run, which will only trigger after final output has already started streaming. The below example uses a `break` statement - note the time elapsed before cancellation occurs:
"""

const startTimer2 = console.time("timer2");

const stream = await chain.stream("what is the current weather in SF?");

for await (const chunk of stream) {
  console.log("chunk", chunk);
  break;
}

console.timeEnd("timer2");
# Output:
#   chunk 

#   timer2: 3.990s


"""
Now compare this to using a signal. Note that you will need to wrap the stream in a `try/catch` block:
"""

const controllerForStream = new AbortController();

const startTimer3 = console.time("timer3");

setTimeout(() => controllerForStream.abort(), 100);

try {
  const streamWithSignal = await chain.stream("what is the current weather in SF?", {
    signal: controllerForStream.signal
  });
  for await (const chunk of streamWithSignal) {
    console.log(chunk);
    break;
  } 
} catch (e) {
  console.log(e);  
}

console.timeEnd("timer3");
# Output:
#   Error: Aborted

#       at EventTarget.<anonymous> (/Users/jacoblee/langchain/langchainjs/langchain-core/dist/utils/signal.cjs:19:24)

#       at [nodejs.internal.kHybridDispatch] (node:internal/event_target:825:20)

#       at EventTarget.dispatchEvent (node:internal/event_target:760:26)

#       at abortSignal (node:internal/abort_controller:370:10)

#       at AbortController.abort (node:internal/abort_controller:392:5)

#       at Timeout._onTimeout (evalmachine.<anonymous>:7:38)

#       at listOnTimeout (node:internal/timers:573:17)

#       at process.processTimers (node:internal/timers:514:7)

#   timer3: 100.684ms


"""
## Related

- [Pass through arguments from one step to the next](/docs/how_to/passthrough)
- [Dispatching custom events](/docs/how_to/callbacks_custom_events)
"""



================================================
FILE: docs/core_docs/docs/how_to/character_text_splitter.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to split by character

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Text splitters](/docs/concepts/text_splitters)

:::

This is the simplest method for splitting text. This splits based on a given character sequence, which defaults to `"\n\n"`. Chunk length is measured by number of characters.

1. How the text is split: by single character separator.
2. How the chunk size is measured: by number of characters.

To obtain the string content directly, use `.splitText()`.

To create LangChain [Document](https://api.js.langchain.com/classes/langchain_core.documents.Document.html) objects (e.g., for use in downstream tasks), use `.createDocuments()`.
"""

import { CharacterTextSplitter } from "@langchain/textsplitters";
import * as fs from "node:fs";

// Load an example document
const rawData = await fs.readFileSync("../../../../examples/state_of_the_union.txt");
const stateOfTheUnion = rawData.toString();

const textSplitter = new CharacterTextSplitter({
    separator: "\n\n",
    chunkSize: 1000,
    chunkOverlap: 200,
});
const texts = await textSplitter.createDocuments([stateOfTheUnion]);
console.log(texts[0])
# Output:
#   Document {

#     pageContent: "Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and th"... 839 more characters,

#     metadata: { loc: { lines: { from: 1, to: 17 } } }

#   }


"""
You can also propagate metadata associated with each document to the output chunks:
"""

const metadatas = [{ document: 1 }, { document: 2 }];

const documents = await textSplitter.createDocuments(
    [stateOfTheUnion, stateOfTheUnion], metadatas
)

console.log(documents[0])
# Output:
#   Document {

#     pageContent: "Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and th"... 839 more characters,

#     metadata: { document: 1, loc: { lines: { from: 1, to: 17 } } }

#   }


"""
To obtain the string content directly, use `.splitText()`:
"""

const chunks = await textSplitter.splitText(stateOfTheUnion);

chunks[0];
# Output:
#   [32m"Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and th"[39m... 839 more characters

"""
## Next steps

You've now learned a method for splitting text by character.

Next, check out a [more advanced way of splitting by character](/docs/how_to/recursive_text_splitter), or the [full tutorial on retrieval-augmented generation](/docs/tutorials/rag).
"""



================================================
FILE: docs/core_docs/docs/how_to/chat_model_caching.mdx
================================================
---
sidebar_position: 3
---

# How to cache chat model responses

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat models](/docs/concepts/chat_models)
- [LLMs](/docs/concepts/text_llms)

:::

LangChain provides an optional caching layer for chat models. This is useful for two reasons:

It can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times.
It can speed up your application by reducing the number of API calls you make to the LLM provider.

import CodeBlock from "@theme/CodeBlock";

```typescript
import { ChatOpenAI } from "@langchain/openai";

// To make the caching really obvious, lets use a slower model.
const model = new ChatOpenAI({
  model: "gpt-4",
  cache: true,
});
```

## In Memory Cache

The default cache is stored in-memory. This means that if you restart your application, the cache will be cleared.

```typescript
console.time();

// The first time, it is not yet in cache, so it should take longer
const res = await model.invoke("Tell me a joke!");
console.log(res);

console.timeEnd();

/*
  AIMessage {
    lc_serializable: true,
    lc_kwargs: {
      content: "Why don't scientists trust atoms?\n\nBecause they make up everything!",
      additional_kwargs: { function_call: undefined, tool_calls: undefined }
    },
    lc_namespace: [ 'langchain_core', 'messages' ],
    content: "Why don't scientists trust atoms?\n\nBecause they make up everything!",
    name: undefined,
    additional_kwargs: { function_call: undefined, tool_calls: undefined }
  }
  default: 2.224s
*/
```

```typescript
console.time();

// The second time it is, so it goes faster
const res2 = await model.invoke("Tell me a joke!");
console.log(res2);

console.timeEnd();
/*
  AIMessage {
    lc_serializable: true,
    lc_kwargs: {
      content: "Why don't scientists trust atoms?\n\nBecause they make up everything!",
      additional_kwargs: { function_call: undefined, tool_calls: undefined }
    },
    lc_namespace: [ 'langchain_core', 'messages' ],
    content: "Why don't scientists trust atoms?\n\nBecause they make up everything!",
    name: undefined,
    additional_kwargs: { function_call: undefined, tool_calls: undefined }
  }
  default: 181.98ms
*/
```

## Caching with Redis

LangChain also provides a Redis-based cache. This is useful if you want to share the cache across multiple processes or servers.
To use it, you'll need to install the `redis` package:

```bash npm2yarn
npm install ioredis @langchain/community @langchain/core
```

Then, you can pass a `cache` option when you instantiate the LLM. For example:

import RedisCacheExample from "@examples/cache/chat_models/redis.ts";

<CodeBlock language="typescript">{RedisCacheExample}</CodeBlock>

## Caching with Upstash Redis

LangChain provides an Upstash Redis-based cache. Like the Redis-based cache, this cache is useful if you want to share the cache across multiple processes or servers. The Upstash Redis client uses HTTP and supports edge environments. To use it, you'll need to install the `@upstash/redis` package:

```bash npm2yarn
npm install @upstash/redis
```

You'll also need an [Upstash account](https://docs.upstash.com/redis#create-account) and a [Redis database](https://docs.upstash.com/redis#create-a-database) to connect to. Once you've done that, retrieve your REST URL and REST token.

Then, you can pass a `cache` option when you instantiate the LLM. For example:

import UpstashRedisCacheExample from "@examples/cache/chat_models/upstash_redis.ts";

<CodeBlock language="typescript">{UpstashRedisCacheExample}</CodeBlock>

You can also directly pass in a previously created [@upstash/redis](https://docs.upstash.com/redis/sdks/javascriptsdk/overview) client instance:

import AdvancedUpstashRedisCacheExample from "@examples/cache/chat_models/upstash_redis_advanced.ts";

<CodeBlock language="typescript">{AdvancedUpstashRedisCacheExample}</CodeBlock>

## Caching with Vercel KV

LangChain provides an Vercel KV-based cache. Like the Redis-based cache, this cache is useful if you want to share the cache across multiple processes or servers. The Vercel KV client uses HTTP and supports edge environments. To use it, you'll need to install the `@vercel/kv` package:

```bash npm2yarn
npm install @vercel/kv
```

You'll also need an Vercel account and a [KV database](https://vercel.com/docs/storage/vercel-kv/kv-reference) to connect to. Once you've done that, retrieve your REST URL and REST token.

Then, you can pass a `cache` option when you instantiate the LLM. For example:

import VercelKVCacheExample from "@examples/cache/chat_models/vercel_kv.ts";

<CodeBlock language="typescript">{VercelKVCacheExample}</CodeBlock>

## Caching with Cloudflare KV

:::info
This integration is only supported in Cloudflare Workers.
:::

If you're deploying your project as a Cloudflare Worker, you can use LangChain's Cloudflare KV-powered LLM cache.

For information on how to set up KV in Cloudflare, see [the official documentation](https://developers.cloudflare.com/kv/).

**Note:** If you are using TypeScript, you may need to install types if they aren't already present:

```bash npm2yarn
npm install -S @cloudflare/workers-types
```

import CloudflareExample from "@examples/cache/chat_models/cloudflare_kv.ts";

<CodeBlock language="typescript">{CloudflareExample}</CodeBlock>

## Caching on the File System

:::warning
This cache is not recommended for production use. It is only intended for local development.
:::

LangChain provides a simple file system cache.
By default the cache is stored a temporary directory, but you can specify a custom directory if you want.

```typescript
const cache = await LocalFileCache.create();
```

## Next steps

You've now learned how to cache model responses to save time and money.

Next, check out the other how-to guides on chat models, like [how to get a model to return structured output](/docs/how_to/structured_output) or [how to create your own custom chat model](/docs/how_to/custom_chat).



================================================
FILE: docs/core_docs/docs/how_to/chat_models_universal_init.mdx
================================================
# How to init any model in one line

import CodeBlock from "@theme/CodeBlock";

Many LLM applications let end users specify what model provider and model they want the application to be powered by.
This requires writing some logic to initialize different ChatModels based on some user configuration.
The `initChatModel()` helper method makes it easy to initialize a number of different model integrations without having to worry about import paths and class names.
Keep in mind this feature is only for chat models.

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat models](/docs/concepts/chat_models)

- [LangChain Expression Language (LCEL)](/docs/concepts/lcel)

- [Tool calling](/docs/concepts/tools)

:::

:::caution Compatibility
**This feature is only intended to be used in Node environments. Use in non Node environments or with bundlers is not guaranteed to work and not officially supported.**

`initChatModel` requires `langchain>=0.2.11`. See [this guide](/docs/how_to/installation/#installing-integration-packages) for some considerations to take when upgrading.

See the [initChatModel()](https://api.js.langchain.com/functions/langchain.chat_models_universal.initChatModel.html) API reference for a full list of supported integrations.

Make sure you have the integration packages installed for any model providers you want to support. E.g. you should have `@langchain/openai` installed to init an OpenAI model.
:::

## Basic usage

import BasicExample from "@examples/models/chat/configurable/basic.ts";

<CodeBlock language="typescript">{BasicExample}</CodeBlock>

## Inferring model provider

For common and distinct model names `initChatModel()` will attempt to infer the model provider.
See the [API reference](https://api.js.langchain.com/functions/langchain.chat_models_universal.initChatModel.html) for a full list of inference behavior.
E.g. any model that starts with `gpt-3...` or `gpt-4...` will be inferred as using model provider `openai`.

import InferringProviderExample from "@examples/models/chat/configurable/inferring_model_provider.ts";

<CodeBlock language="typescript">{InferringProviderExample}</CodeBlock>

## Creating a configurable model

You can also create a runtime-configurable model by specifying `configurableFields`.
If you don't specify a `model` value, then "model" and "modelProvider" be configurable by default.

import ConfigurableModelExample from "@examples/models/chat/configurable/configurable_model.ts";

<CodeBlock language="typescript">{ConfigurableModelExample}</CodeBlock>

### Configurable model with default values

We can create a configurable model with default model values, specify which parameters are configurable, and add prefixes to configurable params:

import ConfigurableModelWithDefaultsExample from "@examples/models/chat/configurable/configurable_model_with_defaults.ts";

<CodeBlock language="typescript">
  {ConfigurableModelWithDefaultsExample}
</CodeBlock>

### Using a configurable model declaratively

We can call declarative operations like `bindTools`, `withStructuredOutput`, `withConfig`, etc. on a configurable model and chain a configurable model in the same way that we would a regularly instantiated chat model object.

import ConfigurableModelDeclarativelyExample from "@examples/models/chat/configurable/configurable_model_declaratively.ts";

<CodeBlock language="typescript">
  {ConfigurableModelDeclarativelyExample}
</CodeBlock>



================================================
FILE: docs/core_docs/docs/how_to/chat_streaming.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_position: 1.5
---
"""

"""
# How to stream chat model responses

All [chat models](https://api.js.langchain.com/classes/langchain_core.language_models_chat_models.BaseChatModel.html) implement the [Runnable interface](https://.api.js.langchain.com/classes/langchain_core.runnables.Runnable.html), which comes with **default** implementations of standard runnable methods (i.e. `invoke`, `batch`, `stream`, `streamEvents`). This guide covers how to use these methods to stream output from chat models.

:::{.callout-tip}

The **default** implementation does **not** provide support for token-by-token streaming, and will instead return an [`AsyncGenerator`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/AsyncGenerator) that will yield all model output in a single chunk. It exists to ensures that the the model can be swapped in for any other model as it supports the same standard interface.

The ability to stream the output token-by-token depends on whether the provider has implemented token-by-token streaming support.

You can see which [integrations support token-by-token streaming here](/docs/integrations/chat/).

:::
"""

"""
## Streaming

Below, we use a `---` to help visualize the delimiter between tokens.
"""

"""
```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs />
```
"""

// @lc-docs-hide-cell
import { ChatAnthropic } from "@langchain/anthropic";

const model = new ChatAnthropic({
  model: "claude-3-5-sonnet-20240620",
});

const stream = await model.stream("Write me a 1 verse song about goldfish on the moon")

for await (const chunk of stream) {
  console.log(`${chunk.content}\n---`);
}
# Output:
#   

#   ---

#   Here's

#   ---

#    a one

#   ---

#   -

#   ---

#   verse song about goldfish on

#   ---

#    the moon:

#   

#   Verse

#   ---

#   :

#   Swimming

#   ---

#    through the stars

#   ---

#   ,

#   ---

#    in

#   ---

#    a cosmic

#   ---

#    lag

#   ---

#   oon

#   ---

#   

#   Little

#   ---

#    golden

#   ---

#    scales

#   ---

#   ,

#   ---

#    reflecting the moon

#   ---

#   

#   No

#   ---

#    gravity to

#   ---

#    hold them,

#   ---

#    they

#   ---

#    float with

#   ---

#    glee

#   Goldfish

#   ---

#    astron

#   ---

#   auts, on a lunar

#   ---

#    sp

#   ---

#   ree

#   ---

#   

#   Bub

#   ---

#   bles rise

#   ---

#    like

#   ---

#    com

#   ---

#   ets, in the

#   ---

#    star

#   ---

#   ry night

#   ---

#   

#   Their fins like

#   ---

#    tiny

#   ---

#    rockets, a

#   ---

#    w

#   ---

#   ondrous sight

#   Who

#   ---

#    knew

#   ---

#    these

#   ---

#    small

#   ---

#    creatures

#   ---

#   ,

#   ---

#    could con

#   ---

#   quer space?

#   ---

#   

#   Goldfish on the moon,

#   ---

#    with

#   ---

#    such

#   ---

#    fis

#   ---

#   hy grace

#   ---

#   

#   ---

#   

#   ---


"""
## Stream events

Chat models also support the standard [`streamEvents()`](https://api.js.langchain.com/classes/langchain_core.runnables.Runnable.html#streamEvents) method to stream more granular events from within chains.

This method is useful if you're streaming output from a larger LLM application that contains multiple steps (e.g., a chain composed of a prompt, chat model and parser):
"""

const eventStream = await model.streamEvents(
  "Write me a 1 verse song about goldfish on the moon",
  {
    version: "v2"
  },
);

const events = [];
for await (const event of eventStream) {
  events.push(event);
}

events.slice(0, 3);
# Output:
#   [

#     {

#       event: [32m"on_chat_model_start"[39m,

#       data: { input: [32m"Write me a 1 verse song about goldfish on the moon"[39m },

#       name: [32m"ChatAnthropic"[39m,

#       tags: [],

#       run_id: [32m"d60a87d6-acf0-4ae1-bf27-e570aa101960"[39m,

#       metadata: {

#         ls_provider: [32m"openai"[39m,

#         ls_model_name: [32m"claude-3-5-sonnet-20240620"[39m,

#         ls_model_type: [32m"chat"[39m,

#         ls_temperature: [33m1[39m,

#         ls_max_tokens: [33m2048[39m,

#         ls_stop: [90mundefined[39m

#       }

#     },

#     {

#       event: [32m"on_chat_model_stream"[39m,

#       run_id: [32m"d60a87d6-acf0-4ae1-bf27-e570aa101960"[39m,

#       name: [32m"ChatAnthropic"[39m,

#       tags: [],

#       metadata: {

#         ls_provider: [32m"openai"[39m,

#         ls_model_name: [32m"claude-3-5-sonnet-20240620"[39m,

#         ls_model_type: [32m"chat"[39m,

#         ls_temperature: [33m1[39m,

#         ls_max_tokens: [33m2048[39m,

#         ls_stop: [90mundefined[39m

#       },

#       data: {

#         chunk: AIMessageChunk {

#           lc_serializable: [33mtrue[39m,

#           lc_kwargs: {

#             content: [32m""[39m,

#             additional_kwargs: [36m[Object][39m,

#             tool_calls: [],

#             invalid_tool_calls: [],

#             tool_call_chunks: [],

#             response_metadata: {}

#           },

#           lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#           content: [32m""[39m,

#           name: [90mundefined[39m,

#           additional_kwargs: {

#             id: [32m"msg_01JaaH9ZUXg7bUnxzktypRak"[39m,

#             type: [32m"message"[39m,

#             role: [32m"assistant"[39m,

#             model: [32m"claude-3-5-sonnet-20240620"[39m

#           },

#           response_metadata: {},

#           id: [90mundefined[39m,

#           tool_calls: [],

#           invalid_tool_calls: [],

#           tool_call_chunks: [],

#           usage_metadata: [90mundefined[39m

#         }

#       }

#     },

#     {

#       event: [32m"on_chat_model_stream"[39m,

#       run_id: [32m"d60a87d6-acf0-4ae1-bf27-e570aa101960"[39m,

#       name: [32m"ChatAnthropic"[39m,

#       tags: [],

#       metadata: {

#         ls_provider: [32m"openai"[39m,

#         ls_model_name: [32m"claude-3-5-sonnet-20240620"[39m,

#         ls_model_type: [32m"chat"[39m,

#         ls_temperature: [33m1[39m,

#         ls_max_tokens: [33m2048[39m,

#         ls_stop: [90mundefined[39m

#       },

#       data: {

#         chunk: AIMessageChunk {

#           lc_serializable: [33mtrue[39m,

#           lc_kwargs: {

#             content: [32m"Here's"[39m,

#             additional_kwargs: {},

#             tool_calls: [],

#             invalid_tool_calls: [],

#             tool_call_chunks: [],

#             response_metadata: {}

#           },

#           lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#           content: [32m"Here's"[39m,

#           name: [90mundefined[39m,

#           additional_kwargs: {},

#           response_metadata: {},

#           id: [90mundefined[39m,

#           tool_calls: [],

#           invalid_tool_calls: [],

#           tool_call_chunks: [],

#           usage_metadata: [90mundefined[39m

#         }

#       }

#     }

#   ]

"""
## Next steps

You've now seen a few ways you can stream chat model responses.

Next, check out this guide for more on [streaming with other LangChain modules](/docs/how_to/streaming).
"""



================================================
FILE: docs/core_docs/docs/how_to/chat_token_usage_tracking.mdx
================================================
---
sidebar_position: 5
---

# How to track token usage

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat models](/docs/concepts/chat_models)

:::

This notebook goes over how to track your token usage for specific calls.

## Using `AIMessage.usage_metadata`

A number of model providers return token usage information as part of the chat generation response. When available, this information will be included on the `AIMessage` objects produced by the corresponding model.

LangChain `AIMessage` objects include a [`usage_metadata`](https://api.js.langchain.com/classes/langchain_core.messages.AIMessage.html#usage_metadata) attribute for supported providers. When populated, this attribute will be an object with standard keys (e.g., "input_tokens" and "output_tokens").

#### OpenAI

import CodeBlock from "@theme/CodeBlock";
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core
```

import UsageMetadataExample from "@examples/models/chat/usage_metadata.ts";

<CodeBlock language="typescript">{UsageMetadataExample}</CodeBlock>

#### Anthropic

```bash npm2yarn
npm install @langchain/anthropic @langchain/core
```

import UsageMetadataExampleAnthropic from "@examples/models/chat/usage_metadata_anthropic.ts";

<CodeBlock language="typescript">{UsageMetadataExampleAnthropic}</CodeBlock>

## Using `AIMessage.response_metadata`

A number of model providers return token usage information as part of the chat generation response. When available, this is included in the `AIMessage.response_metadata` field.

#### OpenAI

import Example from "@examples/models/chat/token_usage_tracking.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

#### Anthropic

import AnthropicExample from "@examples/models/chat/token_usage_tracking_anthropic.ts";

<CodeBlock language="typescript">{AnthropicExample}</CodeBlock>

## Streaming

Some providers support token count metadata in a streaming context.

#### OpenAI

For example, OpenAI will return a message chunk at the end of a stream with token usage information. This behavior is supported by `@langchain/openai` >= 0.1.0 and can be enabled by passing a `stream_options` parameter when making your call.

:::info
By default, the last message chunk in a stream will include a `finish_reason` in the message's `response_metadata` attribute. If we include token usage in streaming mode, an additional chunk containing usage metadata will be added to the end of the stream, such that `finish_reason` appears on the second to last message chunk.
:::

import OpenAIStreamTokens from "@examples/models/chat/integration_openai_stream_tokens.ts";

<CodeBlock language="typescript">{OpenAIStreamTokens}</CodeBlock>

## Using callbacks

You can also use the `handleLLMEnd` callback to get the full output from the LLM, including token usage for supported models.
Here's an example of how you could do that:

import CallbackExample from "@examples/models/chat/token_usage_tracking_callback.ts";

<CodeBlock language="typescript">{CallbackExample}</CodeBlock>

## Next steps

You've now seen a few examples of how to track chat model token usage for supported providers.

Next, check out the other how-to guides on chat models in this section, like [how to get a model to return structured output](/docs/how_to/structured_output) or [how to add caching to your chat models](/docs/how_to/chat_model_caching).



================================================
FILE: docs/core_docs/docs/how_to/chatbots_memory.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_position: 1
---
"""

"""
# How to add memory to chatbots

A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:

- Simply stuffing previous messages into a chat model prompt.
- The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.
- More complex modifications like synthesizing summaries for long running conversations.

We'll go into more detail on a few techniques below!

:::note

This how-to guide previously built a chatbot using [RunnableWithMessageHistory](https://v03.api.js.langchain.com/classes/_langchain_core.runnables.RunnableWithMessageHistory.html). You can access this version of the tutorial in the [v0.2 docs](https://js.langchain.com/v0.2/docs/how_to/chatbots_memory/).

The LangGraph implementation offers a number of advantages over `RunnableWithMessageHistory`, including the ability to persist arbitrary components of an application's state (instead of only messages).

:::

## Setup

You'll need to install a few packages, select your chat model, and set its enviroment variable.

```{=mdx}
import Npm2Yarn from "@theme/Npm2Yarn"

<Npm2Yarn>
  @langchain/core @langchain/langgraph
</Npm2Yarn>
```

Let's set up a chat model that we'll use for the below examples.

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs />
```
"""

"""
## Message passing

The simplest form of memory is simply passing chat history messages into a chain. Here's an example:
"""

// @lc-docs-hide-cell

import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({ model: "gpt-4o" })

import { HumanMessage, AIMessage } from "@langchain/core/messages";
import {
  ChatPromptTemplate,
  MessagesPlaceholder,
} from "@langchain/core/prompts";

const prompt = ChatPromptTemplate.fromMessages([
  [
    "system",
    "You are a helpful assistant. Answer all questions to the best of your ability.",
  ],
  new MessagesPlaceholder("messages"),
]);

const chain = prompt.pipe(llm);

await chain.invoke({
  messages: [
    new HumanMessage(
      "Translate this sentence from English to French: I love programming."
    ),
    new AIMessage("J'adore la programmation."),
    new HumanMessage("What did you just say?"),
  ],
});
# Output:
#   AIMessage {

#     "id": "chatcmpl-ABSxUXVIBitFRBh9MpasB5jeEHfCA",

#     "content": "I said \"J'adore la programmation,\" which means \"I love programming\" in French.",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "completionTokens": 18,

#         "promptTokens": 58,

#         "totalTokens": 76

#       },

#       "finish_reason": "stop",

#       "system_fingerprint": "fp_e375328146"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 58,

#       "output_tokens": 18,

#       "total_tokens": 76

#     }

#   }


"""
We can see that by passing the previous conversation into a chain, it can use it as context to answer questions. This is the basic concept underpinning chatbot memory - the rest of the guide will demonstrate convenient techniques for passing or reformatting messages.
"""

"""
## Automatic history management

The previous examples pass messages to the chain (and model) explicitly. This is a completely acceptable approach, but it does require external management of new messages. LangChain also provides a way to build applications that have memory using LangGraph's persistence. You can enable persistence in LangGraph applications by providing a `checkpointer` when compiling the graph.
"""

import { START, END, MessagesAnnotation, StateGraph, MemorySaver } from "@langchain/langgraph";


// Define the function that calls the model
const callModel = async (state: typeof MessagesAnnotation.State) => {
  const systemPrompt = 
    "You are a helpful assistant. " +
    "Answer all questions to the best of your ability.";
  const messages = [{ role: "system", content: systemPrompt }, ...state.messages];
  const response = await llm.invoke(messages);
  return { messages: response };
};

const workflow = new StateGraph(MessagesAnnotation)
// Define the node and edge
  .addNode("model", callModel)
  .addEdge(START, "model")
  .addEdge("model", END);

// Add simple in-memory checkpointer
// highlight-start
const memory = new MemorySaver();
const app = workflow.compile({ checkpointer: memory });
// highlight-end

"""
 We'll pass the latest input to the conversation here and let the LangGraph keep track of the conversation history using the checkpointer:
"""

await app.invoke(
  {
    messages: [
      {
        role: "user",
        content: "Translate to French: I love programming."
      }
    ]
  },
  {
    configurable: { thread_id: "1" }
  }
);
# Output:
#   {

#     messages: [

#       HumanMessage {

#         "id": "227b82a9-4084-46a5-ac79-ab9a3faa140e",

#         "content": "Translate to French: I love programming.",

#         "additional_kwargs": {},

#         "response_metadata": {}

#       },

#       AIMessage {

#         "id": "chatcmpl-ABSxVrvztgnasTeMSFbpZQmyYqjJZ",

#         "content": "J'adore la programmation.",

#         "additional_kwargs": {},

#         "response_metadata": {

#           "tokenUsage": {

#             "completionTokens": 5,

#             "promptTokens": 35,

#             "totalTokens": 40

#           },

#           "finish_reason": "stop",

#           "system_fingerprint": "fp_52a7f40b0b"

#         },

#         "tool_calls": [],

#         "invalid_tool_calls": [],

#         "usage_metadata": {

#           "input_tokens": 35,

#           "output_tokens": 5,

#           "total_tokens": 40

#         }

#       }

#     ]

#   }


await app.invoke(
  {
    messages: [
      {
        role: "user",
        content: "What did I just ask you?"
      }
    ]
  },
  {
    configurable: { thread_id: "1" }
  }
);
# Output:
#   {

#     messages: [

#       HumanMessage {

#         "id": "1a0560a4-9dcb-47a1-b441-80717e229706",

#         "content": "Translate to French: I love programming.",

#         "additional_kwargs": {},

#         "response_metadata": {}

#       },

#       AIMessage {

#         "id": "chatcmpl-ABSxVrvztgnasTeMSFbpZQmyYqjJZ",

#         "content": "J'adore la programmation.",

#         "additional_kwargs": {},

#         "response_metadata": {

#           "tokenUsage": {

#             "completionTokens": 5,

#             "promptTokens": 35,

#             "totalTokens": 40

#           },

#           "finish_reason": "stop",

#           "system_fingerprint": "fp_52a7f40b0b"

#         },

#         "tool_calls": [],

#         "invalid_tool_calls": []

#       },

#       HumanMessage {

#         "id": "4f233a7d-4b08-4f53-bb60-cf0141a59721",

#         "content": "What did I just ask you?",

#         "additional_kwargs": {},

#         "response_metadata": {}

#       },

#       AIMessage {

#         "id": "chatcmpl-ABSxVs5QnlPfbihTOmJrCVg1Dh7Ol",

#         "content": "You asked me to translate \"I love programming\" into French.",

#         "additional_kwargs": {},

#         "response_metadata": {

#           "tokenUsage": {

#             "completionTokens": 13,

#             "promptTokens": 55,

#             "totalTokens": 68

#           },

#           "finish_reason": "stop",

#           "system_fingerprint": "fp_9f2bfdaa89"

#         },

#         "tool_calls": [],

#         "invalid_tool_calls": [],

#         "usage_metadata": {

#           "input_tokens": 55,

#           "output_tokens": 13,

#           "total_tokens": 68

#         }

#       }

#     ]

#   }


"""
## Modifying chat history

Modifying stored chat messages can help your chatbot handle a variety of situations. Here are some examples:

### Trimming messages

LLMs and chat models have limited context windows, and even if you're not directly hitting limits, you may want to limit the amount of distraction the model has to deal with. One solution is trim the history messages before passing them to the model. Let's use an example history with the `app` we declared above:
"""

const demoEphemeralChatHistory = [
  { role: "user", content: "Hey there! I'm Nemo." },
  { role: "assistant", content: "Hello!" },
  { role: "user", content: "How are you today?" },
  { role: "assistant", content: "Fine thanks!" },
];

await app.invoke(
  {
    messages: [
      ...demoEphemeralChatHistory,
      { role: "user", content: "What's my name?" }
    ]
  },
  {
    configurable: { thread_id: "2" }
  }
);
# Output:
#   {

#     messages: [

#       HumanMessage {

#         "id": "63057c3d-f980-4640-97d6-497a9f83ddee",

#         "content": "Hey there! I'm Nemo.",

#         "additional_kwargs": {},

#         "response_metadata": {}

#       },

#       AIMessage {

#         "id": "c9f0c20a-8f55-4909-b281-88f2a45c4f05",

#         "content": "Hello!",

#         "additional_kwargs": {},

#         "response_metadata": {},

#         "tool_calls": [],

#         "invalid_tool_calls": []

#       },

#       HumanMessage {

#         "id": "fd7fb3a0-7bc7-4e84-99a9-731b30637b55",

#         "content": "How are you today?",

#         "additional_kwargs": {},

#         "response_metadata": {}

#       },

#       AIMessage {

#         "id": "09b0debb-1d4a-4856-8821-b037f5d96ecf",

#         "content": "Fine thanks!",

#         "additional_kwargs": {},

#         "response_metadata": {},

#         "tool_calls": [],

#         "invalid_tool_calls": []

#       },

#       HumanMessage {

#         "id": "edc13b69-25a0-40ac-81b3-175e65dc1a9a",

#         "content": "What's my name?",

#         "additional_kwargs": {},

#         "response_metadata": {}

#       },

#       AIMessage {

#         "id": "chatcmpl-ABSxWKCTdRuh2ZifXsvFHSo5z5I0J",

#         "content": "Your name is Nemo! How can I assist you today, Nemo?",

#         "additional_kwargs": {},

#         "response_metadata": {

#           "tokenUsage": {

#             "completionTokens": 14,

#             "promptTokens": 63,

#             "totalTokens": 77

#           },

#           "finish_reason": "stop",

#           "system_fingerprint": "fp_a5d11b2ef2"

#         },

#         "tool_calls": [],

#         "invalid_tool_calls": [],

#         "usage_metadata": {

#           "input_tokens": 63,

#           "output_tokens": 14,

#           "total_tokens": 77

#         }

#       }

#     ]

#   }


"""
We can see the app remembers the preloaded name.

But let's say we have a very small context window, and we want to trim the number of messages passed to the model to only the 2 most recent ones. We can use the built in [trimMessages](/docs/how_to/trim_messages/) util to trim messages based on their token count before they reach our prompt. In this case we'll count each message as 1 "token" and keep only the last two messages:
"""

import { START, END, MessagesAnnotation, StateGraph, MemorySaver } from "@langchain/langgraph";
import { trimMessages } from "@langchain/core/messages";

// Define trimmer
// highlight-start
// count each message as 1 "token" (tokenCounter: (msgs) => msgs.length) and keep only the last two messages
const trimmer = trimMessages({ strategy: "last", maxTokens: 2, tokenCounter: (msgs) => msgs.length });
// highlight-end

// Define the function that calls the model
const callModel2 = async (state: typeof MessagesAnnotation.State) => {
  // highlight-start
  const trimmedMessages = await trimmer.invoke(state.messages);
  const systemPrompt = 
    "You are a helpful assistant. " +
    "Answer all questions to the best of your ability.";
  const messages = [{ role: "system", content: systemPrompt }, ...trimmedMessages];
  // highlight-end
  const response = await llm.invoke(messages);
  return { messages: response };
};

const workflow2 = new StateGraph(MessagesAnnotation)
  // Define the node and edge
  .addNode("model", callModel2)
  .addEdge(START, "model")
  .addEdge("model", END);

// Add simple in-memory checkpointer
const app2 = workflow2.compile({ checkpointer: new MemorySaver() });

"""
Let's call this new app and check the response
"""

await app2.invoke(
  {
    messages: [
      ...demoEphemeralChatHistory,
      { role: "user", content: "What is my name?" }
    ]
  },
  {
    configurable: { thread_id: "3" }
  }
);
# Output:
#   {

#     messages: [

#       HumanMessage {

#         "id": "0d9330a0-d9d1-4aaf-8171-ca1ac6344f7c",

#         "content": "What is my name?",

#         "additional_kwargs": {},

#         "response_metadata": {}

#       },

#       AIMessage {

#         "id": "3a24e88b-7525-4797-9fcd-d751a378d22c",

#         "content": "Fine thanks!",

#         "additional_kwargs": {},

#         "response_metadata": {},

#         "tool_calls": [],

#         "invalid_tool_calls": []

#       },

#       HumanMessage {

#         "id": "276039c8-eba8-4c68-b015-81ec7704140d",

#         "content": "How are you today?",

#         "additional_kwargs": {},

#         "response_metadata": {}

#       },

#       AIMessage {

#         "id": "2ad4f461-20e1-4982-ba3b-235cb6b02abd",

#         "content": "Hello!",

#         "additional_kwargs": {},

#         "response_metadata": {},

#         "tool_calls": [],

#         "invalid_tool_calls": []

#       },

#       HumanMessage {

#         "id": "52213cae-953a-463d-a4a0-a7368c9ee4db",

#         "content": "Hey there! I'm Nemo.",

#         "additional_kwargs": {},

#         "response_metadata": {}

#       },

#       AIMessage {

#         "id": "chatcmpl-ABSxWe9BRDl1pmzkNIDawWwU3hvKm",

#         "content": "I'm sorry, but I don't have access to personal information about you unless you've shared it with me during our conversation. How can I assist you today?",

#         "additional_kwargs": {},

#         "response_metadata": {

#           "tokenUsage": {

#             "completionTokens": 30,

#             "promptTokens": 39,

#             "totalTokens": 69

#           },

#           "finish_reason": "stop",

#           "system_fingerprint": "fp_3537616b13"

#         },

#         "tool_calls": [],

#         "invalid_tool_calls": [],

#         "usage_metadata": {

#           "input_tokens": 39,

#           "output_tokens": 30,

#           "total_tokens": 69

#         }

#       }

#     ]

#   }


"""
We can see that `trimMessages` was called and only the two most recent messages will be passed to the model. In this case, this means that the model forgot the name we gave it.

Check out our [how to guide on trimming messages](/docs/how_to/trim_messages/) for more.
"""

"""
### Summary memory

We can use this same pattern in other ways too. For example, we could use an additional LLM call to generate a summary of the conversation before calling our app. Let's recreate our chat history:
"""

const demoEphemeralChatHistory2 = [
  { role: "user", content: "Hey there! I'm Nemo." },
  { role: "assistant", content: "Hello!" },
  { role: "user", content: "How are you today?" },
  { role: "assistant", content: "Fine thanks!" },
];

"""
And now, let's update the model-calling function to distill previous interactions into a summary:
"""

import { START, END, MessagesAnnotation, StateGraph, MemorySaver } from "@langchain/langgraph";
import { RemoveMessage } from "@langchain/core/messages";


// Define the function that calls the model
const callModel3 = async (state: typeof MessagesAnnotation.State) => {
  const systemPrompt = 
    "You are a helpful assistant. " +
    "Answer all questions to the best of your ability. " +
    "The provided chat history includes a summary of the earlier conversation.";
  const systemMessage = { role: "system", content: systemPrompt };
  const messageHistory = state.messages.slice(0, -1); // exclude the most recent user input
  
  // Summarize the messages if the chat history reaches a certain size
  if (messageHistory.length >= 4) {
    const lastHumanMessage = state.messages[state.messages.length - 1];
    // Invoke the model to generate conversation summary
    const summaryPrompt = 
      "Distill the above chat messages into a single summary message. " +
      "Include as many specific details as you can.";
    const summaryMessage = await llm.invoke([
      ...messageHistory,
      { role: "user", content: summaryPrompt }
    ]);

    // Delete messages that we no longer want to show up
    const deleteMessages = state.messages.map(m => new RemoveMessage({ id: m.id }));
    // Re-add user message
    const humanMessage = { role: "user", content: lastHumanMessage.content };
    // Call the model with summary & response
    const response = await llm.invoke([systemMessage, summaryMessage, humanMessage]);
    return { messages: [summaryMessage, humanMessage, response, ...deleteMessages] };
  } else {
    const response = await llm.invoke([systemMessage, ...state.messages]);
    return { messages: response };
  }
};

const workflow3 = new StateGraph(MessagesAnnotation)
  // Define the node and edge
  .addNode("model", callModel3)
  .addEdge(START, "model")
  .addEdge("model", END);

// Add simple in-memory checkpointer
const app3 = workflow3.compile({ checkpointer: new MemorySaver() });

"""
Let's see if it remembers the name we gave it:
"""

await app3.invoke(
  {
    messages: [
      ...demoEphemeralChatHistory2,
      { role: "user", content: "What did I say my name was?" }
    ]
  },
  {
    configurable: { thread_id: "4" }
  }
);
# Output:
#   {

#     messages: [

#       AIMessage {

#         "id": "chatcmpl-ABSxXjFDj6WRo7VLSneBtlAxUumPE",

#         "content": "Nemo greeted the assistant and asked how it was doing, to which the assistant responded that it was fine.",

#         "additional_kwargs": {},

#         "response_metadata": {

#           "tokenUsage": {

#             "completionTokens": 22,

#             "promptTokens": 60,

#             "totalTokens": 82

#           },

#           "finish_reason": "stop",

#           "system_fingerprint": "fp_e375328146"

#         },

#         "tool_calls": [],

#         "invalid_tool_calls": [],

#         "usage_metadata": {

#           "input_tokens": 60,

#           "output_tokens": 22,

#           "total_tokens": 82

#         }

#       },

#       HumanMessage {

#         "id": "8b1309b7-c09e-47fb-9ab3-34047f6973e3",

#         "content": "What did I say my name was?",

#         "additional_kwargs": {},

#         "response_metadata": {}

#       },

#       AIMessage {

#         "id": "chatcmpl-ABSxYAQKiBsQ6oVypO4CLFDsi1HRH",

#         "content": "You mentioned that your name is Nemo.",

#         "additional_kwargs": {},

#         "response_metadata": {

#           "tokenUsage": {

#             "completionTokens": 8,

#             "promptTokens": 73,

#             "totalTokens": 81

#           },

#           "finish_reason": "stop",

#           "system_fingerprint": "fp_52a7f40b0b"

#         },

#         "tool_calls": [],

#         "invalid_tool_calls": [],

#         "usage_metadata": {

#           "input_tokens": 73,

#           "output_tokens": 8,

#           "total_tokens": 81

#         }

#       }

#     ]

#   }


"""
Note that invoking the app again will keep accumulating the history until it reaches the specified number of messages (four in our case). At that point we will generate another summary generated from the initial summary plus new messages and so on.
"""



================================================
FILE: docs/core_docs/docs/how_to/chatbots_retrieval.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to do retrieval

:::info Prerequisites

This guide assumes familiarity with the following:

- [Chatbots](/docs/tutorials/chatbot)
- [Retrieval-augmented generation](/docs/tutorials/rag)

:::

Retrieval is a common technique chatbots use to augment their responses with data outside a chat model’s training data. This section will cover how to implement retrieval in the context of chatbots, but it’s worth noting that retrieval is a very subtle and deep topic.

## Setup

You’ll need to install a few packages, and set any LLM API keys:

```{=mdx}
import Npm2Yarn from "@theme/Npm2Yarn";

<Npm2Yarn>
  @langchain/openai @langchain/core cheerio
</Npm2Yarn>
```

Let’s also set up a chat model that we’ll use for the below examples.

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

// @lc-docs-hide-cell

import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0,
});

"""
## Creating a retriever

We’ll use [the LangSmith documentation](https://docs.smith.langchain.com) as source material and store the content in a vectorstore for later retrieval. Note that this example will gloss over some of the specifics around parsing and storing a data source - you can see more [in-depth documentation on creating retrieval systems here](/docs/how_to/#qa-with-rag).

Let’s use a document loader to pull text from the docs:
"""

import "cheerio";
import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio";

const loader = new CheerioWebBaseLoader(
  "https://docs.smith.langchain.com/user_guide"
);

const rawDocs = await loader.load();

rawDocs[0].pageContent.length;
# Output:
#   [33m36687[39m

"""
Next, we split it into smaller chunks that the LLM’s context window can handle and store it in a vector database:
"""

import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";

const textSplitter = new RecursiveCharacterTextSplitter({
  chunkSize: 500,
  chunkOverlap: 0,
});

const allSplits = await textSplitter.splitDocuments(rawDocs);

"""
Then we embed and store those chunks in a vector database:
"""

import { OpenAIEmbeddings } from "@langchain/openai";
import { MemoryVectorStore } from "langchain/vectorstores/memory";

const vectorstore = await MemoryVectorStore.fromDocuments(
  allSplits,
  new OpenAIEmbeddings()
);

"""
And finally, let’s create a retriever from our initialized vectorstore:
"""

const retriever = vectorstore.asRetriever(4);

const docs = await retriever.invoke("how can langsmith help with testing?");

console.log(docs);
# Output:
#   [

#     Document {

#       pageContent: "These test cases can be uploaded in bulk, created on the fly, or exported from application traces. L"... 294 more characters,

#       metadata: {

#         source: "https://docs.smith.langchain.com/user_guide",

#         loc: { lines: { from: 7, to: 7 } }

#       }

#     },

#     Document {

#       pageContent: "We provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set​Whi"... 347 more characters,

#       metadata: {

#         source: "https://docs.smith.langchain.com/user_guide",

#         loc: { lines: { from: 6, to: 6 } }

#       }

#     },

#     Document {

#       pageContent: "will help in curation of test cases that can help track regressions/improvements and development of "... 393 more characters,

#       metadata: {

#         source: "https://docs.smith.langchain.com/user_guide",

#         loc: { lines: { from: 11, to: 11 } }

#       }

#     },

#     Document {

#       pageContent: "that time period — this is especially handy for debugging production issues.LangSmith also allows fo"... 396 more characters,

#       metadata: {

#         source: "https://docs.smith.langchain.com/user_guide",

#         loc: { lines: { from: 11, to: 11 } }

#       }

#     }

#   ]


"""
We can see that invoking the retriever above results in some parts of the LangSmith docs that contain information about testing that our chatbot can use as context when answering questions. And now we’ve got a retriever that can return related data from the LangSmith docs!

## Document chains

Now that we have a retriever that can return LangChain docs, let’s create a chain that can use them as context to answer questions. We’ll use a `createStuffDocumentsChain` helper function to "stuff" all of the input documents into the prompt. It will also handle formatting the docs as strings.

In addition to a chat model, the function also expects a prompt that has a `context` variable, as well as a placeholder for chat history messages named `messages`. We’ll create an appropriate prompt and pass it as shown below:
"""

import { createStuffDocumentsChain } from "langchain/chains/combine_documents";
import {
  ChatPromptTemplate,
  MessagesPlaceholder,
} from "@langchain/core/prompts";

const SYSTEM_TEMPLATE = `Answer the user's questions based on the below context. 
If the context doesn't contain any relevant information to the question, don't make something up and just say "I don't know":

<context>
{context}
</context>
`;

const questionAnsweringPrompt = ChatPromptTemplate.fromMessages([
  ["system", SYSTEM_TEMPLATE],
  new MessagesPlaceholder("messages"),
]);

const documentChain = await createStuffDocumentsChain({
  llm,
  prompt: questionAnsweringPrompt,
});

"""
We can invoke this `documentChain` by itself to answer questions. Let’s use the docs we retrieved above and the same question, `how can langsmith help with testing?`:
"""

import { HumanMessage, AIMessage } from "@langchain/core/messages";

await documentChain.invoke({
  messages: [new HumanMessage("Can LangSmith help test my LLM applications?")],
  context: docs,
});
# Output:
#   [32m"Yes, LangSmith can help test your LLM applications. It allows developers to create datasets, which a"[39m... 229 more characters

"""
Looks good! For comparison, we can try it with no context docs and compare the result:
"""

await documentChain.invoke({
  messages: [new HumanMessage("Can LangSmith help test my LLM applications?")],
  context: [],
});
# Output:
#   [32m"I don't know."[39m

"""
We can see that the LLM does not return any results.

## Retrieval chains

Let’s combine this document chain with the retriever. Here’s one way this can look:
"""

import type { BaseMessage } from "@langchain/core/messages";
import {
  RunnablePassthrough,
  RunnableSequence,
} from "@langchain/core/runnables";

const parseRetrieverInput = (params: { messages: BaseMessage[] }) => {
  return params.messages[params.messages.length - 1].content;
};

const retrievalChain = RunnablePassthrough.assign({
  context: RunnableSequence.from([parseRetrieverInput, retriever]),
}).assign({
  answer: documentChain,
});

"""
Given a list of input messages, we extract the content of the last message in the list and pass that to the retriever to fetch some documents. Then, we pass those documents as context to our document chain to generate a final response.

Invoking this chain combines both steps outlined above:
"""

await retrievalChain.invoke({
  messages: [new HumanMessage("Can LangSmith help test my LLM applications?")],
});
# Output:
#   {

#     messages: [

#       HumanMessage {

#         lc_serializable: [33mtrue[39m,

#         lc_kwargs: {

#           content: [32m"Can LangSmith help test my LLM applications?"[39m,

#           additional_kwargs: {},

#           response_metadata: {}

#         },

#         lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#         content: [32m"Can LangSmith help test my LLM applications?"[39m,

#         name: [90mundefined[39m,

#         additional_kwargs: {},

#         response_metadata: {}

#       }

#     ],

#     context: [

#       Document {

#         pageContent: [32m"These test cases can be uploaded in bulk, created on the fly, or exported from application traces. L"[39m... 294 more characters,

#         metadata: {

#           source: [32m"https://docs.smith.langchain.com/user_guide"[39m,

#           loc: { lines: [36m[Object][39m }

#         }

#       },

#       Document {

#         pageContent: [32m"this guide, we’ll highlight the breadth of workflows LangSmith supports and how they fit into each s"[39m... 343 more characters,

#         metadata: {

#           source: [32m"https://docs.smith.langchain.com/user_guide"[39m,

#           loc: { lines: [36m[Object][39m }

#         }

#       },

#       Document {

#         pageContent: [32m"We provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set​Whi"[39m... 347 more characters,

#         metadata: {

#           source: [32m"https://docs.smith.langchain.com/user_guide"[39m,

#           loc: { lines: [36m[Object][39m }

#         }

#       },

#       Document {

#         pageContent: [32m"The ability to rapidly understand how the model is performing — and debug where it is failing — is i"[39m... 138 more characters,

#         metadata: {

#           source: [32m"https://docs.smith.langchain.com/user_guide"[39m,

#           loc: { lines: [36m[Object][39m }

#         }

#       }

#     ],

#     answer: [32m"Yes, LangSmith can help test your LLM applications. It allows developers to create datasets, which a"[39m... 297 more characters

#   }

"""
Looks good!

## Query transformation

Our retrieval chain is capable of answering questions about LangSmith, but there’s a problem - chatbots interact with users conversationally, and therefore have to deal with followup questions.

The chain in its current form will struggle with this. Consider a followup question to our original question like `Tell me more!`. If we invoke our retriever with that query directly, we get documents irrelevant to LLM application testing:
"""

await retriever.invoke("Tell me more!");
# Output:
#   [

#     Document {

#       pageContent: [32m"Oftentimes, changes in the prompt, retrieval strategy, or model choice can have huge implications in"[39m... 40 more characters,

#       metadata: {

#         source: [32m"https://docs.smith.langchain.com/user_guide"[39m,

#         loc: { lines: { from: [33m8[39m, to: [33m8[39m } }

#       }

#     },

#     Document {

#       pageContent: [32m"This allows you to quickly test out different prompts and models. You can open the playground from a"[39m... 37 more characters,

#       metadata: {

#         source: [32m"https://docs.smith.langchain.com/user_guide"[39m,

#         loc: { lines: { from: [33m10[39m, to: [33m10[39m } }

#       }

#     },

#     Document {

#       pageContent: [32m"We provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set​Whi"[39m... 347 more characters,

#       metadata: {

#         source: [32m"https://docs.smith.langchain.com/user_guide"[39m,

#         loc: { lines: { from: [33m6[39m, to: [33m6[39m } }

#       }

#     },

#     Document {

#       pageContent: [32m"together, making it easier to track the performance of and annotate your application across multiple"[39m... 244 more characters,

#       metadata: {

#         source: [32m"https://docs.smith.langchain.com/user_guide"[39m,

#         loc: { lines: { from: [33m11[39m, to: [33m11[39m } }

#       }

#     }

#   ]

"""
This is because the retriever has no innate concept of state, and will only pull documents most similar to the query given. To solve this, we can transform the query into a standalone query without any external references an LLM.

Here’s an example:
"""

const queryTransformPrompt = ChatPromptTemplate.fromMessages([
  new MessagesPlaceholder("messages"),
  [
    "user",
    "Given the above conversation, generate a search query to look up in order to get information relevant to the conversation. Only respond with the query, nothing else.",
  ],
]);

const queryTransformationChain = queryTransformPrompt.pipe(llm);

await queryTransformationChain.invoke({
  messages: [
    new HumanMessage("Can LangSmith help test my LLM applications?"),
    new AIMessage(
      "Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise."
    ),
    new HumanMessage("Tell me more!"),
  ],
});
# Output:
#   AIMessage {

#     lc_serializable: [33mtrue[39m,

#     lc_kwargs: {

#       content: [32m'"LangSmith LLM application testing and evaluation features"'[39m,

#       tool_calls: [],

#       invalid_tool_calls: [],

#       additional_kwargs: { function_call: [90mundefined[39m, tool_calls: [90mundefined[39m },

#       response_metadata: {}

#     },

#     lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#     content: [32m'"LangSmith LLM application testing and evaluation features"'[39m,

#     name: [90mundefined[39m,

#     additional_kwargs: { function_call: [90mundefined[39m, tool_calls: [90mundefined[39m },

#     response_metadata: {

#       tokenUsage: { completionTokens: [33m11[39m, promptTokens: [33m144[39m, totalTokens: [33m155[39m },

#       finish_reason: [32m"stop"[39m

#     },

#     tool_calls: [],

#     invalid_tool_calls: []

#   }

"""
Awesome! That transformed query would pull up context documents related to LLM application testing.

Let’s add this to our retrieval chain. We can wrap our retriever as follows:
"""

import { RunnableBranch } from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers";

const queryTransformingRetrieverChain = RunnableBranch.from([
  [
    (params: { messages: BaseMessage[] }) => params.messages.length === 1,
    RunnableSequence.from([parseRetrieverInput, retriever]),
  ],
  queryTransformPrompt
    .pipe(llm)
    .pipe(new StringOutputParser())
    .pipe(retriever),
]).withConfig({ runName: "chat_retriever_chain" });

"""
Then, we can use this query transformation chain to make our retrieval chain better able to handle such followup questions:

"""

const conversationalRetrievalChain = RunnablePassthrough.assign({
  context: queryTransformingRetrieverChain,
}).assign({
  answer: documentChain,
});

"""
Awesome! Let’s invoke this new chain with the same inputs as earlier:

"""

await conversationalRetrievalChain.invoke({
  messages: [new HumanMessage("Can LangSmith help test my LLM applications?")],
});
# Output:
#   {

#     messages: [

#       HumanMessage {

#         lc_serializable: [33mtrue[39m,

#         lc_kwargs: {

#           content: [32m"Can LangSmith help test my LLM applications?"[39m,

#           additional_kwargs: {},

#           response_metadata: {}

#         },

#         lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#         content: [32m"Can LangSmith help test my LLM applications?"[39m,

#         name: [90mundefined[39m,

#         additional_kwargs: {},

#         response_metadata: {}

#       }

#     ],

#     context: [

#       Document {

#         pageContent: [32m"These test cases can be uploaded in bulk, created on the fly, or exported from application traces. L"[39m... 294 more characters,

#         metadata: {

#           source: [32m"https://docs.smith.langchain.com/user_guide"[39m,

#           loc: { lines: [36m[Object][39m }

#         }

#       },

#       Document {

#         pageContent: [32m"this guide, we’ll highlight the breadth of workflows LangSmith supports and how they fit into each s"[39m... 343 more characters,

#         metadata: {

#           source: [32m"https://docs.smith.langchain.com/user_guide"[39m,

#           loc: { lines: [36m[Object][39m }

#         }

#       },

#       Document {

#         pageContent: [32m"We provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set​Whi"[39m... 347 more characters,

#         metadata: {

#           source: [32m"https://docs.smith.langchain.com/user_guide"[39m,

#           loc: { lines: [36m[Object][39m }

#         }

#       },

#       Document {

#         pageContent: [32m"The ability to rapidly understand how the model is performing — and debug where it is failing — is i"[39m... 138 more characters,

#         metadata: {

#           source: [32m"https://docs.smith.langchain.com/user_guide"[39m,

#           loc: { lines: [36m[Object][39m }

#         }

#       }

#     ],

#     answer: [32m"Yes, LangSmith can help test your LLM applications. It allows developers to create datasets, which a"[39m... 297 more characters

#   }

await conversationalRetrievalChain.invoke({
  messages: [
    new HumanMessage("Can LangSmith help test my LLM applications?"),
    new AIMessage(
      "Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise."
    ),
    new HumanMessage("Tell me more!"),
  ],
});
# Output:
#   {

#     messages: [

#       HumanMessage {

#         lc_serializable: [33mtrue[39m,

#         lc_kwargs: {

#           content: [32m"Can LangSmith help test my LLM applications?"[39m,

#           additional_kwargs: {},

#           response_metadata: {}

#         },

#         lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#         content: [32m"Can LangSmith help test my LLM applications?"[39m,

#         name: [90mundefined[39m,

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       AIMessage {

#         lc_serializable: [33mtrue[39m,

#         lc_kwargs: {

#           content: [32m"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examp"[39m... 317 more characters,

#           tool_calls: [],

#           invalid_tool_calls: [],

#           additional_kwargs: {},

#           response_metadata: {}

#         },

#         lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#         content: [32m"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examp"[39m... 317 more characters,

#         name: [90mundefined[39m,

#         additional_kwargs: {},

#         response_metadata: {},

#         tool_calls: [],

#         invalid_tool_calls: []

#       },

#       HumanMessage {

#         lc_serializable: [33mtrue[39m,

#         lc_kwargs: {

#           content: [32m"Tell me more!"[39m,

#           additional_kwargs: {},

#           response_metadata: {}

#         },

#         lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#         content: [32m"Tell me more!"[39m,

#         name: [90mundefined[39m,

#         additional_kwargs: {},

#         response_metadata: {}

#       }

#     ],

#     context: [

#       Document {

#         pageContent: [32m"These test cases can be uploaded in bulk, created on the fly, or exported from application traces. L"[39m... 294 more characters,

#         metadata: {

#           source: [32m"https://docs.smith.langchain.com/user_guide"[39m,

#           loc: { lines: [36m[Object][39m }

#         }

#       },

#       Document {

#         pageContent: [32m"We provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set​Whi"[39m... 347 more characters,

#         metadata: {

#           source: [32m"https://docs.smith.langchain.com/user_guide"[39m,

#           loc: { lines: [36m[Object][39m }

#         }

#       },

#       Document {

#         pageContent: [32m"this guide, we’ll highlight the breadth of workflows LangSmith supports and how they fit into each s"[39m... 343 more characters,

#         metadata: {

#           source: [32m"https://docs.smith.langchain.com/user_guide"[39m,

#           loc: { lines: [36m[Object][39m }

#         }

#       },

#       Document {

#         pageContent: [32m"will help in curation of test cases that can help track regressions/improvements and development of "[39m... 393 more characters,

#         metadata: {

#           source: [32m"https://docs.smith.langchain.com/user_guide"[39m,

#           loc: { lines: [36m[Object][39m }

#         }

#       }

#     ],

#     answer: [32m"LangSmith supports a variety of workflows to aid in the development of your applications, from creat"[39m... 607 more characters

#   }

"""
You can check out [this LangSmith trace](https://smith.langchain.com/public/dc4d6bd4-fea5-45df-be94-06ad18882ae9/r) to see the internal query transformation step for yourself.

## Streaming

Because this chain is constructed with LCEL, you can use familiar methods like `.stream()` with it:
"""

const stream = await conversationalRetrievalChain.stream({
  messages: [
    new HumanMessage("Can LangSmith help test my LLM applications?"),
    new AIMessage(
      "Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise."
    ),
    new HumanMessage("Tell me more!"),
  ],
});

for await (const chunk of stream) {
  console.log(chunk);
}
# Output:
#   {

#     messages: [

#       HumanMessage {

#         lc_serializable: true,

#         lc_kwargs: {

#           content: "Can LangSmith help test my LLM applications?",

#           additional_kwargs: {},

#           response_metadata: {}

#         },

#         lc_namespace: [ "langchain_core", "messages" ],

#         content: "Can LangSmith help test my LLM applications?",

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       AIMessage {

#         lc_serializable: true,

#         lc_kwargs: {

#           content: "Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examp"... 317 more characters,

#           tool_calls: [],

#           invalid_tool_calls: [],

#           additional_kwargs: {},

#           response_metadata: {}

#         },

#         lc_namespace: [ "langchain_core", "messages" ],

#         content: "Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examp"... 317 more characters,

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: {},

#         tool_calls: [],

#         invalid_tool_calls: []

#       },

#       HumanMessage {

#         lc_serializable: true,

#         lc_kwargs: {

#           content: "Tell me more!",

#           additional_kwargs: {},

#           response_metadata: {}

#         },

#         lc_namespace: [ "langchain_core", "messages" ],

#         content: "Tell me more!",

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: {}

#       }

#     ]

#   }

#   {

#     context: [

#       Document {

#         pageContent: "These test cases can be uploaded in bulk, created on the fly, or exported from application traces. L"... 294 more characters,

#         metadata: {

#           source: "https://docs.smith.langchain.com/user_guide",

#           loc: { lines: [Object] }

#         }

#       },

#       Document {

#         pageContent: "We provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set​Whi"... 347 more characters,

#         metadata: {

#           source: "https://docs.smith.langchain.com/user_guide",

#           loc: { lines: [Object] }

#         }

#       },

#       Document {

#         pageContent: "this guide, we’ll highlight the breadth of workflows LangSmith supports and how they fit into each s"... 343 more characters,

#         metadata: {

#           source: "https://docs.smith.langchain.com/user_guide",

#           loc: { lines: [Object] }

#         }

#       },

#       Document {

#         pageContent: "will help in curation of test cases that can help track regressions/improvements and development of "... 393 more characters,

#         metadata: {

#           source: "https://docs.smith.langchain.com/user_guide",

#           loc: { lines: [Object] }

#         }

#       }

#     ]

#   }

#   { answer: "" }

#   { answer: "Lang" }

#   { answer: "Smith" }

#   { answer: " offers" }

#   { answer: " a" }

#   { answer: " comprehensive" }

#   { answer: " suite" }

#   { answer: " of" }

#   { answer: " tools" }

#   { answer: " and" }

#   { answer: " workflows" }

#   { answer: " to" }

#   { answer: " support" }

#   { answer: " the" }

#   { answer: " development" }

#   { answer: " and" }

#   { answer: " testing" }

#   { answer: " of" }

#   { answer: " L" }

#   { answer: "LM" }

#   { answer: " applications" }

#   { answer: "." }

#   { answer: " Here" }

#   { answer: " are" }

#   { answer: " some" }

#   { answer: " key" }

#   { answer: " features" }

#   { answer: " and" }

#   { answer: " functionalities" }

#   { answer: ":\n\n" }

#   { answer: "1" }

#   { answer: "." }

#   { answer: " **" }

#   { answer: "Test" }

#   { answer: " Case" }

#   { answer: " Management" }

#   { answer: "**" }

#   { answer: ":\n" }

#   { answer: "  " }

#   { answer: " -" }

#   { answer: " **" }

#   { answer: "Bulk" }

#   { answer: " Upload" }

#   { answer: " and" }

#   { answer: " Creation" }

#   { answer: "**" }

#   { answer: ":" }

#   { answer: " You" }

#   { answer: " can" }

#   { answer: " upload" }

#   { answer: " test" }

#   { answer: " cases" }

#   { answer: " in" }

#   { answer: " bulk" }

#   { answer: "," }

#   { answer: " create" }

#   { answer: " them" }

#   { answer: " on" }

#   { answer: " the" }

#   { answer: " fly" }

#   { answer: "," }

#   { answer: " or" }

#   { answer: " export" }

#   { answer: " them" }

#   { answer: " from" }

#   { answer: " application" }

#   { answer: " traces" }

#   { answer: ".\n" }

#   { answer: "  " }

#   { answer: " -" }

#   { answer: " **" }

#   { answer: "Datas" }

#   { answer: "ets" }

#   { answer: "**" }

#   { answer: ":" }

#   { answer: " Lang" }

#   { answer: "Smith" }

#   { answer: " allows" }

#   { answer: " you" }

#   { answer: " to" }

#   { answer: " create" }

#   { answer: " datasets" }

#   { answer: "," }

#   { answer: " which" }

#   { answer: " are" }

#   { answer: " collections" }

#   { answer: " of" }

#   { answer: " inputs" }

#   { answer: " and" }

#   { answer: " reference" }

#   { answer: " outputs" }

#   { answer: "." }

#   { answer: " These" }

#   { answer: " datasets" }

#   { answer: " can" }

#   { answer: " be" }

#   { answer: " used" }

#   { answer: " to" }

#   { answer: " run" }

#   { answer: " tests" }

#   { answer: " on" }

#   { answer: " your" }

#   { answer: " L" }

#   { answer: "LM" }

#   { answer: " applications" }

#   { answer: ".\n\n" }

#   { answer: "2" }

#   { answer: "." }

#   { answer: " **" }

#   { answer: "Custom" }

#   { answer: " Evalu" }

#   { answer: "ations" }

#   { answer: "**" }

#   { answer: ":\n" }

#   { answer: "  " }

#   { answer: " -" }

#   { answer: " **" }

#   { answer: "LL" }

#   { answer: "M" }

#   { answer: " and" }

#   { answer: " He" }

#   { answer: "uristic" }

#   { answer: " Based" }

#   { answer: "**" }

#   { answer: ":" }

#   { answer: " You" }

#   { answer: " can" }

#   { answer: " run" }

#   { answer: " custom" }

#   { answer: " evaluations" }

#   { answer: " using" }

#   { answer: " both" }

#   { answer: " L" }

#   { answer: "LM" }

#   { answer: "-based" }

#   { answer: " and" }

#   { answer: " heuristic" }

#   { answer: "-based" }

#   { answer: " methods" }

#   { answer: " to" }

#   { answer: " score" }

#   { answer: " test" }

#   { answer: " results" }

#   { answer: ".\n\n" }

#   { answer: "3" }

#   { answer: "." }

#   { answer: " **" }

#   { answer: "Comparison" }

#   { answer: " View" }

#   { answer: "**" }

#   { answer: ":\n" }

#   { answer: "  " }

#   { answer: " -" }

#   { answer: " **" }

#   { answer: "Pro" }

#   { answer: "tot" }

#   { answer: "yp" }

#   { answer: "ing" }

#   { answer: " and" }

#   { answer: " Regression" }

#   { answer: " Tracking" }

#   { answer: "**" }

#   { answer: ":" }

#   { answer: " When" }

#   { answer: " prot" }

#   { answer: "otyping" }

#   { answer: " different" }

#   { answer: " versions" }

#   { answer: " of" }

#   { answer: " your" }

#   { answer: " applications" }

#   { answer: "," }

#   { answer: " Lang" }

#   { answer: "Smith" }

#   { answer: " provides" }

#   { answer: " a" }

#   { answer: " comparison" }

#   { answer: " view" }

#   { answer: " to" }

#   { answer: " see" }

#   { answer: " if" }

#   { answer: " there" }

#   { answer: " have" }

#   { answer: " been" }

#   { answer: " any" }

#   { answer: " regress" }

#   { answer: "ions" }

#   { answer: " with" }

#   { answer: " respect" }

#   { answer: " to" }

#   { answer: " your" }

#   { answer: " initial" }

#   { answer: " test" }

#   { answer: " cases" }

#   { answer: ".\n\n" }

#   { answer: "4" }

#   { answer: "." }

#   { answer: " **" }

#   { answer: "Native" }

#   { answer: " Rendering" }

#   { answer: "**" }

#   { answer: ":\n" }

#   { answer: "  " }

#   { answer: " -" }

#   { answer: " **" }

#   { answer: "Chat" }

#   { answer: " Messages" }

#   { answer: "," }

#   { answer: " Functions" }

#   { answer: "," }

#   { answer: " and" }

#   { answer: " Documents" }

#   { answer: "**" }

#   { answer: ":" }

#   { answer: " Lang" }

#   { answer: "Smith" }

#   { answer: " provides" }

#   { answer: " native" }

#   { answer: " rendering" }

#   { answer: " of" }

#   { answer: " chat" }

#   { answer: " messages" }

#   { answer: "," }

#   { answer: " functions" }

#   { answer: "," }

#   { answer: " and" }

#   { answer: " retrieved" }

#   { answer: " documents" }

#   { answer: "," }

#   { answer: " making" }

#   { answer: " it" }

#   { answer: " easier" }

#   { answer: " to" }

#   { answer: " visualize" }

#   { answer: " and" }

#   { answer: " understand" }

#   { answer: " the" }

#   { answer: " outputs" }

#   { answer: ".\n\n" }

#   { answer: "5" }

#   { answer: "." }

#   { answer: " **" }

#   { answer: "Pro" }

#   { answer: "tot" }

#   { answer: "yp" }

#   { answer: "ing" }

#   { answer: " Support" }

#   { answer: "**" }

#   { answer: ":\n" }

#   { answer: "  " }

#   { answer: " -" }

#   { answer: " **" }

#   { answer: "Quick" }

#   { answer: " Experiment" }

#   { answer: "ation" }

#   { answer: "**" }

#   { answer: ":" }

#   { answer: " The" }

#   { answer: " platform" }

#   { answer: " supports" }

#   { answer: " quick" }

#   { answer: " experimentation" }

#   { answer: " with" }

#   { answer: " different" }

#   { answer: " prompts" }

#   { answer: "," }

#   { answer: " model" }

#   { answer: " types" }

#   { answer: "," }

#   { answer: " retrieval" }

#   { answer: " strategies" }

#   { answer: "," }

#   { answer: " and" }

#   { answer: " other" }

#   { answer: " parameters" }

#   { answer: ".\n\n" }

#   { answer: "6" }

#   { answer: "." }

#   { answer: " **" }

#   { answer: "Feedback" }

#   { answer: " Capture" }

#   { answer: "**" }

#   { answer: ":\n" }

#   { answer: "  " }

#   { answer: " -" }

#   { answer: " **" }

#   { answer: "Human" }

#   { answer: " Feedback" }

#   { answer: "**" }

#   { answer: ":" }

#   { answer: " When" }

#   { answer: " launching" }

#   { answer: " your" }

#   { answer: " application" }

#   { answer: " to" }

#   { answer: " an" }

#   { answer: " initial" }

#   { answer: " set" }

#   { answer: " of" }

#   { answer: " users" }

#   { answer: "," }

#   { answer: " Lang" }

#   { answer: "Smith" }

#   { answer: " allows" }

#   { answer: " you" }

#   { answer: " to" }

#   { answer: " gather" }

#   { answer: " human" }

#   { answer: " feedback" }

#   { answer: " on" }

#   { answer: " the" }

#   { answer: " responses" }

#   { answer: "." }

#   { answer: " This" }

#   { answer: " helps" }

#   { answer: " identify" }

#   { answer: " interesting" }

#   { answer: " runs" }

#   { answer: " and" }

#   { answer: " highlight" }

#   { answer: " edge" }

#   { answer: " cases" }

#   { answer: " causing" }

#   { answer: " problematic" }

#   { answer: " responses" }

#   { answer: ".\n" }

#   { answer: "  " }

#   { answer: " -" }

#   { answer: " **" }

#   { answer: "Feedback" }

#   { answer: " Scores" }

#   { answer: "**" }

#   { answer: ":" }

#   { answer: " You" }

#   { answer: " can" }

#   { answer: " attach" }

#   { answer: " feedback" }

#   { answer: " scores" }

#   { answer: " to" }

#   { answer: " logged" }

#   { answer: " traces" }

#   { answer: "," }

#   { answer: " often" }

#   { answer: " integrated" }

#   { answer: " into" }

#   { answer: " the" }

#   { answer: " system" }

#   { answer: ".\n\n" }

#   { answer: "7" }

#   { answer: "." }

#   { answer: " **" }

#   { answer: "Monitoring" }

#   { answer: " and" }

#   { answer: " Troubles" }

#   { answer: "hooting" }

#   { answer: "**" }

#   { answer: ":\n" }

#   { answer: "  " }

#   { answer: " -" }

#   { answer: " **" }

#   { answer: "Logging" }

#   { answer: " and" }

#   { answer: " Visualization" }

#   { answer: "**" }

#   { answer: ":" }

#   { answer: " Lang" }

#   { answer: "Smith" }

#   { answer: " logs" }

#   { answer: " all" }

#   { answer: " traces" }

#   { answer: "," }

#   { answer: " visual" }

#   { answer: "izes" }

#   { answer: " latency" }

#   { answer: " and" }

#   { answer: " token" }

#   { answer: " usage" }

#   { answer: " statistics" }

#   { answer: "," }

#   { answer: " and" }

#   { answer: " helps" }

#   { answer: " troubleshoot" }

#   { answer: " specific" }

#   { answer: " issues" }

#   { answer: " as" }

#   { answer: " they" }

#   { answer: " arise" }

#   { answer: ".\n\n" }

#   { answer: "Overall" }

#   { answer: "," }

#   { answer: " Lang" }

#   { answer: "Smith" }

#   { answer: " is" }

#   { answer: " designed" }

#   { answer: " to" }

#   { answer: " support" }

#   { answer: " the" }

#   { answer: " entire" }

#   { answer: " lifecycle" }

#   { answer: " of" }

#   { answer: " L" }

#   { answer: "LM" }

#   { answer: " application" }

#   { answer: " development" }

#   { answer: "," }

#   { answer: " from" }

#   { answer: " initial" }

#   { answer: " prot" }

#   { answer: "otyping" }

#   { answer: " to" }

#   { answer: " deployment" }

#   { answer: " and" }

#   { answer: " ongoing" }

#   { answer: " monitoring" }

#   { answer: "," }

#   { answer: " making" }

#   { answer: " it" }

#   { answer: " a" }

#   { answer: " powerful" }

#   { answer: " tool" }

#   { answer: " for" }

#   { answer: " developers" }

#   { answer: " looking" }

#   { answer: " to" }

#   { answer: " build" }

#   { answer: " and" }

#   { answer: " maintain" }

#   { answer: " high" }

#   { answer: "-quality" }

#   { answer: " L" }

#   { answer: "LM" }

#   { answer: " applications" }

#   { answer: "." }

#   { answer: "" }


"""
## Next steps

You've now learned some techniques for adding personal data as context to your chatbots.

This guide only scratches the surface of retrieval techniques. For more on different ways of ingesting, preparing, and retrieving the most relevant data, check out our [how to guides on retrieval](/docs/how_to/#retrievers).
"""



================================================
FILE: docs/core_docs/docs/how_to/chatbots_tools.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to add tools to chatbots

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Chatbots](/docs/concepts/messages)
- [Agents](https://langchain-ai.github.io/langgraphjs/tutorials/multi_agent/agent_supervisor/)
- [Chat history](/docs/concepts/chat_history)

:::

This section will cover how to create conversational agents: chatbots that can interact with other systems and APIs using tools.

:::note

This how-to guide previously built a chatbot using [RunnableWithMessageHistory](https://api.js.langchain.com/classes/_langchain_core.runnables.RunnableWithMessageHistory.html). You can access this version of the tutorial in the [v0.2 docs](https://js.langchain.com/v0.2/docs/how_to/chatbots_tools/).

The LangGraph implementation offers a number of advantages over `RunnableWithMessageHistory`, including the ability to persist arbitrary components of an application's state (instead of only messages).

:::

## Setup

For this guide, we'll be using a [tool calling agent](https://langchain-ai.github.io/langgraphjs/concepts/agentic_concepts/#tool-calling-agent) with a single tool for searching the web. The default will be powered by [Tavily](/docs/integrations/tools/tavily_search), but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.

You'll need to [sign up for an account](https://tavily.com/) on the Tavily website, and install the following packages:

```{=mdx}
import Npm2Yarn from "@theme/Npm2Yarn";

<Npm2Yarn>
  @langchain/core @langchain/langgraph @langchain/community
</Npm2Yarn>
```

Let’s also set up a chat model that we’ll use for the below examples.

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```

```typescript
process.env.TAVILY_API_KEY = "YOUR_API_KEY";
```
"""

"""
## Creating an agent

Our end goal is to create an agent that can respond conversationally to user questions while looking up information as needed.

First, let's initialize Tavily and an OpenAI chat model capable of tool calling:
"""

// @lc-docs-hide-cell

import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0,
});

import { TavilySearchResults } from "@langchain/community/tools/tavily_search";

const tools = [
  new TavilySearchResults({
    maxResults: 1,
  }),
];

"""
To make our agent conversational, we can also specify a prompt. Here's an example:
"""

import {
  ChatPromptTemplate,
} from "@langchain/core/prompts";

// Adapted from https://smith.langchain.com/hub/jacob/tool-calling-agent
const prompt = ChatPromptTemplate.fromMessages([
  [
    "system",
    "You are a helpful assistant. You may not need to use tools for every query - the user may just want to chat!",
  ],
]);

"""
Great! Now let's assemble our agent using LangGraph's prebuilt [createReactAgent](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph_prebuilt.createReactAgent.html), which allows you to create a [tool-calling agent](https://langchain-ai.github.io/langgraphjs/concepts/agentic_concepts/#tool-calling-agent):
"""

import { createReactAgent } from "@langchain/langgraph/prebuilt"

// messageModifier allows you to preprocess the inputs to the model inside ReAct agent
// in this case, since we're passing a prompt string, we'll just always add a SystemMessage
// with this prompt string before any other messages sent to the model
const agent = createReactAgent({ llm, tools, messageModifier: prompt })

"""
## Running the agent

Now that we've set up our agent, let's try interacting with it! It can handle both trivial queries that require no lookup:
"""

await agent.invoke({ messages: [{ role: "user", content: "I'm Nemo!" }]})
# Output:
#   {

#     messages: [

#       HumanMessage {

#         "id": "8c5fa465-e8d8-472a-9434-f574bf74537f",

#         "content": "I'm Nemo!",

#         "additional_kwargs": {},

#         "response_metadata": {}

#       },

#       AIMessage {

#         "id": "chatcmpl-ABTKLLriRcZin65zLAMB3WUf9Sg1t",

#         "content": "How can I assist you today?",

#         "additional_kwargs": {},

#         "response_metadata": {

#           "tokenUsage": {

#             "completionTokens": 8,

#             "promptTokens": 93,

#             "totalTokens": 101

#           },

#           "finish_reason": "stop",

#           "system_fingerprint": "fp_3537616b13"

#         },

#         "tool_calls": [],

#         "invalid_tool_calls": [],

#         "usage_metadata": {

#           "input_tokens": 93,

#           "output_tokens": 8,

#           "total_tokens": 101

#         }

#       }

#     ]

#   }


"""
Or, it can use of the passed search tool to get up to date information if needed:
"""

await agent.invoke({ messages: [{ role: "user", content: "What is the current conservation status of the Great Barrier Reef?" }]})
# Output:
#   {

#     messages: [

#       HumanMessage {

#         "id": "65c315b6-2433-4cb1-97c7-b60b5546f518",

#         "content": "What is the current conservation status of the Great Barrier Reef?",

#         "additional_kwargs": {},

#         "response_metadata": {}

#       },

#       AIMessage {

#         "id": "chatcmpl-ABTKLQn1e4axRhqIhpKMyzWWTGauO",

#         "content": "How can I assist you today?",

#         "additional_kwargs": {},

#         "response_metadata": {

#           "tokenUsage": {

#             "completionTokens": 8,

#             "promptTokens": 93,

#             "totalTokens": 101

#           },

#           "finish_reason": "stop",

#           "system_fingerprint": "fp_3537616b13"

#         },

#         "tool_calls": [],

#         "invalid_tool_calls": [],

#         "usage_metadata": {

#           "input_tokens": 93,

#           "output_tokens": 8,

#           "total_tokens": 101

#         }

#       }

#     ]

#   }


"""
## Conversational responses

Because our prompt contains a placeholder for chat history messages, our agent can also take previous interactions into account and respond conversationally like a standard chatbot:
"""

await agent.invoke({
  messages: [
    { role: "user", content: "I'm Nemo!" },
    { role: "user", content: "Hello Nemo! How can I assist you today?" },
    { role: "user", content: "What is my name?" }
  ]
})
# Output:
#   {

#     messages: [

#       HumanMessage {

#         "id": "6433afc5-31bd-44b3-b34c-f11647e1677d",

#         "content": "I'm Nemo!",

#         "additional_kwargs": {},

#         "response_metadata": {}

#       },

#       HumanMessage {

#         "id": "f163b5f1-ea29-4d7a-9965-7c7c563d9cea",

#         "content": "Hello Nemo! How can I assist you today?",

#         "additional_kwargs": {},

#         "response_metadata": {}

#       },

#       HumanMessage {

#         "id": "382c3354-d02b-4888-98d8-44d75d045044",

#         "content": "What is my name?",

#         "additional_kwargs": {},

#         "response_metadata": {}

#       },

#       AIMessage {

#         "id": "chatcmpl-ABTKMKu7ThZDZW09yMIPTq2N723Cj",

#         "content": "How can I assist you today?",

#         "additional_kwargs": {},

#         "response_metadata": {

#           "tokenUsage": {

#             "completionTokens": 8,

#             "promptTokens": 93,

#             "totalTokens": 101

#           },

#           "finish_reason": "stop",

#           "system_fingerprint": "fp_e375328146"

#         },

#         "tool_calls": [],

#         "invalid_tool_calls": [],

#         "usage_metadata": {

#           "input_tokens": 93,

#           "output_tokens": 8,

#           "total_tokens": 101

#         }

#       }

#     ]

#   }


"""
If preferred, you can also add memory to the LangGraph agent to manage the history of messages. Let's redeclare it this way:
"""

import { MemorySaver } from "@langchain/langgraph"

// highlight-start
const memory = new MemorySaver()
const agent2 = createReactAgent({ llm, tools, messageModifier: prompt, checkpointSaver: memory })
// highlight-end

await agent2.invoke({ messages: [{ role: "user", content: "I'm Nemo!" }]}, { configurable: { thread_id: "1" } })
# Output:
#   {

#     messages: [

#       HumanMessage {

#         "id": "a4a4f663-8192-4179-afcc-88d9d186aa80",

#         "content": "I'm Nemo!",

#         "additional_kwargs": {},

#         "response_metadata": {}

#       },

#       AIMessage {

#         "id": "chatcmpl-ABTKi4tBzOWMh3hgA46xXo7bJzb8r",

#         "content": "How can I assist you today?",

#         "additional_kwargs": {},

#         "response_metadata": {

#           "tokenUsage": {

#             "completionTokens": 8,

#             "promptTokens": 93,

#             "totalTokens": 101

#           },

#           "finish_reason": "stop",

#           "system_fingerprint": "fp_e375328146"

#         },

#         "tool_calls": [],

#         "invalid_tool_calls": [],

#         "usage_metadata": {

#           "input_tokens": 93,

#           "output_tokens": 8,

#           "total_tokens": 101

#         }

#       }

#     ]

#   }


"""
And then if we rerun our wrapped agent executor:
"""

await agent2.invoke({ messages: [{ role: "user", content: "What is my name?" }]}, { configurable: { thread_id: "1" } })
# Output:
#   {

#     messages: [

#       HumanMessage {

#         "id": "c5fd303c-eb49-41a0-868e-bc8c5aa02cf6",

#         "content": "I'm Nemo!",

#         "additional_kwargs": {},

#         "response_metadata": {}

#       },

#       AIMessage {

#         "id": "chatcmpl-ABTKi4tBzOWMh3hgA46xXo7bJzb8r",

#         "content": "How can I assist you today?",

#         "additional_kwargs": {},

#         "response_metadata": {

#           "tokenUsage": {

#             "completionTokens": 8,

#             "promptTokens": 93,

#             "totalTokens": 101

#           },

#           "finish_reason": "stop",

#           "system_fingerprint": "fp_e375328146"

#         },

#         "tool_calls": [],

#         "invalid_tool_calls": []

#       },

#       HumanMessage {

#         "id": "635b17b9-2ec7-412f-bf45-85d0e9944430",

#         "content": "What is my name?",

#         "additional_kwargs": {},

#         "response_metadata": {}

#       },

#       AIMessage {

#         "id": "chatcmpl-ABTKjBbmFlPb5t37aJ8p4NtoHb8YG",

#         "content": "How can I assist you today?",

#         "additional_kwargs": {},

#         "response_metadata": {

#           "tokenUsage": {

#             "completionTokens": 8,

#             "promptTokens": 93,

#             "totalTokens": 101

#           },

#           "finish_reason": "stop",

#           "system_fingerprint": "fp_e375328146"

#         },

#         "tool_calls": [],

#         "invalid_tool_calls": [],

#         "usage_metadata": {

#           "input_tokens": 93,

#           "output_tokens": 8,

#           "total_tokens": 101

#         }

#       }

#     ]

#   }


"""
This [LangSmith trace](https://smith.langchain.com/public/16cbcfa5-5ef1-4d4c-92c9-538a6e71f23d/r) shows what's going on under the hood.

## Further reading

For more on how to build agents, check these [LangGraph](https://langchain-ai.github.io/langgraphjs/) guides:

* [agents conceptual guide](https://langchain-ai.github.io/langgraphjs/concepts/agentic_concepts/)
* [agents tutorials](https://langchain-ai.github.io/langgraphjs/tutorials/multi_agent/multi_agent_collaboration/)
* [createReactAgent](https://langchain-ai.github.io/langgraphjs/how-tos/create-react-agent/)

For more on tool usage, you can also check out [this use case section](/docs/how_to#tools).
"""



================================================
FILE: docs/core_docs/docs/how_to/code_splitter.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to split code

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Text splitters](/docs/concepts/text_splitters)
- [Recursively splitting text by character](/docs/how_to/recursive_text_splitter)

:::


[RecursiveCharacterTextSplitter](https://api.js.langchain.com/classes/langchain_textsplitters.RecursiveCharacterTextSplitter.html) includes pre-built lists of separators that are useful for splitting text in a specific programming language.

Supported languages include:

```
"html" | "cpp" | "go" | "java" | "js" | "php" | "proto" | "python" | "rst" | "ruby" | "rust" | "scala" | "swift" | "markdown" | "latex" | "sol"
```

To view the list of separators for a given language, pass one of the values from the list above into the `getSeparatorsForLanguage()` static method
"""

import {
  RecursiveCharacterTextSplitter,
} from "@langchain/textsplitters";

RecursiveCharacterTextSplitter.getSeparatorsForLanguage("js");
# Output:
#   [

#     [32m"\nfunction "[39m, [32m"\nconst "[39m,

#     [32m"\nlet "[39m,      [32m"\nvar "[39m,

#     [32m"\nclass "[39m,    [32m"\nif "[39m,

#     [32m"\nfor "[39m,      [32m"\nwhile "[39m,

#     [32m"\nswitch "[39m,   [32m"\ncase "[39m,

#     [32m"\ndefault "[39m,  [32m"\n\n"[39m,

#     [32m"\n"[39m,          [32m" "[39m,

#     [32m""[39m

#   ]

"""
## JS
Here's an example using the JS text splitter:
"""

const JS_CODE = `
function helloWorld() {
  console.log("Hello, World!");
}

// Call the function
helloWorld();
`

const jsSplitter = RecursiveCharacterTextSplitter.fromLanguage(
    "js", {
      chunkSize: 60,
      chunkOverlap: 0,
    }
)
const jsDocs = await jsSplitter.createDocuments([JS_CODE]);

jsDocs
# Output:
#   [

#     Document {

#       pageContent: [32m'function helloWorld() {\n  console.log("Hello, World!");\n}'[39m,

#       metadata: { loc: { lines: { from: [33m2[39m, to: [33m4[39m } } }

#     },

#     Document {

#       pageContent: [32m"// Call the function\nhelloWorld();"[39m,

#       metadata: { loc: { lines: { from: [33m6[39m, to: [33m7[39m } } }

#     }

#   ]

"""
## Python

Here's an example for Python:
"""

const PYTHON_CODE = `
def hello_world():
    print("Hello, World!")

# Call the function
hello_world()
`

const pythonSplitter = RecursiveCharacterTextSplitter.fromLanguage(
    "python", {
        chunkSize: 50,
        chunkOverlap: 0,
    }
)
const pythonDocs = await pythonSplitter.createDocuments([PYTHON_CODE])
pythonDocs
# Output:
#   [

#     Document {

#       pageContent: [32m'def hello_world():\n    print("Hello, World!")'[39m,

#       metadata: { loc: { lines: { from: [33m2[39m, to: [33m3[39m } } }

#     },

#     Document {

#       pageContent: [32m"# Call the function\nhello_world()"[39m,

#       metadata: { loc: { lines: { from: [33m5[39m, to: [33m6[39m } } }

#     }

#   ]

"""
## Markdown

Here's an example of splitting on markdown separators:
"""

const markdownText = `
# 🦜️🔗 LangChain

⚡ Building applications with LLMs through composability ⚡

## Quick Install

\`\`\`bash
# Hopefully this code block isn't split
pip install langchain
\`\`\`

As an open-source project in a rapidly developing field, we are extremely open to contributions.
`;

const mdSplitter = RecursiveCharacterTextSplitter.fromLanguage(
  "markdown", {
      chunkSize: 60,
      chunkOverlap: 0,
    }
)
const mdDocs = await mdSplitter.createDocuments([markdownText])

mdDocs
# Output:
#   [

#     Document {

#       pageContent: [32m"# 🦜️🔗 LangChain"[39m,

#       metadata: { loc: { lines: { from: [33m2[39m, to: [33m2[39m } } }

#     },

#     Document {

#       pageContent: [32m"⚡ Building applications with LLMs through composability ⚡"[39m,

#       metadata: { loc: { lines: { from: [33m4[39m, to: [33m4[39m } } }

#     },

#     Document {

#       pageContent: [32m"## Quick Install"[39m,

#       metadata: { loc: { lines: { from: [33m6[39m, to: [33m6[39m } } }

#     },

#     Document {

#       pageContent: [32m"```bash\n# Hopefully this code block isn't split"[39m,

#       metadata: { loc: { lines: { from: [33m8[39m, to: [33m9[39m } } }

#     },

#     Document {

#       pageContent: [32m"pip install langchain"[39m,

#       metadata: { loc: { lines: { from: [33m10[39m, to: [33m10[39m } } }

#     },

#     Document {

#       pageContent: [32m"```"[39m,

#       metadata: { loc: { lines: { from: [33m11[39m, to: [33m11[39m } } }

#     },

#     Document {

#       pageContent: [32m"As an open-source project in a rapidly developing field, we"[39m,

#       metadata: { loc: { lines: { from: [33m13[39m, to: [33m13[39m } } }

#     },

#     Document {

#       pageContent: [32m"are extremely open to contributions."[39m,

#       metadata: { loc: { lines: { from: [33m13[39m, to: [33m13[39m } } }

#     }

#   ]

"""
## Latex

Here's an example on Latex text:

"""

const latexText = `
\documentclass{article}

\begin{document}

\maketitle

\section{Introduction}
Large language models (LLMs) are a type of machine learning model that can be trained on vast amounts of text data to generate human-like language. In recent years, LLMs have made significant advances in a variety of natural language processing tasks, including language translation, text generation, and sentiment analysis.

\subsection{History of LLMs}
The earliest LLMs were developed in the 1980s and 1990s, but they were limited by the amount of data that could be processed and the computational power available at the time. In the past decade, however, advances in hardware and software have made it possible to train LLMs on massive datasets, leading to significant improvements in performance.

\subsection{Applications of LLMs}
LLMs have many applications in industry, including chatbots, content creation, and virtual assistants. They can also be used in academia for research in linguistics, psychology, and computational linguistics.

\end{document}
`

const latexSplitter = RecursiveCharacterTextSplitter.fromLanguage(
  "latex", {
      chunkSize: 60,
      chunkOverlap: 0,
    }
)
const latexDocs = await latexSplitter.createDocuments([latexText])

latexDocs
# Output:
#   [

#     Document {

#       pageContent: [32m"documentclass{article}\n\n\begin{document}\n\nmaketitle"[39m,

#       metadata: { loc: { lines: { from: [33m2[39m, to: [33m6[39m } } }

#     },

#     Document {

#       pageContent: [32m"section{Introduction}"[39m,

#       metadata: { loc: { lines: { from: [33m8[39m, to: [33m8[39m } } }

#     },

#     Document {

#       pageContent: [32m"Large language models (LLMs) are a type of machine learning"[39m,

#       metadata: { loc: { lines: { from: [33m9[39m, to: [33m9[39m } } }

#     },

#     Document {

#       pageContent: [32m"model that can be trained on vast amounts of text data to"[39m,

#       metadata: { loc: { lines: { from: [33m9[39m, to: [33m9[39m } } }

#     },

#     Document {

#       pageContent: [32m"generate human-like language. In recent years, LLMs have"[39m,

#       metadata: { loc: { lines: { from: [33m9[39m, to: [33m9[39m } } }

#     },

#     Document {

#       pageContent: [32m"made significant advances in a variety of natural language"[39m,

#       metadata: { loc: { lines: { from: [33m9[39m, to: [33m9[39m } } }

#     },

#     Document {

#       pageContent: [32m"processing tasks, including language translation, text"[39m,

#       metadata: { loc: { lines: { from: [33m9[39m, to: [33m9[39m } } }

#     },

#     Document {

#       pageContent: [32m"generation, and sentiment analysis."[39m,

#       metadata: { loc: { lines: { from: [33m9[39m, to: [33m9[39m } } }

#     },

#     Document {

#       pageContent: [32m"subsection{History of LLMs}"[39m,

#       metadata: { loc: { lines: { from: [33m11[39m, to: [33m11[39m } } }

#     },

#     Document {

#       pageContent: [32m"The earliest LLMs were developed in the 1980s and 1990s,"[39m,

#       metadata: { loc: { lines: { from: [33m12[39m, to: [33m12[39m } } }

#     },

#     Document {

#       pageContent: [32m"but they were limited by the amount of data that could be"[39m,

#       metadata: { loc: { lines: { from: [33m12[39m, to: [33m12[39m } } }

#     },

#     Document {

#       pageContent: [32m"processed and the computational power available at the"[39m,

#       metadata: { loc: { lines: { from: [33m12[39m, to: [33m12[39m } } }

#     },

#     Document {

#       pageContent: [32m"time. In the past decade, however, advances in hardware and"[39m,

#       metadata: { loc: { lines: { from: [33m12[39m, to: [33m12[39m } } }

#     },

#     Document {

#       pageContent: [32m"software have made it possible to train LLMs on massive"[39m,

#       metadata: { loc: { lines: { from: [33m12[39m, to: [33m12[39m } } }

#     },

#     Document {

#       pageContent: [32m"datasets, leading to significant improvements in"[39m,

#       metadata: { loc: { lines: { from: [33m12[39m, to: [33m12[39m } } }

#     },

#     Document {

#       pageContent: [32m"performance."[39m,

#       metadata: { loc: { lines: { from: [33m12[39m, to: [33m12[39m } } }

#     },

#     Document {

#       pageContent: [32m"subsection{Applications of LLMs}"[39m,

#       metadata: { loc: { lines: { from: [33m14[39m, to: [33m14[39m } } }

#     },

#     Document {

#       pageContent: [32m"LLMs have many applications in industry, including"[39m,

#       metadata: { loc: { lines: { from: [33m15[39m, to: [33m15[39m } } }

#     },

#     Document {

#       pageContent: [32m"chatbots, content creation, and virtual assistants. They"[39m,

#       metadata: { loc: { lines: { from: [33m15[39m, to: [33m15[39m } } }

#     },

#     Document {

#       pageContent: [32m"can also be used in academia for research in linguistics,"[39m,

#       metadata: { loc: { lines: { from: [33m15[39m, to: [33m15[39m } } }

#     },

#     Document {

#       pageContent: [32m"psychology, and computational linguistics."[39m,

#       metadata: { loc: { lines: { from: [33m15[39m, to: [33m15[39m } } }

#     },

#     Document {

#       pageContent: [32m"end{document}"[39m,

#       metadata: { loc: { lines: { from: [33m17[39m, to: [33m17[39m } } }

#     }

#   ]

"""
## HTML

Here's an example using an HTML text splitter:

"""

const htmlText = `
<!DOCTYPE html>
<html>
    <head>
        <title>🦜️🔗 LangChain</title>
        <style>
            body {
                font-family: Arial, sans-serif;
            }
            h1 {
                color: darkblue;
            }
        </style>
    </head>
    <body>
        <div>
            <h1>🦜️🔗 LangChain</h1>
            <p>⚡ Building applications with LLMs through composability ⚡</p>
        </div>
        <div>
            As an open-source project in a rapidly developing field, we are extremely open to contributions.
        </div>
    </body>
</html>
`

const htmlSplitter = RecursiveCharacterTextSplitter.fromLanguage(
  "html", {
      chunkSize: 60,
      chunkOverlap: 0,
    }
)
const htmlDocs = await htmlSplitter.createDocuments([htmlText])
htmlDocs
# Output:
#   [

#     Document {

#       pageContent: [32m"<!DOCTYPE html>\n<html>"[39m,

#       metadata: { loc: { lines: { from: [33m2[39m, to: [33m3[39m } } }

#     },

#     Document {

#       pageContent: [32m"<head>\n        <title>🦜️🔗 LangChain</title>"[39m,

#       metadata: { loc: { lines: { from: [33m4[39m, to: [33m5[39m } } }

#     },

#     Document {

#       pageContent: [32m"<style>\n            body {\n                font-family:"[39m,

#       metadata: { loc: { lines: { from: [33m6[39m, to: [33m8[39m } } }

#     },

#     Document {

#       pageContent: [32m"Arial, sans-serif;\n            }\n            h1 {"[39m,

#       metadata: { loc: { lines: { from: [33m8[39m, to: [33m10[39m } } }

#     },

#     Document {

#       pageContent: [32m"color: darkblue;\n            }\n        </style>"[39m,

#       metadata: { loc: { lines: { from: [33m11[39m, to: [33m13[39m } } }

#     },

#     Document {

#       pageContent: [32m"</head>"[39m,

#       metadata: { loc: { lines: { from: [33m14[39m, to: [33m14[39m } } }

#     },

#     Document {

#       pageContent: [32m"<body>"[39m,

#       metadata: { loc: { lines: { from: [33m15[39m, to: [33m15[39m } } }

#     },

#     Document {

#       pageContent: [32m"<div>\n            <h1>🦜️🔗 LangChain</h1>"[39m,

#       metadata: { loc: { lines: { from: [33m16[39m, to: [33m17[39m } } }

#     },

#     Document {

#       pageContent: [32m"<p>⚡ Building applications with LLMs through composability"[39m,

#       metadata: { loc: { lines: { from: [33m18[39m, to: [33m18[39m } } }

#     },

#     Document {

#       pageContent: [32m"⚡</p>\n        </div>"[39m,

#       metadata: { loc: { lines: { from: [33m18[39m, to: [33m19[39m } } }

#     },

#     Document {

#       pageContent: [32m"<div>\n            As an open-source project in a rapidly"[39m,

#       metadata: { loc: { lines: { from: [33m20[39m, to: [33m21[39m } } }

#     },

#     Document {

#       pageContent: [32m"developing field, we are extremely open to contributions."[39m,

#       metadata: { loc: { lines: { from: [33m21[39m, to: [33m21[39m } } }

#     },

#     Document {

#       pageContent: [32m"</div>\n    </body>\n</html>"[39m,

#       metadata: { loc: { lines: { from: [33m22[39m, to: [33m24[39m } } }

#     }

#   ]

"""
## Solidity
Here's an example using of splitting on [Solidity](https://soliditylang.org/) code:
"""

const SOL_CODE = `
pragma solidity ^0.8.20;
contract HelloWorld {
   function add(uint a, uint b) pure public returns(uint) {
       return a + b;
   }
}
`

const solSplitter = RecursiveCharacterTextSplitter.fromLanguage(
    "sol", {
        chunkSize: 128,
        chunkOverlap: 0,
    }
)
const solDocs = await solSplitter.createDocuments([SOL_CODE])
solDocs
# Output:
#   [

#     Document {

#       pageContent: [32m"pragma solidity ^0.8.20;"[39m,

#       metadata: { loc: { lines: { from: [33m2[39m, to: [33m2[39m } } }

#     },

#     Document {

#       pageContent: [32m"contract HelloWorld {\n"[39m +

#         [32m"   function add(uint a, uint b) pure public returns(uint) {\n"[39m +

#         [32m"       return a + "[39m... 9 more characters,

#       metadata: { loc: { lines: { from: [33m3[39m, to: [33m7[39m } } }

#     }

#   ]

"""
## PHP
Here's an example of splitting on PHP code:
"""

const PHP_CODE = `<?php
namespace foo;
class Hello {
    public function __construct() { }
}
function hello() {
    echo "Hello World!";
}
interface Human {
    public function breath();
}
trait Foo { }
enum Color
{
    case Red;
    case Blue;
}`

const phpSplitter = RecursiveCharacterTextSplitter.fromLanguage(
    "php", {
        chunkSize: 50,
        chunkOverlap: 0,
    } 
)
const phpDocs = await phpSplitter.createDocuments([PHP_CODE])

phpDocs
# Output:
#   [

#     Document {

#       pageContent: [32m"<?php\nnamespace foo;"[39m,

#       metadata: { loc: { lines: { from: [33m1[39m, to: [33m2[39m } } }

#     },

#     Document {

#       pageContent: [32m"class Hello {"[39m,

#       metadata: { loc: { lines: { from: [33m3[39m, to: [33m3[39m } } }

#     },

#     Document {

#       pageContent: [32m"public function __construct() { }\n}"[39m,

#       metadata: { loc: { lines: { from: [33m4[39m, to: [33m5[39m } } }

#     },

#     Document {

#       pageContent: [32m'function hello() {\n    echo "Hello World!";\n}'[39m,

#       metadata: { loc: { lines: { from: [33m6[39m, to: [33m8[39m } } }

#     },

#     Document {

#       pageContent: [32m"interface Human {\n    public function breath();\n}"[39m,

#       metadata: { loc: { lines: { from: [33m9[39m, to: [33m11[39m } } }

#     },

#     Document {

#       pageContent: [32m"trait Foo { }\nenum Color\n{\n    case Red;"[39m,

#       metadata: { loc: { lines: { from: [33m12[39m, to: [33m15[39m } } }

#     },

#     Document {

#       pageContent: [32m"case Blue;\n}"[39m,

#       metadata: { loc: { lines: { from: [33m16[39m, to: [33m17[39m } } }

#     }

#   ]

"""
## Next steps

You've now learned a method for splitting text on code-specific separators.

Next, check out the [full tutorial on retrieval-augmented generation](/docs/tutorials/rag).
"""



================================================
FILE: docs/core_docs/docs/how_to/contextual_compression.mdx
================================================
# How to do retrieval with contextual compression

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Retrievers](/docs/concepts/retrievers)
- [Retrieval-augmented generation (RAG)](/docs/tutorials/rag)

:::

One challenge with retrieval is that usually you don't know the specific queries your document storage system will face when you ingest data into the system. This means that the information most relevant to a query may be buried in a document with a lot of irrelevant text. Passing that full document through your application can lead to more expensive LLM calls and poorer responses.

Contextual compression is meant to fix this. The idea is simple: instead of immediately returning retrieved documents as-is, you can compress them using the context of the given query, so that only the relevant information is returned. “Compressing” here refers to both compressing the contents of an individual document and filtering out documents wholesale.

To use the Contextual Compression Retriever, you'll need:

- a base retriever
- a Document Compressor

The Contextual Compression Retriever passes queries to the base retriever, takes the initial documents and passes them through the Document Compressor. The Document Compressor takes a list of documents and shortens it by reducing the contents of documents or dropping documents altogether.

## Using a vanilla vector store retriever

Let's start by initializing a simple vector store retriever and storing the 2023 State of the Union speech (in chunks).
Given an example question, our retriever returns one or two relevant docs and a few irrelevant docs, and even the relevant docs have a lot of irrelevant information in them.
To extract all the context we can, we use an `LLMChainExtractor`, which will iterate over the initially returned documents and extract from each only the content that is relevant to the query.

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/retrievers/contextual_compression.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## `EmbeddingsFilter`

Making an extra LLM call over each retrieved document is expensive and slow. The `EmbeddingsFilter` provides a cheaper and faster option by embedding the documents and query and only returning those documents which have sufficiently similar embeddings to the query.

This is most useful for non-vector store retrievers where we may not have control over the returned chunk size, or as part of a pipeline, outlined below.

Here's an example:

import EmbeddingsFilterExample from "@examples/retrievers/embeddings_filter.ts";

<CodeBlock language="typescript">{EmbeddingsFilterExample}</CodeBlock>

## Stringing compressors and document transformers together

Using the `DocumentCompressorPipeline` we can also easily combine multiple compressors in sequence. Along with compressors we can add BaseDocumentTransformers to our pipeline, which don't perform any contextual compression but simply perform some transformation on a set of documents.
For example `TextSplitters` can be used as document transformers to split documents into smaller pieces, and the `EmbeddingsFilter` can be used to filter out documents based on similarity of the individual chunks to the input query.

Below we create a compressor pipeline by first splitting raw webpage documents retrieved from the [Tavily web search API retriever](/docs/integrations/retrievers/tavily) into smaller chunks, then filtering based on relevance to the query.
The result is smaller chunks that are semantically similar to the input query.
This skips the need to add documents to a vector store to perform similarity search, which can be useful for one-off use cases:

import DocumentCompressorPipelineExample from "@examples/retrievers/document_compressor_pipeline.ts";

<CodeBlock language="typescript">{DocumentCompressorPipelineExample}</CodeBlock>

## Next steps

You've now learned a few ways to use contextual compression to remove bad data from your results.

See the individual sections for deeper dives on specific retrievers, the [broader tutorial on RAG](/docs/tutorials/rag), or this section to learn how to
[create your own custom retriever over any data source](/docs/how_to/custom_retriever/).



================================================
FILE: docs/core_docs/docs/how_to/convert_runnable_to_tool.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to convert Runnables to Tools

```{=mdx}

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Runnables](/docs/concepts/runnables)
- [Tools](/docs/concepts/tools)
- [Agents](https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/)

:::

```

For convenience, `Runnables` that accept a string or object input can be converted to tools using the [`asTool`](https://api.js.langchain.com/classes/langchain_core.runnables.Runnable.html#asTool) method, which allows for the specification of names, descriptions, and additional schema information for arguments.

Here we will demonstrate how to use this method to convert a LangChain `Runnable` into a tool that can be used by agents, chains, or chat models.

```{=mdx}
:::caution Compatibility

This functionality requires `@langchain/core>=0.2.16`. Please see here for a [guide on upgrading](/docs/how_to/installation/#installing-integration-packages).

:::
```

## `asTool`

Tools have some additional requirements over general Runnables:

- Their inputs are constrained to be serializable, specifically strings and objects;
- They contain names and descriptions indicating how and when they should be used;
- They contain a detailed `schema` property for their arguments. That is, while a tool (as a `Runnable`) might accept a single object input, the specific keys and type information needed to populate an object should be specified in the `schema` field.

The `asTool()` method therefore requires this additional information to create a tool from a runnable. Here's a basic example:
"""

import { RunnableLambda } from "@langchain/core/runnables";
import { z } from "zod";

const schema = z.object({
  a: z.number(),
  b: z.array(z.number()),
});


const runnable = RunnableLambda.from((input: z.infer<typeof schema>) => {
  return input.a * Math.max(...input.b);
});

const asTool = runnable.asTool({
  name: "My tool",
  description: "Explanation of when to use the tool.",
  schema,
});

asTool.description
# Output:
#   Explanation of when to use the tool.


await asTool.invoke({ a: 3, b: [1, 2] })
# Output:
#   6


"""
Runnables that take string inputs are also supported:
"""

const firstRunnable = RunnableLambda.from<string, string>((input) => {
  return input + "a";
})

const secondRunnable = RunnableLambda.from<string, string>((input) => {
  return input + "z";
})

const runnable = firstRunnable.pipe(secondRunnable)
const asTool = runnable.asTool({
  name: "append_letters",
  description: "Adds letters to a string.",
  schema: z.string(),
})

asTool.description;
# Output:
#   Adds letters to a string.


await asTool.invoke("b")
# Output:
#   baz


"""
## In an agents

Below we will incorporate LangChain Runnables as tools in an [agent](/docs/concepts/agents) application. We will demonstrate with:

- a document [retriever](/docs/concepts/retrievers);
- a simple [RAG](/docs/tutorials/rag/) chain, allowing an agent to delegate relevant queries to it.

We first instantiate a chat model that supports [tool calling](/docs/how_to/tool_calling/):

```{=mdx}
<ChatModelTabs
  customVarName="llm"
/>
```
"""

"""
Following the [RAG tutorial](/docs/tutorials/rag/), let's first construct a retriever:
"""

import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({ model: "gpt-3.5-turbo-0125", temperature: 0 })

import { Document } from "@langchain/core/documents"
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "@langchain/openai";

const documents = [
  new Document({
    pageContent: "Dogs are great companions, known for their loyalty and friendliness.",
  }),
  new Document({
    pageContent: "Cats are independent pets that often enjoy their own space.",
  }),
]

const vectorstore = await MemoryVectorStore.fromDocuments(
  documents, new OpenAIEmbeddings(),
);

const retriever = vectorstore.asRetriever({
  k: 1,
  searchType: "similarity",
});

"""
We next create a pre-built [LangGraph agent](/docs/how_to/migrate_agent/) and provide it with the tool:
"""

import { createReactAgent } from "@langchain/langgraph/prebuilt";

const tools = [
  retriever.asTool({
    name: "pet_info_retriever",
    description: "Get information about pets.",
    schema: z.string(),
  })
];

const agent = createReactAgent({ llm: llm, tools });

const stream = await agent.stream({"messages": [["human", "What are dogs known for?"]]});

for await (const chunk of stream) {
  // Log output from the agent or tools node
  if (chunk.agent) {
    console.log("AGENT:", chunk.agent.messages[0]);
  } else if (chunk.tools) {
    console.log("TOOLS:", chunk.tools.messages[0]);
  }
  console.log("----");
}
# Output:
#   AGENT: AIMessage {

#     "id": "chatcmpl-9m9RIN1GQVeXcrVdp0lNBTcZFVHb9",

#     "content": "",

#     "additional_kwargs": {

#       "tool_calls": [

#         {

#           "id": "call_n30LPDbegmytrj5GdUxZt9xn",

#           "type": "function",

#           "function": "[Object]"

#         }

#       ]

#     },

#     "response_metadata": {

#       "tokenUsage": {

#         "completionTokens": 17,

#         "promptTokens": 52,

#         "totalTokens": 69

#       },

#       "finish_reason": "tool_calls"

#     },

#     "tool_calls": [

#       {

#         "name": "pet_info_retriever",

#         "args": {

#           "input": "dogs"

#         },

#         "type": "tool_call",

#         "id": "call_n30LPDbegmytrj5GdUxZt9xn"

#       }

#     ],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 52,

#       "output_tokens": 17,

#       "total_tokens": 69

#     }

#   }

#   ----

#   TOOLS: ToolMessage {

#     "content": "[{\"pageContent\":\"Dogs are great companions, known for their loyalty and friendliness.\",\"metadata\":{}}]",

#     "name": "pet_info_retriever",

#     "additional_kwargs": {},

#     "response_metadata": {},

#     "tool_call_id": "call_n30LPDbegmytrj5GdUxZt9xn"

#   }

#   ----

#   AGENT: AIMessage {

#     "id": "chatcmpl-9m9RJ3TT3ITfv6R0Tb7pcrNOUtnm8",

#     "content": "Dogs are known for being great companions, known for their loyalty and friendliness.",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "completionTokens": 18,

#         "promptTokens": 104,

#         "totalTokens": 122

#       },

#       "finish_reason": "stop"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 104,

#       "output_tokens": 18,

#       "total_tokens": 122

#     }

#   }

#   ----


"""
This [LangSmith trace](https://smith.langchain.com/public/5e141617-ae82-44af-8fe0-b64dbd007826/r) shows what's going on under the hood for the above run.

Going further, we can even create a tool from a full [RAG chain](/docs/tutorials/rag/):
"""

import { StringOutputParser } from "@langchain/core/output_parsers";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnableSequence } from "@langchain/core/runnables";

const SYSTEM_TEMPLATE = `
You are an assistant for question-answering tasks.
Use the below context to answer the question. If
you don't know the answer, say you don't know.
Use three sentences maximum and keep the answer
concise.

Answer in the style of {answer_style}.

Context: {context}`;

const prompt = ChatPromptTemplate.fromMessages([
  ["system", SYSTEM_TEMPLATE],
  ["human", "{question}"],
]);

const ragChain = RunnableSequence.from([
  {
    context: (input, config) => retriever.invoke(input.question, config),
    question: (input) => input.question,
    answer_style: (input) => input.answer_style,
  },
  prompt,
  llm,
  new StringOutputParser(),
]);

"""
Below we again invoke the agent. Note that the agent populates the required parameters in its `tool_calls`:
"""

const ragTool = ragChain.asTool({
  name: "pet_expert",
  description: "Get information about pets.",
  schema: z.object({
    context: z.string(),
    question: z.string(),
    answer_style: z.string(),
  }),
});

const agent = createReactAgent({ llm: llm, tools: [ragTool] });

const stream = await agent.stream({
  messages: [
    ["human", "What would a pirate say dogs are known for?"]
  ]
});

for await (const chunk of stream) {
  // Log output from the agent or tools node
  if (chunk.agent) {
    console.log("AGENT:", chunk.agent.messages[0]);
  } else if (chunk.tools) {
    console.log("TOOLS:", chunk.tools.messages[0]);
  }
  console.log("----");
}
# Output:
#   AGENT: AIMessage {

#     "id": "chatcmpl-9m9RKY2nAa8LeGoBiO7N1SR4nAoED",

#     "content": "",

#     "additional_kwargs": {

#       "tool_calls": [

#         {

#           "id": "call_ukzivO4jRn1XdDpuVTI6CvtU",

#           "type": "function",

#           "function": "[Object]"

#         }

#       ]

#     },

#     "response_metadata": {

#       "tokenUsage": {

#         "completionTokens": 30,

#         "promptTokens": 63,

#         "totalTokens": 93

#       },

#       "finish_reason": "tool_calls"

#     },

#     "tool_calls": [

#       {

#         "name": "pet_expert",

#         "args": {

#           "context": "pirate",

#           "question": "What are dogs known for?",

#           "answer_style": "short"

#         },

#         "type": "tool_call",

#         "id": "call_ukzivO4jRn1XdDpuVTI6CvtU"

#       }

#     ],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 63,

#       "output_tokens": 30,

#       "total_tokens": 93

#     }

#   }

#   ----

#   TOOLS: ToolMessage {

#     "content": "Dogs are known for their loyalty, companionship, and ability to provide emotional support to their owners.",

#     "name": "pet_expert",

#     "additional_kwargs": {},

#     "response_metadata": {},

#     "tool_call_id": "call_ukzivO4jRn1XdDpuVTI6CvtU"

#   }

#   ----

#   AGENT: AIMessage {

#     "id": "chatcmpl-9m9RMwAEc14TTKtitq3CH2x9wpGik",

#     "content": "A pirate would say that dogs are known for their loyalty, companionship, and ability to provide emotional support to their owners.",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "completionTokens": 26,

#         "promptTokens": 123,

#         "totalTokens": 149

#       },

#       "finish_reason": "stop"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 123,

#       "output_tokens": 26,

#       "total_tokens": 149

#     }

#   }

#   ----


"""
See this [LangSmith trace](https://smith.langchain.com/public/147ae4e6-4dfb-4dd9-8ca0-5c5b954f08ac/r) for the above run to see what's going on internally.

## Related

- [How to: create custom tools](/docs/how_to/custom_tools)
- [How to: pass tool results back to model](/docs/how_to/tool_results_pass_to_model/)
- [How to: stream events from child runs within a custom tool](/docs/how_to/tool_stream_events)

"""



================================================
FILE: docs/core_docs/docs/how_to/custom_callbacks.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to create custom callback handlers

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Callbacks](/docs/concepts/callbacks)

:::

LangChain has some built-in callback handlers, but you will often want to create your own handlers with custom logic.

To create a custom callback handler, we need to determine the [event(s)](https://api.js.langchain.com/interfaces/langchain_core.callbacks_base.CallbackHandlerMethods.html) we want our callback handler to handle as well as what we want our callback handler to do when the event is triggered. Then all we need to do is attach the callback handler to the object, for example via [the constructor](/docs/how_to/callbacks_constructor) or [at runtime](/docs/how_to/callbacks_runtime).

An easy way to construct a custom callback handler is to initialize it as an object whose keys are functions with names matching the events we want to handle. Here's an example that only handles the start of a chat model and streamed tokens from the model run:
"""

import { ChatPromptTemplate } from "@langchain/core/prompts";
import { ChatAnthropic } from "@langchain/anthropic";

const prompt = ChatPromptTemplate.fromTemplate(`What is 1 + {number}?`);
const model = new ChatAnthropic({
  model: "claude-3-sonnet-20240229",
});

const chain = prompt.pipe(model);

const customHandler = {
  handleChatModelStart: async (llm, inputMessages, runId) => {
    console.log("Chat model start:", llm, inputMessages, runId)
  },
  handleLLMNewToken: async (token) => {
    console.log("Chat model new token", token);
  }
};

const stream = await chain.stream({ number: "2" }, { callbacks: [customHandler] });

for await (const _ of stream) {
  // Just consume the stream so the callbacks run
}
# Output:
#   Chat model start: {

#     lc: 1,

#     type: "constructor",

#     id: [ "langchain", "chat_models", "anthropic", "ChatAnthropic" ],

#     kwargs: {

#       callbacks: undefined,

#       model: "claude-3-sonnet-20240229",

#       verbose: undefined,

#       anthropic_api_key: { lc: 1, type: "secret", id: [ "ANTHROPIC_API_KEY" ] },

#       api_key: { lc: 1, type: "secret", id: [ "ANTHROPIC_API_KEY" ] }

#     }

#   } [

#     [

#       HumanMessage {

#         lc_serializable: true,

#         lc_kwargs: {

#           content: "What is 1 + 2?",

#           additional_kwargs: {},

#           response_metadata: {}

#         },

#         lc_namespace: [ "langchain_core", "messages" ],

#         content: "What is 1 + 2?",

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: {}

#       }

#     ]

#   ] b6e3b7ad-c602-4cef-9652-d51781a657b7

#   Chat model new token The

#   Chat model new token  sum

#   Chat model new token  of

#   Chat model new token  

#   Chat model new token 1

#   Chat model new token  

#   Chat model new token an

#   Chat model new token d 

#   Chat model new token 2

#   Chat model new token  

#   Chat model new token is

#   Chat model new token  

#   Chat model new token 3

#   Chat model new token .


"""
You can see [this reference page](https://api.js.langchain.com/interfaces/langchain_core.callbacks_base.CallbackHandlerMethods.html) for a list of events you can handle. Note that the `handleChain*` events run for most LCEL runnables.

## Next steps

You've now learned how to create your own custom callback handlers.

Next, check out the other how-to guides in this section, such as [how to await callbacks in serverless environments](/docs/how_to/callbacks_serverless).
"""



================================================
FILE: docs/core_docs/docs/how_to/custom_chat.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_position: 4
---

# How to create a custom chat model class

```{=mdx}
:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat models](/docs/concepts/chat_models)

:::
```

This notebook goes over how to create a custom chat model wrapper, in case you want to use your own chat model or a different wrapper than one that is directly supported in LangChain.

There are a few required things that a chat model needs to implement after extending the [`SimpleChatModel` class](https://api.js.langchain.com/classes/langchain_core.language_models_chat_models.SimpleChatModel.html):

- A `_call` method that takes in a list of messages and call options (which includes things like `stop` sequences), and returns a string.
- A `_llmType` method that returns a string. Used for logging purposes only.

You can also implement the following optional method:

- A `_streamResponseChunks` method that returns an `AsyncGenerator` and yields [`ChatGenerationChunks`](https://api.js.langchain.com/classes/langchain_core.outputs.ChatGenerationChunk.html). This allows the LLM to support streaming outputs.

Let's implement a very simple custom chat model that just echoes back the first `n` characters of the input.
"""

import {
  SimpleChatModel,
  type BaseChatModelParams,
} from "@langchain/core/language_models/chat_models";
import { CallbackManagerForLLMRun } from "@langchain/core/callbacks/manager";
import { AIMessageChunk, type BaseMessage } from "@langchain/core/messages";
import { ChatGenerationChunk } from "@langchain/core/outputs";

interface CustomChatModelInput extends BaseChatModelParams {
  n: number;
}

class CustomChatModel extends SimpleChatModel {
  n: number;

  constructor(fields: CustomChatModelInput) {
    super(fields);
    this.n = fields.n;
  }

  _llmType() {
    return "custom";
  }

  async _call(
    messages: BaseMessage[],
    options: this["ParsedCallOptions"],
    runManager?: CallbackManagerForLLMRun
  ): Promise<string> {
    if (!messages.length) {
      throw new Error("No messages provided.");
    }
    // Pass `runManager?.getChild()` when invoking internal runnables to enable tracing
    // await subRunnable.invoke(params, runManager?.getChild());
    if (typeof messages[0].content !== "string") {
      throw new Error("Multimodal messages are not supported.");
    }
    return messages[0].content.slice(0, this.n);
  }

  async *_streamResponseChunks(
    messages: BaseMessage[],
    options: this["ParsedCallOptions"],
    runManager?: CallbackManagerForLLMRun
  ): AsyncGenerator<ChatGenerationChunk> {
    if (!messages.length) {
      throw new Error("No messages provided.");
    }
    if (typeof messages[0].content !== "string") {
      throw new Error("Multimodal messages are not supported.");
    }
    // Pass `runManager?.getChild()` when invoking internal runnables to enable tracing
    // await subRunnable.invoke(params, runManager?.getChild());
    for (const letter of messages[0].content.slice(0, this.n)) {
      yield new ChatGenerationChunk({
        message: new AIMessageChunk({
          content: letter,
        }),
        text: letter,
      });
      // Trigger the appropriate callback for new chunks
      await runManager?.handleLLMNewToken(letter);
    }
  }
}

"""
We can now use this as any other chat model:
"""

const chatModel = new CustomChatModel({ n: 4 });

await chatModel.invoke([["human", "I am an LLM"]]);
# Output:
#   AIMessage {

#     lc_serializable: true,

#     lc_kwargs: {

#       content: 'I am',

#       tool_calls: [],

#       invalid_tool_calls: [],

#       additional_kwargs: {},

#       response_metadata: {}

#     },

#     lc_namespace: [ 'langchain_core', 'messages' ],

#     content: 'I am',

#     name: undefined,

#     additional_kwargs: {},

#     response_metadata: {},

#     id: undefined,

#     tool_calls: [],

#     invalid_tool_calls: [],

#     usage_metadata: undefined

#   }


"""
And support streaming:
"""

const stream = await chatModel.stream([["human", "I am an LLM"]]);

for await (const chunk of stream) {
  console.log(chunk);
}
# Output:
#   AIMessageChunk {

#     lc_serializable: true,

#     lc_kwargs: {

#       content: 'I',

#       tool_calls: [],

#       invalid_tool_calls: [],

#       tool_call_chunks: [],

#       additional_kwargs: {},

#       response_metadata: {}

#     },

#     lc_namespace: [ 'langchain_core', 'messages' ],

#     content: 'I',

#     name: undefined,

#     additional_kwargs: {},

#     response_metadata: {},

#     id: undefined,

#     tool_calls: [],

#     invalid_tool_calls: [],

#     tool_call_chunks: [],

#     usage_metadata: undefined

#   }

#   AIMessageChunk {

#     lc_serializable: true,

#     lc_kwargs: {

#       content: ' ',

#       tool_calls: [],

#       invalid_tool_calls: [],

#       tool_call_chunks: [],

#       additional_kwargs: {},

#       response_metadata: {}

#     },

#     lc_namespace: [ 'langchain_core', 'messages' ],

#     content: ' ',

#     name: undefined,

#     additional_kwargs: {},

#     response_metadata: {},

#     id: undefined,

#     tool_calls: [],

#     invalid_tool_calls: [],

#     tool_call_chunks: [],

#     usage_metadata: undefined

#   }

#   AIMessageChunk {

#     lc_serializable: true,

#     lc_kwargs: {

#       content: 'a',

#       tool_calls: [],

#       invalid_tool_calls: [],

#       tool_call_chunks: [],

#       additional_kwargs: {},

#       response_metadata: {}

#     },

#     lc_namespace: [ 'langchain_core', 'messages' ],

#     content: 'a',

#     name: undefined,

#     additional_kwargs: {},

#     response_metadata: {},

#     id: undefined,

#     tool_calls: [],

#     invalid_tool_calls: [],

#     tool_call_chunks: [],

#     usage_metadata: undefined

#   }

#   AIMessageChunk {

#     lc_serializable: true,

#     lc_kwargs: {

#       content: 'm',

#       tool_calls: [],

#       invalid_tool_calls: [],

#       tool_call_chunks: [],

#       additional_kwargs: {},

#       response_metadata: {}

#     },

#     lc_namespace: [ 'langchain_core', 'messages' ],

#     content: 'm',

#     name: undefined,

#     additional_kwargs: {},

#     response_metadata: {},

#     id: undefined,

#     tool_calls: [],

#     invalid_tool_calls: [],

#     tool_call_chunks: [],

#     usage_metadata: undefined

#   }


"""
## Richer outputs

If you want to take advantage of LangChain's callback system for functionality like token tracking, you can extend the [`BaseChatModel`](https://api.js.langchain.com/classes/langchain_core.language_models_chat_models.BaseChatModel.html) class and implement the lower level
`_generate` method. It also takes a list of `BaseMessage`s as input, but requires you to construct and return a `ChatGeneration` object that permits additional metadata.
Here's an example:
"""

import { AIMessage, BaseMessage } from "@langchain/core/messages";
import { ChatResult } from "@langchain/core/outputs";
import {
  BaseChatModel,
  BaseChatModelCallOptions,
  BaseChatModelParams,
} from "@langchain/core/language_models/chat_models";
import { CallbackManagerForLLMRun } from "@langchain/core/callbacks/manager";

interface AdvancedCustomChatModelOptions
  extends BaseChatModelCallOptions {}

interface AdvancedCustomChatModelParams extends BaseChatModelParams {
  n: number;
}

class AdvancedCustomChatModel extends BaseChatModel<AdvancedCustomChatModelOptions> {
  n: number;

  static lc_name(): string {
    return "AdvancedCustomChatModel";
  }

  constructor(fields: AdvancedCustomChatModelParams) {
    super(fields);
    this.n = fields.n;
  }

  async _generate(
    messages: BaseMessage[],
    options: this["ParsedCallOptions"],
    runManager?: CallbackManagerForLLMRun
  ): Promise<ChatResult> {
    if (!messages.length) {
      throw new Error("No messages provided.");
    }
    if (typeof messages[0].content !== "string") {
      throw new Error("Multimodal messages are not supported.");
    }
    // Pass `runManager?.getChild()` when invoking internal runnables to enable tracing
    // await subRunnable.invoke(params, runManager?.getChild());
    const content = messages[0].content.slice(0, this.n);
    const tokenUsage = {
      usedTokens: this.n,
    };
    return {
      generations: [{ message: new AIMessage({ content }), text: content }],
      llmOutput: { tokenUsage },
    };
  }

  _llmType(): string {
    return "advanced_custom_chat_model";
  }
}

"""
This will pass the additional returned information in callback events and in the `streamEvents method:
"""

const chatModel = new AdvancedCustomChatModel({ n: 4 });

const eventStream = await chatModel.streamEvents([["human", "I am an LLM"]], {
  version: "v2",
});

for await (const event of eventStream) {
  if (event.event === "on_chat_model_end") {
    console.log(JSON.stringify(event, null, 2));
  }
}
# Output:
#   {

#     "event": "on_chat_model_end",

#     "data": {

#       "output": {

#         "lc": 1,

#         "type": "constructor",

#         "id": [

#           "langchain_core",

#           "messages",

#           "AIMessage"

#         ],

#         "kwargs": {

#           "content": "I am",

#           "tool_calls": [],

#           "invalid_tool_calls": [],

#           "additional_kwargs": {},

#           "response_metadata": {

#             "tokenUsage": {

#               "usedTokens": 4

#             }

#           }

#         }

#       }

#     },

#     "run_id": "11dbdef6-1b91-407e-a497-1a1ce2974788",

#     "name": "AdvancedCustomChatModel",

#     "tags": [],

#     "metadata": {

#       "ls_model_type": "chat"

#     }

#   }


"""
## Tracing (advanced)

If you are implementing a custom chat model and want to use it with a tracing service like [LangSmith](https://smith.langchain.com/),
you can automatically log params used for a given invocation by implementing the `invocationParams()` method on the model.

This method is purely optional, but anything it returns will be logged as metadata for the trace.

Here's one pattern you might use:
"""

import { CallbackManagerForLLMRun } from "@langchain/core/callbacks/manager";
import { BaseChatModel, type BaseChatModelCallOptions, type BaseChatModelParams } from "@langchain/core/language_models/chat_models";
import { BaseMessage } from "@langchain/core/messages";
import { ChatResult } from "@langchain/core/outputs";

interface CustomChatModelOptions extends BaseChatModelCallOptions {
  // Some required or optional inner args
  tools: Record<string, any>[];
}

interface CustomChatModelParams extends BaseChatModelParams {
  temperature: number;
  n: number;
}

class CustomChatModel extends BaseChatModel<CustomChatModelOptions> {
  temperature: number;

  n: number;

  static lc_name(): string {
    return "CustomChatModel";
  }

  constructor(fields: CustomChatModelParams) {
    super(fields);
    this.temperature = fields.temperature;
    this.n = fields.n;
  }

  // Anything returned in this method will be logged as metadata in the trace.
  // It is common to pass it any options used to invoke the function.
  invocationParams(options?: this["ParsedCallOptions"]) {
    return {
      tools: options?.tools,
      n: this.n,
    };
  }

  async _generate(
    messages: BaseMessage[],
    options: this["ParsedCallOptions"],
    runManager?: CallbackManagerForLLMRun
  ): Promise<ChatResult> {
    if (!messages.length) {
      throw new Error("No messages provided.");
    }
    if (typeof messages[0].content !== "string") {
      throw new Error("Multimodal messages are not supported.");
    }
    const additionalParams = this.invocationParams(options);
    const content = await someAPIRequest(messages, additionalParams);
    return {
      generations: [{ message: new AIMessage({ content }), text: content }],
      llmOutput: {},
    };
  }

  _llmType(): string {
    return "advanced_custom_chat_model";
  }
}



================================================
FILE: docs/core_docs/docs/how_to/custom_llm.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_position: 3
---

# How to create a custom LLM class

```{=mdx}
:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [LLMs](/docs/concepts/text_llms)

:::
```

This notebook goes over how to create a custom LLM wrapper, in case you want to use your own LLM or a different wrapper than one that is directly supported in LangChain.

There are a few required things that a custom LLM needs to implement after extending the [`LLM` class](https://api.js.langchain.com/classes/langchain_core.language_models_llms.LLM.html):

- A `_call` method that takes in a string and call options (which includes things like `stop` sequences), and returns a string.
- A `_llmType` method that returns a string. Used for logging purposes only.

You can also implement the following optional method:

- A `_streamResponseChunks` method that returns an `AsyncIterator` and yields [`GenerationChunks`](https://api.js.langchain.com/classes/langchain_core.outputs.GenerationChunk.html). This allows the LLM to support streaming outputs.

Let's implement a very simple custom LLM that just echoes back the first `n` characters of the input.
"""

import { LLM, type BaseLLMParams } from "@langchain/core/language_models/llms";
import type { CallbackManagerForLLMRun } from "@langchain/core/callbacks/manager";
import { GenerationChunk } from "@langchain/core/outputs";

interface CustomLLMInput extends BaseLLMParams {
  n: number;
}

class CustomLLM extends LLM {
  n: number;

  constructor(fields: CustomLLMInput) {
    super(fields);
    this.n = fields.n;
  }

  _llmType() {
    return "custom";
  }

  async _call(
    prompt: string,
    options: this["ParsedCallOptions"],
    runManager: CallbackManagerForLLMRun
  ): Promise<string> {
    // Pass `runManager?.getChild()` when invoking internal runnables to enable tracing
    // await subRunnable.invoke(params, runManager?.getChild());
    return prompt.slice(0, this.n);
  }

  async *_streamResponseChunks(
    prompt: string,
    options: this["ParsedCallOptions"],
    runManager?: CallbackManagerForLLMRun
  ): AsyncGenerator<GenerationChunk> {
    // Pass `runManager?.getChild()` when invoking internal runnables to enable tracing
    // await subRunnable.invoke(params, runManager?.getChild());
    for (const letter of prompt.slice(0, this.n)) {
      yield new GenerationChunk({
        text: letter,
      });
      // Trigger the appropriate callback
      await runManager?.handleLLMNewToken(letter);
    }
  }
}

"""
We can now use this as any other LLM:
"""

const llm = new CustomLLM({ n: 4 });

await llm.invoke("I am an LLM");
# Output:
#   I am


"""
And support streaming:
"""

const stream = await llm.stream("I am an LLM");

for await (const chunk of stream) {
  console.log(chunk);
}
# Output:
#   I

#    

#   a

#   m


"""
## Richer outputs

If you want to take advantage of LangChain's callback system for functionality like token tracking, you can extend the [`BaseLLM`](https://api.js.langchain.com/classes/langchain_core.language_models_llms.BaseLLM.html) class and implement the lower level
`_generate` method. Rather than taking a single string as input and a single string output, it can take multiple input strings and map each to multiple string outputs.
Additionally, it returns a `Generation` output with fields for additional metadata rather than just a string.
"""

import { CallbackManagerForLLMRun } from "@langchain/core/callbacks/manager";
import { LLMResult } from "@langchain/core/outputs";
import {
  BaseLLM,
  BaseLLMCallOptions,
  BaseLLMParams,
} from "@langchain/core/language_models/llms";

interface AdvancedCustomLLMCallOptions extends BaseLLMCallOptions {}

interface AdvancedCustomLLMParams extends BaseLLMParams {
  n: number;
}

class AdvancedCustomLLM extends BaseLLM<AdvancedCustomLLMCallOptions> {
  n: number;

  constructor(fields: AdvancedCustomLLMParams) {
    super(fields);
    this.n = fields.n;
  }

  _llmType() {
    return "advanced_custom_llm";
  }

  async _generate(
    inputs: string[],
    options: this["ParsedCallOptions"],
    runManager?: CallbackManagerForLLMRun
  ): Promise<LLMResult> {
    const outputs = inputs.map((input) => input.slice(0, this.n));
    // Pass `runManager?.getChild()` when invoking internal runnables to enable tracing
    // await subRunnable.invoke(params, runManager?.getChild());

    // One input could generate multiple outputs.
    const generations = outputs.map((output) => [
      {
        text: output,
        // Optional additional metadata for the generation
        generationInfo: { outputCount: 1 },
      },
    ]);
    const tokenUsage = {
      usedTokens: this.n,
    };
    return {
      generations,
      llmOutput: { tokenUsage },
    };
  }
}

"""
This will pass the additional returned information in callback events and in the `streamEvents method:
"""

const llm = new AdvancedCustomLLM({ n: 4 });

const eventStream = await llm.streamEvents("I am an LLM", {
  version: "v2",
});

for await (const event of eventStream) {
  if (event.event === "on_llm_end") {
    console.log(JSON.stringify(event, null, 2));
  }
}
# Output:
#   {

#     "event": "on_llm_end",

#     "data": {

#       "output": {

#         "generations": [

#           [

#             {

#               "text": "I am",

#               "generationInfo": {

#                 "outputCount": 1

#               }

#             }

#           ]

#         ],

#         "llmOutput": {

#           "tokenUsage": {

#             "usedTokens": 4

#           }

#         }

#       }

#     },

#     "run_id": "a9ce50e4-f85b-41eb-bcbe-793efc52f9d8",

#     "name": "AdvancedCustomLLM",

#     "tags": [],

#     "metadata": {}

#   }




================================================
FILE: docs/core_docs/docs/how_to/custom_retriever.mdx
================================================
# How to write a custom retriever class

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Retrievers](/docs/concepts/retrievers)

:::

To create your own retriever, you need to extend the [`BaseRetriever`](https://api.js.langchain.com/classes/langchain_core.retrievers.BaseRetriever.html) class
and implement a `_getRelevantDocuments` method that takes a `string` as its first parameter (and an optional `runManager` for tracing).
This method should return an array of `Document`s fetched from some source. This process can involve calls to a database, to the web using `fetch`, or any other source.
Note the underscore before `_getRelevantDocuments()`. The base class wraps the non-prefixed version in order to automatically handle tracing of the original call.

Here's an example of a custom retriever that returns static documents:

```ts
import {
  BaseRetriever,
  type BaseRetrieverInput,
} from "@langchain/core/retrievers";
import type { CallbackManagerForRetrieverRun } from "@langchain/core/callbacks/manager";
import { Document } from "@langchain/core/documents";

export interface CustomRetrieverInput extends BaseRetrieverInput {}

export class CustomRetriever extends BaseRetriever {
  lc_namespace = ["langchain", "retrievers"];

  constructor(fields?: CustomRetrieverInput) {
    super(fields);
  }

  async _getRelevantDocuments(
    query: string,
    runManager?: CallbackManagerForRetrieverRun
  ): Promise<Document[]> {
    // Pass `runManager?.getChild()` when invoking internal runnables to enable tracing
    // const additionalDocs = await someOtherRunnable.invoke(params, runManager?.getChild());
    return [
      // ...additionalDocs,
      new Document({
        pageContent: `Some document pertaining to ${query}`,
        metadata: {},
      }),
      new Document({
        pageContent: `Some other document pertaining to ${query}`,
        metadata: {},
      }),
    ];
  }
}
```

Then, you can call `.invoke()` as follows:

```ts
const retriever = new CustomRetriever({});

await retriever.invoke("LangChain docs");
```

```
[
  Document {
    pageContent: 'Some document pertaining to LangChain docs',
    metadata: {}
  },
  Document {
    pageContent: 'Some other document pertaining to LangChain docs',
    metadata: {}
  }
]
```

## Next steps

You've now seen an example of implementing your own custom retriever.

Next, check out the individual sections for deeper dives on specific retrievers, or the [broader tutorial on RAG](/docs/tutorials/rag).



================================================
FILE: docs/core_docs/docs/how_to/custom_tools.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
keywords: [custom tool, custom tools]
---
"""

"""
# How to create Tools

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [LangChain tools](/docs/concepts/tools)
- [Agents](/docs/concepts/agents)

:::

When constructing your own agent, you will need to provide it with a list of Tools that it can use. While LangChain includes some prebuilt tools, it can often be more useful to use tools that use custom logic. This guide will walk you through some ways you can create custom tools.

The biggest difference here is that the first function requires an object with multiple input fields, while the second one only accepts an object with a single field. Some older agents only work with functions that require single inputs, so it's important to understand the distinction.

LangChain has a handful of ways to construct tools for different applications. Below I'll show the two most common ways to create tools, and where you might use each.
"""

"""
## Tool schema

```{=mdx}
:::caution Compatibility
Only available in `@langchain/core` version 0.2.19 and above.
:::
```

The simplest way to create a tool is through the [`StructuredToolParams`](https://api.js.langchain.com/interfaces/_langchain_core.tools.StructuredToolParams.html) schema. Every chat model which supports tool calling in LangChain accepts binding tools to the model through this schema. This schema has only three fields

- `name` - The name of the tool.
- `schema` - The schema of the tool, defined with a Zod object.
- `description` (optional) - A description of the tool.

This schema does not include a function to pair with the tool, and for this reason it should only be used in situations where the generated output does not need to be passed as the input argument to a function.
"""

import { z } from "zod";
import { StructuredToolParams } from "@langchain/core/tools";

const simpleToolSchema: StructuredToolParams = {
  name: "get_current_weather",
  description: "Get the current weather for a location",
  schema: z.object({
    city: z.string().describe("The city to get the weather for"),
    state: z.string().optional().describe("The state to get the weather for"),
  })
}

"""
## `tool` function

```{=mdx}
:::caution Compatibility
Only available in `@langchain/core` version 0.2.7 and above.
:::
```

The [`tool`](https://api.js.langchain.com/classes/langchain_core.tools.Tool.html) wrapper function is a convenience method for turning a JavaScript function into a tool. It requires the function itself along with some additional arguments that define your tool. You should use this over `StructuredToolParams` tools when the resulting tool call executes a function. The most important are:

- The tool's `name`, which the LLM will use as context as well as to reference the tool
- An optional, but recommended `description`, which the LLM will use as context to know when to use the tool
- A `schema`, which defines the shape of the tool's input

The `tool` function will return an instance of the [`StructuredTool`](https://api.js.langchain.com/classes/langchain_core.tools.StructuredTool.html) class, so it is compatible with all the existing tool calling infrastructure in the LangChain library.
"""

import { z } from "zod";
import { tool } from "@langchain/core/tools";

const adderSchema = z.object({
  a: z.number(),
  b: z.number(),
});
const adderTool = tool(async (input): Promise<string> => {
  const sum = input.a + input.b;
  return `The sum of ${input.a} and ${input.b} is ${sum}`;
}, {
  name: "adder",
  description: "Adds two numbers together",
  schema: adderSchema,
});

await adderTool.invoke({ a: 1, b: 2 });
# Output:
#   [32m"The sum of 1 and 2 is 3"[39m

"""
## `DynamicStructuredTool`

You can also use the [`DynamicStructuredTool`](https://api.js.langchain.com/classes/langchain_core.tools.DynamicStructuredTool.html) class to declare tools. Here's an example - note that tools must always return strings!
"""

import { DynamicStructuredTool } from "@langchain/core/tools";
import { z } from "zod";

const multiplyTool = new DynamicStructuredTool({
  name: "multiply",
  description: "multiply two numbers together",
  schema: z.object({
    a: z.number().describe("the first number to multiply"),
    b: z.number().describe("the second number to multiply"),
  }),
  func: async ({ a, b }: { a: number; b: number; }) => {
    return (a * b).toString();
  },
});

await multiplyTool.invoke({ a: 8, b: 9, });
# Output:
#   [32m"72"[39m

"""
## `DynamicTool`

For older agents that require tools which accept only a single input, you can pass the relevant parameters to the [`DynamicTool`](https://api.js.langchain.com/classes/langchain_core.tools.DynamicTool.html) class. This is useful when working with older agents that only support tools that accept a single input. In this case, no schema is required:
"""

import { DynamicTool } from "@langchain/core/tools";

const searchTool = new DynamicTool({
  name: "search",
  description: "look things up online",
  func: async (_input: string) => {
    return "LangChain";
  },
});

await searchTool.invoke("foo");
# Output:
#   [32m"LangChain"[39m

"""
# Returning artifacts of Tool execution

Sometimes there are artifacts of a tool's execution that we want to make accessible to downstream components in our chain or agent, but that we don't want to expose to the model itself. For example if a tool returns custom objects like Documents, we may want to pass some view or metadata about this output to the model without passing the raw output to the model. At the same time, we may want to be able to access this full output elsewhere, for example in downstream tools.

The Tool and `ToolMessage` interfaces make it possible to distinguish between the parts of the tool output meant for the model (`ToolMessage.content`) and those parts which are meant for use outside the model (`ToolMessage.artifact`).

```{=mdx}
:::caution Compatibility
This functionality was added in `@langchain/core>=0.2.16`. Please make sure your package is up to date.
:::
```

If you want your tool to distinguish between message content and other artifacts, we need to do three things:

- Set the `response_format` parameter to `"content_and_artifact"` when defining the tool.
- Make sure that we return a tuple of `[content, artifact]`.
- Call the tool with a a [`ToolCall`](https://api.js.langchain.com/types/langchain_core.messages_tool.ToolCall.html)    (like the ones generated by tool-calling models) rather than with the required schema directly.

Here's an example of what this looks like. First, create a new tool:
"""

import { z } from "zod";
import { tool } from "@langchain/core/tools";

const randomIntToolSchema = z.object({
  min: z.number(),
  max: z.number(),
  size: z.number(),
});

const generateRandomInts = tool(async ({ min, max, size }) => {
  const array: number[] = [];
  for (let i = 0; i < size; i++) {
    array.push(Math.floor(Math.random() * (max - min + 1)) + min);
  }
  return [
    `Successfully generated array of ${size} random ints in [${min}, ${max}].`,
    array,
  ];
}, {
  name: "generateRandomInts",
  description: "Generate size random ints in the range [min, max].",
  schema: randomIntToolSchema,
  responseFormat: "content_and_artifact",
});

"""
If you invoke our tool directly with the tool arguments, you'll get back just the `content` part of the output:
"""

await generateRandomInts.invoke({ min: 0, max: 9, size: 10 });
# Output:
#   [32m"Successfully generated array of 10 random ints in [0, 9]."[39m

"""
But if you invoke our tool with a `ToolCall`, you'll get back a ToolMessage that contains both the content and artifact generated by the `Tool`:
"""

await generateRandomInts.invoke({
  name: "generateRandomInts",
  args: { min: 0, max: 9, size: 10 },
  id: "123", // required
  type: "tool_call",
});
# Output:
#   ToolMessage {

#     lc_serializable: [33mtrue[39m,

#     lc_kwargs: {

#       content: [32m"Successfully generated array of 10 random ints in [0, 9]."[39m,

#       artifact: [

#         [33m7[39m, [33m7[39m, [33m1[39m, [33m4[39m, [33m8[39m,

#         [33m4[39m, [33m8[39m, [33m3[39m, [33m0[39m, [33m9[39m

#       ],

#       tool_call_id: [32m"123"[39m,

#       name: [32m"generateRandomInts"[39m,

#       additional_kwargs: {},

#       response_metadata: {}

#     },

#     lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#     content: [32m"Successfully generated array of 10 random ints in [0, 9]."[39m,

#     name: [32m"generateRandomInts"[39m,

#     additional_kwargs: {},

#     response_metadata: {},

#     id: [90mundefined[39m,

#     tool_call_id: [32m"123"[39m,

#     artifact: [

#       [33m7[39m, [33m7[39m, [33m1[39m, [33m4[39m, [33m8[39m,

#       [33m4[39m, [33m8[39m, [33m3[39m, [33m0[39m, [33m9[39m

#     ]

#   }

"""
## Related

You've now seen a few ways to create custom tools in LangChain.

Next, you might be interested in learning [how to use a chat model to call tools](/docs/how_to/tool_calling/).

You can also check out how to create your own [custom versions of other modules](/docs/how_to/#custom).
"""



================================================
FILE: docs/core_docs/docs/how_to/debugging.mdx
================================================
# How to debug your LLM apps

import CodeBlock from "@theme/CodeBlock";

Like building any type of software, at some point you'll need to debug when building with LLMs.
A model call will fail, or model output will be misformatted, or there will be some nested model calls and it won't be clear where along the way an incorrect output was created.

Here are a few different tools and functionalities to aid in debugging.

## Tracing

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.
As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.
The best way to do this is with [LangSmith](https://smith.langchain.com).

After you sign up at the link above, make sure to set your environment variables to start logging traces:

```shell
export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."

# Reduce tracing latency if you are not in a serverless environment
# export LANGCHAIN_CALLBACKS_BACKGROUND=true
```

Let's suppose we have an agent, and want to visualize the actions it takes and tool outputs it receives. Without any debugging, here's what we see:

import SimpleAgent from "@examples/guides/debugging/simple_agent.ts";

<CodeBlock language="typescript">{SimpleAgent}</CodeBlock>

```bash
{
  input: 'Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?',
  output: 'So Christopher Nolan, the director of the 2023 film Oppenheimer, is 53 years old, which is approximately 19,345 days old (assuming 365 days per year).'
}
```

We don't get much output, but since we set up LangSmith we can easily see what happened under the hood:

https://smith.langchain.com/public/fd3a4aa1-dfea-4d17-9d44-a306e7b230d3/r

## `verbose`

If you're prototyping in Jupyter Notebooks or running Node scripts, it can be helpful to print out the intermediate steps of a chain run.

There are a number of ways to enable printing at varying degrees of verbosity.

### `{ verbose: true }`

Setting the `verbose` parameter will cause any LangChain component with callback support (chains, models, agents, tools, retrievers) to print the inputs they receive and outputs they generate.
This is the most verbose setting and will fully log raw inputs and outputs.

import SimpleAgentVerbose from "@examples/guides/debugging/simple_agent_verbose.ts";

<CodeBlock language="typescript">{SimpleAgentVerbose}</CodeBlock>

<details> 
  <summary>Console output</summary>

```bash
[chain/start] [1:chain:AgentExecutor] Entering Chain run with input: {
  "input": "Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?"
}
[chain/start] [1:chain:AgentExecutor > 2:chain:ToolCallingAgent] Entering Chain run with input: {
  "input": "Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?",
  "steps": []
}
[chain/start] [1:chain:AgentExecutor > 2:chain:ToolCallingAgent > 3:chain:RunnableAssign] Entering Chain run with input: {
  "input": ""
}
[chain/start] [1:chain:AgentExecutor > 2:chain:ToolCallingAgent > 3:chain:RunnableAssign > 4:chain:RunnableMap] Entering Chain run with input: {
  "input": ""
}
[chain/start] [1:chain:AgentExecutor > 2:chain:ToolCallingAgent > 3:chain:RunnableAssign > 4:chain:RunnableMap > 5:chain:RunnableLambda] Entering Chain run with input: {
  "input": ""
}
[chain/end] [1:chain:AgentExecutor > 2:chain:ToolCallingAgent > 3:chain:RunnableAssign > 4:chain:RunnableMap > 5:chain:RunnableLambda] [0ms] Exiting Chain run with output: {
  "output": []
}
[chain/end] [1:chain:AgentExecutor > 2:chain:ToolCallingAgent > 3:chain:RunnableAssign > 4:chain:RunnableMap] [1ms] Exiting Chain run with output: {
  "agent_scratchpad": []
}
[chain/end] [1:chain:AgentExecutor > 2:chain:ToolCallingAgent > 3:chain:RunnableAssign] [1ms] Exiting Chain run with output: {
  "input": "Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?",
  "steps": [],
  "agent_scratchpad": []
}
[chain/start] [1:chain:AgentExecutor > 2:chain:ToolCallingAgent > 6:prompt:ChatPromptTemplate] Entering Chain run with input: {
  "input": "Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?",
  "steps": [],
  "agent_scratchpad": []
}
[chain/end] [1:chain:AgentExecutor > 2:chain:ToolCallingAgent > 6:prompt:ChatPromptTemplate] [0ms] Exiting Chain run with output: {
  "lc": 1,
  "type": "constructor",
  "id": [
    "langchain_core",
    "prompt_values",
    "ChatPromptValue"
  ],
  "kwargs": {
    "messages": [
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "SystemMessage"
        ],
        "kwargs": {
          "content": "You are a helpful assistant",
          "additional_kwargs": {},
          "response_metadata": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "HumanMessage"
        ],
        "kwargs": {
          "content": "Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?",
          "additional_kwargs": {},
          "response_metadata": {}
        }
      }
    ]
  }
}
[llm/start] [1:chain:AgentExecutor > 2:chain:ToolCallingAgent > 7:llm:ChatAnthropic] Entering LLM run with input: {
  "messages": [
    [
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "SystemMessage"
        ],
        "kwargs": {
          "content": "You are a helpful assistant",
          "additional_kwargs": {},
          "response_metadata": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "HumanMessage"
        ],
        "kwargs": {
          "content": "Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?",
          "additional_kwargs": {},
          "response_metadata": {}
        }
      }
    ]
  ]
}
[llm/start] [1:llm:ChatAnthropic] Entering LLM run with input: {
  "messages": [
    [
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "SystemMessage"
        ],
        "kwargs": {
          "content": "You are a helpful assistant",
          "additional_kwargs": {},
          "response_metadata": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "HumanMessage"
        ],
        "kwargs": {
          "content": "Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?",
          "additional_kwargs": {},
          "response_metadata": {}
        }
      }
    ]
  ]
}
[llm/end] [1:chain:AgentExecutor > 2:chain:ToolCallingAgent > 7:llm:ChatAnthropic] [1.98s] Exiting LLM run with output: {
  "generations": [
    [
      {
        "text": "",
        "message": {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain_core",
            "messages",
            "AIMessageChunk"
          ],
          "kwargs": {
            "content": [
              {
                "type": "tool_use",
                "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
                "name": "tavily_search_results_json",
                "input": {
                  "input": "Oppenheimer 2023 film director age"
                }
              }
            ],
            "additional_kwargs": {
              "id": "msg_015MqAHr84dBCAqBgjou41Km",
              "type": "message",
              "role": "assistant",
              "model": "claude-3-sonnet-20240229",
              "stop_sequence": null,
              "usage": {
                "input_tokens": 409,
                "output_tokens": 68
              },
              "stop_reason": "tool_use"
            },
            "tool_call_chunks": [
              {
                "name": "tavily_search_results_json",
                "args": "{\"input\":\"Oppenheimer 2023 film director age\"}",
                "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
                "index": 0
              }
            ],
            "tool_calls": [
              {
                "name": "tavily_search_results_json",
                "args": {
                  "input": "Oppenheimer 2023 film director age"
                },
                "id": "toolu_01NUVejujVo2y8WGVtZ49KAN"
              }
            ],
            "invalid_tool_calls": [],
            "response_metadata": {}
          }
        }
      }
    ]
  ]
}
[llm/end] [1:llm:ChatAnthropic] [1.98s] Exiting LLM run with output: {
  "generations": [
    [
      {
        "text": "",
        "message": {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain_core",
            "messages",
            "AIMessageChunk"
          ],
          "kwargs": {
            "content": [
              {
                "type": "tool_use",
                "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
                "name": "tavily_search_results_json",
                "input": {
                  "input": "Oppenheimer 2023 film director age"
                }
              }
            ],
            "additional_kwargs": {
              "id": "msg_015MqAHr84dBCAqBgjou41Km",
              "type": "message",
              "role": "assistant",
              "model": "claude-3-sonnet-20240229",
              "stop_sequence": null,
              "usage": {
                "input_tokens": 409,
                "output_tokens": 68
              },
              "stop_reason": "tool_use"
            },
            "tool_call_chunks": [
              {
                "name": "tavily_search_results_json",
                "args": "{\"input\":\"Oppenheimer 2023 film director age\"}",
                "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
                "index": 0
              }
            ],
            "tool_calls": [
              {
                "name": "tavily_search_results_json",
                "args": {
                  "input": "Oppenheimer 2023 film director age"
                },
                "id": "toolu_01NUVejujVo2y8WGVtZ49KAN"
              }
            ],
            "invalid_tool_calls": [],
            "response_metadata": {}
          }
        }
      }
    ]
  ]
}
[chain/start] [1:chain:AgentExecutor > 2:chain:ToolCallingAgent > 8:parser:ToolCallingAgentOutputParser] Entering Chain run with input: {
  "lc": 1,
  "type": "constructor",
  "id": [
    "langchain_core",
    "messages",
    "AIMessageChunk"
  ],
  "kwargs": {
    "content": [
      {
        "type": "tool_use",
        "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
        "name": "tavily_search_results_json",
        "input": {
          "input": "Oppenheimer 2023 film director age"
        }
      }
    ],
    "additional_kwargs": {
      "id": "msg_015MqAHr84dBCAqBgjou41Km",
      "type": "message",
      "role": "assistant",
      "model": "claude-3-sonnet-20240229",
      "stop_sequence": null,
      "usage": {
        "input_tokens": 409,
        "output_tokens": 68
      },
      "stop_reason": "tool_use"
    },
    "tool_call_chunks": [
      {
        "name": "tavily_search_results_json",
        "args": "{\"input\":\"Oppenheimer 2023 film director age\"}",
        "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
        "index": 0
      }
    ],
    "tool_calls": [
      {
        "name": "tavily_search_results_json",
        "args": {
          "input": "Oppenheimer 2023 film director age"
        },
        "id": "toolu_01NUVejujVo2y8WGVtZ49KAN"
      }
    ],
    "invalid_tool_calls": [],
    "response_metadata": {}
  }
}
[chain/end] [1:chain:AgentExecutor > 2:chain:ToolCallingAgent > 8:parser:ToolCallingAgentOutputParser] [0ms] Exiting Chain run with output: {
  "output": [
    {
      "tool": "tavily_search_results_json",
      "toolInput": {
        "input": "Oppenheimer 2023 film director age"
      },
      "toolCallId": "toolu_01NUVejujVo2y8WGVtZ49KAN",
      "log": "Invoking \"tavily_search_results_json\" with {\"input\":\"Oppenheimer 2023 film director age\"}\n[{\"type\":\"tool_use\",\"id\":\"toolu_01NUVejujVo2y8WGVtZ49KAN\",\"name\":\"tavily_search_results_json\",\"input\":{\"input\":\"Oppenheimer 2023 film director age\"}}]",
      "messageLog": [
        {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain_core",
            "messages",
            "AIMessageChunk"
          ],
          "kwargs": {
            "content": [
              {
                "type": "tool_use",
                "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
                "name": "tavily_search_results_json",
                "input": {
                  "input": "Oppenheimer 2023 film director age"
                }
              }
            ],
            "additional_kwargs": {
              "id": "msg_015MqAHr84dBCAqBgjou41Km",
              "type": "message",
              "role": "assistant",
              "model": "claude-3-sonnet-20240229",
              "stop_sequence": null,
              "usage": {
                "input_tokens": 409,
                "output_tokens": 68
              },
              "stop_reason": "tool_use"
            },
            "tool_call_chunks": [
              {
                "name": "tavily_search_results_json",
                "args": "{\"input\":\"Oppenheimer 2023 film director age\"}",
                "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
                "index": 0
              }
            ],
            "tool_calls": [
              {
                "name": "tavily_search_results_json",
                "args": {
                  "input": "Oppenheimer 2023 film director age"
                },
                "id": "toolu_01NUVejujVo2y8WGVtZ49KAN"
              }
            ],
            "invalid_tool_calls": [],
            "response_metadata": {}
          }
        }
      ]
    }
  ]
}
[chain/end] [1:chain:AgentExecutor > 2:chain:ToolCallingAgent] [1.98s] Exiting Chain run with output: {
  "output": [
    {
      "tool": "tavily_search_results_json",
      "toolInput": {
        "input": "Oppenheimer 2023 film director age"
      },
      "toolCallId": "toolu_01NUVejujVo2y8WGVtZ49KAN",
      "log": "Invoking \"tavily_search_results_json\" with {\"input\":\"Oppenheimer 2023 film director age\"}\n[{\"type\":\"tool_use\",\"id\":\"toolu_01NUVejujVo2y8WGVtZ49KAN\",\"name\":\"tavily_search_results_json\",\"input\":{\"input\":\"Oppenheimer 2023 film director age\"}}]",
      "messageLog": [
        {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain_core",
            "messages",
            "AIMessageChunk"
          ],
          "kwargs": {
            "content": [
              {
                "type": "tool_use",
                "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
                "name": "tavily_search_results_json",
                "input": {
                  "input": "Oppenheimer 2023 film director age"
                }
              }
            ],
            "additional_kwargs": {
              "id": "msg_015MqAHr84dBCAqBgjou41Km",
              "type": "message",
              "role": "assistant",
              "model": "claude-3-sonnet-20240229",
              "stop_sequence": null,
              "usage": {
                "input_tokens": 409,
                "output_tokens": 68
              },
              "stop_reason": "tool_use"
            },
            "tool_call_chunks": [
              {
                "name": "tavily_search_results_json",
                "args": "{\"input\":\"Oppenheimer 2023 film director age\"}",
                "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
                "index": 0
              }
            ],
            "tool_calls": [
              {
                "name": "tavily_search_results_json",
                "args": {
                  "input": "Oppenheimer 2023 film director age"
                },
                "id": "toolu_01NUVejujVo2y8WGVtZ49KAN"
              }
            ],
            "invalid_tool_calls": [],
            "response_metadata": {}
          }
        }
      ]
    }
  ]
}
[agent/action] [1:chain:AgentExecutor] Agent selected action: {
  "tool": "tavily_search_results_json",
  "toolInput": {
    "input": "Oppenheimer 2023 film director age"
  },
  "toolCallId": "toolu_01NUVejujVo2y8WGVtZ49KAN",
  "log": "Invoking \"tavily_search_results_json\" with {\"input\":\"Oppenheimer 2023 film director age\"}\n[{\"type\":\"tool_use\",\"id\":\"toolu_01NUVejujVo2y8WGVtZ49KAN\",\"name\":\"tavily_search_results_json\",\"input\":{\"input\":\"Oppenheimer 2023 film director age\"}}]",
  "messageLog": [
    {
      "lc": 1,
      "type": "constructor",
      "id": [
        "langchain_core",
        "messages",
        "AIMessageChunk"
      ],
      "kwargs": {
        "content": [
          {
            "type": "tool_use",
            "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
            "name": "tavily_search_results_json",
            "input": {
              "input": "Oppenheimer 2023 film director age"
            }
          }
        ],
        "additional_kwargs": {
          "id": "msg_015MqAHr84dBCAqBgjou41Km",
          "type": "message",
          "role": "assistant",
          "model": "claude-3-sonnet-20240229",
          "stop_sequence": null,
          "usage": {
            "input_tokens": 409,
            "output_tokens": 68
          },
          "stop_reason": "tool_use"
        },
        "tool_call_chunks": [
          {
            "name": "tavily_search_results_json",
            "args": "{\"input\":\"Oppenheimer 2023 film director age\"}",
            "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
            "index": 0
          }
        ],
        "tool_calls": [
          {
            "name": "tavily_search_results_json",
            "args": {
              "input": "Oppenheimer 2023 film director age"
            },
            "id": "toolu_01NUVejujVo2y8WGVtZ49KAN"
          }
        ],
        "invalid_tool_calls": [],
        "response_metadata": {}
      }
    }
  ]
}
[tool/start] [1:chain:AgentExecutor > 9:tool:TavilySearchResults] Entering Tool run with input: "Oppenheimer 2023 film director age"
[tool/start] [1:tool:TavilySearchResults] Entering Tool run with input: "Oppenheimer 2023 film director age"
[tool/end] [1:chain:AgentExecutor > 9:tool:TavilySearchResults] [2.20s] Exiting Tool run with output: "[{"title":"Oppenheimer (2023) - IMDb","url":"https://www.imdb.com/title/tt15398776/","content":"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.","score":0.96643,"raw_content":null},{"title":"Christopher Nolan's Oppenheimer - Rotten Tomatoes","url":"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/","content":"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.","score":0.92804,"raw_content":null},{"title":"Oppenheimer (film) - Wikipedia","url":"https://en.wikipedia.org/wiki/Oppenheimer_(film)","content":"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\nCritical response\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \"more objective view of his story from a different character's point of view\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \"big-atures\", since the special effects team had tried to build the models as physically large as possible. He felt that \"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \"emotional\" and resembling that of a thriller, while also remarking that Nolan had \"Trojan-Horsed a biopic into a thriller\".[72]\nCasting\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\", while also underscoring that it is a \"huge shift in perception about the reality of Oppenheimer's perception\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.","score":0.92404,"raw_content":null},{"title":"'Oppenheimer' Director Christopher Nolan On Filmmaking at 53: \"I Try to ...","url":"https://www.everythingzoomer.com/arts-entertainment/2023/11/21/oppenheimer-director-christopher-nolan-on-filmmaking-at-53-i-try-to-challenge-myself-with-every-film/","content":"Oppenheimer will be available to own on 4K Ultra HD, Blu-ray and DVD — including more than three hours of bonus features — on November 21.\nRELATED:\nVisiting the Trinity Site Featured in ‘Oppenheimer’ Is a Sobering Reminder of the Horror of Nuclear Weapons\nBarbenheimer: How ‘Barbie’ and ‘Oppenheimer’ Became the Unlikely Movie Marriage of the Summer\nBlast From the Past: ‘Asteroid City’ & ‘Oppenheimer’ and the Age of Nuclear Anxiety\nEXPLORE  HealthMoneyTravelFoodStyleBook ClubClassifieds#ZoomerDailyPolicy & PerspectiveArts & EntertainmentStars & RoyaltySex & Love\nCONNECT  FacebookTwitterInstagram\nSUBSCRIBE  Terms of Subscription ServiceE-NewslettersSubscribe to Zoomer Magazine\nBROWSE  AboutMastheadContact UsAdvertise with UsPrivacy Policy\nEverythingZoomer.com is part of the ZoomerMedia Digital Network “I think with experience — and with the experience of watching your films with an audience over the years — you do more and more recognize the human elements that people respond to, and the things that move you and the things that move the audience.”\n “What’s interesting, as you watch the films over time, is that some of his preoccupations are the same, but then some of them have changed over time with who he is as a person and what’s going on in his own life,” Thomas said.\n The British-American director’s latest explosive drama, Oppenheimer, which has earned upwards of US$940 million at the global box office, follows theoretical physicist J. Robert Oppenheimer (played by Cillian Murphy) as he leads the team creating the first atomic bomb, as director of the Manhattan Project’s Los Alamos Laboratory.\n Subscribe\nEverything Zoomer\n‘Oppenheimer’ Director Christopher Nolan On Filmmaking at 53: “I Try to Challenge Myself with Every Film”\nDirector Christopher Nolan poses upon his arrival for the premiere of the movie 'Oppenheimer' in Paris on July 11, 2023.","score":0.92002,"raw_content":null},{"title":"'Oppenheimer' Review: A Man for Our Time - The New York Times","url":"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html","content":"Instead, it is here that the film’s complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era’s most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation — Strauss’s role in the hearing and his relationship with Oppenheimer directly affected the confirmation’s outcome — to create a dialectical synthesis. To signal his conceit, he stamps the film with the words “fission” (a splitting into parts) and “fusion” (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology — it is a lot.\n It’s also at Berkeley that Oppenheimer meets the project’s military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos’s director, despite the leftist causes he supported — among them, the fight against fascism during the Spanish Civil War — and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\n","score":0.91831,"raw_content":null}]"
[tool/end] [1:tool:TavilySearchResults] [2.20s] Exiting Tool run with output: "[{"title":"Oppenheimer (2023) - IMDb","url":"https://www.imdb.com/title/tt15398776/","content":"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.","score":0.96643,"raw_content":null},{"title":"Christopher Nolan's Oppenheimer - Rotten Tomatoes","url":"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/","content":"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.","score":0.92804,"raw_content":null},{"title":"Oppenheimer (film) - Wikipedia","url":"https://en.wikipedia.org/wiki/Oppenheimer_(film)","content":"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\nCritical response\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \"more objective view of his story from a different character's point of view\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \"big-atures\", since the special effects team had tried to build the models as physically large as possible. He felt that \"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \"emotional\" and resembling that of a thriller, while also remarking that Nolan had \"Trojan-Horsed a biopic into a thriller\".[72]\nCasting\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\", while also underscoring that it is a \"huge shift in perception about the reality of Oppenheimer's perception\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.","score":0.92404,"raw_content":null},{"title":"'Oppenheimer' Director Christopher Nolan On Filmmaking at 53: \"I Try to ...","url":"https://www.everythingzoomer.com/arts-entertainment/2023/11/21/oppenheimer-director-christopher-nolan-on-filmmaking-at-53-i-try-to-challenge-myself-with-every-film/","content":"Oppenheimer will be available to own on 4K Ultra HD, Blu-ray and DVD — including more than three hours of bonus features — on November 21.\nRELATED:\nVisiting the Trinity Site Featured in ‘Oppenheimer’ Is a Sobering Reminder of the Horror of Nuclear Weapons\nBarbenheimer: How ‘Barbie’ and ‘Oppenheimer’ Became the Unlikely Movie Marriage of the Summer\nBlast From the Past: ‘Asteroid City’ & ‘Oppenheimer’ and the Age of Nuclear Anxiety\nEXPLORE  HealthMoneyTravelFoodStyleBook ClubClassifieds#ZoomerDailyPolicy & PerspectiveArts & EntertainmentStars & RoyaltySex & Love\nCONNECT  FacebookTwitterInstagram\nSUBSCRIBE  Terms of Subscription ServiceE-NewslettersSubscribe to Zoomer Magazine\nBROWSE  AboutMastheadContact UsAdvertise with UsPrivacy Policy\nEverythingZoomer.com is part of the ZoomerMedia Digital Network “I think with experience — and with the experience of watching your films with an audience over the years — you do more and more recognize the human elements that people respond to, and the things that move you and the things that move the audience.”\n “What’s interesting, as you watch the films over time, is that some of his preoccupations are the same, but then some of them have changed over time with who he is as a person and what’s going on in his own life,” Thomas said.\n The British-American director’s latest explosive drama, Oppenheimer, which has earned upwards of US$940 million at the global box office, follows theoretical physicist J. Robert Oppenheimer (played by Cillian Murphy) as he leads the team creating the first atomic bomb, as director of the Manhattan Project’s Los Alamos Laboratory.\n Subscribe\nEverything Zoomer\n‘Oppenheimer’ Director Christopher Nolan On Filmmaking at 53: “I Try to Challenge Myself with Every Film”\nDirector Christopher Nolan poses upon his arrival for the premiere of the movie 'Oppenheimer' in Paris on July 11, 2023.","score":0.92002,"raw_content":null},{"title":"'Oppenheimer' Review: A Man for Our Time - The New York Times","url":"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html","content":"Instead, it is here that the film’s complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era’s most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation — Strauss’s role in the hearing and his relationship with Oppenheimer directly affected the confirmation’s outcome — to create a dialectical synthesis. To signal his conceit, he stamps the film with the words “fission” (a splitting into parts) and “fusion” (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology — it is a lot.\n It’s also at Berkeley that Oppenheimer meets the project’s military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos’s director, despite the leftist causes he supported — among them, the fight against fascism during the Spanish Civil War — and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\n","score":0.91831,"raw_content":null}]"
[chain/start] [1:chain:AgentExecutor > 10:chain:ToolCallingAgent] Entering Chain run with input: {
  "input": "Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?",
  "steps": [
    {
      "action": {
        "tool": "tavily_search_results_json",
        "toolInput": {
          "input": "Oppenheimer 2023 film director age"
        },
        "toolCallId": "toolu_01NUVejujVo2y8WGVtZ49KAN",
        "log": "Invoking \"tavily_search_results_json\" with {\"input\":\"Oppenheimer 2023 film director age\"}\n[{\"type\":\"tool_use\",\"id\":\"toolu_01NUVejujVo2y8WGVtZ49KAN\",\"name\":\"tavily_search_results_json\",\"input\":{\"input\":\"Oppenheimer 2023 film director age\"}}]",
        "messageLog": [
          {
            "lc": 1,
            "type": "constructor",
            "id": [
              "langchain_core",
              "messages",
              "AIMessageChunk"
            ],
            "kwargs": {
              "content": [
                {
                  "type": "tool_use",
                  "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
                  "name": "tavily_search_results_json",
                  "input": {
                    "input": "Oppenheimer 2023 film director age"
                  }
                }
              ],
              "additional_kwargs": {
                "id": "msg_015MqAHr84dBCAqBgjou41Km",
                "type": "message",
                "role": "assistant",
                "model": "claude-3-sonnet-20240229",
                "stop_sequence": null,
                "usage": {
                  "input_tokens": 409,
                  "output_tokens": 68
                },
                "stop_reason": "tool_use"
              },
              "tool_call_chunks": [
                {
                  "name": "tavily_search_results_json",
                  "args": "{\"input\":\"Oppenheimer 2023 film director age\"}",
                  "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
                  "index": 0
                }
              ],
              "tool_calls": [
                {
                  "name": "tavily_search_results_json",
                  "args": {
                    "input": "Oppenheimer 2023 film director age"
                  },
                  "id": "toolu_01NUVejujVo2y8WGVtZ49KAN"
                }
              ],
              "invalid_tool_calls": [],
              "response_metadata": {}
            }
          }
        ]
      },
      "observation": "[{\"title\":\"Oppenheimer (2023) - IMDb\",\"url\":\"https://www.imdb.com/title/tt15398776/\",\"content\":\"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\",\"score\":0.96643,\"raw_content\":null},{\"title\":\"Christopher Nolan's Oppenheimer - Rotten Tomatoes\",\"url\":\"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/\",\"content\":\"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.\",\"score\":0.92804,\"raw_content\":null},{\"title\":\"Oppenheimer (film) - Wikipedia\",\"url\":\"https://en.wikipedia.org/wiki/Oppenheimer_(film)\",\"content\":\"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\\nCritical response\\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \\\"more objective view of his story from a different character's point of view\\\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \\\"big-atures\\\", since the special effects team had tried to build the models as physically large as possible. He felt that \\\"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\\\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \\\"emotional\\\" and resembling that of a thriller, while also remarking that Nolan had \\\"Trojan-Horsed a biopic into a thriller\\\".[72]\\nCasting\\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\\\", while also underscoring that it is a \\\"huge shift in perception about the reality of Oppenheimer's perception\\\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \\\"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\\\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.\",\"score\":0.92404,\"raw_content\":null},{\"title\":\"'Oppenheimer' Director Christopher Nolan On Filmmaking at 53: \\\"I Try to ...\",\"url\":\"https://www.everythingzoomer.com/arts-entertainment/2023/11/21/oppenheimer-director-christopher-nolan-on-filmmaking-at-53-i-try-to-challenge-myself-with-every-film/\",\"content\":\"Oppenheimer will be available to own on 4K Ultra HD, Blu-ray and DVD — including more than three hours of bonus features — on November 21.\\nRELATED:\\nVisiting the Trinity Site Featured in ‘Oppenheimer’ Is a Sobering Reminder of the Horror of Nuclear Weapons\\nBarbenheimer: How ‘Barbie’ and ‘Oppenheimer’ Became the Unlikely Movie Marriage of the Summer\\nBlast From the Past: ‘Asteroid City’ & ‘Oppenheimer’ and the Age of Nuclear Anxiety\\nEXPLORE  HealthMoneyTravelFoodStyleBook ClubClassifieds#ZoomerDailyPolicy & PerspectiveArts & EntertainmentStars & RoyaltySex & Love\\nCONNECT  FacebookTwitterInstagram\\nSUBSCRIBE  Terms of Subscription ServiceE-NewslettersSubscribe to Zoomer Magazine\\nBROWSE  AboutMastheadContact UsAdvertise with UsPrivacy Policy\\nEverythingZoomer.com is part of the ZoomerMedia Digital Network “I think with experience — and with the experience of watching your films with an audience over the years — you do more and more recognize the human elements that people respond to, and the things that move you and the things that move the audience.”\\n “What’s interesting, as you watch the films over time, is that some of his preoccupations are the same, but then some of them have changed over time with who he is as a person and what’s going on in his own life,” Thomas said.\\n The British-American director’s latest explosive drama, Oppenheimer, which has earned upwards of US$940 million at the global box office, follows theoretical physicist J. Robert Oppenheimer (played by Cillian Murphy) as he leads the team creating the first atomic bomb, as director of the Manhattan Project’s Los Alamos Laboratory.\\n Subscribe\\nEverything Zoomer\\n‘Oppenheimer’ Director Christopher Nolan On Filmmaking at 53: “I Try to Challenge Myself with Every Film”\\nDirector Christopher Nolan poses upon his arrival for the premiere of the movie 'Oppenheimer' in Paris on July 11, 2023.\",\"score\":0.92002,\"raw_content\":null},{\"title\":\"'Oppenheimer' Review: A Man for Our Time - The New York Times\",\"url\":\"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html\",\"content\":\"Instead, it is here that the film’s complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era’s most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation — Strauss’s role in the hearing and his relationship with Oppenheimer directly affected the confirmation’s outcome — to create a dialectical synthesis. To signal his conceit, he stamps the film with the words “fission” (a splitting into parts) and “fusion” (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology — it is a lot.\\n It’s also at Berkeley that Oppenheimer meets the project’s military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos’s director, despite the leftist causes he supported — among them, the fight against fascism during the Spanish Civil War — and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\\n\",\"score\":0.91831,\"raw_content\":null}]"
    }
  ]
}
[chain/start] [1:chain:AgentExecutor > 10:chain:ToolCallingAgent > 11:chain:RunnableAssign] Entering Chain run with input: {
  "input": ""
}
[chain/start] [1:chain:AgentExecutor > 10:chain:ToolCallingAgent > 11:chain:RunnableAssign > 12:chain:RunnableMap] Entering Chain run with input: {
  "input": ""
}
[chain/start] [1:chain:AgentExecutor > 10:chain:ToolCallingAgent > 11:chain:RunnableAssign > 12:chain:RunnableMap > 13:chain:RunnableLambda] Entering Chain run with input: {
  "input": ""
}
[chain/end] [1:chain:AgentExecutor > 10:chain:ToolCallingAgent > 11:chain:RunnableAssign > 12:chain:RunnableMap > 13:chain:RunnableLambda] [1ms] Exiting Chain run with output: {
  "output": [
    {
      "lc": 1,
      "type": "constructor",
      "id": [
        "langchain_core",
        "messages",
        "AIMessageChunk"
      ],
      "kwargs": {
        "content": [
          {
            "type": "tool_use",
            "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
            "name": "tavily_search_results_json",
            "input": {
              "input": "Oppenheimer 2023 film director age"
            }
          }
        ],
        "additional_kwargs": {
          "id": "msg_015MqAHr84dBCAqBgjou41Km",
          "type": "message",
          "role": "assistant",
          "model": "claude-3-sonnet-20240229",
          "stop_sequence": null,
          "usage": {
            "input_tokens": 409,
            "output_tokens": 68
          },
          "stop_reason": "tool_use"
        },
        "tool_call_chunks": [
          {
            "name": "tavily_search_results_json",
            "args": "{\"input\":\"Oppenheimer 2023 film director age\"}",
            "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
            "index": 0
          }
        ],
        "tool_calls": [
          {
            "name": "tavily_search_results_json",
            "args": {
              "input": "Oppenheimer 2023 film director age"
            },
            "id": "toolu_01NUVejujVo2y8WGVtZ49KAN"
          }
        ],
        "invalid_tool_calls": [],
        "response_metadata": {}
      }
    },
    {
      "lc": 1,
      "type": "constructor",
      "id": [
        "langchain_core",
        "messages",
        "ToolMessage"
      ],
      "kwargs": {
        "tool_call_id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
        "content": "[{\"title\":\"Oppenheimer (2023) - IMDb\",\"url\":\"https://www.imdb.com/title/tt15398776/\",\"content\":\"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\",\"score\":0.96643,\"raw_content\":null},{\"title\":\"Christopher Nolan's Oppenheimer - Rotten Tomatoes\",\"url\":\"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/\",\"content\":\"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.\",\"score\":0.92804,\"raw_content\":null},{\"title\":\"Oppenheimer (film) - Wikipedia\",\"url\":\"https://en.wikipedia.org/wiki/Oppenheimer_(film)\",\"content\":\"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\\nCritical response\\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \\\"more objective view of his story from a different character's point of view\\\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \\\"big-atures\\\", since the special effects team had tried to build the models as physically large as possible. He felt that \\\"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\\\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \\\"emotional\\\" and resembling that of a thriller, while also remarking that Nolan had \\\"Trojan-Horsed a biopic into a thriller\\\".[72]\\nCasting\\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\\\", while also underscoring that it is a \\\"huge shift in perception about the reality of Oppenheimer's perception\\\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \\\"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\\\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.\",\"score\":0.92404,\"raw_content\":null},{\"title\":\"'Oppenheimer' Director Christopher Nolan On Filmmaking at 53: \\\"I Try to ...\",\"url\":\"https://www.everythingzoomer.com/arts-entertainment/2023/11/21/oppenheimer-director-christopher-nolan-on-filmmaking-at-53-i-try-to-challenge-myself-with-every-film/\",\"content\":\"Oppenheimer will be available to own on 4K Ultra HD, Blu-ray and DVD — including more than three hours of bonus features — on November 21.\\nRELATED:\\nVisiting the Trinity Site Featured in ‘Oppenheimer’ Is a Sobering Reminder of the Horror of Nuclear Weapons\\nBarbenheimer: How ‘Barbie’ and ‘Oppenheimer’ Became the Unlikely Movie Marriage of the Summer\\nBlast From the Past: ‘Asteroid City’ & ‘Oppenheimer’ and the Age of Nuclear Anxiety\\nEXPLORE  HealthMoneyTravelFoodStyleBook ClubClassifieds#ZoomerDailyPolicy & PerspectiveArts & EntertainmentStars & RoyaltySex & Love\\nCONNECT  FacebookTwitterInstagram\\nSUBSCRIBE  Terms of Subscription ServiceE-NewslettersSubscribe to Zoomer Magazine\\nBROWSE  AboutMastheadContact UsAdvertise with UsPrivacy Policy\\nEverythingZoomer.com is part of the ZoomerMedia Digital Network “I think with experience — and with the experience of watching your films with an audience over the years — you do more and more recognize the human elements that people respond to, and the things that move you and the things that move the audience.”\\n “What’s interesting, as you watch the films over time, is that some of his preoccupations are the same, but then some of them have changed over time with who he is as a person and what’s going on in his own life,” Thomas said.\\n The British-American director’s latest explosive drama, Oppenheimer, which has earned upwards of US$940 million at the global box office, follows theoretical physicist J. Robert Oppenheimer (played by Cillian Murphy) as he leads the team creating the first atomic bomb, as director of the Manhattan Project’s Los Alamos Laboratory.\\n Subscribe\\nEverything Zoomer\\n‘Oppenheimer’ Director Christopher Nolan On Filmmaking at 53: “I Try to Challenge Myself with Every Film”\\nDirector Christopher Nolan poses upon his arrival for the premiere of the movie 'Oppenheimer' in Paris on July 11, 2023.\",\"score\":0.92002,\"raw_content\":null},{\"title\":\"'Oppenheimer' Review: A Man for Our Time - The New York Times\",\"url\":\"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html\",\"content\":\"Instead, it is here that the film’s complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era’s most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation — Strauss’s role in the hearing and his relationship with Oppenheimer directly affected the confirmation’s outcome — to create a dialectical synthesis. To signal his conceit, he stamps the film with the words “fission” (a splitting into parts) and “fusion” (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology — it is a lot.\\n It’s also at Berkeley that Oppenheimer meets the project’s military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos’s director, despite the leftist causes he supported — among them, the fight against fascism during the Spanish Civil War — and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\\n\",\"score\":0.91831,\"raw_content\":null}]",
        "additional_kwargs": {
          "name": "tavily_search_results_json"
        },
        "response_metadata": {}
      }
    }
  ]
}
[chain/end] [1:chain:AgentExecutor > 10:chain:ToolCallingAgent > 11:chain:RunnableAssign > 12:chain:RunnableMap] [2ms] Exiting Chain run with output: {
  "agent_scratchpad": [
    {
      "lc": 1,
      "type": "constructor",
      "id": [
        "langchain_core",
        "messages",
        "AIMessageChunk"
      ],
      "kwargs": {
        "content": [
          {
            "type": "tool_use",
            "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
            "name": "tavily_search_results_json",
            "input": {
              "input": "Oppenheimer 2023 film director age"
            }
          }
        ],
        "additional_kwargs": {
          "id": "msg_015MqAHr84dBCAqBgjou41Km",
          "type": "message",
          "role": "assistant",
          "model": "claude-3-sonnet-20240229",
          "stop_sequence": null,
          "usage": {
            "input_tokens": 409,
            "output_tokens": 68
          },
          "stop_reason": "tool_use"
        },
        "tool_call_chunks": [
          {
            "name": "tavily_search_results_json",
            "args": "{\"input\":\"Oppenheimer 2023 film director age\"}",
            "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
            "index": 0
          }
        ],
        "tool_calls": [
          {
            "name": "tavily_search_results_json",
            "args": {
              "input": "Oppenheimer 2023 film director age"
            },
            "id": "toolu_01NUVejujVo2y8WGVtZ49KAN"
          }
        ],
        "invalid_tool_calls": [],
        "response_metadata": {}
      }
    },
    {
      "lc": 1,
      "type": "constructor",
      "id": [
        "langchain_core",
        "messages",
        "ToolMessage"
      ],
      "kwargs": {
        "tool_call_id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
        "content": "[{\"title\":\"Oppenheimer (2023) - IMDb\",\"url\":\"https://www.imdb.com/title/tt15398776/\",\"content\":\"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\",\"score\":0.96643,\"raw_content\":null},{\"title\":\"Christopher Nolan's Oppenheimer - Rotten Tomatoes\",\"url\":\"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/\",\"content\":\"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.\",\"score\":0.92804,\"raw_content\":null},{\"title\":\"Oppenheimer (film) - Wikipedia\",\"url\":\"https://en.wikipedia.org/wiki/Oppenheimer_(film)\",\"content\":\"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\\nCritical response\\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \\\"more objective view of his story from a different character's point of view\\\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \\\"big-atures\\\", since the special effects team had tried to build the models as physically large as possible. He felt that \\\"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\\\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \\\"emotional\\\" and resembling that of a thriller, while also remarking that Nolan had \\\"Trojan-Horsed a biopic into a thriller\\\".[72]\\nCasting\\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\\\", while also underscoring that it is a \\\"huge shift in perception about the reality of Oppenheimer's perception\\\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \\\"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\\\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.\",\"score\":0.92404,\"raw_content\":null},{\"title\":\"'Oppenheimer' Director Christopher Nolan On Filmmaking at 53: \\\"I Try to ...\",\"url\":\"https://www.everythingzoomer.com/arts-entertainment/2023/11/21/oppenheimer-director-christopher-nolan-on-filmmaking-at-53-i-try-to-challenge-myself-with-every-film/\",\"content\":\"Oppenheimer will be available to own on 4K Ultra HD, Blu-ray and DVD — including more than three hours of bonus features — on November 21.\\nRELATED:\\nVisiting the Trinity Site Featured in ‘Oppenheimer’ Is a Sobering Reminder of the Horror of Nuclear Weapons\\nBarbenheimer: How ‘Barbie’ and ‘Oppenheimer’ Became the Unlikely Movie Marriage of the Summer\\nBlast From the Past: ‘Asteroid City’ & ‘Oppenheimer’ and the Age of Nuclear Anxiety\\nEXPLORE  HealthMoneyTravelFoodStyleBook ClubClassifieds#ZoomerDailyPolicy & PerspectiveArts & EntertainmentStars & RoyaltySex & Love\\nCONNECT  FacebookTwitterInstagram\\nSUBSCRIBE  Terms of Subscription ServiceE-NewslettersSubscribe to Zoomer Magazine\\nBROWSE  AboutMastheadContact UsAdvertise with UsPrivacy Policy\\nEverythingZoomer.com is part of the ZoomerMedia Digital Network “I think with experience — and with the experience of watching your films with an audience over the years — you do more and more recognize the human elements that people respond to, and the things that move you and the things that move the audience.”\\n “What’s interesting, as you watch the films over time, is that some of his preoccupations are the same, but then some of them have changed over time with who he is as a person and what’s going on in his own life,” Thomas said.\\n The British-American director’s latest explosive drama, Oppenheimer, which has earned upwards of US$940 million at the global box office, follows theoretical physicist J. Robert Oppenheimer (played by Cillian Murphy) as he leads the team creating the first atomic bomb, as director of the Manhattan Project’s Los Alamos Laboratory.\\n Subscribe\\nEverything Zoomer\\n‘Oppenheimer’ Director Christopher Nolan On Filmmaking at 53: “I Try to Challenge Myself with Every Film”\\nDirector Christopher Nolan poses upon his arrival for the premiere of the movie 'Oppenheimer' in Paris on July 11, 2023.\",\"score\":0.92002,\"raw_content\":null},{\"title\":\"'Oppenheimer' Review: A Man for Our Time - The New York Times\",\"url\":\"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html\",\"content\":\"Instead, it is here that the film’s complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era’s most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation — Strauss’s role in the hearing and his relationship with Oppenheimer directly affected the confirmation’s outcome — to create a dialectical synthesis. To signal his conceit, he stamps the film with the words “fission” (a splitting into parts) and “fusion” (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology — it is a lot.\\n It’s also at Berkeley that Oppenheimer meets the project’s military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos’s director, despite the leftist causes he supported — among them, the fight against fascism during the Spanish Civil War — and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\\n\",\"score\":0.91831,\"raw_content\":null}]",
        "additional_kwargs": {
          "name": "tavily_search_results_json"
        },
        "response_metadata": {}
      }
    }
  ]
}
[chain/end] [1:chain:AgentExecutor > 10:chain:ToolCallingAgent > 11:chain:RunnableAssign] [3ms] Exiting Chain run with output: {
  "input": "Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?",
  "steps": [
    {
      "action": {
        "tool": "tavily_search_results_json",
        "toolInput": {
          "input": "Oppenheimer 2023 film director age"
        },
        "toolCallId": "toolu_01NUVejujVo2y8WGVtZ49KAN",
        "log": "Invoking \"tavily_search_results_json\" with {\"input\":\"Oppenheimer 2023 film director age\"}\n[{\"type\":\"tool_use\",\"id\":\"toolu_01NUVejujVo2y8WGVtZ49KAN\",\"name\":\"tavily_search_results_json\",\"input\":{\"input\":\"Oppenheimer 2023 film director age\"}}]",
        "messageLog": [
          {
            "lc": 1,
            "type": "constructor",
            "id": [
              "langchain_core",
              "messages",
              "AIMessageChunk"
            ],
            "kwargs": {
              "content": [
                {
                  "type": "tool_use",
                  "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
                  "name": "tavily_search_results_json",
                  "input": {
                    "input": "Oppenheimer 2023 film director age"
                  }
                }
              ],
              "additional_kwargs": {
                "id": "msg_015MqAHr84dBCAqBgjou41Km",
                "type": "message",
                "role": "assistant",
                "model": "claude-3-sonnet-20240229",
                "stop_sequence": null,
                "usage": {
                  "input_tokens": 409,
                  "output_tokens": 68
                },
                "stop_reason": "tool_use"
              },
              "tool_call_chunks": [
                {
                  "name": "tavily_search_results_json",
                  "args": "{\"input\":\"Oppenheimer 2023 film director age\"}",
                  "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
                  "index": 0
                }
              ],
              "tool_calls": [
                {
                  "name": "tavily_search_results_json",
                  "args": {
                    "input": "Oppenheimer 2023 film director age"
                  },
                  "id": "toolu_01NUVejujVo2y8WGVtZ49KAN"
                }
              ],
              "invalid_tool_calls": [],
              "response_metadata": {}
            }
          }
        ]
      },
      "observation": "[{\"title\":\"Oppenheimer (2023) - IMDb\",\"url\":\"https://www.imdb.com/title/tt15398776/\",\"content\":\"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\",\"score\":0.96643,\"raw_content\":null},{\"title\":\"Christopher Nolan's Oppenheimer - Rotten Tomatoes\",\"url\":\"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/\",\"content\":\"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.\",\"score\":0.92804,\"raw_content\":null},{\"title\":\"Oppenheimer (film) - Wikipedia\",\"url\":\"https://en.wikipedia.org/wiki/Oppenheimer_(film)\",\"content\":\"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\\nCritical response\\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \\\"more objective view of his story from a different character's point of view\\\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \\\"big-atures\\\", since the special effects team had tried to build the models as physically large as possible. He felt that \\\"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\\\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \\\"emotional\\\" and resembling that of a thriller, while also remarking that Nolan had \\\"Trojan-Horsed a biopic into a thriller\\\".[72]\\nCasting\\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\\\", while also underscoring that it is a \\\"huge shift in perception about the reality of Oppenheimer's perception\\\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \\\"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\\\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.\",\"score\":0.92404,\"raw_content\":null},{\"title\":\"'Oppenheimer' Director Christopher Nolan On Filmmaking at 53: \\\"I Try to ...\",\"url\":\"https://www.everythingzoomer.com/arts-entertainment/2023/11/21/oppenheimer-director-christopher-nolan-on-filmmaking-at-53-i-try-to-challenge-myself-with-every-film/\",\"content\":\"Oppenheimer will be available to own on 4K Ultra HD, Blu-ray and DVD — including more than three hours of bonus features — on November 21.\\nRELATED:\\nVisiting the Trinity Site Featured in ‘Oppenheimer’ Is a Sobering Reminder of the Horror of Nuclear Weapons\\nBarbenheimer: How ‘Barbie’ and ‘Oppenheimer’ Became the Unlikely Movie Marriage of the Summer\\nBlast From the Past: ‘Asteroid City’ & ‘Oppenheimer’ and the Age of Nuclear Anxiety\\nEXPLORE  HealthMoneyTravelFoodStyleBook ClubClassifieds#ZoomerDailyPolicy & PerspectiveArts & EntertainmentStars & RoyaltySex & Love\\nCONNECT  FacebookTwitterInstagram\\nSUBSCRIBE  Terms of Subscription ServiceE-NewslettersSubscribe to Zoomer Magazine\\nBROWSE  AboutMastheadContact UsAdvertise with UsPrivacy Policy\\nEverythingZoomer.com is part of the ZoomerMedia Digital Network “I think with experience — and with the experience of watching your films with an audience over the years — you do more and more recognize the human elements that people respond to, and the things that move you and the things that move the audience.”\\n “What’s interesting, as you watch the films over time, is that some of his preoccupations are the same, but then some of them have changed over time with who he is as a person and what’s going on in his own life,” Thomas said.\\n The British-American director’s latest explosive drama, Oppenheimer, which has earned upwards of US$940 million at the global box office, follows theoretical physicist J. Robert Oppenheimer (played by Cillian Murphy) as he leads the team creating the first atomic bomb, as director of the Manhattan Project’s Los Alamos Laboratory.\\n Subscribe\\nEverything Zoomer\\n‘Oppenheimer’ Director Christopher Nolan On Filmmaking at 53: “I Try to Challenge Myself with Every Film”\\nDirector Christopher Nolan poses upon his arrival for the premiere of the movie 'Oppenheimer' in Paris on July 11, 2023.\",\"score\":0.92002,\"raw_content\":null},{\"title\":\"'Oppenheimer' Review: A Man for Our Time - The New York Times\",\"url\":\"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html\",\"content\":\"Instead, it is here that the film’s complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era’s most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation — Strauss’s role in the hearing and his relationship with Oppenheimer directly affected the confirmation’s outcome — to create a dialectical synthesis. To signal his conceit, he stamps the film with the words “fission” (a splitting into parts) and “fusion” (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology — it is a lot.\\n It’s also at Berkeley that Oppenheimer meets the project’s military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos’s director, despite the leftist causes he supported — among them, the fight against fascism during the Spanish Civil War — and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\\n\",\"score\":0.91831,\"raw_content\":null}]"
    }
  ],
  "agent_scratchpad": [
    {
      "lc": 1,
      "type": "constructor",
      "id": [
        "langchain_core",
        "messages",
        "AIMessageChunk"
      ],
      "kwargs": {
        "content": [
          {
            "type": "tool_use",
            "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
            "name": "tavily_search_results_json",
            "input": {
              "input": "Oppenheimer 2023 film director age"
            }
          }
        ],
        "additional_kwargs": {
          "id": "msg_015MqAHr84dBCAqBgjou41Km",
          "type": "message",
          "role": "assistant",
          "model": "claude-3-sonnet-20240229",
          "stop_sequence": null,
          "usage": {
            "input_tokens": 409,
            "output_tokens": 68
          },
          "stop_reason": "tool_use"
        },
        "tool_call_chunks": [
          {
            "name": "tavily_search_results_json",
            "args": "{\"input\":\"Oppenheimer 2023 film director age\"}",
            "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
            "index": 0
          }
        ],
        "tool_calls": [
          {
            "name": "tavily_search_results_json",
            "args": {
              "input": "Oppenheimer 2023 film director age"
            },
            "id": "toolu_01NUVejujVo2y8WGVtZ49KAN"
          }
        ],
        "invalid_tool_calls": [],
        "response_metadata": {}
      }
    },
    {
      "lc": 1,
      "type": "constructor",
      "id": [
        "langchain_core",
        "messages",
        "ToolMessage"
      ],
      "kwargs": {
        "tool_call_id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
        "content": "[{\"title\":\"Oppenheimer (2023) - IMDb\",\"url\":\"https://www.imdb.com/title/tt15398776/\",\"content\":\"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\",\"score\":0.96643,\"raw_content\":null},{\"title\":\"Christopher Nolan's Oppenheimer - Rotten Tomatoes\",\"url\":\"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/\",\"content\":\"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.\",\"score\":0.92804,\"raw_content\":null},{\"title\":\"Oppenheimer (film) - Wikipedia\",\"url\":\"https://en.wikipedia.org/wiki/Oppenheimer_(film)\",\"content\":\"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\\nCritical response\\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \\\"more objective view of his story from a different character's point of view\\\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \\\"big-atures\\\", since the special effects team had tried to build the models as physically large as possible. He felt that \\\"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\\\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \\\"emotional\\\" and resembling that of a thriller, while also remarking that Nolan had \\\"Trojan-Horsed a biopic into a thriller\\\".[72]\\nCasting\\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\\\", while also underscoring that it is a \\\"huge shift in perception about the reality of Oppenheimer's perception\\\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \\\"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\\\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.\",\"score\":0.92404,\"raw_content\":null},{\"title\":\"'Oppenheimer' Director Christopher Nolan On Filmmaking at 53: \\\"I Try to ...\",\"url\":\"https://www.everythingzoomer.com/arts-entertainment/2023/11/21/oppenheimer-director-christopher-nolan-on-filmmaking-at-53-i-try-to-challenge-myself-with-every-film/\",\"content\":\"Oppenheimer will be available to own on 4K Ultra HD, Blu-ray and DVD — including more than three hours of bonus features — on November 21.\\nRELATED:\\nVisiting the Trinity Site Featured in ‘Oppenheimer’ Is a Sobering Reminder of the Horror of Nuclear Weapons\\nBarbenheimer: How ‘Barbie’ and ‘Oppenheimer’ Became the Unlikely Movie Marriage of the Summer\\nBlast From the Past: ‘Asteroid City’ & ‘Oppenheimer’ and the Age of Nuclear Anxiety\\nEXPLORE  HealthMoneyTravelFoodStyleBook ClubClassifieds#ZoomerDailyPolicy & PerspectiveArts & EntertainmentStars & RoyaltySex & Love\\nCONNECT  FacebookTwitterInstagram\\nSUBSCRIBE  Terms of Subscription ServiceE-NewslettersSubscribe to Zoomer Magazine\\nBROWSE  AboutMastheadContact UsAdvertise with UsPrivacy Policy\\nEverythingZoomer.com is part of the ZoomerMedia Digital Network “I think with experience — and with the experience of watching your films with an audience over the years — you do more and more recognize the human elements that people respond to, and the things that move you and the things that move the audience.”\\n “What’s interesting, as you watch the films over time, is that some of his preoccupations are the same, but then some of them have changed over time with who he is as a person and what’s going on in his own life,” Thomas said.\\n The British-American director’s latest explosive drama, Oppenheimer, which has earned upwards of US$940 million at the global box office, follows theoretical physicist J. Robert Oppenheimer (played by Cillian Murphy) as he leads the team creating the first atomic bomb, as director of the Manhattan Project’s Los Alamos Laboratory.\\n Subscribe\\nEverything Zoomer\\n‘Oppenheimer’ Director Christopher Nolan On Filmmaking at 53: “I Try to Challenge Myself with Every Film”\\nDirector Christopher Nolan poses upon his arrival for the premiere of the movie 'Oppenheimer' in Paris on July 11, 2023.\",\"score\":0.92002,\"raw_content\":null},{\"title\":\"'Oppenheimer' Review: A Man for Our Time - The New York Times\",\"url\":\"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html\",\"content\":\"Instead, it is here that the film’s complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era’s most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation — Strauss’s role in the hearing and his relationship with Oppenheimer directly affected the confirmation’s outcome — to create a dialectical synthesis. To signal his conceit, he stamps the film with the words “fission” (a splitting into parts) and “fusion” (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology — it is a lot.\\n It’s also at Berkeley that Oppenheimer meets the project’s military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos’s director, despite the leftist causes he supported — among them, the fight against fascism during the Spanish Civil War — and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\\n\",\"score\":0.91831,\"raw_content\":null}]",
        "additional_kwargs": {
          "name": "tavily_search_results_json"
        },
        "response_metadata": {}
      }
    }
  ]
}
[chain/start] [1:chain:AgentExecutor > 10:chain:ToolCallingAgent > 14:prompt:ChatPromptTemplate] Entering Chain run with input: {
  "input": "Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?",
  "steps": [
    {
      "action": {
        "tool": "tavily_search_results_json",
        "toolInput": {
          "input": "Oppenheimer 2023 film director age"
        },
        "toolCallId": "toolu_01NUVejujVo2y8WGVtZ49KAN",
        "log": "Invoking \"tavily_search_results_json\" with {\"input\":\"Oppenheimer 2023 film director age\"}\n[{\"type\":\"tool_use\",\"id\":\"toolu_01NUVejujVo2y8WGVtZ49KAN\",\"name\":\"tavily_search_results_json\",\"input\":{\"input\":\"Oppenheimer 2023 film director age\"}}]",
        "messageLog": [
          {
            "lc": 1,
            "type": "constructor",
            "id": [
              "langchain_core",
              "messages",
              "AIMessageChunk"
            ],
            "kwargs": {
              "content": [
                {
                  "type": "tool_use",
                  "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
                  "name": "tavily_search_results_json",
                  "input": {
                    "input": "Oppenheimer 2023 film director age"
                  }
                }
              ],
              "additional_kwargs": {
                "id": "msg_015MqAHr84dBCAqBgjou41Km",
                "type": "message",
                "role": "assistant",
                "model": "claude-3-sonnet-20240229",
                "stop_sequence": null,
                "usage": {
                  "input_tokens": 409,
                  "output_tokens": 68
                },
                "stop_reason": "tool_use"
              },
              "tool_call_chunks": [
                {
                  "name": "tavily_search_results_json",
                  "args": "{\"input\":\"Oppenheimer 2023 film director age\"}",
                  "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
                  "index": 0
                }
              ],
              "tool_calls": [
                {
                  "name": "tavily_search_results_json",
                  "args": {
                    "input": "Oppenheimer 2023 film director age"
                  },
                  "id": "toolu_01NUVejujVo2y8WGVtZ49KAN"
                }
              ],
              "invalid_tool_calls": [],
              "response_metadata": {}
            }
          }
        ]
      },
      "observation": "[{\"title\":\"Oppenheimer (2023) - IMDb\",\"url\":\"https://www.imdb.com/title/tt15398776/\",\"content\":\"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\",\"score\":0.96643,\"raw_content\":null},{\"title\":\"Christopher Nolan's Oppenheimer - Rotten Tomatoes\",\"url\":\"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/\",\"content\":\"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.\",\"score\":0.92804,\"raw_content\":null},{\"title\":\"Oppenheimer (film) - Wikipedia\",\"url\":\"https://en.wikipedia.org/wiki/Oppenheimer_(film)\",\"content\":\"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\\nCritical response\\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \\\"more objective view of his story from a different character's point of view\\\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \\\"big-atures\\\", since the special effects team had tried to build the models as physically large as possible. He felt that \\\"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\\\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \\\"emotional\\\" and resembling that of a thriller, while also remarking that Nolan had \\\"Trojan-Horsed a biopic into a thriller\\\".[72]\\nCasting\\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\\\", while also underscoring that it is a \\\"huge shift in perception about the reality of Oppenheimer's perception\\\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \\\"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\\\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.\",\"score\":0.92404,\"raw_content\":null},{\"title\":\"'Oppenheimer' Director Christopher Nolan On Filmmaking at 53: \\\"I Try to ...\",\"url\":\"https://www.everythingzoomer.com/arts-entertainment/2023/11/21/oppenheimer-director-christopher-nolan-on-filmmaking-at-53-i-try-to-challenge-myself-with-every-film/\",\"content\":\"Oppenheimer will be available to own on 4K Ultra HD, Blu-ray and DVD — including more than three hours of bonus features — on November 21.\\nRELATED:\\nVisiting the Trinity Site Featured in ‘Oppenheimer’ Is a Sobering Reminder of the Horror of Nuclear Weapons\\nBarbenheimer: How ‘Barbie’ and ‘Oppenheimer’ Became the Unlikely Movie Marriage of the Summer\\nBlast From the Past: ‘Asteroid City’ & ‘Oppenheimer’ and the Age of Nuclear Anxiety\\nEXPLORE  HealthMoneyTravelFoodStyleBook ClubClassifieds#ZoomerDailyPolicy & PerspectiveArts & EntertainmentStars & RoyaltySex & Love\\nCONNECT  FacebookTwitterInstagram\\nSUBSCRIBE  Terms of Subscription ServiceE-NewslettersSubscribe to Zoomer Magazine\\nBROWSE  AboutMastheadContact UsAdvertise with UsPrivacy Policy\\nEverythingZoomer.com is part of the ZoomerMedia Digital Network “I think with experience — and with the experience of watching your films with an audience over the years — you do more and more recognize the human elements that people respond to, and the things that move you and the things that move the audience.”\\n “What’s interesting, as you watch the films over time, is that some of his preoccupations are the same, but then some of them have changed over time with who he is as a person and what’s going on in his own life,” Thomas said.\\n The British-American director’s latest explosive drama, Oppenheimer, which has earned upwards of US$940 million at the global box office, follows theoretical physicist J. Robert Oppenheimer (played by Cillian Murphy) as he leads the team creating the first atomic bomb, as director of the Manhattan Project’s Los Alamos Laboratory.\\n Subscribe\\nEverything Zoomer\\n‘Oppenheimer’ Director Christopher Nolan On Filmmaking at 53: “I Try to Challenge Myself with Every Film”\\nDirector Christopher Nolan poses upon his arrival for the premiere of the movie 'Oppenheimer' in Paris on July 11, 2023.\",\"score\":0.92002,\"raw_content\":null},{\"title\":\"'Oppenheimer' Review: A Man for Our Time - The New York Times\",\"url\":\"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html\",\"content\":\"Instead, it is here that the film’s complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era’s most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation — Strauss’s role in the hearing and his relationship with Oppenheimer directly affected the confirmation’s outcome — to create a dialectical synthesis. To signal his conceit, he stamps the film with the words “fission” (a splitting into parts) and “fusion” (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology — it is a lot.\\n It’s also at Berkeley that Oppenheimer meets the project’s military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos’s director, despite the leftist causes he supported — among them, the fight against fascism during the Spanish Civil War — and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\\n\",\"score\":0.91831,\"raw_content\":null}]"
    }
  ],
  "agent_scratchpad": [
    {
      "lc": 1,
      "type": "constructor",
      "id": [
        "langchain_core",
        "messages",
        "AIMessageChunk"
      ],
      "kwargs": {
        "content": [
          {
            "type": "tool_use",
            "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
            "name": "tavily_search_results_json",
            "input": {
              "input": "Oppenheimer 2023 film director age"
            }
          }
        ],
        "additional_kwargs": {
          "id": "msg_015MqAHr84dBCAqBgjou41Km",
          "type": "message",
          "role": "assistant",
          "model": "claude-3-sonnet-20240229",
          "stop_sequence": null,
          "usage": {
            "input_tokens": 409,
            "output_tokens": 68
          },
          "stop_reason": "tool_use"
        },
        "tool_call_chunks": [
          {
            "name": "tavily_search_results_json",
            "args": "{\"input\":\"Oppenheimer 2023 film director age\"}",
            "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
            "index": 0
          }
        ],
        "tool_calls": [
          {
            "name": "tavily_search_results_json",
            "args": {
              "input": "Oppenheimer 2023 film director age"
            },
            "id": "toolu_01NUVejujVo2y8WGVtZ49KAN"
          }
        ],
        "invalid_tool_calls": [],
        "response_metadata": {}
      }
    },
    {
      "lc": 1,
      "type": "constructor",
      "id": [
        "langchain_core",
        "messages",
        "ToolMessage"
      ],
      "kwargs": {
        "tool_call_id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
        "content": "[{\"title\":\"Oppenheimer (2023) - IMDb\",\"url\":\"https://www.imdb.com/title/tt15398776/\",\"content\":\"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\",\"score\":0.96643,\"raw_content\":null},{\"title\":\"Christopher Nolan's Oppenheimer - Rotten Tomatoes\",\"url\":\"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/\",\"content\":\"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.\",\"score\":0.92804,\"raw_content\":null},{\"title\":\"Oppenheimer (film) - Wikipedia\",\"url\":\"https://en.wikipedia.org/wiki/Oppenheimer_(film)\",\"content\":\"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\\nCritical response\\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \\\"more objective view of his story from a different character's point of view\\\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \\\"big-atures\\\", since the special effects team had tried to build the models as physically large as possible. He felt that \\\"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\\\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \\\"emotional\\\" and resembling that of a thriller, while also remarking that Nolan had \\\"Trojan-Horsed a biopic into a thriller\\\".[72]\\nCasting\\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\\\", while also underscoring that it is a \\\"huge shift in perception about the reality of Oppenheimer's perception\\\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \\\"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\\\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.\",\"score\":0.92404,\"raw_content\":null},{\"title\":\"'Oppenheimer' Director Christopher Nolan On Filmmaking at 53: \\\"I Try to ...\",\"url\":\"https://www.everythingzoomer.com/arts-entertainment/2023/11/21/oppenheimer-director-christopher-nolan-on-filmmaking-at-53-i-try-to-challenge-myself-with-every-film/\",\"content\":\"Oppenheimer will be available to own on 4K Ultra HD, Blu-ray and DVD — including more than three hours of bonus features — on November 21.\\nRELATED:\\nVisiting the Trinity Site Featured in ‘Oppenheimer’ Is a Sobering Reminder of the Horror of Nuclear Weapons\\nBarbenheimer: How ‘Barbie’ and ‘Oppenheimer’ Became the Unlikely Movie Marriage of the Summer\\nBlast From the Past: ‘Asteroid City’ & ‘Oppenheimer’ and the Age of Nuclear Anxiety\\nEXPLORE  HealthMoneyTravelFoodStyleBook ClubClassifieds#ZoomerDailyPolicy & PerspectiveArts & EntertainmentStars & RoyaltySex & Love\\nCONNECT  FacebookTwitterInstagram\\nSUBSCRIBE  Terms of Subscription ServiceE-NewslettersSubscribe to Zoomer Magazine\\nBROWSE  AboutMastheadContact UsAdvertise with UsPrivacy Policy\\nEverythingZoomer.com is part of the ZoomerMedia Digital Network “I think with experience — and with the experience of watching your films with an audience over the years — you do more and more recognize the human elements that people respond to, and the things that move you and the things that move the audience.”\\n “What’s interesting, as you watch the films over time, is that some of his preoccupations are the same, but then some of them have changed over time with who he is as a person and what’s going on in his own life,” Thomas said.\\n The British-American director’s latest explosive drama, Oppenheimer, which has earned upwards of US$940 million at the global box office, follows theoretical physicist J. Robert Oppenheimer (played by Cillian Murphy) as he leads the team creating the first atomic bomb, as director of the Manhattan Project’s Los Alamos Laboratory.\\n Subscribe\\nEverything Zoomer\\n‘Oppenheimer’ Director Christopher Nolan On Filmmaking at 53: “I Try to Challenge Myself with Every Film”\\nDirector Christopher Nolan poses upon his arrival for the premiere of the movie 'Oppenheimer' in Paris on July 11, 2023.\",\"score\":0.92002,\"raw_content\":null},{\"title\":\"'Oppenheimer' Review: A Man for Our Time - The New York Times\",\"url\":\"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html\",\"content\":\"Instead, it is here that the film’s complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era’s most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation — Strauss’s role in the hearing and his relationship with Oppenheimer directly affected the confirmation’s outcome — to create a dialectical synthesis. To signal his conceit, he stamps the film with the words “fission” (a splitting into parts) and “fusion” (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology — it is a lot.\\n It’s also at Berkeley that Oppenheimer meets the project’s military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos’s director, despite the leftist causes he supported — among them, the fight against fascism during the Spanish Civil War — and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\\n\",\"score\":0.91831,\"raw_content\":null}]",
        "additional_kwargs": {
          "name": "tavily_search_results_json"
        },
        "response_metadata": {}
      }
    }
  ]
}
[chain/end] [1:chain:AgentExecutor > 10:chain:ToolCallingAgent > 14:prompt:ChatPromptTemplate] [2ms] Exiting Chain run with output: {
  "lc": 1,
  "type": "constructor",
  "id": [
    "langchain_core",
    "prompt_values",
    "ChatPromptValue"
  ],
  "kwargs": {
    "messages": [
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "SystemMessage"
        ],
        "kwargs": {
          "content": "You are a helpful assistant",
          "additional_kwargs": {},
          "response_metadata": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "HumanMessage"
        ],
        "kwargs": {
          "content": "Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?",
          "additional_kwargs": {},
          "response_metadata": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "AIMessageChunk"
        ],
        "kwargs": {
          "content": [
            {
              "type": "tool_use",
              "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
              "name": "tavily_search_results_json",
              "input": {
                "input": "Oppenheimer 2023 film director age"
              }
            }
          ],
          "additional_kwargs": {
            "id": "msg_015MqAHr84dBCAqBgjou41Km",
            "type": "message",
            "role": "assistant",
            "model": "claude-3-sonnet-20240229",
            "stop_sequence": null,
            "usage": {
              "input_tokens": 409,
              "output_tokens": 68
            },
            "stop_reason": "tool_use"
          },
          "tool_call_chunks": [
            {
              "name": "tavily_search_results_json",
              "args": "{\"input\":\"Oppenheimer 2023 film director age\"}",
              "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
              "index": 0
            }
          ],
          "tool_calls": [
            {
              "name": "tavily_search_results_json",
              "args": {
                "input": "Oppenheimer 2023 film director age"
              },
              "id": "toolu_01NUVejujVo2y8WGVtZ49KAN"
            }
          ],
          "invalid_tool_calls": [],
          "response_metadata": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "ToolMessage"
        ],
        "kwargs": {
          "tool_call_id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
          "content": "[{\"title\":\"Oppenheimer (2023) - IMDb\",\"url\":\"https://www.imdb.com/title/tt15398776/\",\"content\":\"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\",\"score\":0.96643,\"raw_content\":null},{\"title\":\"Christopher Nolan's Oppenheimer - Rotten Tomatoes\",\"url\":\"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/\",\"content\":\"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.\",\"score\":0.92804,\"raw_content\":null},{\"title\":\"Oppenheimer (film) - Wikipedia\",\"url\":\"https://en.wikipedia.org/wiki/Oppenheimer_(film)\",\"content\":\"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\\nCritical response\\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \\\"more objective view of his story from a different character's point of view\\\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \\\"big-atures\\\", since the special effects team had tried to build the models as physically large as possible. He felt that \\\"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\\\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \\\"emotional\\\" and resembling that of a thriller, while also remarking that Nolan had \\\"Trojan-Horsed a biopic into a thriller\\\".[72]\\nCasting\\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\\\", while also underscoring that it is a \\\"huge shift in perception about the reality of Oppenheimer's perception\\\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \\\"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\\\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.\",\"score\":0.92404,\"raw_content\":null},{\"title\":\"'Oppenheimer' Director Christopher Nolan On Filmmaking at 53: \\\"I Try to ...\",\"url\":\"https://www.everythingzoomer.com/arts-entertainment/2023/11/21/oppenheimer-director-christopher-nolan-on-filmmaking-at-53-i-try-to-challenge-myself-with-every-film/\",\"content\":\"Oppenheimer will be available to own on 4K Ultra HD, Blu-ray and DVD — including more than three hours of bonus features — on November 21.\\nRELATED:\\nVisiting the Trinity Site Featured in ‘Oppenheimer’ Is a Sobering Reminder of the Horror of Nuclear Weapons\\nBarbenheimer: How ‘Barbie’ and ‘Oppenheimer’ Became the Unlikely Movie Marriage of the Summer\\nBlast From the Past: ‘Asteroid City’ & ‘Oppenheimer’ and the Age of Nuclear Anxiety\\nEXPLORE  HealthMoneyTravelFoodStyleBook ClubClassifieds#ZoomerDailyPolicy & PerspectiveArts & EntertainmentStars & RoyaltySex & Love\\nCONNECT  FacebookTwitterInstagram\\nSUBSCRIBE  Terms of Subscription ServiceE-NewslettersSubscribe to Zoomer Magazine\\nBROWSE  AboutMastheadContact UsAdvertise with UsPrivacy Policy\\nEverythingZoomer.com is part of the ZoomerMedia Digital Network “I think with experience — and with the experience of watching your films with an audience over the years — you do more and more recognize the human elements that people respond to, and the things that move you and the things that move the audience.”\\n “What’s interesting, as you watch the films over time, is that some of his preoccupations are the same, but then some of them have changed over time with who he is as a person and what’s going on in his own life,” Thomas said.\\n The British-American director’s latest explosive drama, Oppenheimer, which has earned upwards of US$940 million at the global box office, follows theoretical physicist J. Robert Oppenheimer (played by Cillian Murphy) as he leads the team creating the first atomic bomb, as director of the Manhattan Project’s Los Alamos Laboratory.\\n Subscribe\\nEverything Zoomer\\n‘Oppenheimer’ Director Christopher Nolan On Filmmaking at 53: “I Try to Challenge Myself with Every Film”\\nDirector Christopher Nolan poses upon his arrival for the premiere of the movie 'Oppenheimer' in Paris on July 11, 2023.\",\"score\":0.92002,\"raw_content\":null},{\"title\":\"'Oppenheimer' Review: A Man for Our Time - The New York Times\",\"url\":\"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html\",\"content\":\"Instead, it is here that the film’s complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era’s most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation — Strauss’s role in the hearing and his relationship with Oppenheimer directly affected the confirmation’s outcome — to create a dialectical synthesis. To signal his conceit, he stamps the film with the words “fission” (a splitting into parts) and “fusion” (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology — it is a lot.\\n It’s also at Berkeley that Oppenheimer meets the project’s military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos’s director, despite the leftist causes he supported — among them, the fight against fascism during the Spanish Civil War — and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\\n\",\"score\":0.91831,\"raw_content\":null}]",
          "additional_kwargs": {
            "name": "tavily_search_results_json"
          },
          "response_metadata": {}
        }
      }
    ]
  }
}
[llm/start] [1:chain:AgentExecutor > 10:chain:ToolCallingAgent > 15:llm:ChatAnthropic] Entering LLM run with input: {
  "messages": [
    [
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "SystemMessage"
        ],
        "kwargs": {
          "content": "You are a helpful assistant",
          "additional_kwargs": {},
          "response_metadata": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "HumanMessage"
        ],
        "kwargs": {
          "content": "Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?",
          "additional_kwargs": {},
          "response_metadata": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "AIMessageChunk"
        ],
        "kwargs": {
          "content": [
            {
              "type": "tool_use",
              "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
              "name": "tavily_search_results_json",
              "input": {
                "input": "Oppenheimer 2023 film director age"
              }
            }
          ],
          "additional_kwargs": {
            "id": "msg_015MqAHr84dBCAqBgjou41Km",
            "type": "message",
            "role": "assistant",
            "model": "claude-3-sonnet-20240229",
            "stop_sequence": null,
            "usage": {
              "input_tokens": 409,
              "output_tokens": 68
            },
            "stop_reason": "tool_use"
          },
          "tool_call_chunks": [
            {
              "name": "tavily_search_results_json",
              "args": "{\"input\":\"Oppenheimer 2023 film director age\"}",
              "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
              "index": 0
            }
          ],
          "tool_calls": [
            {
              "name": "tavily_search_results_json",
              "args": {
                "input": "Oppenheimer 2023 film director age"
              },
              "id": "toolu_01NUVejujVo2y8WGVtZ49KAN"
            }
          ],
          "invalid_tool_calls": [],
          "response_metadata": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "ToolMessage"
        ],
        "kwargs": {
          "tool_call_id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
          "content": "[{\"title\":\"Oppenheimer (2023) - IMDb\",\"url\":\"https://www.imdb.com/title/tt15398776/\",\"content\":\"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\",\"score\":0.96643,\"raw_content\":null},{\"title\":\"Christopher Nolan's Oppenheimer - Rotten Tomatoes\",\"url\":\"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/\",\"content\":\"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.\",\"score\":0.92804,\"raw_content\":null},{\"title\":\"Oppenheimer (film) - Wikipedia\",\"url\":\"https://en.wikipedia.org/wiki/Oppenheimer_(film)\",\"content\":\"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\\nCritical response\\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \\\"more objective view of his story from a different character's point of view\\\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \\\"big-atures\\\", since the special effects team had tried to build the models as physically large as possible. He felt that \\\"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\\\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \\\"emotional\\\" and resembling that of a thriller, while also remarking that Nolan had \\\"Trojan-Horsed a biopic into a thriller\\\".[72]\\nCasting\\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\\\", while also underscoring that it is a \\\"huge shift in perception about the reality of Oppenheimer's perception\\\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \\\"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\\\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.\",\"score\":0.92404,\"raw_content\":null},{\"title\":\"'Oppenheimer' Director Christopher Nolan On Filmmaking at 53: \\\"I Try to ...\",\"url\":\"https://www.everythingzoomer.com/arts-entertainment/2023/11/21/oppenheimer-director-christopher-nolan-on-filmmaking-at-53-i-try-to-challenge-myself-with-every-film/\",\"content\":\"Oppenheimer will be available to own on 4K Ultra HD, Blu-ray and DVD — including more than three hours of bonus features — on November 21.\\nRELATED:\\nVisiting the Trinity Site Featured in ‘Oppenheimer’ Is a Sobering Reminder of the Horror of Nuclear Weapons\\nBarbenheimer: How ‘Barbie’ and ‘Oppenheimer’ Became the Unlikely Movie Marriage of the Summer\\nBlast From the Past: ‘Asteroid City’ & ‘Oppenheimer’ and the Age of Nuclear Anxiety\\nEXPLORE  HealthMoneyTravelFoodStyleBook ClubClassifieds#ZoomerDailyPolicy & PerspectiveArts & EntertainmentStars & RoyaltySex & Love\\nCONNECT  FacebookTwitterInstagram\\nSUBSCRIBE  Terms of Subscription ServiceE-NewslettersSubscribe to Zoomer Magazine\\nBROWSE  AboutMastheadContact UsAdvertise with UsPrivacy Policy\\nEverythingZoomer.com is part of the ZoomerMedia Digital Network “I think with experience — and with the experience of watching your films with an audience over the years — you do more and more recognize the human elements that people respond to, and the things that move you and the things that move the audience.”\\n “What’s interesting, as you watch the films over time, is that some of his preoccupations are the same, but then some of them have changed over time with who he is as a person and what’s going on in his own life,” Thomas said.\\n The British-American director’s latest explosive drama, Oppenheimer, which has earned upwards of US$940 million at the global box office, follows theoretical physicist J. Robert Oppenheimer (played by Cillian Murphy) as he leads the team creating the first atomic bomb, as director of the Manhattan Project’s Los Alamos Laboratory.\\n Subscribe\\nEverything Zoomer\\n‘Oppenheimer’ Director Christopher Nolan On Filmmaking at 53: “I Try to Challenge Myself with Every Film”\\nDirector Christopher Nolan poses upon his arrival for the premiere of the movie 'Oppenheimer' in Paris on July 11, 2023.\",\"score\":0.92002,\"raw_content\":null},{\"title\":\"'Oppenheimer' Review: A Man for Our Time - The New York Times\",\"url\":\"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html\",\"content\":\"Instead, it is here that the film’s complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era’s most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation — Strauss’s role in the hearing and his relationship with Oppenheimer directly affected the confirmation’s outcome — to create a dialectical synthesis. To signal his conceit, he stamps the film with the words “fission” (a splitting into parts) and “fusion” (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology — it is a lot.\\n It’s also at Berkeley that Oppenheimer meets the project’s military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos’s director, despite the leftist causes he supported — among them, the fight against fascism during the Spanish Civil War — and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\\n\",\"score\":0.91831,\"raw_content\":null}]",
          "additional_kwargs": {
            "name": "tavily_search_results_json"
          },
          "response_metadata": {}
        }
      }
    ]
  ]
}
[llm/start] [1:llm:ChatAnthropic] Entering LLM run with input: {
  "messages": [
    [
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "SystemMessage"
        ],
        "kwargs": {
          "content": "You are a helpful assistant",
          "additional_kwargs": {},
          "response_metadata": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "HumanMessage"
        ],
        "kwargs": {
          "content": "Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?",
          "additional_kwargs": {},
          "response_metadata": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "AIMessageChunk"
        ],
        "kwargs": {
          "content": [
            {
              "type": "tool_use",
              "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
              "name": "tavily_search_results_json",
              "input": {
                "input": "Oppenheimer 2023 film director age"
              }
            }
          ],
          "additional_kwargs": {
            "id": "msg_015MqAHr84dBCAqBgjou41Km",
            "type": "message",
            "role": "assistant",
            "model": "claude-3-sonnet-20240229",
            "stop_sequence": null,
            "usage": {
              "input_tokens": 409,
              "output_tokens": 68
            },
            "stop_reason": "tool_use"
          },
          "tool_call_chunks": [
            {
              "name": "tavily_search_results_json",
              "args": "{\"input\":\"Oppenheimer 2023 film director age\"}",
              "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
              "index": 0
            }
          ],
          "tool_calls": [
            {
              "name": "tavily_search_results_json",
              "args": {
                "input": "Oppenheimer 2023 film director age"
              },
              "id": "toolu_01NUVejujVo2y8WGVtZ49KAN"
            }
          ],
          "invalid_tool_calls": [],
          "response_metadata": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "ToolMessage"
        ],
        "kwargs": {
          "tool_call_id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
          "content": "[{\"title\":\"Oppenheimer (2023) - IMDb\",\"url\":\"https://www.imdb.com/title/tt15398776/\",\"content\":\"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\",\"score\":0.96643,\"raw_content\":null},{\"title\":\"Christopher Nolan's Oppenheimer - Rotten Tomatoes\",\"url\":\"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/\",\"content\":\"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.\",\"score\":0.92804,\"raw_content\":null},{\"title\":\"Oppenheimer (film) - Wikipedia\",\"url\":\"https://en.wikipedia.org/wiki/Oppenheimer_(film)\",\"content\":\"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\\nCritical response\\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \\\"more objective view of his story from a different character's point of view\\\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \\\"big-atures\\\", since the special effects team had tried to build the models as physically large as possible. He felt that \\\"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\\\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \\\"emotional\\\" and resembling that of a thriller, while also remarking that Nolan had \\\"Trojan-Horsed a biopic into a thriller\\\".[72]\\nCasting\\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\\\", while also underscoring that it is a \\\"huge shift in perception about the reality of Oppenheimer's perception\\\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \\\"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\\\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.\",\"score\":0.92404,\"raw_content\":null},{\"title\":\"'Oppenheimer' Director Christopher Nolan On Filmmaking at 53: \\\"I Try to ...\",\"url\":\"https://www.everythingzoomer.com/arts-entertainment/2023/11/21/oppenheimer-director-christopher-nolan-on-filmmaking-at-53-i-try-to-challenge-myself-with-every-film/\",\"content\":\"Oppenheimer will be available to own on 4K Ultra HD, Blu-ray and DVD — including more than three hours of bonus features — on November 21.\\nRELATED:\\nVisiting the Trinity Site Featured in ‘Oppenheimer’ Is a Sobering Reminder of the Horror of Nuclear Weapons\\nBarbenheimer: How ‘Barbie’ and ‘Oppenheimer’ Became the Unlikely Movie Marriage of the Summer\\nBlast From the Past: ‘Asteroid City’ & ‘Oppenheimer’ and the Age of Nuclear Anxiety\\nEXPLORE  HealthMoneyTravelFoodStyleBook ClubClassifieds#ZoomerDailyPolicy & PerspectiveArts & EntertainmentStars & RoyaltySex & Love\\nCONNECT  FacebookTwitterInstagram\\nSUBSCRIBE  Terms of Subscription ServiceE-NewslettersSubscribe to Zoomer Magazine\\nBROWSE  AboutMastheadContact UsAdvertise with UsPrivacy Policy\\nEverythingZoomer.com is part of the ZoomerMedia Digital Network “I think with experience — and with the experience of watching your films with an audience over the years — you do more and more recognize the human elements that people respond to, and the things that move you and the things that move the audience.”\\n “What’s interesting, as you watch the films over time, is that some of his preoccupations are the same, but then some of them have changed over time with who he is as a person and what’s going on in his own life,” Thomas said.\\n The British-American director’s latest explosive drama, Oppenheimer, which has earned upwards of US$940 million at the global box office, follows theoretical physicist J. Robert Oppenheimer (played by Cillian Murphy) as he leads the team creating the first atomic bomb, as director of the Manhattan Project’s Los Alamos Laboratory.\\n Subscribe\\nEverything Zoomer\\n‘Oppenheimer’ Director Christopher Nolan On Filmmaking at 53: “I Try to Challenge Myself with Every Film”\\nDirector Christopher Nolan poses upon his arrival for the premiere of the movie 'Oppenheimer' in Paris on July 11, 2023.\",\"score\":0.92002,\"raw_content\":null},{\"title\":\"'Oppenheimer' Review: A Man for Our Time - The New York Times\",\"url\":\"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html\",\"content\":\"Instead, it is here that the film’s complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era’s most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation — Strauss’s role in the hearing and his relationship with Oppenheimer directly affected the confirmation’s outcome — to create a dialectical synthesis. To signal his conceit, he stamps the film with the words “fission” (a splitting into parts) and “fusion” (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology — it is a lot.\\n It’s also at Berkeley that Oppenheimer meets the project’s military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos’s director, despite the leftist causes he supported — among them, the fight against fascism during the Spanish Civil War — and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\\n\",\"score\":0.91831,\"raw_content\":null}]",
          "additional_kwargs": {
            "name": "tavily_search_results_json"
          },
          "response_metadata": {}
        }
      }
    ]
  ]
}
[llm/end] [1:chain:AgentExecutor > 10:chain:ToolCallingAgent > 15:llm:ChatAnthropic] [3.50s] Exiting LLM run with output: {
  "generations": [
    [
      {
        "text": "",
        "message": {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain_core",
            "messages",
            "AIMessageChunk"
          ],
          "kwargs": {
            "content": [
              {
                "type": "text",
                "text": "Based on the search results, the 2023 film Oppenheimer was directed by Christopher Nolan. Some key information about Christopher Nolan:\n\n- He is a British-American film director, producer and screenwriter.\n- He was born on July 30, 1970, making him currently 52 years old.\n\nTo calculate his age in days:"
              },
              {
                "type": "tool_use",
                "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
                "name": "calculator",
                "input": {
                  "input": "52 * 365"
                }
              }
            ],
            "additional_kwargs": {
              "id": "msg_01RBDqmJKNXiEjgt5Xrng4mz",
              "type": "message",
              "role": "assistant",
              "model": "claude-3-sonnet-20240229",
              "stop_sequence": null,
              "usage": {
                "input_tokens": 2810,
                "output_tokens": 137
              },
              "stop_reason": "tool_use"
            },
            "tool_call_chunks": [
              {
                "name": "calculator",
                "args": "{\"input\":\"52 * 365\"}",
                "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
                "index": 0
              }
            ],
            "tool_calls": [
              {
                "name": "calculator",
                "args": {
                  "input": "52 * 365"
                },
                "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE"
              }
            ],
            "invalid_tool_calls": [],
            "response_metadata": {}
          }
        }
      }
    ]
  ]
}
[llm/end] [1:llm:ChatAnthropic] [3.50s] Exiting LLM run with output: {
  "generations": [
    [
      {
        "text": "",
        "message": {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain_core",
            "messages",
            "AIMessageChunk"
          ],
          "kwargs": {
            "content": [
              {
                "type": "text",
                "text": "Based on the search results, the 2023 film Oppenheimer was directed by Christopher Nolan. Some key information about Christopher Nolan:\n\n- He is a British-American film director, producer and screenwriter.\n- He was born on July 30, 1970, making him currently 52 years old.\n\nTo calculate his age in days:"
              },
              {
                "type": "tool_use",
                "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
                "name": "calculator",
                "input": {
                  "input": "52 * 365"
                }
              }
            ],
            "additional_kwargs": {
              "id": "msg_01RBDqmJKNXiEjgt5Xrng4mz",
              "type": "message",
              "role": "assistant",
              "model": "claude-3-sonnet-20240229",
              "stop_sequence": null,
              "usage": {
                "input_tokens": 2810,
                "output_tokens": 137
              },
              "stop_reason": "tool_use"
            },
            "tool_call_chunks": [
              {
                "name": "calculator",
                "args": "{\"input\":\"52 * 365\"}",
                "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
                "index": 0
              }
            ],
            "tool_calls": [
              {
                "name": "calculator",
                "args": {
                  "input": "52 * 365"
                },
                "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE"
              }
            ],
            "invalid_tool_calls": [],
            "response_metadata": {}
          }
        }
      }
    ]
  ]
}
[chain/start] [1:chain:AgentExecutor > 10:chain:ToolCallingAgent > 16:parser:ToolCallingAgentOutputParser] Entering Chain run with input: {
  "lc": 1,
  "type": "constructor",
  "id": [
    "langchain_core",
    "messages",
    "AIMessageChunk"
  ],
  "kwargs": {
    "content": [
      {
        "type": "text",
        "text": "Based on the search results, the 2023 film Oppenheimer was directed by Christopher Nolan. Some key information about Christopher Nolan:\n\n- He is a British-American film director, producer and screenwriter.\n- He was born on July 30, 1970, making him currently 52 years old.\n\nTo calculate his age in days:"
      },
      {
        "type": "tool_use",
        "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
        "name": "calculator",
        "input": {
          "input": "52 * 365"
        }
      }
    ],
    "additional_kwargs": {
      "id": "msg_01RBDqmJKNXiEjgt5Xrng4mz",
      "type": "message",
      "role": "assistant",
      "model": "claude-3-sonnet-20240229",
      "stop_sequence": null,
      "usage": {
        "input_tokens": 2810,
        "output_tokens": 137
      },
      "stop_reason": "tool_use"
    },
    "tool_call_chunks": [
      {
        "name": "calculator",
        "args": "{\"input\":\"52 * 365\"}",
        "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
        "index": 0
      }
    ],
    "tool_calls": [
      {
        "name": "calculator",
        "args": {
          "input": "52 * 365"
        },
        "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE"
      }
    ],
    "invalid_tool_calls": [],
    "response_metadata": {}
  }
}
[chain/end] [1:chain:AgentExecutor > 10:chain:ToolCallingAgent > 16:parser:ToolCallingAgentOutputParser] [1ms] Exiting Chain run with output: {
  "output": [
    {
      "tool": "calculator",
      "toolInput": {
        "input": "52 * 365"
      },
      "toolCallId": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
      "log": "Invoking \"calculator\" with {\"input\":\"52 * 365\"}\n[{\"type\":\"text\",\"text\":\"Based on the search results, the 2023 film Oppenheimer was directed by Christopher Nolan. Some key information about Christopher Nolan:\\n\\n- He is a British-American film director, producer and screenwriter.\\n- He was born on July 30, 1970, making him currently 52 years old.\\n\\nTo calculate his age in days:\"},{\"type\":\"tool_use\",\"id\":\"toolu_01NVTbm5aNYSm1wGYb6XF7jE\",\"name\":\"calculator\",\"input\":{\"input\":\"52 * 365\"}}]",
      "messageLog": [
        {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain_core",
            "messages",
            "AIMessageChunk"
          ],
          "kwargs": {
            "content": [
              {
                "type": "text",
                "text": "Based on the search results, the 2023 film Oppenheimer was directed by Christopher Nolan. Some key information about Christopher Nolan:\n\n- He is a British-American film director, producer and screenwriter.\n- He was born on July 30, 1970, making him currently 52 years old.\n\nTo calculate his age in days:"
              },
              {
                "type": "tool_use",
                "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
                "name": "calculator",
                "input": {
                  "input": "52 * 365"
                }
              }
            ],
            "additional_kwargs": {
              "id": "msg_01RBDqmJKNXiEjgt5Xrng4mz",
              "type": "message",
              "role": "assistant",
              "model": "claude-3-sonnet-20240229",
              "stop_sequence": null,
              "usage": {
                "input_tokens": 2810,
                "output_tokens": 137
              },
              "stop_reason": "tool_use"
            },
            "tool_call_chunks": [
              {
                "name": "calculator",
                "args": "{\"input\":\"52 * 365\"}",
                "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
                "index": 0
              }
            ],
            "tool_calls": [
              {
                "name": "calculator",
                "args": {
                  "input": "52 * 365"
                },
                "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE"
              }
            ],
            "invalid_tool_calls": [],
            "response_metadata": {}
          }
        }
      ]
    }
  ]
}
[chain/end] [1:chain:AgentExecutor > 10:chain:ToolCallingAgent] [3.51s] Exiting Chain run with output: {
  "output": [
    {
      "tool": "calculator",
      "toolInput": {
        "input": "52 * 365"
      },
      "toolCallId": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
      "log": "Invoking \"calculator\" with {\"input\":\"52 * 365\"}\n[{\"type\":\"text\",\"text\":\"Based on the search results, the 2023 film Oppenheimer was directed by Christopher Nolan. Some key information about Christopher Nolan:\\n\\n- He is a British-American film director, producer and screenwriter.\\n- He was born on July 30, 1970, making him currently 52 years old.\\n\\nTo calculate his age in days:\"},{\"type\":\"tool_use\",\"id\":\"toolu_01NVTbm5aNYSm1wGYb6XF7jE\",\"name\":\"calculator\",\"input\":{\"input\":\"52 * 365\"}}]",
      "messageLog": [
        {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain_core",
            "messages",
            "AIMessageChunk"
          ],
          "kwargs": {
            "content": [
              {
                "type": "text",
                "text": "Based on the search results, the 2023 film Oppenheimer was directed by Christopher Nolan. Some key information about Christopher Nolan:\n\n- He is a British-American film director, producer and screenwriter.\n- He was born on July 30, 1970, making him currently 52 years old.\n\nTo calculate his age in days:"
              },
              {
                "type": "tool_use",
                "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
                "name": "calculator",
                "input": {
                  "input": "52 * 365"
                }
              }
            ],
            "additional_kwargs": {
              "id": "msg_01RBDqmJKNXiEjgt5Xrng4mz",
              "type": "message",
              "role": "assistant",
              "model": "claude-3-sonnet-20240229",
              "stop_sequence": null,
              "usage": {
                "input_tokens": 2810,
                "output_tokens": 137
              },
              "stop_reason": "tool_use"
            },
            "tool_call_chunks": [
              {
                "name": "calculator",
                "args": "{\"input\":\"52 * 365\"}",
                "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
                "index": 0
              }
            ],
            "tool_calls": [
              {
                "name": "calculator",
                "args": {
                  "input": "52 * 365"
                },
                "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE"
              }
            ],
            "invalid_tool_calls": [],
            "response_metadata": {}
          }
        }
      ]
    }
  ]
}
[agent/action] [1:chain:AgentExecutor] Agent selected action: {
  "tool": "calculator",
  "toolInput": {
    "input": "52 * 365"
  },
  "toolCallId": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
  "log": "Invoking \"calculator\" with {\"input\":\"52 * 365\"}\n[{\"type\":\"text\",\"text\":\"Based on the search results, the 2023 film Oppenheimer was directed by Christopher Nolan. Some key information about Christopher Nolan:\\n\\n- He is a British-American film director, producer and screenwriter.\\n- He was born on July 30, 1970, making him currently 52 years old.\\n\\nTo calculate his age in days:\"},{\"type\":\"tool_use\",\"id\":\"toolu_01NVTbm5aNYSm1wGYb6XF7jE\",\"name\":\"calculator\",\"input\":{\"input\":\"52 * 365\"}}]",
  "messageLog": [
    {
      "lc": 1,
      "type": "constructor",
      "id": [
        "langchain_core",
        "messages",
        "AIMessageChunk"
      ],
      "kwargs": {
        "content": [
          {
            "type": "text",
            "text": "Based on the search results, the 2023 film Oppenheimer was directed by Christopher Nolan. Some key information about Christopher Nolan:\n\n- He is a British-American film director, producer and screenwriter.\n- He was born on July 30, 1970, making him currently 52 years old.\n\nTo calculate his age in days:"
          },
          {
            "type": "tool_use",
            "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
            "name": "calculator",
            "input": {
              "input": "52 * 365"
            }
          }
        ],
        "additional_kwargs": {
          "id": "msg_01RBDqmJKNXiEjgt5Xrng4mz",
          "type": "message",
          "role": "assistant",
          "model": "claude-3-sonnet-20240229",
          "stop_sequence": null,
          "usage": {
            "input_tokens": 2810,
            "output_tokens": 137
          },
          "stop_reason": "tool_use"
        },
        "tool_call_chunks": [
          {
            "name": "calculator",
            "args": "{\"input\":\"52 * 365\"}",
            "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
            "index": 0
          }
        ],
        "tool_calls": [
          {
            "name": "calculator",
            "args": {
              "input": "52 * 365"
            },
            "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE"
          }
        ],
        "invalid_tool_calls": [],
        "response_metadata": {}
      }
    }
  ]
}
[tool/start] [1:chain:AgentExecutor > 17:tool:Calculator] Entering Tool run with input: "52 * 365"
[tool/start] [1:tool:Calculator] Entering Tool run with input: "52 * 365"
[tool/end] [1:chain:AgentExecutor > 17:tool:Calculator] [3ms] Exiting Tool run with output: "18980"
[tool/end] [1:tool:Calculator] [3ms] Exiting Tool run with output: "18980"
[chain/start] [1:chain:AgentExecutor > 18:chain:ToolCallingAgent] Entering Chain run with input: {
  "input": "Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?",
  "steps": [
    {
      "action": {
        "tool": "tavily_search_results_json",
        "toolInput": {
          "input": "Oppenheimer 2023 film director age"
        },
        "toolCallId": "toolu_01NUVejujVo2y8WGVtZ49KAN",
        "log": "Invoking \"tavily_search_results_json\" with {\"input\":\"Oppenheimer 2023 film director age\"}\n[{\"type\":\"tool_use\",\"id\":\"toolu_01NUVejujVo2y8WGVtZ49KAN\",\"name\":\"tavily_search_results_json\",\"input\":{\"input\":\"Oppenheimer 2023 film director age\"}}]",
        "messageLog": [
          {
            "lc": 1,
            "type": "constructor",
            "id": [
              "langchain_core",
              "messages",
              "AIMessageChunk"
            ],
            "kwargs": {
              "content": [
                {
                  "type": "tool_use",
                  "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
                  "name": "tavily_search_results_json",
                  "input": {
                    "input": "Oppenheimer 2023 film director age"
                  }
                }
              ],
              "additional_kwargs": {
                "id": "msg_015MqAHr84dBCAqBgjou41Km",
                "type": "message",
                "role": "assistant",
                "model": "claude-3-sonnet-20240229",
                "stop_sequence": null,
                "usage": {
                  "input_tokens": 409,
                  "output_tokens": 68
                },
                "stop_reason": "tool_use"
              },
              "tool_call_chunks": [
                {
                  "name": "tavily_search_results_json",
                  "args": "{\"input\":\"Oppenheimer 2023 film director age\"}",
                  "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
                  "index": 0
                }
              ],
              "tool_calls": [
                {
                  "name": "tavily_search_results_json",
                  "args": {
                    "input": "Oppenheimer 2023 film director age"
                  },
                  "id": "toolu_01NUVejujVo2y8WGVtZ49KAN"
                }
              ],
              "invalid_tool_calls": [],
              "response_metadata": {}
            }
          }
        ]
      },
      "observation": "[{\"title\":\"Oppenheimer (2023) - IMDb\",\"url\":\"https://www.imdb.com/title/tt15398776/\",\"content\":\"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\",\"score\":0.96643,\"raw_content\":null},{\"title\":\"Christopher Nolan's Oppenheimer - Rotten Tomatoes\",\"url\":\"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/\",\"content\":\"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.\",\"score\":0.92804,\"raw_content\":null},{\"title\":\"Oppenheimer (film) - Wikipedia\",\"url\":\"https://en.wikipedia.org/wiki/Oppenheimer_(film)\",\"content\":\"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\\nCritical response\\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \\\"more objective view of his story from a different character's point of view\\\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \\\"big-atures\\\", since the special effects team had tried to build the models as physically large as possible. He felt that \\\"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\\\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \\\"emotional\\\" and resembling that of a thriller, while also remarking that Nolan had \\\"Trojan-Horsed a biopic into a thriller\\\".[72]\\nCasting\\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\\\", while also underscoring that it is a \\\"huge shift in perception about the reality of Oppenheimer's perception\\\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \\\"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\\\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.\",\"score\":0.92404,\"raw_content\":null},{\"title\":\"'Oppenheimer' Director Christopher Nolan On Filmmaking at 53: \\\"I Try to ...\",\"url\":\"https://www.everythingzoomer.com/arts-entertainment/2023/11/21/oppenheimer-director-christopher-nolan-on-filmmaking-at-53-i-try-to-challenge-myself-with-every-film/\",\"content\":\"Oppenheimer will be available to own on 4K Ultra HD, Blu-ray and DVD — including more than three hours of bonus features — on November 21.\\nRELATED:\\nVisiting the Trinity Site Featured in ‘Oppenheimer’ Is a Sobering Reminder of the Horror of Nuclear Weapons\\nBarbenheimer: How ‘Barbie’ and ‘Oppenheimer’ Became the Unlikely Movie Marriage of the Summer\\nBlast From the Past: ‘Asteroid City’ & ‘Oppenheimer’ and the Age of Nuclear Anxiety\\nEXPLORE  HealthMoneyTravelFoodStyleBook ClubClassifieds#ZoomerDailyPolicy & PerspectiveArts & EntertainmentStars & RoyaltySex & Love\\nCONNECT  FacebookTwitterInstagram\\nSUBSCRIBE  Terms of Subscription ServiceE-NewslettersSubscribe to Zoomer Magazine\\nBROWSE  AboutMastheadContact UsAdvertise with UsPrivacy Policy\\nEverythingZoomer.com is part of the ZoomerMedia Digital Network “I think with experience — and with the experience of watching your films with an audience over the years — you do more and more recognize the human elements that people respond to, and the things that move you and the things that move the audience.”\\n “What’s interesting, as you watch the films over time, is that some of his preoccupations are the same, but then some of them have changed over time with who he is as a person and what’s going on in his own life,” Thomas said.\\n The British-American director’s latest explosive drama, Oppenheimer, which has earned upwards of US$940 million at the global box office, follows theoretical physicist J. Robert Oppenheimer (played by Cillian Murphy) as he leads the team creating the first atomic bomb, as director of the Manhattan Project’s Los Alamos Laboratory.\\n Subscribe\\nEverything Zoomer\\n‘Oppenheimer’ Director Christopher Nolan On Filmmaking at 53: “I Try to Challenge Myself with Every Film”\\nDirector Christopher Nolan poses upon his arrival for the premiere of the movie 'Oppenheimer' in Paris on July 11, 2023.\",\"score\":0.92002,\"raw_content\":null},{\"title\":\"'Oppenheimer' Review: A Man for Our Time - The New York Times\",\"url\":\"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html\",\"content\":\"Instead, it is here that the film’s complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era’s most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation — Strauss’s role in the hearing and his relationship with Oppenheimer directly affected the confirmation’s outcome — to create a dialectical synthesis. To signal his conceit, he stamps the film with the words “fission” (a splitting into parts) and “fusion” (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology — it is a lot.\\n It’s also at Berkeley that Oppenheimer meets the project’s military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos’s director, despite the leftist causes he supported — among them, the fight against fascism during the Spanish Civil War — and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\\n\",\"score\":0.91831,\"raw_content\":null}]"
    },
    {
      "action": {
        "tool": "calculator",
        "toolInput": {
          "input": "52 * 365"
        },
        "toolCallId": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
        "log": "Invoking \"calculator\" with {\"input\":\"52 * 365\"}\n[{\"type\":\"text\",\"text\":\"Based on the search results, the 2023 film Oppenheimer was directed by Christopher Nolan. Some key information about Christopher Nolan:\\n\\n- He is a British-American film director, producer and screenwriter.\\n- He was born on July 30, 1970, making him currently 52 years old.\\n\\nTo calculate his age in days:\"},{\"type\":\"tool_use\",\"id\":\"toolu_01NVTbm5aNYSm1wGYb6XF7jE\",\"name\":\"calculator\",\"input\":{\"input\":\"52 * 365\"}}]",
        "messageLog": [
          {
            "lc": 1,
            "type": "constructor",
            "id": [
              "langchain_core",
              "messages",
              "AIMessageChunk"
            ],
            "kwargs": {
              "content": [
                {
                  "type": "text",
                  "text": "Based on the search results, the 2023 film Oppenheimer was directed by Christopher Nolan. Some key information about Christopher Nolan:\n\n- He is a British-American film director, producer and screenwriter.\n- He was born on July 30, 1970, making him currently 52 years old.\n\nTo calculate his age in days:"
                },
                {
                  "type": "tool_use",
                  "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
                  "name": "calculator",
                  "input": {
                    "input": "52 * 365"
                  }
                }
              ],
              "additional_kwargs": {
                "id": "msg_01RBDqmJKNXiEjgt5Xrng4mz",
                "type": "message",
                "role": "assistant",
                "model": "claude-3-sonnet-20240229",
                "stop_sequence": null,
                "usage": {
                  "input_tokens": 2810,
                  "output_tokens": 137
                },
                "stop_reason": "tool_use"
              },
              "tool_call_chunks": [
                {
                  "name": "calculator",
                  "args": "{\"input\":\"52 * 365\"}",
                  "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
                  "index": 0
                }
              ],
              "tool_calls": [
                {
                  "name": "calculator",
                  "args": {
                    "input": "52 * 365"
                  },
                  "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE"
                }
              ],
              "invalid_tool_calls": [],
              "response_metadata": {}
            }
          }
        ]
      },
      "observation": "18980"
    }
  ]
}
[chain/start] [1:chain:AgentExecutor > 18:chain:ToolCallingAgent > 19:chain:RunnableAssign] Entering Chain run with input: {
  "input": ""
}
[chain/start] [1:chain:AgentExecutor > 18:chain:ToolCallingAgent > 19:chain:RunnableAssign > 20:chain:RunnableMap] Entering Chain run with input: {
  "input": ""
}
[chain/start] [1:chain:AgentExecutor > 18:chain:ToolCallingAgent > 19:chain:RunnableAssign > 20:chain:RunnableMap > 21:chain:RunnableLambda] Entering Chain run with input: {
  "input": ""
}
[chain/end] [1:chain:AgentExecutor > 18:chain:ToolCallingAgent > 19:chain:RunnableAssign > 20:chain:RunnableMap > 21:chain:RunnableLambda] [1ms] Exiting Chain run with output: {
  "output": [
    {
      "lc": 1,
      "type": "constructor",
      "id": [
        "langchain_core",
        "messages",
        "AIMessageChunk"
      ],
      "kwargs": {
        "content": [
          {
            "type": "tool_use",
            "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
            "name": "tavily_search_results_json",
            "input": {
              "input": "Oppenheimer 2023 film director age"
            }
          }
        ],
        "additional_kwargs": {
          "id": "msg_015MqAHr84dBCAqBgjou41Km",
          "type": "message",
          "role": "assistant",
          "model": "claude-3-sonnet-20240229",
          "stop_sequence": null,
          "usage": {
            "input_tokens": 409,
            "output_tokens": 68
          },
          "stop_reason": "tool_use"
        },
        "tool_call_chunks": [
          {
            "name": "tavily_search_results_json",
            "args": "{\"input\":\"Oppenheimer 2023 film director age\"}",
            "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
            "index": 0
          }
        ],
        "tool_calls": [
          {
            "name": "tavily_search_results_json",
            "args": {
              "input": "Oppenheimer 2023 film director age"
            },
            "id": "toolu_01NUVejujVo2y8WGVtZ49KAN"
          }
        ],
        "invalid_tool_calls": [],
        "response_metadata": {}
      }
    },
    {
      "lc": 1,
      "type": "constructor",
      "id": [
        "langchain_core",
        "messages",
        "ToolMessage"
      ],
      "kwargs": {
        "tool_call_id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
        "content": "[{\"title\":\"Oppenheimer (2023) - IMDb\",\"url\":\"https://www.imdb.com/title/tt15398776/\",\"content\":\"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\",\"score\":0.96643,\"raw_content\":null},{\"title\":\"Christopher Nolan's Oppenheimer - Rotten Tomatoes\",\"url\":\"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/\",\"content\":\"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.\",\"score\":0.92804,\"raw_content\":null},{\"title\":\"Oppenheimer (film) - Wikipedia\",\"url\":\"https://en.wikipedia.org/wiki/Oppenheimer_(film)\",\"content\":\"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\\nCritical response\\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \\\"more objective view of his story from a different character's point of view\\\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \\\"big-atures\\\", since the special effects team had tried to build the models as physically large as possible. He felt that \\\"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\\\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \\\"emotional\\\" and resembling that of a thriller, while also remarking that Nolan had \\\"Trojan-Horsed a biopic into a thriller\\\".[72]\\nCasting\\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\\\", while also underscoring that it is a \\\"huge shift in perception about the reality of Oppenheimer's perception\\\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \\\"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\\\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.\",\"score\":0.92404,\"raw_content\":null},{\"title\":\"'Oppenheimer' Director Christopher Nolan On Filmmaking at 53: \\\"I Try to ...\",\"url\":\"https://www.everythingzoomer.com/arts-entertainment/2023/11/21/oppenheimer-director-christopher-nolan-on-filmmaking-at-53-i-try-to-challenge-myself-with-every-film/\",\"content\":\"Oppenheimer will be available to own on 4K Ultra HD, Blu-ray and DVD — including more than three hours of bonus features — on November 21.\\nRELATED:\\nVisiting the Trinity Site Featured in ‘Oppenheimer’ Is a Sobering Reminder of the Horror of Nuclear Weapons\\nBarbenheimer: How ‘Barbie’ and ‘Oppenheimer’ Became the Unlikely Movie Marriage of the Summer\\nBlast From the Past: ‘Asteroid City’ & ‘Oppenheimer’ and the Age of Nuclear Anxiety\\nEXPLORE  HealthMoneyTravelFoodStyleBook ClubClassifieds#ZoomerDailyPolicy & PerspectiveArts & EntertainmentStars & RoyaltySex & Love\\nCONNECT  FacebookTwitterInstagram\\nSUBSCRIBE  Terms of Subscription ServiceE-NewslettersSubscribe to Zoomer Magazine\\nBROWSE  AboutMastheadContact UsAdvertise with UsPrivacy Policy\\nEverythingZoomer.com is part of the ZoomerMedia Digital Network “I think with experience — and with the experience of watching your films with an audience over the years — you do more and more recognize the human elements that people respond to, and the things that move you and the things that move the audience.”\\n “What’s interesting, as you watch the films over time, is that some of his preoccupations are the same, but then some of them have changed over time with who he is as a person and what’s going on in his own life,” Thomas said.\\n The British-American director’s latest explosive drama, Oppenheimer, which has earned upwards of US$940 million at the global box office, follows theoretical physicist J. Robert Oppenheimer (played by Cillian Murphy) as he leads the team creating the first atomic bomb, as director of the Manhattan Project’s Los Alamos Laboratory.\\n Subscribe\\nEverything Zoomer\\n‘Oppenheimer’ Director Christopher Nolan On Filmmaking at 53: “I Try to Challenge Myself with Every Film”\\nDirector Christopher Nolan poses upon his arrival for the premiere of the movie 'Oppenheimer' in Paris on July 11, 2023.\",\"score\":0.92002,\"raw_content\":null},{\"title\":\"'Oppenheimer' Review: A Man for Our Time - The New York Times\",\"url\":\"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html\",\"content\":\"Instead, it is here that the film’s complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era’s most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation — Strauss’s role in the hearing and his relationship with Oppenheimer directly affected the confirmation’s outcome — to create a dialectical synthesis. To signal his conceit, he stamps the film with the words “fission” (a splitting into parts) and “fusion” (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology — it is a lot.\\n It’s also at Berkeley that Oppenheimer meets the project’s military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos’s director, despite the leftist causes he supported — among them, the fight against fascism during the Spanish Civil War — and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\\n\",\"score\":0.91831,\"raw_content\":null}]",
        "additional_kwargs": {
          "name": "tavily_search_results_json"
        },
        "response_metadata": {}
      }
    },
    {
      "lc": 1,
      "type": "constructor",
      "id": [
        "langchain_core",
        "messages",
        "AIMessageChunk"
      ],
      "kwargs": {
        "content": [
          {
            "type": "text",
            "text": "Based on the search results, the 2023 film Oppenheimer was directed by Christopher Nolan. Some key information about Christopher Nolan:\n\n- He is a British-American film director, producer and screenwriter.\n- He was born on July 30, 1970, making him currently 52 years old.\n\nTo calculate his age in days:"
          },
          {
            "type": "tool_use",
            "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
            "name": "calculator",
            "input": {
              "input": "52 * 365"
            }
          }
        ],
        "additional_kwargs": {
          "id": "msg_01RBDqmJKNXiEjgt5Xrng4mz",
          "type": "message",
          "role": "assistant",
          "model": "claude-3-sonnet-20240229",
          "stop_sequence": null,
          "usage": {
            "input_tokens": 2810,
            "output_tokens": 137
          },
          "stop_reason": "tool_use"
        },
        "tool_call_chunks": [
          {
            "name": "calculator",
            "args": "{\"input\":\"52 * 365\"}",
            "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
            "index": 0
          }
        ],
        "tool_calls": [
          {
            "name": "calculator",
            "args": {
              "input": "52 * 365"
            },
            "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE"
          }
        ],
        "invalid_tool_calls": [],
        "response_metadata": {}
      }
    },
    {
      "lc": 1,
      "type": "constructor",
      "id": [
        "langchain_core",
        "messages",
        "ToolMessage"
      ],
      "kwargs": {
        "tool_call_id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
        "content": "18980",
        "additional_kwargs": {
          "name": "calculator"
        },
        "response_metadata": {}
      }
    }
  ]
}
[chain/end] [1:chain:AgentExecutor > 18:chain:ToolCallingAgent > 19:chain:RunnableAssign > 20:chain:RunnableMap] [2ms] Exiting Chain run with output: {
  "agent_scratchpad": [
    {
      "lc": 1,
      "type": "constructor",
      "id": [
        "langchain_core",
        "messages",
        "AIMessageChunk"
      ],
      "kwargs": {
        "content": [
          {
            "type": "tool_use",
            "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
            "name": "tavily_search_results_json",
            "input": {
              "input": "Oppenheimer 2023 film director age"
            }
          }
        ],
        "additional_kwargs": {
          "id": "msg_015MqAHr84dBCAqBgjou41Km",
          "type": "message",
          "role": "assistant",
          "model": "claude-3-sonnet-20240229",
          "stop_sequence": null,
          "usage": {
            "input_tokens": 409,
            "output_tokens": 68
          },
          "stop_reason": "tool_use"
        },
        "tool_call_chunks": [
          {
            "name": "tavily_search_results_json",
            "args": "{\"input\":\"Oppenheimer 2023 film director age\"}",
            "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
            "index": 0
          }
        ],
        "tool_calls": [
          {
            "name": "tavily_search_results_json",
            "args": {
              "input": "Oppenheimer 2023 film director age"
            },
            "id": "toolu_01NUVejujVo2y8WGVtZ49KAN"
          }
        ],
        "invalid_tool_calls": [],
        "response_metadata": {}
      }
    },
    {
      "lc": 1,
      "type": "constructor",
      "id": [
        "langchain_core",
        "messages",
        "ToolMessage"
      ],
      "kwargs": {
        "tool_call_id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
        "content": "[{\"title\":\"Oppenheimer (2023) - IMDb\",\"url\":\"https://www.imdb.com/title/tt15398776/\",\"content\":\"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\",\"score\":0.96643,\"raw_content\":null},{\"title\":\"Christopher Nolan's Oppenheimer - Rotten Tomatoes\",\"url\":\"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/\",\"content\":\"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.\",\"score\":0.92804,\"raw_content\":null},{\"title\":\"Oppenheimer (film) - Wikipedia\",\"url\":\"https://en.wikipedia.org/wiki/Oppenheimer_(film)\",\"content\":\"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\\nCritical response\\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \\\"more objective view of his story from a different character's point of view\\\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \\\"big-atures\\\", since the special effects team had tried to build the models as physically large as possible. He felt that \\\"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\\\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \\\"emotional\\\" and resembling that of a thriller, while also remarking that Nolan had \\\"Trojan-Horsed a biopic into a thriller\\\".[72]\\nCasting\\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\\\", while also underscoring that it is a \\\"huge shift in perception about the reality of Oppenheimer's perception\\\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \\\"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\\\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.\",\"score\":0.92404,\"raw_content\":null},{\"title\":\"'Oppenheimer' Director Christopher Nolan On Filmmaking at 53: \\\"I Try to ...\",\"url\":\"https://www.everythingzoomer.com/arts-entertainment/2023/11/21/oppenheimer-director-christopher-nolan-on-filmmaking-at-53-i-try-to-challenge-myself-with-every-film/\",\"content\":\"Oppenheimer will be available to own on 4K Ultra HD, Blu-ray and DVD — including more than three hours of bonus features — on November 21.\\nRELATED:\\nVisiting the Trinity Site Featured in ‘Oppenheimer’ Is a Sobering Reminder of the Horror of Nuclear Weapons\\nBarbenheimer: How ‘Barbie’ and ‘Oppenheimer’ Became the Unlikely Movie Marriage of the Summer\\nBlast From the Past: ‘Asteroid City’ & ‘Oppenheimer’ and the Age of Nuclear Anxiety\\nEXPLORE  HealthMoneyTravelFoodStyleBook ClubClassifieds#ZoomerDailyPolicy & PerspectiveArts & EntertainmentStars & RoyaltySex & Love\\nCONNECT  FacebookTwitterInstagram\\nSUBSCRIBE  Terms of Subscription ServiceE-NewslettersSubscribe to Zoomer Magazine\\nBROWSE  AboutMastheadContact UsAdvertise with UsPrivacy Policy\\nEverythingZoomer.com is part of the ZoomerMedia Digital Network “I think with experience — and with the experience of watching your films with an audience over the years — you do more and more recognize the human elements that people respond to, and the things that move you and the things that move the audience.”\\n “What’s interesting, as you watch the films over time, is that some of his preoccupations are the same, but then some of them have changed over time with who he is as a person and what’s going on in his own life,” Thomas said.\\n The British-American director’s latest explosive drama, Oppenheimer, which has earned upwards of US$940 million at the global box office, follows theoretical physicist J. Robert Oppenheimer (played by Cillian Murphy) as he leads the team creating the first atomic bomb, as director of the Manhattan Project’s Los Alamos Laboratory.\\n Subscribe\\nEverything Zoomer\\n‘Oppenheimer’ Director Christopher Nolan On Filmmaking at 53: “I Try to Challenge Myself with Every Film”\\nDirector Christopher Nolan poses upon his arrival for the premiere of the movie 'Oppenheimer' in Paris on July 11, 2023.\",\"score\":0.92002,\"raw_content\":null},{\"title\":\"'Oppenheimer' Review: A Man for Our Time - The New York Times\",\"url\":\"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html\",\"content\":\"Instead, it is here that the film’s complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era’s most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation — Strauss’s role in the hearing and his relationship with Oppenheimer directly affected the confirmation’s outcome — to create a dialectical synthesis. To signal his conceit, he stamps the film with the words “fission” (a splitting into parts) and “fusion” (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology — it is a lot.\\n It’s also at Berkeley that Oppenheimer meets the project’s military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos’s director, despite the leftist causes he supported — among them, the fight against fascism during the Spanish Civil War — and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\\n\",\"score\":0.91831,\"raw_content\":null}]",
        "additional_kwargs": {
          "name": "tavily_search_results_json"
        },
        "response_metadata": {}
      }
    },
    {
      "lc": 1,
      "type": "constructor",
      "id": [
        "langchain_core",
        "messages",
        "AIMessageChunk"
      ],
      "kwargs": {
        "content": [
          {
            "type": "text",
            "text": "Based on the search results, the 2023 film Oppenheimer was directed by Christopher Nolan. Some key information about Christopher Nolan:\n\n- He is a British-American film director, producer and screenwriter.\n- He was born on July 30, 1970, making him currently 52 years old.\n\nTo calculate his age in days:"
          },
          {
            "type": "tool_use",
            "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
            "name": "calculator",
            "input": {
              "input": "52 * 365"
            }
          }
        ],
        "additional_kwargs": {
          "id": "msg_01RBDqmJKNXiEjgt5Xrng4mz",
          "type": "message",
          "role": "assistant",
          "model": "claude-3-sonnet-20240229",
          "stop_sequence": null,
          "usage": {
            "input_tokens": 2810,
            "output_tokens": 137
          },
          "stop_reason": "tool_use"
        },
        "tool_call_chunks": [
          {
            "name": "calculator",
            "args": "{\"input\":\"52 * 365\"}",
            "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
            "index": 0
          }
        ],
        "tool_calls": [
          {
            "name": "calculator",
            "args": {
              "input": "52 * 365"
            },
            "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE"
          }
        ],
        "invalid_tool_calls": [],
        "response_metadata": {}
      }
    },
    {
      "lc": 1,
      "type": "constructor",
      "id": [
        "langchain_core",
        "messages",
        "ToolMessage"
      ],
      "kwargs": {
        "tool_call_id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
        "content": "18980",
        "additional_kwargs": {
          "name": "calculator"
        },
        "response_metadata": {}
      }
    }
  ]
}
[chain/end] [1:chain:AgentExecutor > 18:chain:ToolCallingAgent > 19:chain:RunnableAssign] [4ms] Exiting Chain run with output: {
  "input": "Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?",
  "steps": [
    {
      "action": {
        "tool": "tavily_search_results_json",
        "toolInput": {
          "input": "Oppenheimer 2023 film director age"
        },
        "toolCallId": "toolu_01NUVejujVo2y8WGVtZ49KAN",
        "log": "Invoking \"tavily_search_results_json\" with {\"input\":\"Oppenheimer 2023 film director age\"}\n[{\"type\":\"tool_use\",\"id\":\"toolu_01NUVejujVo2y8WGVtZ49KAN\",\"name\":\"tavily_search_results_json\",\"input\":{\"input\":\"Oppenheimer 2023 film director age\"}}]",
        "messageLog": [
          {
            "lc": 1,
            "type": "constructor",
            "id": [
              "langchain_core",
              "messages",
              "AIMessageChunk"
            ],
            "kwargs": {
              "content": [
                {
                  "type": "tool_use",
                  "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
                  "name": "tavily_search_results_json",
                  "input": {
                    "input": "Oppenheimer 2023 film director age"
                  }
                }
              ],
              "additional_kwargs": {
                "id": "msg_015MqAHr84dBCAqBgjou41Km",
                "type": "message",
                "role": "assistant",
                "model": "claude-3-sonnet-20240229",
                "stop_sequence": null,
                "usage": {
                  "input_tokens": 409,
                  "output_tokens": 68
                },
                "stop_reason": "tool_use"
              },
              "tool_call_chunks": [
                {
                  "name": "tavily_search_results_json",
                  "args": "{\"input\":\"Oppenheimer 2023 film director age\"}",
                  "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
                  "index": 0
                }
              ],
              "tool_calls": [
                {
                  "name": "tavily_search_results_json",
                  "args": {
                    "input": "Oppenheimer 2023 film director age"
                  },
                  "id": "toolu_01NUVejujVo2y8WGVtZ49KAN"
                }
              ],
              "invalid_tool_calls": [],
              "response_metadata": {}
            }
          }
        ]
      },
      "observation": "[{\"title\":\"Oppenheimer (2023) - IMDb\",\"url\":\"https://www.imdb.com/title/tt15398776/\",\"content\":\"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\",\"score\":0.96643,\"raw_content\":null},{\"title\":\"Christopher Nolan's Oppenheimer - Rotten Tomatoes\",\"url\":\"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/\",\"content\":\"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.\",\"score\":0.92804,\"raw_content\":null},{\"title\":\"Oppenheimer (film) - Wikipedia\",\"url\":\"https://en.wikipedia.org/wiki/Oppenheimer_(film)\",\"content\":\"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\\nCritical response\\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \\\"more objective view of his story from a different character's point of view\\\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \\\"big-atures\\\", since the special effects team had tried to build the models as physically large as possible. He felt that \\\"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\\\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \\\"emotional\\\" and resembling that of a thriller, while also remarking that Nolan had \\\"Trojan-Horsed a biopic into a thriller\\\".[72]\\nCasting\\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\\\", while also underscoring that it is a \\\"huge shift in perception about the reality of Oppenheimer's perception\\\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \\\"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\\\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.\",\"score\":0.92404,\"raw_content\":null},{\"title\":\"'Oppenheimer' Director Christopher Nolan On Filmmaking at 53: \\\"I Try to ...\",\"url\":\"https://www.everythingzoomer.com/arts-entertainment/2023/11/21/oppenheimer-director-christopher-nolan-on-filmmaking-at-53-i-try-to-challenge-myself-with-every-film/\",\"content\":\"Oppenheimer will be available to own on 4K Ultra HD, Blu-ray and DVD — including more than three hours of bonus features — on November 21.\\nRELATED:\\nVisiting the Trinity Site Featured in ‘Oppenheimer’ Is a Sobering Reminder of the Horror of Nuclear Weapons\\nBarbenheimer: How ‘Barbie’ and ‘Oppenheimer’ Became the Unlikely Movie Marriage of the Summer\\nBlast From the Past: ‘Asteroid City’ & ‘Oppenheimer’ and the Age of Nuclear Anxiety\\nEXPLORE  HealthMoneyTravelFoodStyleBook ClubClassifieds#ZoomerDailyPolicy & PerspectiveArts & EntertainmentStars & RoyaltySex & Love\\nCONNECT  FacebookTwitterInstagram\\nSUBSCRIBE  Terms of Subscription ServiceE-NewslettersSubscribe to Zoomer Magazine\\nBROWSE  AboutMastheadContact UsAdvertise with UsPrivacy Policy\\nEverythingZoomer.com is part of the ZoomerMedia Digital Network “I think with experience — and with the experience of watching your films with an audience over the years — you do more and more recognize the human elements that people respond to, and the things that move you and the things that move the audience.”\\n “What’s interesting, as you watch the films over time, is that some of his preoccupations are the same, but then some of them have changed over time with who he is as a person and what’s going on in his own life,” Thomas said.\\n The British-American director’s latest explosive drama, Oppenheimer, which has earned upwards of US$940 million at the global box office, follows theoretical physicist J. Robert Oppenheimer (played by Cillian Murphy) as he leads the team creating the first atomic bomb, as director of the Manhattan Project’s Los Alamos Laboratory.\\n Subscribe\\nEverything Zoomer\\n‘Oppenheimer’ Director Christopher Nolan On Filmmaking at 53: “I Try to Challenge Myself with Every Film”\\nDirector Christopher Nolan poses upon his arrival for the premiere of the movie 'Oppenheimer' in Paris on July 11, 2023.\",\"score\":0.92002,\"raw_content\":null},{\"title\":\"'Oppenheimer' Review: A Man for Our Time - The New York Times\",\"url\":\"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html\",\"content\":\"Instead, it is here that the film’s complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era’s most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation — Strauss’s role in the hearing and his relationship with Oppenheimer directly affected the confirmation’s outcome — to create a dialectical synthesis. To signal his conceit, he stamps the film with the words “fission” (a splitting into parts) and “fusion” (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology — it is a lot.\\n It’s also at Berkeley that Oppenheimer meets the project’s military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos’s director, despite the leftist causes he supported — among them, the fight against fascism during the Spanish Civil War — and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\\n\",\"score\":0.91831,\"raw_content\":null}]"
    },
    {
      "action": {
        "tool": "calculator",
        "toolInput": {
          "input": "52 * 365"
        },
        "toolCallId": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
        "log": "Invoking \"calculator\" with {\"input\":\"52 * 365\"}\n[{\"type\":\"text\",\"text\":\"Based on the search results, the 2023 film Oppenheimer was directed by Christopher Nolan. Some key information about Christopher Nolan:\\n\\n- He is a British-American film director, producer and screenwriter.\\n- He was born on July 30, 1970, making him currently 52 years old.\\n\\nTo calculate his age in days:\"},{\"type\":\"tool_use\",\"id\":\"toolu_01NVTbm5aNYSm1wGYb6XF7jE\",\"name\":\"calculator\",\"input\":{\"input\":\"52 * 365\"}}]",
        "messageLog": [
          {
            "lc": 1,
            "type": "constructor",
            "id": [
              "langchain_core",
              "messages",
              "AIMessageChunk"
            ],
            "kwargs": {
              "content": [
                {
                  "type": "text",
                  "text": "Based on the search results, the 2023 film Oppenheimer was directed by Christopher Nolan. Some key information about Christopher Nolan:\n\n- He is a British-American film director, producer and screenwriter.\n- He was born on July 30, 1970, making him currently 52 years old.\n\nTo calculate his age in days:"
                },
                {
                  "type": "tool_use",
                  "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
                  "name": "calculator",
                  "input": {
                    "input": "52 * 365"
                  }
                }
              ],
              "additional_kwargs": {
                "id": "msg_01RBDqmJKNXiEjgt5Xrng4mz",
                "type": "message",
                "role": "assistant",
                "model": "claude-3-sonnet-20240229",
                "stop_sequence": null,
                "usage": {
                  "input_tokens": 2810,
                  "output_tokens": 137
                },
                "stop_reason": "tool_use"
              },
              "tool_call_chunks": [
                {
                  "name": "calculator",
                  "args": "{\"input\":\"52 * 365\"}",
                  "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
                  "index": 0
                }
              ],
              "tool_calls": [
                {
                  "name": "calculator",
                  "args": {
                    "input": "52 * 365"
                  },
                  "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE"
                }
              ],
              "invalid_tool_calls": [],
              "response_metadata": {}
            }
          }
        ]
      },
      "observation": "18980"
    }
  ],
  "agent_scratchpad": [
    {
      "lc": 1,
      "type": "constructor",
      "id": [
        "langchain_core",
        "messages",
        "AIMessageChunk"
      ],
      "kwargs": {
        "content": [
          {
            "type": "tool_use",
            "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
            "name": "tavily_search_results_json",
            "input": {
              "input": "Oppenheimer 2023 film director age"
            }
          }
        ],
        "additional_kwargs": {
          "id": "msg_015MqAHr84dBCAqBgjou41Km",
          "type": "message",
          "role": "assistant",
          "model": "claude-3-sonnet-20240229",
          "stop_sequence": null,
          "usage": {
            "input_tokens": 409,
            "output_tokens": 68
          },
          "stop_reason": "tool_use"
        },
        "tool_call_chunks": [
          {
            "name": "tavily_search_results_json",
            "args": "{\"input\":\"Oppenheimer 2023 film director age\"}",
            "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
            "index": 0
          }
        ],
        "tool_calls": [
          {
            "name": "tavily_search_results_json",
            "args": {
              "input": "Oppenheimer 2023 film director age"
            },
            "id": "toolu_01NUVejujVo2y8WGVtZ49KAN"
          }
        ],
        "invalid_tool_calls": [],
        "response_metadata": {}
      }
    },
    {
      "lc": 1,
      "type": "constructor",
      "id": [
        "langchain_core",
        "messages",
        "ToolMessage"
      ],
      "kwargs": {
        "tool_call_id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
        "content": "[{\"title\":\"Oppenheimer (2023) - IMDb\",\"url\":\"https://www.imdb.com/title/tt15398776/\",\"content\":\"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\",\"score\":0.96643,\"raw_content\":null},{\"title\":\"Christopher Nolan's Oppenheimer - Rotten Tomatoes\",\"url\":\"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/\",\"content\":\"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.\",\"score\":0.92804,\"raw_content\":null},{\"title\":\"Oppenheimer (film) - Wikipedia\",\"url\":\"https://en.wikipedia.org/wiki/Oppenheimer_(film)\",\"content\":\"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\\nCritical response\\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \\\"more objective view of his story from a different character's point of view\\\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \\\"big-atures\\\", since the special effects team had tried to build the models as physically large as possible. He felt that \\\"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\\\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \\\"emotional\\\" and resembling that of a thriller, while also remarking that Nolan had \\\"Trojan-Horsed a biopic into a thriller\\\".[72]\\nCasting\\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\\\", while also underscoring that it is a \\\"huge shift in perception about the reality of Oppenheimer's perception\\\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \\\"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\\\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.\",\"score\":0.92404,\"raw_content\":null},{\"title\":\"'Oppenheimer' Director Christopher Nolan On Filmmaking at 53: \\\"I Try to ...\",\"url\":\"https://www.everythingzoomer.com/arts-entertainment/2023/11/21/oppenheimer-director-christopher-nolan-on-filmmaking-at-53-i-try-to-challenge-myself-with-every-film/\",\"content\":\"Oppenheimer will be available to own on 4K Ultra HD, Blu-ray and DVD — including more than three hours of bonus features — on November 21.\\nRELATED:\\nVisiting the Trinity Site Featured in ‘Oppenheimer’ Is a Sobering Reminder of the Horror of Nuclear Weapons\\nBarbenheimer: How ‘Barbie’ and ‘Oppenheimer’ Became the Unlikely Movie Marriage of the Summer\\nBlast From the Past: ‘Asteroid City’ & ‘Oppenheimer’ and the Age of Nuclear Anxiety\\nEXPLORE  HealthMoneyTravelFoodStyleBook ClubClassifieds#ZoomerDailyPolicy & PerspectiveArts & EntertainmentStars & RoyaltySex & Love\\nCONNECT  FacebookTwitterInstagram\\nSUBSCRIBE  Terms of Subscription ServiceE-NewslettersSubscribe to Zoomer Magazine\\nBROWSE  AboutMastheadContact UsAdvertise with UsPrivacy Policy\\nEverythingZoomer.com is part of the ZoomerMedia Digital Network “I think with experience — and with the experience of watching your films with an audience over the years — you do more and more recognize the human elements that people respond to, and the things that move you and the things that move the audience.”\\n “What’s interesting, as you watch the films over time, is that some of his preoccupations are the same, but then some of them have changed over time with who he is as a person and what’s going on in his own life,” Thomas said.\\n The British-American director’s latest explosive drama, Oppenheimer, which has earned upwards of US$940 million at the global box office, follows theoretical physicist J. Robert Oppenheimer (played by Cillian Murphy) as he leads the team creating the first atomic bomb, as director of the Manhattan Project’s Los Alamos Laboratory.\\n Subscribe\\nEverything Zoomer\\n‘Oppenheimer’ Director Christopher Nolan On Filmmaking at 53: “I Try to Challenge Myself with Every Film”\\nDirector Christopher Nolan poses upon his arrival for the premiere of the movie 'Oppenheimer' in Paris on July 11, 2023.\",\"score\":0.92002,\"raw_content\":null},{\"title\":\"'Oppenheimer' Review: A Man for Our Time - The New York Times\",\"url\":\"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html\",\"content\":\"Instead, it is here that the film’s complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era’s most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation — Strauss’s role in the hearing and his relationship with Oppenheimer directly affected the confirmation’s outcome — to create a dialectical synthesis. To signal his conceit, he stamps the film with the words “fission” (a splitting into parts) and “fusion” (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology — it is a lot.\\n It’s also at Berkeley that Oppenheimer meets the project’s military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos’s director, despite the leftist causes he supported — among them, the fight against fascism during the Spanish Civil War — and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\\n\",\"score\":0.91831,\"raw_content\":null}]",
        "additional_kwargs": {
          "name": "tavily_search_results_json"
        },
        "response_metadata": {}
      }
    },
    {
      "lc": 1,
      "type": "constructor",
      "id": [
        "langchain_core",
        "messages",
        "AIMessageChunk"
      ],
      "kwargs": {
        "content": [
          {
            "type": "text",
            "text": "Based on the search results, the 2023 film Oppenheimer was directed by Christopher Nolan. Some key information about Christopher Nolan:\n\n- He is a British-American film director, producer and screenwriter.\n- He was born on July 30, 1970, making him currently 52 years old.\n\nTo calculate his age in days:"
          },
          {
            "type": "tool_use",
            "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
            "name": "calculator",
            "input": {
              "input": "52 * 365"
            }
          }
        ],
        "additional_kwargs": {
          "id": "msg_01RBDqmJKNXiEjgt5Xrng4mz",
          "type": "message",
          "role": "assistant",
          "model": "claude-3-sonnet-20240229",
          "stop_sequence": null,
          "usage": {
            "input_tokens": 2810,
            "output_tokens": 137
          },
          "stop_reason": "tool_use"
        },
        "tool_call_chunks": [
          {
            "name": "calculator",
            "args": "{\"input\":\"52 * 365\"}",
            "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
            "index": 0
          }
        ],
        "tool_calls": [
          {
            "name": "calculator",
            "args": {
              "input": "52 * 365"
            },
            "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE"
          }
        ],
        "invalid_tool_calls": [],
        "response_metadata": {}
      }
    },
    {
      "lc": 1,
      "type": "constructor",
      "id": [
        "langchain_core",
        "messages",
        "ToolMessage"
      ],
      "kwargs": {
        "tool_call_id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
        "content": "18980",
        "additional_kwargs": {
          "name": "calculator"
        },
        "response_metadata": {}
      }
    }
  ]
}
[chain/start] [1:chain:AgentExecutor > 18:chain:ToolCallingAgent > 22:prompt:ChatPromptTemplate] Entering Chain run with input: {
  "input": "Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?",
  "steps": [
    {
      "action": {
        "tool": "tavily_search_results_json",
        "toolInput": {
          "input": "Oppenheimer 2023 film director age"
        },
        "toolCallId": "toolu_01NUVejujVo2y8WGVtZ49KAN",
        "log": "Invoking \"tavily_search_results_json\" with {\"input\":\"Oppenheimer 2023 film director age\"}\n[{\"type\":\"tool_use\",\"id\":\"toolu_01NUVejujVo2y8WGVtZ49KAN\",\"name\":\"tavily_search_results_json\",\"input\":{\"input\":\"Oppenheimer 2023 film director age\"}}]",
        "messageLog": [
          {
            "lc": 1,
            "type": "constructor",
            "id": [
              "langchain_core",
              "messages",
              "AIMessageChunk"
            ],
            "kwargs": {
              "content": [
                {
                  "type": "tool_use",
                  "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
                  "name": "tavily_search_results_json",
                  "input": {
                    "input": "Oppenheimer 2023 film director age"
                  }
                }
              ],
              "additional_kwargs": {
                "id": "msg_015MqAHr84dBCAqBgjou41Km",
                "type": "message",
                "role": "assistant",
                "model": "claude-3-sonnet-20240229",
                "stop_sequence": null,
                "usage": {
                  "input_tokens": 409,
                  "output_tokens": 68
                },
                "stop_reason": "tool_use"
              },
              "tool_call_chunks": [
                {
                  "name": "tavily_search_results_json",
                  "args": "{\"input\":\"Oppenheimer 2023 film director age\"}",
                  "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
                  "index": 0
                }
              ],
              "tool_calls": [
                {
                  "name": "tavily_search_results_json",
                  "args": {
                    "input": "Oppenheimer 2023 film director age"
                  },
                  "id": "toolu_01NUVejujVo2y8WGVtZ49KAN"
                }
              ],
              "invalid_tool_calls": [],
              "response_metadata": {}
            }
          }
        ]
      },
      "observation": "[{\"title\":\"Oppenheimer (2023) - IMDb\",\"url\":\"https://www.imdb.com/title/tt15398776/\",\"content\":\"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\",\"score\":0.96643,\"raw_content\":null},{\"title\":\"Christopher Nolan's Oppenheimer - Rotten Tomatoes\",\"url\":\"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/\",\"content\":\"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.\",\"score\":0.92804,\"raw_content\":null},{\"title\":\"Oppenheimer (film) - Wikipedia\",\"url\":\"https://en.wikipedia.org/wiki/Oppenheimer_(film)\",\"content\":\"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\\nCritical response\\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \\\"more objective view of his story from a different character's point of view\\\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \\\"big-atures\\\", since the special effects team had tried to build the models as physically large as possible. He felt that \\\"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\\\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \\\"emotional\\\" and resembling that of a thriller, while also remarking that Nolan had \\\"Trojan-Horsed a biopic into a thriller\\\".[72]\\nCasting\\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\\\", while also underscoring that it is a \\\"huge shift in perception about the reality of Oppenheimer's perception\\\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \\\"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\\\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.\",\"score\":0.92404,\"raw_content\":null},{\"title\":\"'Oppenheimer' Director Christopher Nolan On Filmmaking at 53: \\\"I Try to ...\",\"url\":\"https://www.everythingzoomer.com/arts-entertainment/2023/11/21/oppenheimer-director-christopher-nolan-on-filmmaking-at-53-i-try-to-challenge-myself-with-every-film/\",\"content\":\"Oppenheimer will be available to own on 4K Ultra HD, Blu-ray and DVD — including more than three hours of bonus features — on November 21.\\nRELATED:\\nVisiting the Trinity Site Featured in ‘Oppenheimer’ Is a Sobering Reminder of the Horror of Nuclear Weapons\\nBarbenheimer: How ‘Barbie’ and ‘Oppenheimer’ Became the Unlikely Movie Marriage of the Summer\\nBlast From the Past: ‘Asteroid City’ & ‘Oppenheimer’ and the Age of Nuclear Anxiety\\nEXPLORE  HealthMoneyTravelFoodStyleBook ClubClassifieds#ZoomerDailyPolicy & PerspectiveArts & EntertainmentStars & RoyaltySex & Love\\nCONNECT  FacebookTwitterInstagram\\nSUBSCRIBE  Terms of Subscription ServiceE-NewslettersSubscribe to Zoomer Magazine\\nBROWSE  AboutMastheadContact UsAdvertise with UsPrivacy Policy\\nEverythingZoomer.com is part of the ZoomerMedia Digital Network “I think with experience — and with the experience of watching your films with an audience over the years — you do more and more recognize the human elements that people respond to, and the things that move you and the things that move the audience.”\\n “What’s interesting, as you watch the films over time, is that some of his preoccupations are the same, but then some of them have changed over time with who he is as a person and what’s going on in his own life,” Thomas said.\\n The British-American director’s latest explosive drama, Oppenheimer, which has earned upwards of US$940 million at the global box office, follows theoretical physicist J. Robert Oppenheimer (played by Cillian Murphy) as he leads the team creating the first atomic bomb, as director of the Manhattan Project’s Los Alamos Laboratory.\\n Subscribe\\nEverything Zoomer\\n‘Oppenheimer’ Director Christopher Nolan On Filmmaking at 53: “I Try to Challenge Myself with Every Film”\\nDirector Christopher Nolan poses upon his arrival for the premiere of the movie 'Oppenheimer' in Paris on July 11, 2023.\",\"score\":0.92002,\"raw_content\":null},{\"title\":\"'Oppenheimer' Review: A Man for Our Time - The New York Times\",\"url\":\"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html\",\"content\":\"Instead, it is here that the film’s complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era’s most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation — Strauss’s role in the hearing and his relationship with Oppenheimer directly affected the confirmation’s outcome — to create a dialectical synthesis. To signal his conceit, he stamps the film with the words “fission” (a splitting into parts) and “fusion” (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology — it is a lot.\\n It’s also at Berkeley that Oppenheimer meets the project’s military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos’s director, despite the leftist causes he supported — among them, the fight against fascism during the Spanish Civil War — and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\\n\",\"score\":0.91831,\"raw_content\":null}]"
    },
    {
      "action": {
        "tool": "calculator",
        "toolInput": {
          "input": "52 * 365"
        },
        "toolCallId": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
        "log": "Invoking \"calculator\" with {\"input\":\"52 * 365\"}\n[{\"type\":\"text\",\"text\":\"Based on the search results, the 2023 film Oppenheimer was directed by Christopher Nolan. Some key information about Christopher Nolan:\\n\\n- He is a British-American film director, producer and screenwriter.\\n- He was born on July 30, 1970, making him currently 52 years old.\\n\\nTo calculate his age in days:\"},{\"type\":\"tool_use\",\"id\":\"toolu_01NVTbm5aNYSm1wGYb6XF7jE\",\"name\":\"calculator\",\"input\":{\"input\":\"52 * 365\"}}]",
        "messageLog": [
          {
            "lc": 1,
            "type": "constructor",
            "id": [
              "langchain_core",
              "messages",
              "AIMessageChunk"
            ],
            "kwargs": {
              "content": [
                {
                  "type": "text",
                  "text": "Based on the search results, the 2023 film Oppenheimer was directed by Christopher Nolan. Some key information about Christopher Nolan:\n\n- He is a British-American film director, producer and screenwriter.\n- He was born on July 30, 1970, making him currently 52 years old.\n\nTo calculate his age in days:"
                },
                {
                  "type": "tool_use",
                  "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
                  "name": "calculator",
                  "input": {
                    "input": "52 * 365"
                  }
                }
              ],
              "additional_kwargs": {
                "id": "msg_01RBDqmJKNXiEjgt5Xrng4mz",
                "type": "message",
                "role": "assistant",
                "model": "claude-3-sonnet-20240229",
                "stop_sequence": null,
                "usage": {
                  "input_tokens": 2810,
                  "output_tokens": 137
                },
                "stop_reason": "tool_use"
              },
              "tool_call_chunks": [
                {
                  "name": "calculator",
                  "args": "{\"input\":\"52 * 365\"}",
                  "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
                  "index": 0
                }
              ],
              "tool_calls": [
                {
                  "name": "calculator",
                  "args": {
                    "input": "52 * 365"
                  },
                  "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE"
                }
              ],
              "invalid_tool_calls": [],
              "response_metadata": {}
            }
          }
        ]
      },
      "observation": "18980"
    }
  ],
  "agent_scratchpad": [
    {
      "lc": 1,
      "type": "constructor",
      "id": [
        "langchain_core",
        "messages",
        "AIMessageChunk"
      ],
      "kwargs": {
        "content": [
          {
            "type": "tool_use",
            "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
            "name": "tavily_search_results_json",
            "input": {
              "input": "Oppenheimer 2023 film director age"
            }
          }
        ],
        "additional_kwargs": {
          "id": "msg_015MqAHr84dBCAqBgjou41Km",
          "type": "message",
          "role": "assistant",
          "model": "claude-3-sonnet-20240229",
          "stop_sequence": null,
          "usage": {
            "input_tokens": 409,
            "output_tokens": 68
          },
          "stop_reason": "tool_use"
        },
        "tool_call_chunks": [
          {
            "name": "tavily_search_results_json",
            "args": "{\"input\":\"Oppenheimer 2023 film director age\"}",
            "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
            "index": 0
          }
        ],
        "tool_calls": [
          {
            "name": "tavily_search_results_json",
            "args": {
              "input": "Oppenheimer 2023 film director age"
            },
            "id": "toolu_01NUVejujVo2y8WGVtZ49KAN"
          }
        ],
        "invalid_tool_calls": [],
        "response_metadata": {}
      }
    },
    {
      "lc": 1,
      "type": "constructor",
      "id": [
        "langchain_core",
        "messages",
        "ToolMessage"
      ],
      "kwargs": {
        "tool_call_id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
        "content": "[{\"title\":\"Oppenheimer (2023) - IMDb\",\"url\":\"https://www.imdb.com/title/tt15398776/\",\"content\":\"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\",\"score\":0.96643,\"raw_content\":null},{\"title\":\"Christopher Nolan's Oppenheimer - Rotten Tomatoes\",\"url\":\"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/\",\"content\":\"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.\",\"score\":0.92804,\"raw_content\":null},{\"title\":\"Oppenheimer (film) - Wikipedia\",\"url\":\"https://en.wikipedia.org/wiki/Oppenheimer_(film)\",\"content\":\"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\\nCritical response\\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \\\"more objective view of his story from a different character's point of view\\\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \\\"big-atures\\\", since the special effects team had tried to build the models as physically large as possible. He felt that \\\"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\\\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \\\"emotional\\\" and resembling that of a thriller, while also remarking that Nolan had \\\"Trojan-Horsed a biopic into a thriller\\\".[72]\\nCasting\\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\\\", while also underscoring that it is a \\\"huge shift in perception about the reality of Oppenheimer's perception\\\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \\\"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\\\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.\",\"score\":0.92404,\"raw_content\":null},{\"title\":\"'Oppenheimer' Director Christopher Nolan On Filmmaking at 53: \\\"I Try to ...\",\"url\":\"https://www.everythingzoomer.com/arts-entertainment/2023/11/21/oppenheimer-director-christopher-nolan-on-filmmaking-at-53-i-try-to-challenge-myself-with-every-film/\",\"content\":\"Oppenheimer will be available to own on 4K Ultra HD, Blu-ray and DVD — including more than three hours of bonus features — on November 21.\\nRELATED:\\nVisiting the Trinity Site Featured in ‘Oppenheimer’ Is a Sobering Reminder of the Horror of Nuclear Weapons\\nBarbenheimer: How ‘Barbie’ and ‘Oppenheimer’ Became the Unlikely Movie Marriage of the Summer\\nBlast From the Past: ‘Asteroid City’ & ‘Oppenheimer’ and the Age of Nuclear Anxiety\\nEXPLORE  HealthMoneyTravelFoodStyleBook ClubClassifieds#ZoomerDailyPolicy & PerspectiveArts & EntertainmentStars & RoyaltySex & Love\\nCONNECT  FacebookTwitterInstagram\\nSUBSCRIBE  Terms of Subscription ServiceE-NewslettersSubscribe to Zoomer Magazine\\nBROWSE  AboutMastheadContact UsAdvertise with UsPrivacy Policy\\nEverythingZoomer.com is part of the ZoomerMedia Digital Network “I think with experience — and with the experience of watching your films with an audience over the years — you do more and more recognize the human elements that people respond to, and the things that move you and the things that move the audience.”\\n “What’s interesting, as you watch the films over time, is that some of his preoccupations are the same, but then some of them have changed over time with who he is as a person and what’s going on in his own life,” Thomas said.\\n The British-American director’s latest explosive drama, Oppenheimer, which has earned upwards of US$940 million at the global box office, follows theoretical physicist J. Robert Oppenheimer (played by Cillian Murphy) as he leads the team creating the first atomic bomb, as director of the Manhattan Project’s Los Alamos Laboratory.\\n Subscribe\\nEverything Zoomer\\n‘Oppenheimer’ Director Christopher Nolan On Filmmaking at 53: “I Try to Challenge Myself with Every Film”\\nDirector Christopher Nolan poses upon his arrival for the premiere of the movie 'Oppenheimer' in Paris on July 11, 2023.\",\"score\":0.92002,\"raw_content\":null},{\"title\":\"'Oppenheimer' Review: A Man for Our Time - The New York Times\",\"url\":\"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html\",\"content\":\"Instead, it is here that the film’s complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era’s most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation — Strauss’s role in the hearing and his relationship with Oppenheimer directly affected the confirmation’s outcome — to create a dialectical synthesis. To signal his conceit, he stamps the film with the words “fission” (a splitting into parts) and “fusion” (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology — it is a lot.\\n It’s also at Berkeley that Oppenheimer meets the project’s military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos’s director, despite the leftist causes he supported — among them, the fight against fascism during the Spanish Civil War — and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\\n\",\"score\":0.91831,\"raw_content\":null}]",
        "additional_kwargs": {
          "name": "tavily_search_results_json"
        },
        "response_metadata": {}
      }
    },
    {
      "lc": 1,
      "type": "constructor",
      "id": [
        "langchain_core",
        "messages",
        "AIMessageChunk"
      ],
      "kwargs": {
        "content": [
          {
            "type": "text",
            "text": "Based on the search results, the 2023 film Oppenheimer was directed by Christopher Nolan. Some key information about Christopher Nolan:\n\n- He is a British-American film director, producer and screenwriter.\n- He was born on July 30, 1970, making him currently 52 years old.\n\nTo calculate his age in days:"
          },
          {
            "type": "tool_use",
            "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
            "name": "calculator",
            "input": {
              "input": "52 * 365"
            }
          }
        ],
        "additional_kwargs": {
          "id": "msg_01RBDqmJKNXiEjgt5Xrng4mz",
          "type": "message",
          "role": "assistant",
          "model": "claude-3-sonnet-20240229",
          "stop_sequence": null,
          "usage": {
            "input_tokens": 2810,
            "output_tokens": 137
          },
          "stop_reason": "tool_use"
        },
        "tool_call_chunks": [
          {
            "name": "calculator",
            "args": "{\"input\":\"52 * 365\"}",
            "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
            "index": 0
          }
        ],
        "tool_calls": [
          {
            "name": "calculator",
            "args": {
              "input": "52 * 365"
            },
            "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE"
          }
        ],
        "invalid_tool_calls": [],
        "response_metadata": {}
      }
    },
    {
      "lc": 1,
      "type": "constructor",
      "id": [
        "langchain_core",
        "messages",
        "ToolMessage"
      ],
      "kwargs": {
        "tool_call_id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
        "content": "18980",
        "additional_kwargs": {
          "name": "calculator"
        },
        "response_metadata": {}
      }
    }
  ]
}
[chain/end] [1:chain:AgentExecutor > 18:chain:ToolCallingAgent > 22:prompt:ChatPromptTemplate] [2ms] Exiting Chain run with output: {
  "lc": 1,
  "type": "constructor",
  "id": [
    "langchain_core",
    "prompt_values",
    "ChatPromptValue"
  ],
  "kwargs": {
    "messages": [
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "SystemMessage"
        ],
        "kwargs": {
          "content": "You are a helpful assistant",
          "additional_kwargs": {},
          "response_metadata": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "HumanMessage"
        ],
        "kwargs": {
          "content": "Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?",
          "additional_kwargs": {},
          "response_metadata": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "AIMessageChunk"
        ],
        "kwargs": {
          "content": [
            {
              "type": "tool_use",
              "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
              "name": "tavily_search_results_json",
              "input": {
                "input": "Oppenheimer 2023 film director age"
              }
            }
          ],
          "additional_kwargs": {
            "id": "msg_015MqAHr84dBCAqBgjou41Km",
            "type": "message",
            "role": "assistant",
            "model": "claude-3-sonnet-20240229",
            "stop_sequence": null,
            "usage": {
              "input_tokens": 409,
              "output_tokens": 68
            },
            "stop_reason": "tool_use"
          },
          "tool_call_chunks": [
            {
              "name": "tavily_search_results_json",
              "args": "{\"input\":\"Oppenheimer 2023 film director age\"}",
              "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
              "index": 0
            }
          ],
          "tool_calls": [
            {
              "name": "tavily_search_results_json",
              "args": {
                "input": "Oppenheimer 2023 film director age"
              },
              "id": "toolu_01NUVejujVo2y8WGVtZ49KAN"
            }
          ],
          "invalid_tool_calls": [],
          "response_metadata": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "ToolMessage"
        ],
        "kwargs": {
          "tool_call_id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
          "content": "[{\"title\":\"Oppenheimer (2023) - IMDb\",\"url\":\"https://www.imdb.com/title/tt15398776/\",\"content\":\"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\",\"score\":0.96643,\"raw_content\":null},{\"title\":\"Christopher Nolan's Oppenheimer - Rotten Tomatoes\",\"url\":\"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/\",\"content\":\"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.\",\"score\":0.92804,\"raw_content\":null},{\"title\":\"Oppenheimer (film) - Wikipedia\",\"url\":\"https://en.wikipedia.org/wiki/Oppenheimer_(film)\",\"content\":\"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\\nCritical response\\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \\\"more objective view of his story from a different character's point of view\\\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \\\"big-atures\\\", since the special effects team had tried to build the models as physically large as possible. He felt that \\\"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\\\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \\\"emotional\\\" and resembling that of a thriller, while also remarking that Nolan had \\\"Trojan-Horsed a biopic into a thriller\\\".[72]\\nCasting\\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\\\", while also underscoring that it is a \\\"huge shift in perception about the reality of Oppenheimer's perception\\\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \\\"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\\\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.\",\"score\":0.92404,\"raw_content\":null},{\"title\":\"'Oppenheimer' Director Christopher Nolan On Filmmaking at 53: \\\"I Try to ...\",\"url\":\"https://www.everythingzoomer.com/arts-entertainment/2023/11/21/oppenheimer-director-christopher-nolan-on-filmmaking-at-53-i-try-to-challenge-myself-with-every-film/\",\"content\":\"Oppenheimer will be available to own on 4K Ultra HD, Blu-ray and DVD — including more than three hours of bonus features — on November 21.\\nRELATED:\\nVisiting the Trinity Site Featured in ‘Oppenheimer’ Is a Sobering Reminder of the Horror of Nuclear Weapons\\nBarbenheimer: How ‘Barbie’ and ‘Oppenheimer’ Became the Unlikely Movie Marriage of the Summer\\nBlast From the Past: ‘Asteroid City’ & ‘Oppenheimer’ and the Age of Nuclear Anxiety\\nEXPLORE  HealthMoneyTravelFoodStyleBook ClubClassifieds#ZoomerDailyPolicy & PerspectiveArts & EntertainmentStars & RoyaltySex & Love\\nCONNECT  FacebookTwitterInstagram\\nSUBSCRIBE  Terms of Subscription ServiceE-NewslettersSubscribe to Zoomer Magazine\\nBROWSE  AboutMastheadContact UsAdvertise with UsPrivacy Policy\\nEverythingZoomer.com is part of the ZoomerMedia Digital Network “I think with experience — and with the experience of watching your films with an audience over the years — you do more and more recognize the human elements that people respond to, and the things that move you and the things that move the audience.”\\n “What’s interesting, as you watch the films over time, is that some of his preoccupations are the same, but then some of them have changed over time with who he is as a person and what’s going on in his own life,” Thomas said.\\n The British-American director’s latest explosive drama, Oppenheimer, which has earned upwards of US$940 million at the global box office, follows theoretical physicist J. Robert Oppenheimer (played by Cillian Murphy) as he leads the team creating the first atomic bomb, as director of the Manhattan Project’s Los Alamos Laboratory.\\n Subscribe\\nEverything Zoomer\\n‘Oppenheimer’ Director Christopher Nolan On Filmmaking at 53: “I Try to Challenge Myself with Every Film”\\nDirector Christopher Nolan poses upon his arrival for the premiere of the movie 'Oppenheimer' in Paris on July 11, 2023.\",\"score\":0.92002,\"raw_content\":null},{\"title\":\"'Oppenheimer' Review: A Man for Our Time - The New York Times\",\"url\":\"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html\",\"content\":\"Instead, it is here that the film’s complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era’s most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation — Strauss’s role in the hearing and his relationship with Oppenheimer directly affected the confirmation’s outcome — to create a dialectical synthesis. To signal his conceit, he stamps the film with the words “fission” (a splitting into parts) and “fusion” (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology — it is a lot.\\n It’s also at Berkeley that Oppenheimer meets the project’s military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos’s director, despite the leftist causes he supported — among them, the fight against fascism during the Spanish Civil War — and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\\n\",\"score\":0.91831,\"raw_content\":null}]",
          "additional_kwargs": {
            "name": "tavily_search_results_json"
          },
          "response_metadata": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "AIMessageChunk"
        ],
        "kwargs": {
          "content": [
            {
              "type": "text",
              "text": "Based on the search results, the 2023 film Oppenheimer was directed by Christopher Nolan. Some key information about Christopher Nolan:\n\n- He is a British-American film director, producer and screenwriter.\n- He was born on July 30, 1970, making him currently 52 years old.\n\nTo calculate his age in days:"
            },
            {
              "type": "tool_use",
              "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
              "name": "calculator",
              "input": {
                "input": "52 * 365"
              }
            }
          ],
          "additional_kwargs": {
            "id": "msg_01RBDqmJKNXiEjgt5Xrng4mz",
            "type": "message",
            "role": "assistant",
            "model": "claude-3-sonnet-20240229",
            "stop_sequence": null,
            "usage": {
              "input_tokens": 2810,
              "output_tokens": 137
            },
            "stop_reason": "tool_use"
          },
          "tool_call_chunks": [
            {
              "name": "calculator",
              "args": "{\"input\":\"52 * 365\"}",
              "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
              "index": 0
            }
          ],
          "tool_calls": [
            {
              "name": "calculator",
              "args": {
                "input": "52 * 365"
              },
              "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE"
            }
          ],
          "invalid_tool_calls": [],
          "response_metadata": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "ToolMessage"
        ],
        "kwargs": {
          "tool_call_id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
          "content": "18980",
          "additional_kwargs": {
            "name": "calculator"
          },
          "response_metadata": {}
        }
      }
    ]
  }
}
[llm/start] [1:chain:AgentExecutor > 18:chain:ToolCallingAgent > 23:llm:ChatAnthropic] Entering LLM run with input: {
  "messages": [
    [
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "SystemMessage"
        ],
        "kwargs": {
          "content": "You are a helpful assistant",
          "additional_kwargs": {},
          "response_metadata": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "HumanMessage"
        ],
        "kwargs": {
          "content": "Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?",
          "additional_kwargs": {},
          "response_metadata": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "AIMessageChunk"
        ],
        "kwargs": {
          "content": [
            {
              "type": "tool_use",
              "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
              "name": "tavily_search_results_json",
              "input": {
                "input": "Oppenheimer 2023 film director age"
              }
            }
          ],
          "additional_kwargs": {
            "id": "msg_015MqAHr84dBCAqBgjou41Km",
            "type": "message",
            "role": "assistant",
            "model": "claude-3-sonnet-20240229",
            "stop_sequence": null,
            "usage": {
              "input_tokens": 409,
              "output_tokens": 68
            },
            "stop_reason": "tool_use"
          },
          "tool_call_chunks": [
            {
              "name": "tavily_search_results_json",
              "args": "{\"input\":\"Oppenheimer 2023 film director age\"}",
              "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
              "index": 0
            }
          ],
          "tool_calls": [
            {
              "name": "tavily_search_results_json",
              "args": {
                "input": "Oppenheimer 2023 film director age"
              },
              "id": "toolu_01NUVejujVo2y8WGVtZ49KAN"
            }
          ],
          "invalid_tool_calls": [],
          "response_metadata": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "ToolMessage"
        ],
        "kwargs": {
          "tool_call_id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
          "content": "[{\"title\":\"Oppenheimer (2023) - IMDb\",\"url\":\"https://www.imdb.com/title/tt15398776/\",\"content\":\"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\",\"score\":0.96643,\"raw_content\":null},{\"title\":\"Christopher Nolan's Oppenheimer - Rotten Tomatoes\",\"url\":\"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/\",\"content\":\"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.\",\"score\":0.92804,\"raw_content\":null},{\"title\":\"Oppenheimer (film) - Wikipedia\",\"url\":\"https://en.wikipedia.org/wiki/Oppenheimer_(film)\",\"content\":\"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\\nCritical response\\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \\\"more objective view of his story from a different character's point of view\\\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \\\"big-atures\\\", since the special effects team had tried to build the models as physically large as possible. He felt that \\\"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\\\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \\\"emotional\\\" and resembling that of a thriller, while also remarking that Nolan had \\\"Trojan-Horsed a biopic into a thriller\\\".[72]\\nCasting\\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\\\", while also underscoring that it is a \\\"huge shift in perception about the reality of Oppenheimer's perception\\\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \\\"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\\\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.\",\"score\":0.92404,\"raw_content\":null},{\"title\":\"'Oppenheimer' Director Christopher Nolan On Filmmaking at 53: \\\"I Try to ...\",\"url\":\"https://www.everythingzoomer.com/arts-entertainment/2023/11/21/oppenheimer-director-christopher-nolan-on-filmmaking-at-53-i-try-to-challenge-myself-with-every-film/\",\"content\":\"Oppenheimer will be available to own on 4K Ultra HD, Blu-ray and DVD — including more than three hours of bonus features — on November 21.\\nRELATED:\\nVisiting the Trinity Site Featured in ‘Oppenheimer’ Is a Sobering Reminder of the Horror of Nuclear Weapons\\nBarbenheimer: How ‘Barbie’ and ‘Oppenheimer’ Became the Unlikely Movie Marriage of the Summer\\nBlast From the Past: ‘Asteroid City’ & ‘Oppenheimer’ and the Age of Nuclear Anxiety\\nEXPLORE  HealthMoneyTravelFoodStyleBook ClubClassifieds#ZoomerDailyPolicy & PerspectiveArts & EntertainmentStars & RoyaltySex & Love\\nCONNECT  FacebookTwitterInstagram\\nSUBSCRIBE  Terms of Subscription ServiceE-NewslettersSubscribe to Zoomer Magazine\\nBROWSE  AboutMastheadContact UsAdvertise with UsPrivacy Policy\\nEverythingZoomer.com is part of the ZoomerMedia Digital Network “I think with experience — and with the experience of watching your films with an audience over the years — you do more and more recognize the human elements that people respond to, and the things that move you and the things that move the audience.”\\n “What’s interesting, as you watch the films over time, is that some of his preoccupations are the same, but then some of them have changed over time with who he is as a person and what’s going on in his own life,” Thomas said.\\n The British-American director’s latest explosive drama, Oppenheimer, which has earned upwards of US$940 million at the global box office, follows theoretical physicist J. Robert Oppenheimer (played by Cillian Murphy) as he leads the team creating the first atomic bomb, as director of the Manhattan Project’s Los Alamos Laboratory.\\n Subscribe\\nEverything Zoomer\\n‘Oppenheimer’ Director Christopher Nolan On Filmmaking at 53: “I Try to Challenge Myself with Every Film”\\nDirector Christopher Nolan poses upon his arrival for the premiere of the movie 'Oppenheimer' in Paris on July 11, 2023.\",\"score\":0.92002,\"raw_content\":null},{\"title\":\"'Oppenheimer' Review: A Man for Our Time - The New York Times\",\"url\":\"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html\",\"content\":\"Instead, it is here that the film’s complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era’s most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation — Strauss’s role in the hearing and his relationship with Oppenheimer directly affected the confirmation’s outcome — to create a dialectical synthesis. To signal his conceit, he stamps the film with the words “fission” (a splitting into parts) and “fusion” (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology — it is a lot.\\n It’s also at Berkeley that Oppenheimer meets the project’s military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos’s director, despite the leftist causes he supported — among them, the fight against fascism during the Spanish Civil War — and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\\n\",\"score\":0.91831,\"raw_content\":null}]",
          "additional_kwargs": {
            "name": "tavily_search_results_json"
          },
          "response_metadata": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "AIMessageChunk"
        ],
        "kwargs": {
          "content": [
            {
              "type": "text",
              "text": "Based on the search results, the 2023 film Oppenheimer was directed by Christopher Nolan. Some key information about Christopher Nolan:\n\n- He is a British-American film director, producer and screenwriter.\n- He was born on July 30, 1970, making him currently 52 years old.\n\nTo calculate his age in days:"
            },
            {
              "type": "tool_use",
              "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
              "name": "calculator",
              "input": {
                "input": "52 * 365"
              }
            }
          ],
          "additional_kwargs": {
            "id": "msg_01RBDqmJKNXiEjgt5Xrng4mz",
            "type": "message",
            "role": "assistant",
            "model": "claude-3-sonnet-20240229",
            "stop_sequence": null,
            "usage": {
              "input_tokens": 2810,
              "output_tokens": 137
            },
            "stop_reason": "tool_use"
          },
          "tool_call_chunks": [
            {
              "name": "calculator",
              "args": "{\"input\":\"52 * 365\"}",
              "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
              "index": 0
            }
          ],
          "tool_calls": [
            {
              "name": "calculator",
              "args": {
                "input": "52 * 365"
              },
              "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE"
            }
          ],
          "invalid_tool_calls": [],
          "response_metadata": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "ToolMessage"
        ],
        "kwargs": {
          "tool_call_id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
          "content": "18980",
          "additional_kwargs": {
            "name": "calculator"
          },
          "response_metadata": {}
        }
      }
    ]
  ]
}
[llm/start] [1:llm:ChatAnthropic] Entering LLM run with input: {
  "messages": [
    [
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "SystemMessage"
        ],
        "kwargs": {
          "content": "You are a helpful assistant",
          "additional_kwargs": {},
          "response_metadata": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "HumanMessage"
        ],
        "kwargs": {
          "content": "Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?",
          "additional_kwargs": {},
          "response_metadata": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "AIMessageChunk"
        ],
        "kwargs": {
          "content": [
            {
              "type": "tool_use",
              "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
              "name": "tavily_search_results_json",
              "input": {
                "input": "Oppenheimer 2023 film director age"
              }
            }
          ],
          "additional_kwargs": {
            "id": "msg_015MqAHr84dBCAqBgjou41Km",
            "type": "message",
            "role": "assistant",
            "model": "claude-3-sonnet-20240229",
            "stop_sequence": null,
            "usage": {
              "input_tokens": 409,
              "output_tokens": 68
            },
            "stop_reason": "tool_use"
          },
          "tool_call_chunks": [
            {
              "name": "tavily_search_results_json",
              "args": "{\"input\":\"Oppenheimer 2023 film director age\"}",
              "id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
              "index": 0
            }
          ],
          "tool_calls": [
            {
              "name": "tavily_search_results_json",
              "args": {
                "input": "Oppenheimer 2023 film director age"
              },
              "id": "toolu_01NUVejujVo2y8WGVtZ49KAN"
            }
          ],
          "invalid_tool_calls": [],
          "response_metadata": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "ToolMessage"
        ],
        "kwargs": {
          "tool_call_id": "toolu_01NUVejujVo2y8WGVtZ49KAN",
          "content": "[{\"title\":\"Oppenheimer (2023) - IMDb\",\"url\":\"https://www.imdb.com/title/tt15398776/\",\"content\":\"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\",\"score\":0.96643,\"raw_content\":null},{\"title\":\"Christopher Nolan's Oppenheimer - Rotten Tomatoes\",\"url\":\"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/\",\"content\":\"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.\",\"score\":0.92804,\"raw_content\":null},{\"title\":\"Oppenheimer (film) - Wikipedia\",\"url\":\"https://en.wikipedia.org/wiki/Oppenheimer_(film)\",\"content\":\"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\\nCritical response\\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \\\"more objective view of his story from a different character's point of view\\\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \\\"big-atures\\\", since the special effects team had tried to build the models as physically large as possible. He felt that \\\"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\\\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \\\"emotional\\\" and resembling that of a thriller, while also remarking that Nolan had \\\"Trojan-Horsed a biopic into a thriller\\\".[72]\\nCasting\\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\\\", while also underscoring that it is a \\\"huge shift in perception about the reality of Oppenheimer's perception\\\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \\\"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\\\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.\",\"score\":0.92404,\"raw_content\":null},{\"title\":\"'Oppenheimer' Director Christopher Nolan On Filmmaking at 53: \\\"I Try to ...\",\"url\":\"https://www.everythingzoomer.com/arts-entertainment/2023/11/21/oppenheimer-director-christopher-nolan-on-filmmaking-at-53-i-try-to-challenge-myself-with-every-film/\",\"content\":\"Oppenheimer will be available to own on 4K Ultra HD, Blu-ray and DVD — including more than three hours of bonus features — on November 21.\\nRELATED:\\nVisiting the Trinity Site Featured in ‘Oppenheimer’ Is a Sobering Reminder of the Horror of Nuclear Weapons\\nBarbenheimer: How ‘Barbie’ and ‘Oppenheimer’ Became the Unlikely Movie Marriage of the Summer\\nBlast From the Past: ‘Asteroid City’ & ‘Oppenheimer’ and the Age of Nuclear Anxiety\\nEXPLORE  HealthMoneyTravelFoodStyleBook ClubClassifieds#ZoomerDailyPolicy & PerspectiveArts & EntertainmentStars & RoyaltySex & Love\\nCONNECT  FacebookTwitterInstagram\\nSUBSCRIBE  Terms of Subscription ServiceE-NewslettersSubscribe to Zoomer Magazine\\nBROWSE  AboutMastheadContact UsAdvertise with UsPrivacy Policy\\nEverythingZoomer.com is part of the ZoomerMedia Digital Network “I think with experience — and with the experience of watching your films with an audience over the years — you do more and more recognize the human elements that people respond to, and the things that move you and the things that move the audience.”\\n “What’s interesting, as you watch the films over time, is that some of his preoccupations are the same, but then some of them have changed over time with who he is as a person and what’s going on in his own life,” Thomas said.\\n The British-American director’s latest explosive drama, Oppenheimer, which has earned upwards of US$940 million at the global box office, follows theoretical physicist J. Robert Oppenheimer (played by Cillian Murphy) as he leads the team creating the first atomic bomb, as director of the Manhattan Project’s Los Alamos Laboratory.\\n Subscribe\\nEverything Zoomer\\n‘Oppenheimer’ Director Christopher Nolan On Filmmaking at 53: “I Try to Challenge Myself with Every Film”\\nDirector Christopher Nolan poses upon his arrival for the premiere of the movie 'Oppenheimer' in Paris on July 11, 2023.\",\"score\":0.92002,\"raw_content\":null},{\"title\":\"'Oppenheimer' Review: A Man for Our Time - The New York Times\",\"url\":\"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html\",\"content\":\"Instead, it is here that the film’s complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era’s most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation — Strauss’s role in the hearing and his relationship with Oppenheimer directly affected the confirmation’s outcome — to create a dialectical synthesis. To signal his conceit, he stamps the film with the words “fission” (a splitting into parts) and “fusion” (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology — it is a lot.\\n It’s also at Berkeley that Oppenheimer meets the project’s military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos’s director, despite the leftist causes he supported — among them, the fight against fascism during the Spanish Civil War — and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\\n\",\"score\":0.91831,\"raw_content\":null}]",
          "additional_kwargs": {
            "name": "tavily_search_results_json"
          },
          "response_metadata": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "AIMessageChunk"
        ],
        "kwargs": {
          "content": [
            {
              "type": "text",
              "text": "Based on the search results, the 2023 film Oppenheimer was directed by Christopher Nolan. Some key information about Christopher Nolan:\n\n- He is a British-American film director, producer and screenwriter.\n- He was born on July 30, 1970, making him currently 52 years old.\n\nTo calculate his age in days:"
            },
            {
              "type": "tool_use",
              "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
              "name": "calculator",
              "input": {
                "input": "52 * 365"
              }
            }
          ],
          "additional_kwargs": {
            "id": "msg_01RBDqmJKNXiEjgt5Xrng4mz",
            "type": "message",
            "role": "assistant",
            "model": "claude-3-sonnet-20240229",
            "stop_sequence": null,
            "usage": {
              "input_tokens": 2810,
              "output_tokens": 137
            },
            "stop_reason": "tool_use"
          },
          "tool_call_chunks": [
            {
              "name": "calculator",
              "args": "{\"input\":\"52 * 365\"}",
              "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
              "index": 0
            }
          ],
          "tool_calls": [
            {
              "name": "calculator",
              "args": {
                "input": "52 * 365"
              },
              "id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE"
            }
          ],
          "invalid_tool_calls": [],
          "response_metadata": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain_core",
          "messages",
          "ToolMessage"
        ],
        "kwargs": {
          "tool_call_id": "toolu_01NVTbm5aNYSm1wGYb6XF7jE",
          "content": "18980",
          "additional_kwargs": {
            "name": "calculator"
          },
          "response_metadata": {}
        }
      }
    ]
  ]
}
[llm/end] [1:chain:AgentExecutor > 18:chain:ToolCallingAgent > 23:llm:ChatAnthropic] [2.16s] Exiting LLM run with output: {
  "generations": [
    [
      {
        "text": "So Christopher Nolan, the director of the 2023 film Oppenheimer, is currently 52 years old, which is approximately 18,980 days old (assuming 365 days per year).",
        "message": {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain_core",
            "messages",
            "AIMessageChunk"
          ],
          "kwargs": {
            "content": "So Christopher Nolan, the director of the 2023 film Oppenheimer, is currently 52 years old, which is approximately 18,980 days old (assuming 365 days per year).",
            "additional_kwargs": {
              "id": "msg_01TYp6vJRKJQgXXRoqVrDGTR",
              "type": "message",
              "role": "assistant",
              "model": "claude-3-sonnet-20240229",
              "stop_sequence": null,
              "usage": {
                "input_tokens": 2960,
                "output_tokens": 51
              },
              "stop_reason": "end_turn"
            },
            "tool_call_chunks": [],
            "tool_calls": [],
            "invalid_tool_calls": [],
            "response_metadata": {}
          }
        }
      }
    ]
  ]
}
[llm/end] [1:llm:ChatAnthropic] [2.16s] Exiting LLM run with output: {
  "generations": [
    [
      {
        "text": "So Christopher Nolan, the director of the 2023 film Oppenheimer, is currently 52 years old, which is approximately 18,980 days old (assuming 365 days per year).",
        "message": {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain_core",
            "messages",
            "AIMessageChunk"
          ],
          "kwargs": {
            "content": "So Christopher Nolan, the director of the 2023 film Oppenheimer, is currently 52 years old, which is approximately 18,980 days old (assuming 365 days per year).",
            "additional_kwargs": {
              "id": "msg_01TYp6vJRKJQgXXRoqVrDGTR",
              "type": "message",
              "role": "assistant",
              "model": "claude-3-sonnet-20240229",
              "stop_sequence": null,
              "usage": {
                "input_tokens": 2960,
                "output_tokens": 51
              },
              "stop_reason": "end_turn"
            },
            "tool_call_chunks": [],
            "tool_calls": [],
            "invalid_tool_calls": [],
            "response_metadata": {}
          }
        }
      }
    ]
  ]
}
[chain/start] [1:chain:AgentExecutor > 18:chain:ToolCallingAgent > 24:parser:ToolCallingAgentOutputParser] Entering Chain run with input: {
  "lc": 1,
  "type": "constructor",
  "id": [
    "langchain_core",
    "messages",
    "AIMessageChunk"
  ],
  "kwargs": {
    "content": "So Christopher Nolan, the director of the 2023 film Oppenheimer, is currently 52 years old, which is approximately 18,980 days old (assuming 365 days per year).",
    "additional_kwargs": {
      "id": "msg_01TYp6vJRKJQgXXRoqVrDGTR",
      "type": "message",
      "role": "assistant",
      "model": "claude-3-sonnet-20240229",
      "stop_sequence": null,
      "usage": {
        "input_tokens": 2960,
        "output_tokens": 51
      },
      "stop_reason": "end_turn"
    },
    "tool_call_chunks": [],
    "tool_calls": [],
    "invalid_tool_calls": [],
    "response_metadata": {}
  }
}
[chain/end] [1:chain:AgentExecutor > 18:chain:ToolCallingAgent > 24:parser:ToolCallingAgentOutputParser] [2ms] Exiting Chain run with output: {
  "returnValues": {
    "output": "So Christopher Nolan, the director of the 2023 film Oppenheimer, is currently 52 years old, which is approximately 18,980 days old (assuming 365 days per year)."
  },
  "log": "So Christopher Nolan, the director of the 2023 film Oppenheimer, is currently 52 years old, which is approximately 18,980 days old (assuming 365 days per year)."
}
[chain/end] [1:chain:AgentExecutor > 18:chain:ToolCallingAgent] [2.20s] Exiting Chain run with output: {
  "returnValues": {
    "output": "So Christopher Nolan, the director of the 2023 film Oppenheimer, is currently 52 years old, which is approximately 18,980 days old (assuming 365 days per year)."
  },
  "log": "So Christopher Nolan, the director of the 2023 film Oppenheimer, is currently 52 years old, which is approximately 18,980 days old (assuming 365 days per year)."
}
[chain/end] [1:chain:AgentExecutor] [9.92s] Exiting Chain run with output: {
  "input": "Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?",
  "output": "So Christopher Nolan, the director of the 2023 film Oppenheimer, is currently 52 years old, which is approximately 18,980 days old (assuming 365 days per year)."
}
```

</details>

### `Tool({ ..., verbose: true })`

You can also scope verbosity down to a single object, in which case only the inputs and outputs to that object are printed (along with any additional callbacks calls made specifically by that object).

import SimpleAgentVerboseSome from "@examples/guides/debugging/simple_agent_verbose_some.ts";

<CodeBlock language="typescript">{SimpleAgentVerboseSome}</CodeBlock>

<details> 
  <summary>Console output</summary>

```bash
[tool/start] [1:tool:TavilySearchResults] Entering Tool run with input: "Oppenheimer 2023 film director age"
[tool/end] [1:tool:TavilySearchResults] [1.95s] Exiting Tool run with output: "[{"title":"'Oppenheimer' Review: A Man for Our Time - The New York Times","url":"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html","content":"Instead, it is here that the film’s complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era’s most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation — Strauss’s role in the hearing and his relationship with Oppenheimer directly affected the confirmation’s outcome — to create a dialectical synthesis. To signal his conceit, he stamps the film with the words “fission” (a splitting into parts) and “fusion” (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology — it is a lot.\n It’s also at Berkeley that Oppenheimer meets the project’s military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos’s director, despite the leftist causes he supported — among them, the fight against fascism during the Spanish Civil War — and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\n","score":0.97519,"raw_content":null},{"title":"Oppenheimer's Grandson Reacts to New Christopher Nolan Film | TIME","url":"https://time.com/6297743/oppenheimer-grandson-movie-interview/","content":"July 25, 2023 3:32 PM EDT. M oviegoers turned out in droves this weekend for writer-director Christopher Nolan's new film Oppenheimer, fueling an expectations-shattering domestic box office debut ...","score":0.95166,"raw_content":null},{"title":"Oppenheimer (2023) - IMDb","url":"https://www.imdb.com/title/tt15398776/","content":"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.","score":0.95127,"raw_content":null},{"title":"Oppenheimer (film) - Wikipedia","url":"https://en.wikipedia.org/wiki/Oppenheimer_(film)","content":"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\nCritical response\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \"more objective view of his story from a different character's point of view\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \"big-atures\", since the special effects team had tried to build the models as physically large as possible. He felt that \"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \"emotional\" and resembling that of a thriller, while also remarking that Nolan had \"Trojan-Horsed a biopic into a thriller\".[72]\nCasting\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\", while also underscoring that it is a \"huge shift in perception about the reality of Oppenheimer's perception\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.","score":0.92204,"raw_content":null},{"title":"Oppenheimer (2023) - Full Cast & Crew - IMDb","url":"https://www.imdb.com/title/tt15398776/fullcredits/","content":"Oppenheimer (2023) cast and crew credits, including actors, actresses, directors, writers and more. Menu. Movies. Release Calendar Top 250 Movies Most Popular Movies Browse Movies by Genre Top Box Office Showtimes & Tickets Movie News India Movie Spotlight. ... Peter Oppenheimer - Age 8 (uncredited) Adam Walker Federman ... MIT Student ...","score":0.92179,"raw_content":null}]"
[tool/start] [1:tool:TavilySearchResults] Entering Tool run with input: "Christopher Nolan age"
[tool/end] [1:tool:TavilySearchResults] [1.15s] Exiting Tool run with output: "[{"title":"Christopher Nolan - IMDb","url":"https://www.imdb.com/name/nm0634240/","content":"Christopher Nolan is a British-American writer-director-producer of acclaimed films such as Inception, The Dark Knight, and Interstellar. He was born on July 30, 1970, in London, England.","score":0.96627,"raw_content":null},{"title":"Christopher Nolan: Biography, Movie Director, Filmmaker","url":"https://www.biography.com/movies-tv/christopher-nolan","content":"To meet the team, visit our About Us page: https://www.biography.com/about/a43602329/about-us\nFilmmakers\nMatt Damon\nGreta Gerwig\nMartin Scorsese\nBradley Cooper\nJodie Foster\nDodi Fayed\nDrew Barrymore\nRyan Gosling Was Reluctant to Play Barbie’s Ken\nThe Actors in the Most Wes Anderson Movies\n“The Idol” Raises Eyesbrows at Cannes\n41 Inspiring Famous Women in History\nBen Affleck and Matt Damon’s Lifelong Friendship\nA Part of Hearst Digital Media\nWe may earn commission from links on this page, but we only recommend products we back.\n The Dark Knight and Inception\nIn July 2008, Nolan’s Batman sequel, The Dark Knight, opened and set the record as having the highest weekend gross in the United States, at $158 million; Knight went on to become one of the top five highest-grossing films in America. In the fall of 2014, Nolan returned to the big screen with Interstellar, a nearly three-hour sci-fi epic that follows the journey of a team of astronauts seeking a new world for the inhabitants of a besieged Earth. The director's career then traveled into the stratosphere, when he agreed to helm the re-launch of the comic book hero Batman with the 2005 film Batman Begins, starring Christian Bale as the titular character. Built around three storylines offering different perspectives on a dramatic turn of events in 1940, Dunkirk earned mostly rave reviews for its portrayals of the tensions and terrors of war, picking up Golden Globe nominations for Best Motion Picture—Drama and Best Director, as well as an Academy Award nod for Best Director.\n","score":0.95669,"raw_content":null},{"title":"Christopher Nolan - Biography - IMDb","url":"https://www.imdb.com/name/nm0634240/bio/","content":"Learn about the life and career of acclaimed writer-director Christopher Nolan, who was born on July 30, 1970, in London, England. Find out his filmography, awards, family, trivia and more on IMDb.","score":0.91217,"raw_content":null},{"title":"Christopher Nolan - Wikipedia","url":"https://en.wikipedia.org/wiki/Christopher_Nolan","content":"In early 2003, Nolan approached Warner Bros. with the idea of making a new Batman film, based on the character's origin story.[58] Nolan was fascinated by the notion of grounding it in a more realistic world than a comic-book fantasy.[59] He relied heavily on traditional stunts and miniature effects during filming, with minimal use of computer-generated imagery (CGI).[60] Batman Begins (2005), the biggest project Nolan had undertaken to that point,[61] was released to critical acclaim and commercial success.[62][63] Starring Christian Bale as Bruce Wayne / Batman—along with Michael Caine, Gary Oldman, Morgan Freeman and Liam Neeson—Batman Begins revived the franchise.[64][65] Batman Begins was 2005's ninth-highest-grossing film and was praised for its psychological depth and contemporary relevance;[63][66] it is cited as one of the most influential films of the 2000s.[67] Film author Ian Nathan wrote that within five years of his career, Nolan \"[went] from unknown to indie darling to gaining creative control over one of the biggest properties in Hollywood, and (perhaps unwittingly) fomenting the genre that would redefine the entire industry\".[68]\nNolan directed, co-wrote and produced The Prestige (2006), an adaptation of the Christopher Priest novel about two rival 19th-century magicians.[69] He directed, wrote and edited the short film Larceny (1996),[19] which was filmed over a weekend in black and white with limited equipment and a small cast and crew.[12][20] Funded by Nolan and shot with the UCL Union Film society's equipment, it appeared at the Cambridge Film Festival in 1996 and is considered one of UCL's best shorts.[21] For unknown reasons, the film has since been removed from public view.[19] Nolan filmed a third short, Doodlebug (1997), about a man seemingly chasing an insect with his shoe, only to discover that it is a miniature of himself.[14][22] Nolan and Thomas first attempted to make a feature in the mid-1990s with Larry Mahoney, which they scrapped.[23] During this period in his career, Nolan had little to no success getting his projects off the ground, facing several rejections; he added, \"[T]here's a very limited pool of finance in the UK. Philosophy professor David Kyle Johnson wrote that \"Inception became a classic almost as soon as it was projected on silver screens\", praising its exploration of philosophical ideas, including leap of faith and allegory of the cave.[97] The film grossed over $836 million worldwide.[98] Nominated for eight Academy Awards—including Best Picture and Best Original Screenplay—it won Best Cinematography, Best Sound Mixing, Best Sound Editing and Best Visual Effects.[99] Nolan was nominated for a BAFTA Award and a Golden Globe Award for Best Director, among other accolades.[40]\nAround the release of The Dark Knight Rises (2012), Nolan's third and final Batman film, Joseph Bevan of the British Film Institute wrote a profile on him: \"In the space of just over a decade, Christopher Nolan has shot from promising British indie director to undisputed master of a new brand of intelligent escapism. He further wrote that Nolan's body of work reflect \"a heterogeneity of conditions of products\" extending from low-budget films to lucrative blockbusters, \"a wide range of genres and settings\" and \"a diversity of styles that trumpet his versatility\".[193]\nDavid Bordwell, a film theorist, wrote that Nolan has been able to blend his \"experimental impulses\" with the demands of mainstream entertainment, describing his oeuvre as \"experiments with cinematic time by means of techniques of subjective viewpoint and crosscutting\".[194] Nolan's use of practical, in-camera effects, miniatures and models, as well as shooting on celluloid film, has been highly influential in early 21st century cinema.[195][196] IndieWire wrote in 2019 that, Nolan \"kept a viable alternate model of big-budget filmmaking alive\", in an era where blockbuster filmmaking has become \"a largely computer-generated art form\".[196] Initially reluctant to make a sequel, he agreed after Warner Bros. repeatedly insisted.[78] Nolan wanted to expand on the noir quality of the first film by broadening the canvas and taking on \"the dynamic of a story of the city, a large crime story ... where you're looking at the police, the justice system, the vigilante, the poor people, the rich people, the criminals\".[79] Continuing to minimalise the use of CGI, Nolan employed high-resolution IMAX cameras, making it the first major motion picture to use this technology.[80][81]","score":0.90288,"raw_content":null},{"title":"Christopher Nolan | Biography, Movies, Batman, Oppenheimer, & Facts ...","url":"https://www.britannica.com/biography/Christopher-Nolan-British-director","content":"The sci-fi drama depicted the efforts of a group of scientists to relocate humanity from an Earth vitiated by war and famine to another planet by way of a wormhole. The film turns on this character’s attempt to move past the boundaries of the technology in order to actually plant an idea in a dreamer’s head. His 2023 film Oppenheimer, depicts J. Robert Oppenheimer’s role in the development of the atomic bomb and the later security hearing over his alleged ties to communism. It used a destabilizing reverse-order story line to mirror the fractured mental state of its protagonist, a man with short-term amnesia who is trying to track down the person who murdered his wife. The Dark Knight (2008) leaned even more heavily on the moral and structural decay of its setting, fictional Gotham City, and it revived such classic Batman villains as the Joker (played by Heath Ledger).","score":0.90219,"raw_content":null}]"
[tool/start] [1:tool:Calculator] Entering Tool run with input: "(2023 - 1970) * 365"
[tool/end] [1:tool:Calculator] [3ms] Exiting Tool run with output: "19345"
{
  input: 'Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?',
  output: 'So Christopher Nolan, the director of the 2023 film Oppenheimer, is currently 52 years old, which is 19,345 days old (assuming 365 days per year).'
}
MacBook-Pro-4:examples jacoblee$ yarn start examples/src/guides/debugging/simple_agent_verbose_some.ts
(node:78812) ExperimentalWarning: `--experimental-loader` may be removed in the future; instead use `register()`:
--import 'data:text/javascript,import { register } from "node:module"; import { pathToFileURL } from "node:url"; register("file%3A///Users/jacoblee/langchain/langchainjs/node_modules/tsx/dist/loader.js", pathToFileURL("./"));'
(Use `node --trace-warnings ...` to show where the warning was created)
[WARN]: You have enabled LangSmith tracing without backgrounding callbacks.
[WARN]: If you are not using a serverless environment where you must wait for tracing calls to finish,
[WARN]: we suggest setting "process.env.LANGCHAIN_CALLBACKS_BACKGROUND=true" to avoid additional latency.
[tool/start] [1:tool:TavilySearchResults] Entering Tool run with input: "Oppenheimer 2023 film director age"
[tool/end] [1:tool:TavilySearchResults] [1.76s] Exiting Tool run with output: "[{"title":"Oppenheimer (film) - Wikipedia","url":"https://en.wikipedia.org/wiki/Oppenheimer_(film)","content":"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\nCritical response\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \"more objective view of his story from a different character's point of view\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \"big-atures\", since the special effects team had tried to build the models as physically large as possible. He felt that \"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \"emotional\" and resembling that of a thriller, while also remarking that Nolan had \"Trojan-Horsed a biopic into a thriller\".[72]\nCasting\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\", while also underscoring that it is a \"huge shift in perception about the reality of Oppenheimer's perception\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.","score":0.97075,"raw_content":null},{"title":"Christopher Nolan's Oppenheimer - Rotten Tomatoes","url":"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/","content":"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.","score":0.9684,"raw_content":null},{"title":"Oppenheimer (2023) - Full Cast & Crew - IMDb","url":"https://www.imdb.com/title/tt15398776/fullcredits/","content":"Oppenheimer (2023) cast and crew credits, including actors, actresses, directors, writers and more. Menu. Movies. Release Calendar Top 250 Movies Most Popular Movies Browse Movies by Genre Top Box Office Showtimes & Tickets Movie News India Movie Spotlight. ... Peter Oppenheimer - Age 8 (uncredited) Adam Walker Federman ... MIT Student ...","score":0.94834,"raw_content":null},{"title":"Oppenheimer (2023) - IMDb","url":"https://www.imdb.com/title/tt15398776/","content":"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.","score":0.92995,"raw_content":null},{"title":"'Oppenheimer' Review: A Man for Our Time - The New York Times","url":"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html","content":"Instead, it is here that the film’s complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era’s most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation — Strauss’s role in the hearing and his relationship with Oppenheimer directly affected the confirmation’s outcome — to create a dialectical synthesis. To signal his conceit, he stamps the film with the words “fission” (a splitting into parts) and “fusion” (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology — it is a lot.\n It’s also at Berkeley that Oppenheimer meets the project’s military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos’s director, despite the leftist causes he supported — among them, the fight against fascism during the Spanish Civil War — and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\n","score":0.92512,"raw_content":null}]"
[tool/start] [1:tool:TavilySearchResults] Entering Tool run with input: "Christopher Nolan age"
[tool/end] [1:tool:TavilySearchResults] [1.69s] Exiting Tool run with output: "[{"title":"Christopher Nolan: Biography, Movie Director, Filmmaker","url":"https://www.biography.com/movies-tv/christopher-nolan","content":"To meet the team, visit our About Us page: https://www.biography.com/about/a43602329/about-us\nFilmmakers\nMatt Damon\nGreta Gerwig\nMartin Scorsese\nBradley Cooper\nJodie Foster\nDodi Fayed\nDrew Barrymore\nRyan Gosling Was Reluctant to Play Barbie’s Ken\nThe Actors in the Most Wes Anderson Movies\n“The Idol” Raises Eyesbrows at Cannes\n41 Inspiring Famous Women in History\nBen Affleck and Matt Damon’s Lifelong Friendship\nA Part of Hearst Digital Media\nWe may earn commission from links on this page, but we only recommend products we back.\n The Dark Knight and Inception\nIn July 2008, Nolan’s Batman sequel, The Dark Knight, opened and set the record as having the highest weekend gross in the United States, at $158 million; Knight went on to become one of the top five highest-grossing films in America. In the fall of 2014, Nolan returned to the big screen with Interstellar, a nearly three-hour sci-fi epic that follows the journey of a team of astronauts seeking a new world for the inhabitants of a besieged Earth. The director's career then traveled into the stratosphere, when he agreed to helm the re-launch of the comic book hero Batman with the 2005 film Batman Begins, starring Christian Bale as the titular character. Built around three storylines offering different perspectives on a dramatic turn of events in 1940, Dunkirk earned mostly rave reviews for its portrayals of the tensions and terrors of war, picking up Golden Globe nominations for Best Motion Picture—Drama and Best Director, as well as an Academy Award nod for Best Director.\n","score":0.96408,"raw_content":null},{"title":"Christopher Nolan - Biography - IMDb","url":"https://www.imdb.com/name/nm0634240/bio/","content":"Learn about the life and career of acclaimed writer-director Christopher Nolan, who was born on July 30, 1970, in London, England. Find out his filmography, awards, family, trivia and more on IMDb.","score":0.95409,"raw_content":null},{"title":"Christopher Nolan - IMDb","url":"https://www.imdb.com/name/nm0634240/","content":"Christopher Nolan is a British-American writer-director-producer of acclaimed films such as Inception, The Dark Knight, and Interstellar. He was born on July 30, 1970, in London, England.","score":0.95401,"raw_content":null},{"title":"Christopher Nolan - Wikipedia","url":"https://en.wikipedia.org/wiki/Christopher_Nolan","content":"In early 2003, Nolan approached Warner Bros. with the idea of making a new Batman film, based on the character's origin story.[58] Nolan was fascinated by the notion of grounding it in a more realistic world than a comic-book fantasy.[59] He relied heavily on traditional stunts and miniature effects during filming, with minimal use of computer-generated imagery (CGI).[60] Batman Begins (2005), the biggest project Nolan had undertaken to that point,[61] was released to critical acclaim and commercial success.[62][63] Starring Christian Bale as Bruce Wayne / Batman—along with Michael Caine, Gary Oldman, Morgan Freeman and Liam Neeson—Batman Begins revived the franchise.[64][65] Batman Begins was 2005's ninth-highest-grossing film and was praised for its psychological depth and contemporary relevance;[63][66] it is cited as one of the most influential films of the 2000s.[67] Film author Ian Nathan wrote that within five years of his career, Nolan \"[went] from unknown to indie darling to gaining creative control over one of the biggest properties in Hollywood, and (perhaps unwittingly) fomenting the genre that would redefine the entire industry\".[68]\nNolan directed, co-wrote and produced The Prestige (2006), an adaptation of the Christopher Priest novel about two rival 19th-century magicians.[69] He directed, wrote and edited the short film Larceny (1996),[19] which was filmed over a weekend in black and white with limited equipment and a small cast and crew.[12][20] Funded by Nolan and shot with the UCL Union Film society's equipment, it appeared at the Cambridge Film Festival in 1996 and is considered one of UCL's best shorts.[21] For unknown reasons, the film has since been removed from public view.[19] Nolan filmed a third short, Doodlebug (1997), about a man seemingly chasing an insect with his shoe, only to discover that it is a miniature of himself.[14][22] Nolan and Thomas first attempted to make a feature in the mid-1990s with Larry Mahoney, which they scrapped.[23] During this period in his career, Nolan had little to no success getting his projects off the ground, facing several rejections; he added, \"[T]here's a very limited pool of finance in the UK. Philosophy professor David Kyle Johnson wrote that \"Inception became a classic almost as soon as it was projected on silver screens\", praising its exploration of philosophical ideas, including leap of faith and allegory of the cave.[97] The film grossed over $836 million worldwide.[98] Nominated for eight Academy Awards—including Best Picture and Best Original Screenplay—it won Best Cinematography, Best Sound Mixing, Best Sound Editing and Best Visual Effects.[99] Nolan was nominated for a BAFTA Award and a Golden Globe Award for Best Director, among other accolades.[40]\nAround the release of The Dark Knight Rises (2012), Nolan's third and final Batman film, Joseph Bevan of the British Film Institute wrote a profile on him: \"In the space of just over a decade, Christopher Nolan has shot from promising British indie director to undisputed master of a new brand of intelligent escapism. He further wrote that Nolan's body of work reflect \"a heterogeneity of conditions of products\" extending from low-budget films to lucrative blockbusters, \"a wide range of genres and settings\" and \"a diversity of styles that trumpet his versatility\".[193]\nDavid Bordwell, a film theorist, wrote that Nolan has been able to blend his \"experimental impulses\" with the demands of mainstream entertainment, describing his oeuvre as \"experiments with cinematic time by means of techniques of subjective viewpoint and crosscutting\".[194] Nolan's use of practical, in-camera effects, miniatures and models, as well as shooting on celluloid film, has been highly influential in early 21st century cinema.[195][196] IndieWire wrote in 2019 that, Nolan \"kept a viable alternate model of big-budget filmmaking alive\", in an era where blockbuster filmmaking has become \"a largely computer-generated art form\".[196] Initially reluctant to make a sequel, he agreed after Warner Bros. repeatedly insisted.[78] Nolan wanted to expand on the noir quality of the first film by broadening the canvas and taking on \"the dynamic of a story of the city, a large crime story ... where you're looking at the police, the justice system, the vigilante, the poor people, the rich people, the criminals\".[79] Continuing to minimalise the use of CGI, Nolan employed high-resolution IMAX cameras, making it the first major motion picture to use this technology.[80][81]","score":0.93205,"raw_content":null},{"title":"Christopher Nolan | Biography, Movies, Batman, Oppenheimer, & Facts ...","url":"https://www.britannica.com/biography/Christopher-Nolan-British-director","content":"The sci-fi drama depicted the efforts of a group of scientists to relocate humanity from an Earth vitiated by war and famine to another planet by way of a wormhole. The film turns on this character’s attempt to move past the boundaries of the technology in order to actually plant an idea in a dreamer’s head. His 2023 film Oppenheimer, depicts J. Robert Oppenheimer’s role in the development of the atomic bomb and the later security hearing over his alleged ties to communism. It used a destabilizing reverse-order story line to mirror the fractured mental state of its protagonist, a man with short-term amnesia who is trying to track down the person who murdered his wife. The Dark Knight (2008) leaned even more heavily on the moral and structural decay of its setting, fictional Gotham City, and it revived such classic Batman villains as the Joker (played by Heath Ledger).","score":0.90859,"raw_content":null}]"
 [tool/start] [1:tool:Calculator] Entering Tool run with input: "52 * 365"
[tool/end] [1:tool:Calculator] [2ms] Exiting Tool run with output: "18980"
{
  input: 'Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?',
  output: '<result>\nTherefore, Christopher Nolan is 18,980 days old.\n</result>'
}
MacBook-Pro-4:examples jacoblee$ yarn start examples/src/guides/debugging/simple_agent_verbose_some.ts
(node:78844) ExperimentalWarning: `--experimental-loader` may be removed in the future; instead use `register()`:
--import 'data:text/javascript,import { register } from "node:module"; import { pathToFileURL } from "node:url"; register("file%3A///Users/jacoblee/langchain/langchainjs/node_modules/tsx/dist/loader.js", pathToFileURL("./"));'
(Use `node --trace-warnings ...` to show where the warning was created)
[WARN]: You have enabled LangSmith tracing without backgrounding callbacks.
[WARN]: If you are not using a serverless environment where you must wait for tracing calls to finish,
[WARN]: we suggest setting "process.env.LANGCHAIN_CALLBACKS_BACKGROUND=true" to avoid additional latency.
[tool/start] [1:tool:TavilySearchResults] Entering Tool run with input: "Oppenheimer 2023 film director age"
[tool/end] [1:tool:TavilySearchResults] [2.63s] Exiting Tool run with output: "[{"title":"Oppenheimer (film) - Wikipedia","url":"https://en.wikipedia.org/wiki/Oppenheimer_(film)","content":"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\nCritical response\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \"more objective view of his story from a different character's point of view\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \"big-atures\", since the special effects team had tried to build the models as physically large as possible. He felt that \"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \"emotional\" and resembling that of a thriller, while also remarking that Nolan had \"Trojan-Horsed a biopic into a thriller\".[72]\nCasting\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\", while also underscoring that it is a \"huge shift in perception about the reality of Oppenheimer's perception\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.","score":0.95617,"raw_content":null},{"title":"Oppenheimer (2023) - IMDb","url":"https://www.imdb.com/title/tt15398776/","content":"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.","score":0.95378,"raw_content":null},{"title":"'Oppenheimer' Review: A Man for Our Time - The New York Times","url":"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html","content":"Instead, it is here that the film’s complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era’s most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation — Strauss’s role in the hearing and his relationship with Oppenheimer directly affected the confirmation’s outcome — to create a dialectical synthesis. To signal his conceit, he stamps the film with the words “fission” (a splitting into parts) and “fusion” (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology — it is a lot.\n It’s also at Berkeley that Oppenheimer meets the project’s military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos’s director, despite the leftist causes he supported — among them, the fight against fascism during the Spanish Civil War — and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\n","score":0.92271,"raw_content":null},{"title":"Oppenheimer (2023) - Full Cast & Crew - IMDb","url":"https://www.imdb.com/title/tt15398776/fullcredits/","content":"Oppenheimer (2023) cast and crew credits, including actors, actresses, directors, writers and more. Menu. Movies. Release Calendar Top 250 Movies Most Popular Movies Browse Movies by Genre Top Box Office Showtimes & Tickets Movie News India Movie Spotlight. ... Peter Oppenheimer - Age 8 (uncredited) Adam Walker Federman ... MIT Student ...","score":0.91904,"raw_content":null},{"title":"Oppenheimer's Grandson Reacts to New Christopher Nolan Film | TIME","url":"https://time.com/6297743/oppenheimer-grandson-movie-interview/","content":"July 25, 2023 3:32 PM EDT. M oviegoers turned out in droves this weekend for writer-director Christopher Nolan's new film Oppenheimer, fueling an expectations-shattering domestic box office debut ...","score":0.91248,"raw_content":null}]"
[tool/start] [1:tool:Calculator] Entering Tool run with input: "(2023 - 1970) * 365"
[tool/end] [1:tool:Calculator] [2ms] Exiting Tool run with output: "19345"
```

</details>

```bash
{
  input: 'Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?',
  output: "So as of 2023, Christopher Nolan's age is approximately 19,345 days.\n" +
    '\n' +
    'In summary:\n' +
    '- The 2023 film Oppenheimer was directed by Christopher Nolan\n' +
    '- Nolan was born on July 30, 1970, making his current age around 53 years old\n' +
    '- Converted to days, Nolan is approximately 19,345 days old as of 2023'
}
```

## Other callbacks

`Callbacks` are what we use to execute any functionality within a component outside the primary component logic.
All of the above solutions use `Callbacks` under the hood to log intermediate steps of components.
There are a number of `Callbacks` relevant for debugging that come with LangChain out of the box, like the [`ConsoleCallbackHandler`](https://api.js.langchain.com/classes/langchain_core.tracers_console.ConsoleCallbackHandler.html).
You can also implement your own callbacks to execute custom functionality.



================================================
FILE: docs/core_docs/docs/how_to/document_loader_csv.mdx
================================================
# How to load CSV data

> A [comma-separated values (CSV)](https://en.wikipedia.org/wiki/Comma-separated_values) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas.

Load CSV data with a single row per document.

## Setup

```bash npm2yarn
npm install d3-dsv@2
```

## Usage, extracting all columns

Example CSV file:

```csv
id,text
1,This is a sentence.
2,This is another sentence.
```

Example code:

```typescript
import { CSVLoader } from "@langchain/community/document_loaders/fs/csv";

const loader = new CSVLoader("src/document_loaders/example_data/example.csv");

const docs = await loader.load();
/*
[
  Document {
    "metadata": {
      "line": 1,
      "source": "src/document_loaders/example_data/example.csv",
    },
    "pageContent": "id: 1
text: This is a sentence.",
  },
  Document {
    "metadata": {
      "line": 2,
      "source": "src/document_loaders/example_data/example.csv",
    },
    "pageContent": "id: 2
text: This is another sentence.",
  },
]
*/
```

## Usage, extracting a single column

Example CSV file:

```csv
id,text
1,This is a sentence.
2,This is another sentence.
```

Example code:

```typescript
import { CSVLoader } from "@langchain/community/document_loaders/fs/csv";

const loader = new CSVLoader(
  "src/document_loaders/example_data/example.csv",
  "text"
);

const docs = await loader.load();
/*
[
  Document {
    "metadata": {
      "line": 1,
      "source": "src/document_loaders/example_data/example.csv",
    },
    "pageContent": "This is a sentence.",
  },
  Document {
    "metadata": {
      "line": 2,
      "source": "src/document_loaders/example_data/example.csv",
    },
    "pageContent": "This is another sentence.",
  },
]
*/
```



================================================
FILE: docs/core_docs/docs/how_to/document_loader_custom.mdx
================================================
# How to write a custom document loader

If you want to implement your own Document Loader, you have a few options.

### Subclassing `BaseDocumentLoader`

You can extend the `BaseDocumentLoader` class directly. The `BaseDocumentLoader` class provides a few convenience methods for loading documents from a variety of sources.

```typescript
abstract class BaseDocumentLoader implements DocumentLoader {
  abstract load(): Promise<Document[]>;
}
```

### Subclassing `TextLoader`

If you want to load documents from a text file, you can extend the `TextLoader` class. The `TextLoader` class takes care of reading the file, so all you have to do is implement a parse method.

```typescript
abstract class TextLoader extends BaseDocumentLoader {
  abstract parse(raw: string): Promise<string[]>;
}
```

### Subclassing `BufferLoader`

If you want to load documents from a binary file, you can extend the `BufferLoader` class. The `BufferLoader` class takes care of reading the file, so all you have to do is implement a parse method.

```typescript
abstract class BufferLoader extends BaseDocumentLoader {
  abstract parse(
    raw: Buffer,
    metadata: Document["metadata"]
  ): Promise<Document[]>;
}
```



================================================
FILE: docs/core_docs/docs/how_to/document_loader_directory.mdx
================================================
# How to load data from a directory

This covers how to load all documents in a directory.

The second argument is a map of file extensions to loader factories. Each file will be passed to the matching loader, and the resulting documents will be concatenated together.

Example folder:

```text
src/document_loaders/example_data/example/
├── example.json
├── example.jsonl
├── example.txt
└── example.csv
```

Example code:

```typescript
import { DirectoryLoader } from "langchain/document_loaders/fs/directory";
import {
  JSONLoader,
  JSONLinesLoader,
} from "langchain/document_loaders/fs/json";
import { TextLoader } from "langchain/document_loaders/fs/text";
import { CSVLoader } from "@langchain/community/document_loaders/fs/csv";

const loader = new DirectoryLoader(
  "src/document_loaders/example_data/example",
  {
    ".json": (path) => new JSONLoader(path, "/texts"),
    ".jsonl": (path) => new JSONLinesLoader(path, "/html"),
    ".txt": (path) => new TextLoader(path),
    ".csv": (path) => new CSVLoader(path, "text"),
  }
);
const docs = await loader.load();
console.log({ docs });
```



================================================
FILE: docs/core_docs/docs/how_to/document_loader_html.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to load HTML

The HyperText Markup Language or [HTML](https://en.wikipedia.org/wiki/HTML) is the standard markup language for documents designed to be displayed in a web browser.

This covers how to load `HTML` documents into a LangChain [Document](https://api.js.langchain.com/classes/langchain_core.documents.Document.html) objects that we can use downstream.

Parsing HTML files often requires specialized tools. Here we demonstrate parsing via [Unstructured](https://unstructured-io.github.io/unstructured/). Head over to the integrations page to find integrations with additional services, such as [FireCrawl](/docs/integrations/document_loaders/web_loaders/firecrawl).

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Documents](https://api.js.langchain.com/classes/_langchain_core.documents.Document.html)
- [Document Loaders](/docs/concepts/document_loaders)

:::

## Installation

```{=mdx}
import Npm2Yarn from "@theme/Npm2Yarn"

<Npm2Yarn>
  @langchain/community @langchain/core
</Npm2Yarn>
```
"""

"""
## Setup

Although Unstructured has an open source offering, you're still required to provide an API key to access the service. To get everything up and running, follow these two steps:

1. Download & start the Docker container:
  
```bash
docker run -p 8000:8000 -d --rm --name unstructured-api downloads.unstructured.io/unstructured-io/unstructured-api:latest --port 8000 --host 0.0.0.0
```

2. Get a free API key & API URL [here](https://unstructured.io/api-key), and set it in your environment (as per the Unstructured website, it may take up to an hour to allocate your API key & URL.):

```bash
export UNSTRUCTURED_API_KEY="..."
# Replace with your `Full URL` from the email
export UNSTRUCTURED_API_URL="https://<ORG_NAME>-<SECRET>.api.unstructuredapp.io/general/v0/general" 
```
"""

"""
## Loading HTML with Unstructured
"""

import { UnstructuredLoader } from "@langchain/community/document_loaders/fs/unstructured";

const filePath = "../../../../libs/langchain-community/src/tools/fixtures/wordoftheday.html"

const loader = new UnstructuredLoader(filePath, {
  apiKey: process.env.UNSTRUCTURED_API_KEY,
  apiUrl: process.env.UNSTRUCTURED_API_URL,
});

const data = await loader.load()
console.log(data.slice(0, 5));
# Output:
#   [

#     Document {

#       pageContent: 'Word of the Day',

#       metadata: {

#         category_depth: 0,

#         languages: [Array],

#         filename: 'wordoftheday.html',

#         filetype: 'text/html',

#         category: 'Title'

#       }

#     },

#     Document {

#       pageContent: ': April 10, 2023',

#       metadata: {

#         emphasized_text_contents: [Array],

#         emphasized_text_tags: [Array],

#         languages: [Array],

#         parent_id: 'b845e60d85ff7d10abda4e5f9a37eec8',

#         filename: 'wordoftheday.html',

#         filetype: 'text/html',

#         category: 'UncategorizedText'

#       }

#     },

#     Document {

#       pageContent: 'foible',

#       metadata: {

#         category_depth: 1,

#         languages: [Array],

#         parent_id: 'b845e60d85ff7d10abda4e5f9a37eec8',

#         filename: 'wordoftheday.html',

#         filetype: 'text/html',

#         category: 'Title'

#       }

#     },

#     Document {

#       pageContent: 'play',

#       metadata: {

#         category_depth: 0,

#         link_texts: [Array],

#         link_urls: [Array],

#         link_start_indexes: [Array],

#         languages: [Array],

#         filename: 'wordoftheday.html',

#         filetype: 'text/html',

#         category: 'Title'

#       }

#     },

#     Document {

#       pageContent: 'noun',

#       metadata: {

#         category_depth: 0,

#         emphasized_text_contents: [Array],

#         emphasized_text_tags: [Array],

#         languages: [Array],

#         filename: 'wordoftheday.html',

#         filetype: 'text/html',

#         category: 'Title'

#       }

#     }

#   ]




================================================
FILE: docs/core_docs/docs/how_to/document_loader_markdown.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to load Markdown

[Markdown](https://en.wikipedia.org/wiki/Markdown) is a lightweight markup language for creating formatted text using a plain-text editor.

Here we cover how to load `Markdown` documents into LangChain [Document](https://api.js.langchain.com/classes/langchain_core.documents.Document.html) objects that we can use downstream.

We will cover:

- Basic usage;
- Parsing of Markdown into elements such as titles, list items, and text.

LangChain implements an [UnstructuredLoader](https://api.js.langchain.com/classes/langchain.document_loaders_fs_unstructured.UnstructuredLoader.html) class.

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Documents](https://api.js.langchain.com/classes/_langchain_core.documents.Document.html)
- [Document Loaders](/docs/concepts/document_loaders)

:::

## Installation

```{=mdx}
import Npm2Yarn from "@theme/Npm2Yarn"

<Npm2Yarn>
  @langchain/community @langchain/core
</Npm2Yarn>
```
"""

"""
## Setup

Although Unstructured has an open source offering, you're still required to provide an API key to access the service. To get everything up and running, follow these two steps:

1. Download & start the Docker container:
  
```bash
docker run -p 8000:8000 -d --rm --name unstructured-api downloads.unstructured.io/unstructured-io/unstructured-api:latest --port 8000 --host 0.0.0.0
```

2. Get a free API key & API URL [here](https://unstructured.io/api-key), and set it in your environment (as per the Unstructured website, it may take up to an hour to allocate your API key & URL.):

```bash
export UNSTRUCTURED_API_KEY="..."
# Replace with your `Full URL` from the email
export UNSTRUCTURED_API_URL="https://<ORG_NAME>-<SECRET>.api.unstructuredapp.io/general/v0/general" 
```
"""

"""
Basic usage will ingest a Markdown file to a single document. Here we demonstrate on LangChain's readme:
"""

import { UnstructuredLoader } from "@langchain/community/document_loaders/fs/unstructured";

const markdownPath = "../../../../README.md";

const loader = new UnstructuredLoader(markdownPath, {
  apiKey: process.env.UNSTRUCTURED_API_KEY,
  apiUrl: process.env.UNSTRUCTURED_API_URL,
});

const data = await loader.load()
console.log(data.slice(0, 5));
# Output:
#   [

#     Document {

#       pageContent: '🦜️🔗 LangChain.js',

#       metadata: {

#         languages: [Array],

#         filename: 'README.md',

#         filetype: 'text/markdown',

#         category: 'Title'

#       }

#     },

#     Document {

#       pageContent: '⚡ Building applications with LLMs through composability ⚡',

#       metadata: {

#         languages: [Array],

#         filename: 'README.md',

#         filetype: 'text/markdown',

#         category: 'Title'

#       }

#     },

#     Document {

#       pageContent: 'Looking for the Python version? Check out LangChain.',

#       metadata: {

#         languages: [Array],

#         parent_id: '7ea17bcb17b10f303cbb93b4cb95de93',

#         filename: 'README.md',

#         filetype: 'text/markdown',

#         category: 'NarrativeText'

#       }

#     },

#     Document {

#       pageContent: 'To help you ship LangChain apps to production faster, check out LangSmith.\n' +

#         'LangSmith is a unified developer platform for building, testing, and monitoring LLM applications.\n' +

#         'Fill out this form to get on the waitlist or speak with our sales team.',

#       metadata: {

#         languages: [Array],

#         parent_id: '7ea17bcb17b10f303cbb93b4cb95de93',

#         filename: 'README.md',

#         filetype: 'text/markdown',

#         category: 'NarrativeText'

#       }

#     },

#     Document {

#       pageContent: '⚡️ Quick Install',

#       metadata: {

#         languages: [Array],

#         filename: 'README.md',

#         filetype: 'text/markdown',

#         category: 'Title'

#       }

#     }

#   ]


"""
## Retain Elements

Under the hood, Unstructured creates different "elements" for different chunks of text. By default we combine those together, but you can easily keep that separation by specifying `chunkingStrategy: "by_title"`.
"""

const loaderByTitle = new UnstructuredLoader(markdownPath, {
  chunkingStrategy: "by_title"
});


const loadedDocs = await loaderByTitle.load()

console.log(`Number of documents: ${loadedDocs.length}\n`)

for (const doc of loadedDocs.slice(0, 2)) {
    console.log(doc);
    console.log("\n");
}
# Output:
#   Number of documents: 13

#   

#   Document {

#     pageContent: '🦜️🔗 LangChain.js\n' +

#       '\n' +

#       '⚡ Building applications with LLMs through composability ⚡\n' +

#       '\n' +

#       'Looking for the Python version? Check out LangChain.\n' +

#       '\n' +

#       'To help you ship LangChain apps to production faster, check out LangSmith.\n' +

#       'LangSmith is a unified developer platform for building, testing, and monitoring LLM applications.\n' +

#       'Fill out this form to get on the waitlist or speak with our sales team.',

#     metadata: {

#       filename: 'README.md',

#       filetype: 'text/markdown',

#       languages: [ 'eng' ],

#       orig_elements: 'eJzNUtuO0zAQ/ZVRnquSS3PjBcGyPHURgr5tV2hijxNTJ45ip0u14t8Zp1y6CCF4ACFLlufuc+bcPkRkqKfBv9cyegpREWNZosxS0RRVzmeTCiFlnmRUFZmQ0QqinjxK9Mj5D5HShgbsKRS/vX7+8uZ63S9ZIeBP4xLw9NE/6XxvQsDg0M7YkuPIbURDG919Wp1zQu5+llVGfMta7GdFsVo8MniSErZcfdWhHtYfXOj2dcROe0MRN/oRUUmYlI1o+EpilcWZaJo6azaiqXNJdfYvEKUFJvBi1kbqoQUcR6MFem0HB/fad7Dd3jjw3WTntgNh+9E6bLTR/gTn4t9CmhHFTc1w80oKSUlTpFWaFKWsVR5nFf0dpOwdcfoDvi+p2Vp7CJQoOzF+gjcn39kBjjQ5ZucZXHUkDmBnf7H3Sy5e4zQxkUfahYY/4UQqVcZJpSpspKqSMslVllWJzDdMC6XVf8jJzkJHZoSTncF1evwOPSiHdWJhnKycRRAQKHSephWIR0y961lW6/3w7Q3aAcI8aKVJgqQjGTvSBKNBz+T3ywaaLwpdgSfnlwcOEno7aG+nsCcW6iP58ohX2phlru94xtKLf9iSB/5d2Ok9smC1Y3sCNxIezpq3M5toiAER9r/a6t1n6BJ/zg==',

#       category: 'CompositeElement'

#     }

#   }

#   

#   

#   Document {

#     pageContent: '⚡️ Quick Install\n' +

#       '\n' +

#       'You can use npm, yarn, or pnpm to install LangChain.js\n' +

#       '\n' +

#       'npm install -S langchain or yarn add langchain or pnpm add langchain\n' +

#       '\n' +

#       'typescript\n' +

#       'import { ChatOpenAI } from "langchain/chat_models/openai";\n' +

#       '\n' +

#       '🌐 Supported Environments\n' +

#       '\n' +

#       'LangChain is written in TypeScript and can be used in:\n' +

#       '\n' +

#       'Node.js (ESM and CommonJS) - 18.x, 19.x, 20.x\n' +

#       '\n' +

#       'Cloudflare Workers\n' +

#       '\n' +

#       'Vercel / Next.js (Browser, Serverless and Edge functions)\n' +

#       '\n' +

#       'Supabase Edge Functions\n' +

#       '\n' +

#       'Browser\n' +

#       '\n' +

#       'Deno',

#     metadata: {

#       filename: 'README.md',

#       filetype: 'text/markdown',

#       languages: [ 'eng' ],

#       orig_elements: 'eJzNlm1v2zYQx7/KQa9WwE1Iik/qXnWpB2RoM2wOOgx1URzJY6pVogyJTlME/e6j3KZIhgBzULjIG0Li3VH+/e/BfHNdUUc9pfyuDdUzqGzUjUUda1ZbL7R1UQetnNdMK9swVy2g6iljwIzF/7qKbUcJe5qD/1w+f/FqedSH2Ws25E+bnSHTVT5+n/tuNnSYLrZ4QVOxvKkoXVRvPy+++My+663QyNfbSCzCH9vWf4DTNGXsdsE3J563uaOqxP0XIDSxCdobSZIYd9w7JpQlLU3TaKf4YQDK7gbHB8h4m/jvYQseE2wngrTpF/AJx7SAYYRNeYU8QPtFAHhZvnzyHtt09M90W40zHEfM7SWdz0fep0otuUISLBqMjfNFjMYzI6SWFFWQj1CVGf2G++kK5uP9jD7rMgsEGMLd3Z1ad3YfpJHWsubSchGQeNRItUGPElF7wck2hy/9OWbyY7vJ69T2m2HMcA0l3/n3DaXnp/AZ4jj0sK6+AR6XNb/rh0DddDwUL2zX1c97NUpjVAEOxkh0tbOaN1qU1vG8VtYGe6CSuNvpwda+rJEzWG03MzAFWKbLdhzS/FOnvUhcdChlNC6iKBWuJVrCGMhxIaKMP6i4/1fP2+jfGhnaCT6Obc5UHhOcl4+vdhUAmMJuKjiaB0Mo1mcPKmdBvlFWK6ZMaXfNI2ojIvNORMsUHWiSf5cqZ6WOy2SDn5arVzv+k6Hvh/Tb6gk8BW6PrhbAm3kV7Ojqthgv2ymfZurvrQ4hvRLCSaUEj8YG77TzQTNriYv6B/0hPEiHk24oTdGVePhrGD/QOO0LyxRHKZivAxldS41akzXcxELPm/oxJv01jZ46OIazsrHL/i/j8HGicQErGi9p7GiadtWwDBcEcZt8boc0PdlXE9KlAoSkZh4PtUBZ5oRjTAbiSgd3oLn+XZqUYYgOy3Vgh/zrDfK+xA0rqY6GaQrGo5JM1azcgawzjeOa2CMk/przvXMayvXQEA8meEmCsxiDrkO54/iAVvtHSPiC0nA/3tt/AY+igwk=',

#       category: 'CompositeElement'

#     }

#   }

#   

#   


"""
Note that in this case we recover just one distinct element type:
"""

const categories = new Set(data.map((document) => document.metadata.category));
console.log(categories);
# Output:
#   Set(1) { 'CompositeElement' }




================================================
FILE: docs/core_docs/docs/how_to/document_loader_pdf.mdx
================================================
# How to load PDF files

> [Portable Document Format (PDF)](https://en.wikipedia.org/wiki/PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.

This covers how to load `PDF` documents into the Document format that we use downstream.

By default, one document will be created for each page in the PDF file. You can change this behavior by setting the `splitPages` option to `false`.

## Setup

```bash npm2yarn
npm install pdf-parse
```

## Usage, one document per page

```typescript
import { PDFLoader } from "@langchain/community/document_loaders/fs/pdf";
// Or, in web environments:
// import { WebPDFLoader } from "@langchain/community/document_loaders/web/pdf";
// const blob = new Blob(); // e.g. from a file input
// const loader = new WebPDFLoader(blob);

const loader = new PDFLoader("src/document_loaders/example_data/example.pdf");

const docs = await loader.load();
```

## Usage, one document per file

```typescript
import { PDFLoader } from "@langchain/community/document_loaders/fs/pdf";

const loader = new PDFLoader("src/document_loaders/example_data/example.pdf", {
  splitPages: false,
});

const docs = await loader.load();
```

## Usage, custom `pdfjs` build

By default we use the `pdfjs` build bundled with `pdf-parse`, which is compatible with most environments, including Node.js and modern browsers. If you want to use a more recent version of `pdfjs-dist` or if you want to use a custom build of `pdfjs-dist`, you can do so by providing a custom `pdfjs` function that returns a promise that resolves to the `PDFJS` object.

In the following example we use the "legacy" (see [pdfjs docs](https://github.com/mozilla/pdf.js/wiki/Frequently-Asked-Questions#which-browsersenvironments-are-supported)) build of `pdfjs-dist`, which includes several polyfills not included in the default build.

```bash npm2yarn
npm install pdfjs-dist
```

```typescript
import { PDFLoader } from "@langchain/community/document_loaders/fs/pdf";

const loader = new PDFLoader("src/document_loaders/example_data/example.pdf", {
  // you may need to add `.then(m => m.default)` to the end of the import
  pdfjs: () => import("pdfjs-dist/legacy/build/pdf.js"),
});
```

## Eliminating extra spaces

PDFs come in many varieties, which makes reading them a challenge. The loader parses individual text elements and joins them together with a space by default, but
if you are seeing excessive spaces, this may not be the desired behavior. In that case, you can override the separator with an empty string like this:

```typescript
import { PDFLoader } from "@langchain/community/document_loaders/fs/pdf";

const loader = new PDFLoader("src/document_loaders/example_data/example.pdf", {
  parsedItemSeparator: "",
});

const docs = await loader.load();
```



================================================
FILE: docs/core_docs/docs/how_to/document_loaders_json.mdx
================================================
# How to load JSON data

> [JSON (JavaScript Object Notation)](https://en.wikipedia.org/wiki/JSON) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute–value pairs and arrays (or other serializable values).

> [JSON Lines](https://jsonlines.org/) is a file format where each line is a valid JSON value.

The JSON loader uses [JSON pointer](https://github.com/janl/node-jsonpointer) to target keys in your JSON files you want to target.

### No JSON pointer example

The most simple way of using it is to specify no JSON pointer.
The loader will load all strings it finds in the JSON object.

Example JSON file:

```json
{
  "texts": ["This is a sentence.", "This is another sentence."]
}
```

Example code:

```typescript
import { JSONLoader } from "langchain/document_loaders/fs/json";

const loader = new JSONLoader("src/document_loaders/example_data/example.json");

const docs = await loader.load();
/*
[
  Document {
    "metadata": {
      "blobType": "application/json",
      "line": 1,
      "source": "blob",
    },
    "pageContent": "This is a sentence.",
  },
  Document {
    "metadata": {
      "blobType": "application/json",
      "line": 2,
      "source": "blob",
    },
    "pageContent": "This is another sentence.",
  },
]
*/
```

### Using JSON pointer example

You can do a more advanced scenario by choosing which keys in your JSON object you want to extract string from.

In this example, we want to only extract information from "from" and "surname" entries.

```json
{
  "1": {
    "body": "BD 2023 SUMMER",
    "from": "LinkedIn Job",
    "labels": ["IMPORTANT", "CATEGORY_UPDATES", "INBOX"]
  },
  "2": {
    "body": "Intern, Treasury and other roles are available",
    "from": "LinkedIn Job2",
    "labels": ["IMPORTANT"],
    "other": {
      "name": "plop",
      "surname": "bob"
    }
  }
}
```

Example code:

```typescript
import { JSONLoader } from "langchain/document_loaders/fs/json";

const loader = new JSONLoader(
  "src/document_loaders/example_data/example.json",
  ["/from", "/surname"]
);

const docs = await loader.load();
/*
[
  Document {
    pageContent: 'LinkedIn Job',
    metadata: { source: './src/json/example.json', line: 1 }
  },
  Document {
    pageContent: 'LinkedIn Job2',
    metadata: { source: './src/json/example.json', line: 2 }
  },
  Document {
    pageContent: 'bob',
    metadata: { source: './src/json/example.json', line: 3 }
  }
]
**/
```



================================================
FILE: docs/core_docs/docs/how_to/embed_text.mdx
================================================
---
sidebar_position: 2
---

# How to embed text data

:::info
Head to [Integrations](/docs/integrations/text_embedding) for documentation on built-in integrations with text embedding providers.
:::

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Embeddings](/docs/concepts/embedding_models)

:::

Embeddings create a vector representation of a piece of text. This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space.

The base Embeddings class in LangChain exposes two methods: one for embedding documents and one for embedding a query. The former takes as input multiple texts, while the latter takes a single text. The reason for having these as two separate methods is that some embedding providers have different embedding methods for documents (to be searched over) vs queries (the search query itself).

## Get started

Below is an example of how to use the OpenAI embeddings. Embeddings occasionally have different embedding methods for queries versus documents, so the embedding class exposes a `embedQuery` and `embedDocuments` method.

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core
```

## Get started

```typescript
import { OpenAIEmbeddings } from "@langchain/openai";

const embeddings = new OpenAIEmbeddings();
```

## Embed queries

```typescript
const res = await embeddings.embedQuery("Hello world");
/*
[
   -0.004845875,   0.004899438,  -0.016358767,  -0.024475135, -0.017341806,
    0.012571548,  -0.019156644,   0.009036391,  -0.010227379, -0.026945334,
    0.022861943,   0.010321903,  -0.023479493, -0.0066544134,  0.007977734,
   0.0026371893,   0.025206111,  -0.012048521,   0.012943339,  0.013094575,
   -0.010580265,  -0.003509951,   0.004070787,   0.008639394, -0.020631202,
  ... 1511 more items
]
*/
```

## Embed documents

```typescript
const documentRes = await embeddings.embedDocuments(["Hello world", "Bye bye"]);
/*
[
  [
    -0.004845875,   0.004899438,  -0.016358767,  -0.024475135, -0.017341806,
      0.012571548,  -0.019156644,   0.009036391,  -0.010227379, -0.026945334,
      0.022861943,   0.010321903,  -0.023479493, -0.0066544134,  0.007977734,
    0.0026371893,   0.025206111,  -0.012048521,   0.012943339,  0.013094575,
    -0.010580265,  -0.003509951,   0.004070787,   0.008639394, -0.020631202,
    ... 1511 more items
  ]
  [
      -0.009446913,  -0.013253193,   0.013174579,  0.0057552797,  -0.038993083,
      0.0077763423,    -0.0260478, -0.0114384955, -0.0022683728,  -0.016509168,
      0.041797023,    0.01787183,    0.00552271, -0.0049789557,   0.018146982,
      -0.01542166,   0.033752076,   0.006112323,   0.023872782,  -0.016535373,
      -0.006623321,   0.016116094, -0.0061090477, -0.0044155475,  -0.016627092,
    ... 1511 more items
  ]
]
*/
```

## Next steps

You've now learned how to use embeddings models with queries and text.

Next, check out how to [avoid excessively recomputing embeddings with caching](/docs/how_to/caching_embeddings), or the [full tutorial on retrieval-augmented generation](/docs/tutorials/rag).



================================================
FILE: docs/core_docs/docs/how_to/ensemble_retriever.mdx
================================================
# How to combine results from multiple retrievers

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Documents](https://api.js.langchain.com/classes/_langchain_core.documents.Document.html)
- [Retrievers](/docs/concepts/retrievers)

:::

The [EnsembleRetriever](https://api.js.langchain.com/classes/langchain.retrievers_ensemble.EnsembleRetriever.html) supports ensembling of results from multiple retrievers. It is initialized with a list of [BaseRetriever](https://api.js.langchain.com/classes/langchain_core.retrievers.BaseRetriever.html) objects. EnsembleRetrievers rerank the results of the constituent retrievers based on the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.

By leveraging the strengths of different algorithms, the `EnsembleRetriever` can achieve better performance than any single algorithm.

One useful pattern is to combine a keyword matching retriever with a dense retriever (like embedding similarity), because their strengths are complementary. This can be considered a form of "hybrid search". The sparse retriever is good at finding relevant documents based on keywords, while the dense retriever is good at finding relevant documents based on semantic similarity.

Below we demonstrate ensembling of a [simple custom retriever](/docs/how_to/custom_retriever/) that simply returns documents that directly contain the input query with a retriever derived from a [demo, in-memory, vector store](https://api.js.langchain.com/classes/langchain.vectorstores_memory.MemoryVectorStore.html).

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/retrievers/ensemble_retriever.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## Next steps

You've now learned how to combine results from multiple retrievers.
Next, check out some other retrieval how-to guides, such as how to [improve results using multiple embeddings per document](/docs/how_to/multi_vector)
or how to [create your own custom retriever](/docs/how_to/custom_retriever).



================================================
FILE: docs/core_docs/docs/how_to/example_selectors.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_position: 1
---
"""

"""
# How to use example selectors

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Prompt templates](/docs/concepts/prompt_templates)
- [Few-shot examples](/docs/how_to/few_shot_examples)

:::

If you have a large number of examples, you may need to select which ones to include in the prompt. The Example Selector is the class responsible for doing so.

The base interface is defined as below:

```typescript
class BaseExampleSelector {
  addExample(example: Example): Promise<void | string>;

  selectExamples(input_variables: Example): Promise<Example[]>;
}
```

The only method it needs to define is a `selectExamples` method. This takes in the input variables and then returns a list of examples. It is up to each specific implementation as to how those examples are selected.

LangChain has a few different types of example selectors. For an overview of all these types, see the below table.

In this guide, we will walk through creating a custom example selector.
"""

"""
## Examples

In order to use an example selector, we need to create a list of examples. These should generally be example inputs and outputs. For this demo purpose, let's imagine we are selecting examples of how to translate English to Italian.
"""

const examples = [
    { input: "hi", output: "ciao" },
    { input: "bye", output: "arrivaderci" },
    { input: "soccer", output: "calcio" },
];

"""
## Custom Example Selector

Let's write an example selector that chooses what example to pick based on the length of the word.
"""

import { BaseExampleSelector } from "@langchain/core/example_selectors";
import { Example } from "@langchain/core/prompts";


class CustomExampleSelector extends BaseExampleSelector {
    private examples: Example[];
  
    constructor(examples: Example[]) {
      super();
      this.examples = examples;
    }
  
    async addExample(example: Example): Promise<void | string> {
      this.examples.push(example);
      return;
    }
  
    async selectExamples(inputVariables: Example): Promise<Example[]> {
      // This assumes knowledge that part of the input will be a 'text' key
      const newWord = inputVariables.input;
      const newWordLength = newWord.length;
  
      // Initialize variables to store the best match and its length difference
      let bestMatch: Example | null = null;
      let smallestDiff = Infinity;
  
      // Iterate through each example
      for (const example of this.examples) {
        // Calculate the length difference with the first word of the example
        const currentDiff = Math.abs(example.input.length - newWordLength);
  
        // Update the best match if the current one is closer in length
        if (currentDiff < smallestDiff) {
          smallestDiff = currentDiff;
          bestMatch = example;
        }
      }
  
      return bestMatch ? [bestMatch] : [];
    }
  }

const exampleSelector = new CustomExampleSelector(examples)

await exampleSelector.selectExamples({ input: "okay" })
# Output:
#   [ { input: [32m"bye"[39m, output: [32m"arrivaderci"[39m } ]

await exampleSelector.addExample({ input: "hand", output: "mano" })

await exampleSelector.selectExamples({ input: "okay" })
# Output:
#   [ { input: [32m"hand"[39m, output: [32m"mano"[39m } ]

"""
## Use in a Prompt

We can now use this example selector in a prompt
"""

import { PromptTemplate, FewShotPromptTemplate } from "@langchain/core/prompts"

const examplePrompt = PromptTemplate.fromTemplate("Input: {input} -> Output: {output}")

const prompt = new FewShotPromptTemplate({
    exampleSelector,
    examplePrompt,
    suffix: "Input: {input} -> Output:",
    prefix: "Translate the following words from English to Italian:",
    inputVariables: ["input"],
})

console.log(await prompt.format({ input: "word" }))
# Output:
#   Translate the following words from English to Italian:

#   

#   Input: hand -> Output: mano

#   

#   Input: word -> Output:


"""
## Example Selector Types

| Name       | Description                                                                                 |
|------------|---------------------------------------------------------------------------------------------|
| Similarity | Uses semantic similarity between inputs and examples to decide which examples to choose.    |
| Length     | Selects examples based on how many can fit within a certain length                          |

## Next steps

You've now learned a bit about using example selectors to few shot LLMs.

Next, check out some guides on some other techniques for selecting examples:

- [How to select examples by length](/docs/how_to/example_selectors_length_based)
- [How to select examples by similarity](/docs/how_to/example_selectors_similarity)
"""



================================================
FILE: docs/core_docs/docs/how_to/example_selectors_langsmith.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to select examples from a LangSmith dataset

```{=mdx}

:::tip Prerequisites

- [Chat models](/docs/concepts/chat_models)
- [Few-shot-prompting](/docs/concepts/few_shot_prompting)
- [LangSmith](/docs/concepts/#langsmith)

:::


:::note Compatibility

- `langsmith` >= 0.1.43

:::

```

LangSmith datasets have built-in support for similarity search, making them a great tool for building and querying few-shot examples.

In this guide we'll see how to use an indexed LangSmith dataset as a few-shot example selector.

## Setup

Before getting started make sure you've [created a LangSmith account](https://smith.langchain.com/) and set your credentials:

```typescript
process.env.LANGSMITH_API_KEY="your-api-key"
process.env.LANGSMITH_TRACING="true"
```

We'll need to install the `langsmith` SDK. In this example we'll also make use of `langchain` and `@langchain/anthropic`:

```{=mdx}

import Npm2Yarn from "@theme/Npm2Yarn"

<Npm2Yarn>
  langsmith langchain @langchain/anthropic @langchain/core zod zod-to-json-schema
</Npm2Yarn>

```
"""

"""
Now we'll clone a public dataset and turn on indexing for the dataset. We can also turn on indexing via the [LangSmith UI](https://docs.smith.langchain.com/how_to_guides/datasets/index_datasets_for_dynamic_few_shot_example_selection).

We'll create a clone the [Multiverse math few shot example dataset](https://blog.langchain.dev/few-shot-prompting-to-improve-tool-calling-performance/).

This enables searching over the dataset, and will make sure that anytime we update/add examples they are also indexed.

The first step to creating a clone is to read the JSON file containing the examples and convert them to the format expected by LangSmith for creating examples:
"""

import { Client as LangSmithClient } from 'langsmith';
import { z } from 'zod';
import { zodToJsonSchema } from 'zod-to-json-schema';
import fs from "fs/promises";

// Read the example dataset and convert to the format expected by the LangSmith API
// for creating new examples
const examplesJson = JSON.parse(
  await fs.readFile("../../data/ls_few_shot_example_dataset.json", "utf-8")
);

let inputs: Record<string, any>[] = [];
let outputs: Record<string, any>[] = [];
let metadata: Record<string, any>[] = [];

examplesJson.forEach((ex) => {
  inputs.push(ex.inputs);
  outputs.push(ex.outputs);
  metadata.push(ex.metadata);
});

// Define our input schema as this is required for indexing
const inputsSchema = zodToJsonSchema(z.object({
  input: z.string(),
  system: z.boolean().optional(),
}));

const lsClient = new LangSmithClient();

await lsClient.deleteDataset({ datasetName: "multiverse-math-examples-for-few-shot-example" })

const dataset = await lsClient.createDataset("multiverse-math-examples-for-few-shot-example", {
  inputsSchema,
});

const createdExamples = await lsClient.createExamples({
  inputs,
  outputs,
  metadata,
  datasetId: dataset.id,
})


await lsClient.indexDataset({ datasetId: dataset.id });

"""
Once the dataset is indexed, we can search for similar examples like so:
"""

const examples = await lsClient.similarExamples(
  { input: "whats the negation of the negation of the negation of 3" },
  dataset.id,
  3,
)
console.log(examples.length)
# Output:
#   3


console.log(examples[0].inputs.input)
# Output:
#   evaluate the negation of -100


"""
For this dataset the outputs are an entire chat history:
"""

console.log(examples[1].outputs.output)
# Output:
#   [

#     {

#       id: 'cbe7ed83-86e1-4e46-89de-6646f8b55cef',

#       type: 'system',

#       content: 'You are requested to solve math questions in an alternate mathematical universe. The operations have been altered to yield different results than expected. Do not guess the answer or rely on your  innate knowledge of math. Use the provided tools to answer the question. While associativity and commutativity apply, distributivity does not. Answer the question using the fewest possible tools. Only include the numeric response without any clarifications.',

#       additional_kwargs: {},

#       response_metadata: {}

#     },

#     {

#       id: '04946246-09a8-4465-be95-037efd7dae55',

#       type: 'human',

#       content: 'if one gazoink is 4 badoinks, each of which is 6 foos, each of wich is 3 bars - how many bars in 3 gazoinks?',

#       example: false,

#       additional_kwargs: {},

#       response_metadata: {}

#     },

#     {

#       id: 'run-d6f0954e-b21b-4ea8-ad98-0ee64cfc824e-0',

#       type: 'ai',

#       content: [ [Object] ],

#       example: false,

#       tool_calls: [ [Object] ],

#       usage_metadata: { input_tokens: 916, total_tokens: 984, output_tokens: 68 },

#       additional_kwargs: {},

#       response_metadata: {

#         id: 'msg_01MBWxgouUBzomwTvXhomGVq',

#         model: 'claude-3-sonnet-20240229',

#         usage: [Object],

#         stop_reason: 'tool_use',

#         stop_sequence: null

#       },

#       invalid_tool_calls: []

#     },

#     {

#       id: '3d4c72c4-f009-48ce-b739-1d3f28ee4803',

#       name: 'multiply',

#       type: 'tool',

#       content: '13.2',

#       tool_call_id: 'toolu_016RjRHSEyDZRqKhGrb8uvjJ',

#       additional_kwargs: {},

#       response_metadata: {}

#     },

#     {

#       id: 'run-26dd7e83-f5fb-4c70-8ba1-271300ffeb25-0',

#       type: 'ai',

#       content: [ [Object] ],

#       example: false,

#       tool_calls: [ [Object] ],

#       usage_metadata: { input_tokens: 999, total_tokens: 1070, output_tokens: 71 },

#       additional_kwargs: {},

#       response_metadata: {

#         id: 'msg_01VTFvtCxtR3rN58hCmjt2oH',

#         model: 'claude-3-sonnet-20240229',

#         usage: [Object],

#         stop_reason: 'tool_use',

#         stop_sequence: null

#       },

#       invalid_tool_calls: []

#     },

#     {

#       id: 'ca4e0317-7b3a-4638-933c-1efd98bc4fda',

#       name: 'multiply',

#       type: 'tool',

#       content: '87.12',

#       tool_call_id: 'toolu_01PqvszxiuXrVJ9bwgTWaH3q',

#       additional_kwargs: {},

#       response_metadata: {}

#     },

#     {

#       id: 'run-007794ac-3590-4b9e-b678-008f02e40042-0',

#       type: 'ai',

#       content: [ [Object] ],

#       example: false,

#       tool_calls: [ [Object] ],

#       usage_metadata: { input_tokens: 1084, total_tokens: 1155, output_tokens: 71 },

#       additional_kwargs: {},

#       response_metadata: {

#         id: 'msg_017BEkSqmTsmtJaTxAzfRMEh',

#         model: 'claude-3-sonnet-20240229',

#         usage: [Object],

#         stop_reason: 'tool_use',

#         stop_sequence: null

#       },

#       invalid_tool_calls: []

#     },

#     {

#       id: '7f58c121-6f21-4c7b-ba38-aa820e274ff8',

#       name: 'multiply',

#       type: 'tool',

#       content: '287.496',

#       tool_call_id: 'toolu_01LU3RqRUXZRLRoJ2AZNmPed',

#       additional_kwargs: {},

#       response_metadata: {}

#     },

#     {

#       id: 'run-51e35afb-7ec6-4738-93e2-92f80b5c9377-0',

#       type: 'ai',

#       content: '287.496',

#       example: false,

#       tool_calls: [],

#       usage_metadata: { input_tokens: 1169, total_tokens: 1176, output_tokens: 7 },

#       additional_kwargs: {},

#       response_metadata: {

#         id: 'msg_01Tx9kSNapSg8aUbWZXiS1NL',

#         model: 'claude-3-sonnet-20240229',

#         usage: [Object],

#         stop_reason: 'end_turn',

#         stop_sequence: null

#       },

#       invalid_tool_calls: []

#     }

#   ]


"""
The search returns the examples whose inputs are most similar to the query input. We can use this for few-shot prompting a model. The first step is to create a series of math tools we want to allow the model to call:
"""

import { tool } from '@langchain/core/tools';
import { z } from 'zod';

const add = tool((input) => {
  return (input.a + input.b).toString();
}, {
  name: "add",
  description: "Add two numbers",
  schema: z.object({
    a: z.number().describe("The first number to add"),
    b: z.number().describe("The second number to add"),
  }),
});

const cos = tool((input) => {
  return Math.cos(input.angle).toString();
}, {
  name: "cos",
  description: "Calculate the cosine of an angle (in radians)",
  schema: z.object({
    angle: z.number().describe("The angle in radians"),
  }),
});

const divide = tool((input) => {
  return (input.a / input.b).toString();
}, {
  name: "divide",
  description: "Divide two numbers",
  schema: z.object({
    a: z.number().describe("The dividend"),
    b: z.number().describe("The divisor"),
  }),
});

const log = tool((input) => {
  return Math.log(input.value).toString();
}, {
  name: "log",
  description: "Calculate the natural logarithm of a number",
  schema: z.object({
    value: z.number().describe("The number to calculate the logarithm of"),
  }),
});

const multiply = tool((input) => {
  return (input.a * input.b).toString();
}, {
  name: "multiply",
  description: "Multiply two numbers",
  schema: z.object({
    a: z.number().describe("The first number to multiply"),
    b: z.number().describe("The second number to multiply"),
  }),
});

const negate = tool((input) => {
  return (-input.a).toString();
}, {
  name: "negate",
  description: "Negate a number",
  schema: z.object({
    a: z.number().describe("The number to negate"),
  }),
});

const pi = tool(() => {
  return Math.PI.toString();
}, {
  name: "pi",
  description: "Return the value of pi",
  schema: z.object({}),
});

const power = tool((input) => {
  return Math.pow(input.base, input.exponent).toString();
}, {
  name: "power",
  description: "Raise a number to a power",
  schema: z.object({
    base: z.number().describe("The base number"),
    exponent: z.number().describe("The exponent"),
  }),
});

const sin = tool((input) => {
  return Math.sin(input.angle).toString();
}, {
  name: "sin",
  description: "Calculate the sine of an angle (in radians)",
  schema: z.object({
    angle: z.number().describe("The angle in radians"),
  }),
});

const subtract = tool((input) => {
  return (input.a - input.b).toString();
}, {
  name: "subtract",
  description: "Subtract two numbers",
  schema: z.object({
    a: z.number().describe("The number to subtract from"),
    b: z.number().describe("The number to subtract"),
  }),
});

import { ChatOpenAI } from "@langchain/openai";
import { HumanMessage, SystemMessage, BaseMessage, BaseMessageLike } from "@langchain/core/messages";
import { RunnableLambda } from "@langchain/core/runnables";
import { Client as LangSmithClient, Example } from "langsmith";
import { coerceMessageLikeToMessage } from "@langchain/core/messages";

const client = new LangSmithClient();

async function similarExamples(input: Record<string, any>): Promise<Record<string, any>> {
  const examples = await client.similarExamples(input, dataset.id, 5);
  return { ...input, examples };
}

function constructPrompt(input: { examples: Example[], input: string }): BaseMessage[] {
  const instructions = "You are great at using mathematical tools.";
  let messages: BaseMessage[] = []
  
  for (const ex of input.examples) {
    // Assuming ex.outputs.output is an array of message-like objects
    messages = messages.concat(ex.outputs.output.flatMap((msg: BaseMessageLike) => coerceMessageLikeToMessage(msg)));
  }
  
  const examples = messages.filter(msg => msg._getType() !== 'system');
  examples.forEach((ex) => {
    if (ex._getType() === 'human') {
      ex.name = "example_user";
    } else {
      ex.name = "example_assistant";
    }
  });

  return [new SystemMessage(instructions), ...examples, new HumanMessage(input.input)];
}

const llm = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0,
});
const tools = [add, cos, divide, log, multiply, negate, pi, power, sin, subtract];
const llmWithTools = llm.bindTools(tools);

const exampleSelector = new RunnableLambda(
  { func: similarExamples }
).withConfig({ runName: "similarExamples" });

const chain = exampleSelector.pipe(
  new RunnableLambda({
    func: constructPrompt
  }).withConfig({
    runName: "constructPrompt"
  })
).pipe(llmWithTools);

const aiMsg = await chain.invoke({ input: "whats the negation of the negation of 3", system: false })
console.log(aiMsg.tool_calls)
# Output:
#   [

#     {

#       name: 'negate',

#       args: { a: 3 },

#       type: 'tool_call',

#       id: 'call_SX0dmb4AbFu39KkGQDqPXQwa'

#     }

#   ]


"""
Looking at the LangSmith trace, we can see that relevant examples were pulled in in the `similarExamples` step and passed as messages to ChatOpenAI: https://smith.langchain.com/public/20e09618-0746-4973-9382-5b36c3f27083/r.
"""



================================================
FILE: docs/core_docs/docs/how_to/example_selectors_length_based.mdx
================================================
# How to select examples by length

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Prompt templates](/docs/concepts/prompt_templates)
- [Example selectors](/docs/how_to/example_selectors)

:::

This example selector selects which examples to use based on length.
This is useful when you are worried about constructing a prompt that will go over the length of the context window.
For longer inputs, it will select fewer examples to include, while for shorter inputs it will select more.

import CodeBlock from "@theme/CodeBlock";
import ExampleLength from "@examples/prompts/length_based_example_selector.ts";

<CodeBlock language="typescript">{ExampleLength}</CodeBlock>

## Next steps

You've now learned a bit about using a length based example selector.

Next, check out this guide on how to use a [similarity based example selector](/docs/how_to/example_selectors_similarity).



================================================
FILE: docs/core_docs/docs/how_to/example_selectors_similarity.mdx
================================================
# How to select examples by similarity

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Prompt templates](/docs/concepts/prompt_templates)
- [Example selectors](/docs/how_to/example_selectors)
- [Vector stores](/docs/concepts/vectorstores)

:::

This object selects examples based on similarity to the inputs.
It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs.

import CodeBlock from "@theme/CodeBlock";
import ExampleSimilarity from "@examples/prompts/semantic_similarity_example_selector.ts";

The fields of the examples object will be used as parameters to format the `examplePrompt` passed to the `FewShotPromptTemplate`.
Each example should therefore contain all required fields for the example prompt you are using.

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

<CodeBlock language="typescript">{ExampleSimilarity}</CodeBlock>

By default, each field in the examples object is concatenated together, embedded, and stored in the vectorstore for
later similarity search against user queries.

If you only want to embed specific keys
(e.g., you only want to search for examples that have a similar query to the one the user provides), you can pass an `inputKeys`
array in the final `options` parameter.

## Loading from an existing vectorstore

You can also use a pre-initialized vector store by passing an instance to the `SemanticSimilarityExampleSelector` constructor
directly, as shown below. You can also add more examples via the `addExample` method:

import ExampleSimilarityFromExisting from "@examples/prompts/semantic_similarity_example_selector_from_existing.ts";

<CodeBlock language="typescript">{ExampleSimilarityFromExisting}</CodeBlock>

## Metadata filtering

When adding examples, each field is available as metadata in the produced document. If you would like further control over your
search space, you can add extra fields to your examples and pass a `filter` parameter when initializing your selector:

import ExampleSimilarityMetadataFiltering from "@examples/prompts/semantic_similarity_example_selector_metadata_filtering.ts";

<CodeBlock language="typescript">
  {ExampleSimilarityMetadataFiltering}
</CodeBlock>

## Custom vectorstore retrievers

You can also pass a vectorstore retriever instead of a vectorstore. One way this could be useful is if you want to use retrieval
besides similarity search such as maximal marginal relevance:

import ExampleSimilarityCustomRetriever from "@examples/prompts/semantic_similarity_example_selector_custom_retriever.ts";

<CodeBlock language="typescript">{ExampleSimilarityCustomRetriever}</CodeBlock>

## Next steps

You've now learned a bit about using similarity in an example selector.

Next, check out this guide on how to use a [length-based example selector](/docs/how_to/example_selectors_length_based).



================================================
FILE: docs/core_docs/docs/how_to/extraction_examples.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to use reference examples

:::info Prerequisites

This guide assumes familiarity with the following:

- [Extraction](/docs/tutorials/extraction)

:::

The quality of extraction can often be improved by providing reference examples to the LLM.

:::{.callout-tip}
While this tutorial focuses how to use examples with a tool calling model, this technique is generally applicable, and will work
also with JSON more or prompt based techniques.
:::

We'll use OpenAI's GPT-4 this time for their robust support for `ToolMessages`:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/openai @langchain/core zod uuid
</Npm2Yarn>
```

Let's define a prompt:
"""

import { ChatPromptTemplate, MessagesPlaceholder } from "@langchain/core/prompts";

const SYSTEM_PROMPT_TEMPLATE = `You are an expert extraction algorithm.
Only extract relevant information from the text.
If you do not know the value of an attribute asked to extract, you may omit the attribute's value.`;

// Define a custom prompt to provide instructions and any additional context.
// 1) You can add examples into the prompt template to improve extraction quality
// 2) Introduce additional parameters to take context into account (e.g., include metadata
//    about the document from which the text was extracted.)
const prompt = ChatPromptTemplate.fromMessages([
  ["system", SYSTEM_PROMPT_TEMPLATE],
  // ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓
  new MessagesPlaceholder("examples"),
  // ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑
  ["human", "{text}"]
]);

"""
Test out the template:
"""

import { HumanMessage } from "@langchain/core/messages";

const promptValue = await prompt.invoke({
  text: "this is some text",
  examples: [new HumanMessage("testing 1 2 3")],
});

promptValue.toChatMessages();
# Output:
#   [

#     SystemMessage {

#       lc_serializable: [33mtrue[39m,

#       lc_kwargs: {

#         content: [32m"You are an expert extraction algorithm.\n"[39m +

#           [32m"Only extract relevant information from the text.\n"[39m +

#           [32m"If you do n"[39m... 87 more characters,

#         additional_kwargs: {}

#       },

#       lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#       content: [32m"You are an expert extraction algorithm.\n"[39m +

#         [32m"Only extract relevant information from the text.\n"[39m +

#         [32m"If you do n"[39m... 87 more characters,

#       name: [90mundefined[39m,

#       additional_kwargs: {}

#     },

#     HumanMessage {

#       lc_serializable: [33mtrue[39m,

#       lc_kwargs: { content: [32m"testing 1 2 3"[39m, additional_kwargs: {} },

#       lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#       content: [32m"testing 1 2 3"[39m,

#       name: [90mundefined[39m,

#       additional_kwargs: {}

#     },

#     HumanMessage {

#       lc_serializable: [33mtrue[39m,

#       lc_kwargs: { content: [32m"this is some text"[39m, additional_kwargs: {} },

#       lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#       content: [32m"this is some text"[39m,

#       name: [90mundefined[39m,

#       additional_kwargs: {}

#     }

#   ]

"""
## Define the schema

Let's re-use the people schema from the quickstart.
"""

import { z } from "zod";

const personSchema = z.object({
  name: z.optional(z.string()).describe("The name of the person"),
  hair_color: z.optional(z.string()).describe("The color of the person's hair, if known"),
  height_in_meters: z.optional(z.string()).describe("Height measured in meters")
}).describe("Information about a person.");

const peopleSchema = z.object({
  people: z.array(personSchema)
});

"""
## Define reference examples

Examples can be defined as a list of input-output pairs. 

Each example contains an example `input` text and an example `output` showing what should be extracted from the text.

:::{.callout-important}
The below example is a bit more advanced - the format of the example needs to match the API used (e.g., tool calling or JSON mode etc.).

Here, the formatted examples will match the format expected for the OpenAI tool calling API since that's what we're using.
:::

To provide reference examples to the model, we will mock out a fake chat history containing successful usages of the given tool.
Because the model can choose to call multiple tools at once (or the same tool multiple times), the example's outputs are an array:
"""

import {
  AIMessage,
  type BaseMessage,
  HumanMessage,
  ToolMessage
} from "@langchain/core/messages";
import { v4 as uuid } from "uuid";

type OpenAIToolCall = {
  id: string,
  type: "function",
  function: {
    name: string;
    arguments: string;
  }
};

type Example = {
  input: string;
  toolCallOutputs: Record<string, any>[];
}

/**
 * This function converts an example into a list of messages that can be fed into an LLM.
 *
 * This code serves as an adapter that transforms our example into a list of messages
 * that can be processed by a chat model.
 *
 * The list of messages for each example includes:
 *
 * 1) HumanMessage: This contains the content from which information should be extracted.
 * 2) AIMessage: This contains the information extracted by the model.
 * 3) ToolMessage: This provides confirmation to the model that the tool was requested correctly.
 *
 * The inclusion of ToolMessage is necessary because some chat models are highly optimized for agents,
 * making them less suitable for an extraction use case.
 */
function toolExampleToMessages(example: Example): BaseMessage[] {
  const openAIToolCalls: OpenAIToolCall[] = example.toolCallOutputs.map((output) => {
    return {
      id: uuid(),
      type: "function",
      function: {
        // The name of the function right now corresponds
        // to the passed name.
        name: "extract",
        arguments: JSON.stringify(output),
      },
    };
  });
  const messages: BaseMessage[] = [
    new HumanMessage(example.input),
    new AIMessage({
      content: "",
      additional_kwargs: { tool_calls: openAIToolCalls }
    })
  ];
  const toolMessages = openAIToolCalls.map((toolCall, i) => {
    // Return the mocked successful result for a given tool call.
    return new ToolMessage({
      content: "You have correctly called this tool.",
      tool_call_id: toolCall.id
    });
  });
  return messages.concat(toolMessages);
}


"""
Next let's define our examples and then convert them into message format.
"""

const examples: Example[] = [
  {
    input: "The ocean is vast and blue. It's more than 20,000 feet deep. There are many fish in it.",
    toolCallOutputs: [{}]
  },
  {
    input: "Fiona traveled far from France to Spain.",
    toolCallOutputs: [{
      name: "Fiona",
    }]
  }
];

const exampleMessages = [];
for (const example of examples) {
  exampleMessages.push(...toolExampleToMessages(example));
}
# Output:
#   [33m6[39m

"""
Let's test out the prompt
"""

const promptValueWithExamples = await prompt.invoke({
  text: "this is some text",
  examples: exampleMessages
});

promptValueWithExamples.toChatMessages();
# Output:
#   [

#     SystemMessage {

#       lc_serializable: [33mtrue[39m,

#       lc_kwargs: {

#         content: [32m"You are an expert extraction algorithm.\n"[39m +

#           [32m"Only extract relevant information from the text.\n"[39m +

#           [32m"If you do n"[39m... 87 more characters,

#         additional_kwargs: {}

#       },

#       lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#       content: [32m"You are an expert extraction algorithm.\n"[39m +

#         [32m"Only extract relevant information from the text.\n"[39m +

#         [32m"If you do n"[39m... 87 more characters,

#       name: [90mundefined[39m,

#       additional_kwargs: {}

#     },

#     HumanMessage {

#       lc_serializable: [33mtrue[39m,

#       lc_kwargs: {

#         content: [32m"The ocean is vast and blue. It's more than 20,000 feet deep. There are many fish in it."[39m,

#         additional_kwargs: {}

#       },

#       lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#       content: [32m"The ocean is vast and blue. It's more than 20,000 feet deep. There are many fish in it."[39m,

#       name: [90mundefined[39m,

#       additional_kwargs: {}

#     },

#     AIMessage {

#       lc_serializable: [33mtrue[39m,

#       lc_kwargs: { content: [32m""[39m, additional_kwargs: { tool_calls: [ [36m[Object][39m ] } },

#       lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#       content: [32m""[39m,

#       name: [90mundefined[39m,

#       additional_kwargs: {

#         tool_calls: [

#           {

#             id: [32m"8fa4d00d-801f-470e-8737-51ee9dc82259"[39m,

#             type: [32m"function"[39m,

#             function: [36m[Object][39m

#           }

#         ]

#       }

#     },

#     ToolMessage {

#       lc_serializable: [33mtrue[39m,

#       lc_kwargs: {

#         content: [32m"You have correctly called this tool."[39m,

#         tool_call_id: [32m"8fa4d00d-801f-470e-8737-51ee9dc82259"[39m,

#         additional_kwargs: {}

#       },

#       lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#       content: [32m"You have correctly called this tool."[39m,

#       name: [90mundefined[39m,

#       additional_kwargs: {},

#       tool_call_id: [32m"8fa4d00d-801f-470e-8737-51ee9dc82259"[39m

#     },

#     HumanMessage {

#       lc_serializable: [33mtrue[39m,

#       lc_kwargs: {

#         content: [32m"Fiona traveled far from France to Spain."[39m,

#         additional_kwargs: {}

#       },

#       lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#       content: [32m"Fiona traveled far from France to Spain."[39m,

#       name: [90mundefined[39m,

#       additional_kwargs: {}

#     },

#     AIMessage {

#       lc_serializable: [33mtrue[39m,

#       lc_kwargs: { content: [32m""[39m, additional_kwargs: { tool_calls: [ [36m[Object][39m ] } },

#       lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#       content: [32m""[39m,

#       name: [90mundefined[39m,

#       additional_kwargs: {

#         tool_calls: [

#           {

#             id: [32m"14ad6217-fcbd-47c7-9006-82f612e36c66"[39m,

#             type: [32m"function"[39m,

#             function: [36m[Object][39m

#           }

#         ]

#       }

#     },

#     ToolMessage {

#       lc_serializable: [33mtrue[39m,

#       lc_kwargs: {

#         content: [32m"You have correctly called this tool."[39m,

#         tool_call_id: [32m"14ad6217-fcbd-47c7-9006-82f612e36c66"[39m,

#         additional_kwargs: {}

#       },

#       lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#       content: [32m"You have correctly called this tool."[39m,

#       name: [90mundefined[39m,

#       additional_kwargs: {},

#       tool_call_id: [32m"14ad6217-fcbd-47c7-9006-82f612e36c66"[39m

#     },

#     HumanMessage {

#       lc_serializable: [33mtrue[39m,

#       lc_kwargs: { content: [32m"this is some text"[39m, additional_kwargs: {} },

#       lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#       content: [32m"this is some text"[39m,

#       name: [90mundefined[39m,

#       additional_kwargs: {}

#     }

#   ]

"""
## Create an extractor
Here, we'll create an extractor using **gpt-4**.
"""

import { ChatOpenAI } from "@langchain/openai";

// We will be using tool calling mode, which
// requires a tool calling capable model.
const llm = new ChatOpenAI({
  // Consider benchmarking with the best model you can to get
  // a sense of the best possible quality.
  model: "gpt-4-0125-preview",
  temperature: 0,
});

// For function/tool calling, we can also supply an name for the schema
// to give the LLM additional context about what it's extracting.
const extractionRunnable = prompt.pipe(llm.withStructuredOutput(peopleSchema, { name: "people" }));

"""
## Without examples 😿

Notice that even though we're using `gpt-4`, it's unreliable with a **very simple** test case!

We run it 5 times below to emphasize this:
"""

const text = "The solar system is large, but earth has only 1 moon.";

for (let i = 0; i < 5; i++) {
  const result = await extractionRunnable.invoke({
    text,
    examples: []
  });
  console.log(result);
}
# Output:
#   {

#     people: [ { name: "earth", hair_color: "grey", height_in_meters: "1" } ]

#   }

#   { people: [ { name: "earth", hair_color: "moon" } ] }

#   { people: [ { name: "earth", hair_color: "moon" } ] }

#   { people: [ { name: "earth", hair_color: "1 moon" } ] }

#   { people: [] }


"""
## With examples 😻

Reference examples help fix the failure!
"""

for (let i = 0; i < 5; i++) {
  const result = await extractionRunnable.invoke({
    text,
    // Example messages from above
    examples: exampleMessages
  });
  console.log(result);
}
# Output:
#   { people: [] }

#   { people: [] }

#   { people: [] }

#   { people: [] }

#   { people: [] }


await extractionRunnable.invoke({
  text: "My name is Hair-ison. My hair is black. I am 3 meters tall.",
  examples: exampleMessages,
});
# Output:
#   {

#     people: [ { name: [32m"Hair-ison"[39m, hair_color: [32m"black"[39m, height_in_meters: [32m"3"[39m } ]

#   }

"""
## Next steps

You've now learned how to improve extraction quality using few-shot examples.

Next, check out some of the other guides in this section, such as [some tips on how to perform extraction on long text](/docs/how_to/extraction_long_text).
"""



================================================
FILE: docs/core_docs/docs/how_to/extraction_long_text.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to handle long text

:::info Prerequisites

This guide assumes familiarity with the following:

- [Extraction](/docs/tutorials/extraction)

:::

When working with files, like PDFs, you're likely to encounter text that exceeds your language model's context window. To process this text, consider these strategies:

1. **Change LLM** Choose a different LLM that supports a larger context window.
2. **Brute Force** Chunk the document, and extract content from each chunk.
3. **RAG** Chunk the document, index the chunks, and only extract content from a subset of chunks that look "relevant".

Keep in mind that these strategies have different trade offs and the best strategy likely depends on the application that you're designing!
"""

"""
## Set up

First, let's install some required dependencies:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/openai @langchain/core zod cheerio
</Npm2Yarn>
```

Next, we need some example data! Let's download an article about [cars from Wikipedia](https://en.wikipedia.org/wiki/Car) and load it as a LangChain `Document`.
"""

import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio";
// Only required in a Deno notebook environment to load the peer dep.
import "cheerio";

const loader = new CheerioWebBaseLoader(
  "https://en.wikipedia.org/wiki/Car"
);

const docs = await loader.load();

docs[0].pageContent.length;
# Output:
#   [33m97336[39m

"""
## Define the schema

Here, we'll define schema to extract key developments from the text.
"""

import { z } from "zod";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { ChatOpenAI } from "@langchain/openai";

const keyDevelopmentSchema = z.object({
  year: z.number().describe("The year when there was an important historic development."),
  description: z.string().describe("What happened in this year? What was the development?"),
  evidence: z.string().describe("Repeat verbatim the sentence(s) from which the year and description information were extracted"),
}).describe("Information about a development in the history of cars.");

const extractionDataSchema = z.object({
  key_developments: z.array(keyDevelopmentSchema),
}).describe("Extracted information about key developments in the history of cars");

const SYSTEM_PROMPT_TEMPLATE = [
  "You are an expert at identifying key historic development in text.",
  "Only extract important historic developments. Extract nothing if no important information can be found in the text."
].join("\n");

// Define a custom prompt to provide instructions and any additional context.
// 1) You can add examples into the prompt template to improve extraction quality
// 2) Introduce additional parameters to take context into account (e.g., include metadata
//    about the document from which the text was extracted.)
const prompt = ChatPromptTemplate.fromMessages([
  [
    "system",
    SYSTEM_PROMPT_TEMPLATE,
  ],
  // Keep on reading through this use case to see how to use examples to improve performance
  // MessagesPlaceholder('examples'),
  ["human", "{text}"],
]);

// We will be using tool calling mode, which
// requires a tool calling capable model.
const llm = new ChatOpenAI({
  model: "gpt-4-0125-preview",
  temperature: 0,
});

const extractionChain = prompt.pipe(llm.withStructuredOutput(extractionDataSchema));

"""
## Brute force approach

Split the documents into chunks such that each chunk fits into the context window of the LLMs.
"""

import { TokenTextSplitter } from "langchain/text_splitter";

const textSplitter = new TokenTextSplitter({
  chunkSize: 2000,
  chunkOverlap: 20,
});

// Note that this method takes an array of docs
const splitDocs = await textSplitter.splitDocuments(docs);

"""
Use the `.batch` method present on all runnables to run the extraction in **parallel** across each chunk! 

:::{.callout-tip}
You can often use `.batch()` to parallelize the extractions!

If your model is exposed via an API, this will likely speed up your extraction flow.
:::
"""

// Limit just to the first 3 chunks
// so the code can be re-run quickly
const firstFewTexts = splitDocs.slice(0, 3).map((doc) => doc.pageContent);

const extractionChainParams = firstFewTexts.map((text) => {
  return { text };
});

const results = await extractionChain.batch(extractionChainParams, { maxConcurrency: 5 });

"""
### Merge results

After extracting data from across the chunks, we'll want to merge the extractions together.
"""

const keyDevelopments = results.flatMap((result) => result.key_developments);

keyDevelopments.slice(0, 20);
# Output:
#   [

#     { year: [33m0[39m, description: [32m""[39m, evidence: [32m""[39m },

#     {

#       year: [33m1769[39m,

#       description: [32m"French inventor Nicolas-Joseph Cugnot built the first steam-powered road vehicle."[39m,

#       evidence: [32m"French inventor Nicolas-Joseph Cugnot built the first steam-powered road vehicle in 1769."[39m

#     },

#     {

#       year: [33m1808[39m,

#       description: [32m"French-born Swiss inventor François Isaac de Rivaz designed and constructed the first internal combu"[39m... 25 more characters,

#       evidence: [32m"French-born Swiss inventor François Isaac de Rivaz designed and constructed the first internal combu"[39m... 33 more characters

#     },

#     {

#       year: [33m1886[39m,

#       description: [32m"German inventor Carl Benz patented his Benz Patent-Motorwagen, inventing the modern car—a practical,"[39m... 40 more characters,

#       evidence: [32m"The modern car—a practical, marketable automobile for everyday use—was invented in 1886, when German"[39m... 56 more characters

#     },

#     {

#       year: [33m1908[39m,

#       description: [32m"The 1908 Model T, an American car manufactured by the Ford Motor Company, became one of the first ca"[39m... 28 more characters,

#       evidence: [32m"One of the first cars affordable by the masses was the 1908 Model T, an American car manufactured by"[39m... 24 more characters

#     }

#   ]

"""
## RAG based approach

Another simple idea is to chunk up the text, but instead of extracting information from every chunk, just focus on the the most relevant chunks.

:::{.callout-caution}
It can be difficult to identify which chunks are relevant.

For example, in the `car` article we're using here, most of the article contains key development information. So by using
**RAG**, we'll likely be throwing out a lot of relevant information.

We suggest experimenting with your use case and determining whether this approach works or not.
:::

Here's a simple example that relies on an in-memory demo `MemoryVectorStore` vectorstore.
"""

import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "@langchain/openai";

// Only load the first 10 docs for speed in this demo use-case
const vectorstore = await MemoryVectorStore.fromDocuments(
  splitDocs.slice(0, 10),
  new OpenAIEmbeddings()
);

// Only extract from top document
const retriever = vectorstore.asRetriever({ k: 1 });

"""
In this case the RAG extractor is only looking at the top document.
"""

import { RunnableSequence } from "@langchain/core/runnables";

const ragExtractor = RunnableSequence.from([
  {
    text: retriever.pipe((docs) => docs[0].pageContent)
  },
  extractionChain,
]);

const ragExtractorResults = await ragExtractor.invoke("Key developments associated with cars");

ragExtractorResults.key_developments;
# Output:
#   [

#     {

#       year: [33m2020[39m,

#       description: [32m"The lifetime of a car built in the 2020s is expected to be about 16 years, or about 2 million km (1."[39m... 33 more characters,

#       evidence: [32m"The lifetime of a car built in the 2020s is expected to be about 16 years, or about 2 millionkm (1.2"[39m... 31 more characters

#     },

#     {

#       year: [33m2030[39m,

#       description: [32m"All fossil fuel vehicles will be banned in Amsterdam from 2030."[39m,

#       evidence: [32m"all fossil fuel vehicles will be banned in Amsterdam from 2030."[39m

#     },

#     {

#       year: [33m2020[39m,

#       description: [32m"In 2020, there were 56 million cars manufactured worldwide, down from 67 million the previous year."[39m,

#       evidence: [32m"In 2020, there were 56 million cars manufactured worldwide, down from 67 million the previous year."[39m

#     }

#   ]

"""
## Common issues

Different methods have their own pros and cons related to cost, speed, and accuracy.

Watch out for these issues:

* Chunking content means that the LLM can fail to extract information if the information is spread across multiple chunks.
* Large chunk overlap may cause the same information to be extracted twice, so be prepared to de-duplicate!
* LLMs can make up data. If looking for a single fact across a large text and using a brute force approach, you may end up getting more made up data.

## Next steps

You've now learned how to improve extraction quality using few-shot examples.

Next, check out some of the other guides in this section, such as [some tips on how to improve extraction quality with examples](/docs/how_to/extraction_examples).
"""



================================================
FILE: docs/core_docs/docs/how_to/extraction_parse.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to do extraction without using function calling

:::info Prerequisites

This guide assumes familiarity with the following:

- [Extraction](/docs/tutorials/extraction)

:::

LLMs that are able to follow prompt instructions well can be tasked with outputting information in a given format without using function calling.

This approach relies on designing good prompts and then parsing the output of the LLMs to make them extract information well, though it lacks some of the guarantees provided by function calling or JSON mode.

Here, we'll use Claude which is great at following instructions! See [here for more about Anthropic models](/docs/integrations/chat/anthropic).

First, we'll install the integration package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/anthropic @langchain/core zod zod-to-json-schema
</Npm2Yarn>
```
"""

import { ChatAnthropic } from "@langchain/anthropic";

const model = new ChatAnthropic({
  model: "claude-3-sonnet-20240229",
  temperature: 0,
})

"""
:::{.callout-tip}
All the same considerations for extraction quality apply for parsing approach.

This tutorial is meant to be simple, but generally should really include reference examples to squeeze out performance!
:::
"""

"""
## Using StructuredOutputParser

The following example uses the built-in [`StructuredOutputParser`](/docs/how_to/output_parser_structured/) to parse the output of a chat model. We use the built-in prompt formatting instructions contained in the parser.
"""

import { z } from "zod";
import { StructuredOutputParser } from "langchain/output_parsers";
import { ChatPromptTemplate } from "@langchain/core/prompts";

let personSchema = z.object({
  name: z.optional(z.string()).describe("The name of the person"),
  hair_color: z.optional(z.string()).describe("The color of the person's hair, if known"),
  height_in_meters: z.optional(z.string()).describe("Height measured in meters")
}).describe("Information about a person.");

const parser = StructuredOutputParser.fromZodSchema(personSchema);

const prompt = ChatPromptTemplate.fromMessages([
  ["system", "Answer the user query. Wrap the output in `json` tags\n{format_instructions}"],
  ["human", "{query}"],
]);

const partialedPrompt = await prompt.partial({
  format_instructions: parser.getFormatInstructions(),
});

"""
Let's take a look at what information is sent to the model
"""

const query = "Anna is 23 years old and she is 6 feet tall";

const promptValue = await partialedPrompt.invoke({ query });

console.log(promptValue.toChatMessages());
# Output:
#   [

#     SystemMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: "Answer the user query. Wrap the output in `json` tags\n" +

#           "You must format your output as a JSON value th"... 1444 more characters,

#         additional_kwargs: {}

#       },

#       lc_namespace: [ "langchain_core", "messages" ],

#       content: "Answer the user query. Wrap the output in `json` tags\n" +

#         "You must format your output as a JSON value th"... 1444 more characters,

#       name: undefined,

#       additional_kwargs: {}

#     },

#     HumanMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: "Anna is 23 years old and she is 6 feet tall",

#         additional_kwargs: {}

#       },

#       lc_namespace: [ "langchain_core", "messages" ],

#       content: "Anna is 23 years old and she is 6 feet tall",

#       name: undefined,

#       additional_kwargs: {}

#     }

#   ]


const chain = partialedPrompt.pipe(model).pipe(parser);

await chain.invoke({ query });
# Output:
#   { name: [32m"Anna"[39m, hair_color: [32m""[39m, height_in_meters: [32m"1.83"[39m }

"""
## Custom Parsing

You can also create a custom prompt and parser with `LangChain` and `LCEL`.

You can use a raw function to parse the output from the model.

In the below example, we'll pass the schema into the prompt as JSON schema. For convenience, we'll declare our schema with Zod, then use the [`zod-to-json-schema`](https://github.com/StefanTerdell/zod-to-json-schema) utility to convert it to JSON schema.
"""

import { z } from "zod";
import { zodToJsonSchema } from "zod-to-json-schema";

personSchema = z.object({
  name: z.optional(z.string()).describe("The name of the person"),
  hair_color: z.optional(z.string()).describe("The color of the person's hair, if known"),
  height_in_meters: z.optional(z.string()).describe("Height measured in meters")
}).describe("Information about a person.");

const peopleSchema = z.object({
  people: z.array(personSchema),
});

const SYSTEM_PROMPT_TEMPLATE = [
  "Answer the user's query. You must return your answer as JSON that matches the given schema:",
  "```json\n{schema}\n```.",
  "Make sure to wrap the answer in ```json and ``` tags. Conform to the given schema exactly.",
].join("\n");

const customParsingPrompt = ChatPromptTemplate.fromMessages([
  ["system", SYSTEM_PROMPT_TEMPLATE],
  ["human", "{query}"],
]);

const extractJsonFromOutput = (message) => {
  const text = message.content;

  // Define the regular expression pattern to match JSON blocks
  const pattern = /```json\s*((.|\n)*?)\s*```/gs;

  // Find all non-overlapping matches of the pattern in the string
  const matches = pattern.exec(text);

  if (matches && matches[1]) {
    try {
      return JSON.parse(matches[1].trim());
    } catch (error) {
      throw new Error(`Failed to parse: ${matches[1]}`);
    }
  } else {
    throw new Error(`No JSON found in: ${message}`);
  }
}

const customParsingQuery = "Anna is 23 years old and she is 6 feet tall";

const customParsingPromptValue = await customParsingPrompt.invoke({
  schema: zodToJsonSchema(peopleSchema),
  customParsingQuery
});

customParsingPromptValue.toString();
# Output:
#   [32m"System: Answer the user's query. You must return your answer as JSON that matches the given schema:\n"[39m... 170 more characters

const customParsingChain = prompt.pipe(model).pipe(extractJsonFromOutput);

await customParsingChain.invoke({
  schema: zodToJsonSchema(peopleSchema),
  customParsingQuery,
});
# Output:
#   { name: [32m"Anna"[39m, age: [33m23[39m, height: { feet: [33m6[39m, inches: [33m0[39m } }

"""
## Next steps

You've now learned how to perform extraction without using tool calling.

Next, check out some of the other guides in this section, such as [some tips on how to improve extraction quality with examples](/docs/how_to/extraction_examples).
"""



================================================
FILE: docs/core_docs/docs/how_to/fallbacks.mdx
================================================
import CodeBlock from "@theme/CodeBlock";

# Fallbacks

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [LangChain Expression Language (LCEL)](/docs/concepts/lcel)
- [Chaining runnables](/docs/how_to/sequence/)

:::

When working with language models, you may encounter issues from the underlying APIs, e.g. rate limits or downtime.
As you move your LLM applications into production it becomes more and more important to have contingencies for errors.
That's why we've introduced the concept of fallbacks.

Crucially, fallbacks can be applied not only on the LLM level but on the whole runnable level.
This is important because often times different models require different prompts. So if your call to OpenAI fails, you don't just want to send the same prompt to Anthropic - you probably want want to use e.g. a different prompt template.

## Handling LLM API errors

This is maybe the most common use case for fallbacks. A request to an LLM API can fail for a variety of reasons - the API could be down,
you could have hit a rate limit, or any number of things.

**IMPORTANT:** By default, many of LangChain's LLM wrappers catch errors and retry.
You will most likely want to turn those off when working with fallbacks. Otherwise the first wrapper will keep on retrying rather than failing.

import ModelExample from "@examples/guides/fallbacks/model.ts";

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/anthropic @langchain/openai @langchain/core
```

<CodeBlock language="typescript">{ModelExample}</CodeBlock>

## Fallbacks for RunnableSequences

We can also create fallbacks for sequences, that are sequences themselves.
Here we do that with two different models: ChatOpenAI and then normal OpenAI (which does not use a chat model).
Because OpenAI is NOT a chat model, you likely want a different prompt.

import ChainExample from "@examples/guides/fallbacks/chain.ts";

<CodeBlock language="typescript">{ChainExample}</CodeBlock>

## Handling long inputs

One of the big limiting factors of LLMs in their context window.
Sometimes you can count and track the length of prompts before sending them to an LLM,
but in situations where that is hard/complicated you can fallback to a model with longer context length.

import LongInputExample from "@examples/guides/fallbacks/long_inputs.ts";

<CodeBlock language="typescript">{LongInputExample}</CodeBlock>

## Fallback to a better model

Often times we ask models to output format in a specific format (like JSON). Models like GPT-3.5 can do this okay, but sometimes struggle.
This naturally points to fallbacks - we can try with a faster and cheaper model, but then if parsing fails we can use GPT-4.

import BetterModelExample from "@examples/guides/fallbacks/better_model.ts";

<CodeBlock language="typescript">{BetterModelExample}</CodeBlock>



================================================
FILE: docs/core_docs/docs/how_to/few_shot.mdx
================================================
# Few Shot Prompt Templates

Few shot prompting is a prompting technique which provides the Large Language Model (LLM) with a list of examples, and then asks the LLM to generate some text following the lead of the examples provided.

An example of this is the following:

Say you want your LLM to respond in a specific format. You can few shot prompt the LLM with a list of question answer pairs so it knows what format to respond in.

```txt
Respond to the users question in the with the following format:

Question: What is your name?
Answer: My name is John.

Question: What is your age?
Answer: I am 25 years old.

Question: What is your favorite color?
Answer:
```

Here we left the last `Answer:` undefined so the LLM can fill it in. The LLM will then generate the following:

```txt
Answer: I don't have a favorite color; I don't have preferences.
```

### Use Case

In the following example we're few shotting the LLM to rephrase questions into more general queries.

We provide two sets of examples with specific questions, and rephrased general questions. The `FewShotChatMessagePromptTemplate` will use our examples and when `.format` is called, we'll see those examples formatted into a string we can pass to the LLM.

```typescript
import {
  ChatPromptTemplate,
  FewShotChatMessagePromptTemplate,
} from "langchain/prompts";
```

```typescript
const examples = [
  {
    input: "Could the members of The Police perform lawful arrests?",
    output: "what can the members of The Police do?",
  },
  {
    input: "Jan Sindel's was born in what country?",
    output: "what is Jan Sindel's personal history?",
  },
];
const examplePrompt = ChatPromptTemplate.fromTemplate(`Human: {input}
AI: {output}`);
const fewShotPrompt = new FewShotChatMessagePromptTemplate({
  examplePrompt,
  examples,
  inputVariables: [], // no input variables
});
```

```typescript
const formattedPrompt = await fewShotPrompt.format({});
console.log(formattedPrompt);
```

```typescript
[
  HumanMessage {
    lc_namespace: [ 'langchain', 'schema' ],
    content: 'Human: Could the members of The Police perform lawful arrests?\n' +
      'AI: what can the members of The Police do?',
    additional_kwargs: {}
  },
  HumanMessage {
    lc_namespace: [ 'langchain', 'schema' ],
    content: "Human: Jan Sindel's was born in what country?\n" +
      "AI: what is Jan Sindel's personal history?",
    additional_kwargs: {}
  }
]
```

Then, if we use this with another question, the LLM will rephrase the question how we want.

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core
```

```typescript
import { ChatOpenAI } from "@langchain/openai";
```

```typescript
const model = new ChatOpenAI({});
const examples = [
  {
    input: "Could the members of The Police perform lawful arrests?",
    output: "what can the members of The Police do?",
  },
  {
    input: "Jan Sindel's was born in what country?",
    output: "what is Jan Sindel's personal history?",
  },
];
const examplePrompt = ChatPromptTemplate.fromTemplate(`Human: {input}
AI: {output}`);
const fewShotPrompt = new FewShotChatMessagePromptTemplate({
  prefix:
    "Rephrase the users query to be more general, using the following examples",
  suffix: "Human: {input}",
  examplePrompt,
  examples,
  inputVariables: ["input"],
});
const formattedPrompt = await fewShotPrompt.format({
  input: "What's France's main city?",
});

const response = await model.invoke(formattedPrompt);
console.log(response);
```

```typescript
AIMessage {
  lc_namespace: [ 'langchain', 'schema' ],
  content: 'What is the capital of France?',
  additional_kwargs: { function_call: undefined }
}
```

### Few Shotting With Functions

You can also partial with a function. The use case for this is when you have a variable you know that you always want to fetch in a common way. A prime example of this is with date or time. Imagine you have a prompt which you always want to have the current date. You can't hard code it in the prompt, and passing it along with the other input variables can be tedious. In this case, it's very handy to be able to partial the prompt with a function that always returns the current date.

```typescript
const getCurrentDate = () => {
  return new Date().toISOString();
};

const prompt = new FewShotChatMessagePromptTemplate({
  template: "Tell me a {adjective} joke about the day {date}",
  inputVariables: ["adjective", "date"],
});

const partialPrompt = await prompt.partial({
  date: getCurrentDate,
});

const formattedPrompt = await partialPrompt.format({
  adjective: "funny",
});

console.log(formattedPrompt);

// Tell me a funny joke about the day 2023-07-13T00:54:59.287Z
```

### Few Shot vs Chat Few Shot

The chat and non chat few shot prompt templates act in a similar way. The below example will demonstrate using chat and non chat, and the differences with their outputs.

```typescript
import {
  FewShotPromptTemplate,
  FewShotChatMessagePromptTemplate,
} from "langchain/prompts";
```

```typescript
const examples = [
  {
    input: "Could the members of The Police perform lawful arrests?",
    output: "what can the members of The Police do?",
  },
  {
    input: "Jan Sindel's was born in what country?",
    output: "what is Jan Sindel's personal history?",
  },
];
const prompt = `Human: {input}
AI: {output}`;
const examplePromptTemplate = PromptTemplate.fromTemplate(prompt);
const exampleChatPromptTemplate = ChatPromptTemplate.fromTemplate(prompt);
const chatFewShotPrompt = new FewShotChatMessagePromptTemplate({
  examplePrompt: exampleChatPromptTemplate,
  examples,
  inputVariables: [], // no input variables
});
const fewShotPrompt = new FewShotPromptTemplate({
  examplePrompt: examplePromptTemplate,
  examples,
  inputVariables: [], // no input variables
});
```

```typescript
console.log("Chat Few Shot: ", await chatFewShotPrompt.formatMessages({}));
/**
Chat Few Shot:  [
  HumanMessage {
    lc_namespace: [ 'langchain', 'schema' ],
    content: 'Human: Could the members of The Police perform lawful arrests?\n' +
      'AI: what can the members of The Police do?',
    additional_kwargs: {}
  },
  HumanMessage {
    lc_namespace: [ 'langchain', 'schema' ],
    content: "Human: Jan Sindel's was born in what country?\n" +
      "AI: what is Jan Sindel's personal history?",
    additional_kwargs: {}
  }
]
 */
```

```typescript
console.log("Few Shot: ", await fewShotPrompt.formatPromptValue({}));
/**
Few Shot:

Human: Could the members of The Police perform lawful arrests?
AI: what can the members of The Police do?

Human: Jan Sindel's was born in what country?
AI: what is Jan Sindel's personal history?
 */
```

Here we can see the main distinctions between `FewShotChatMessagePromptTemplate` and `FewShotPromptTemplate`: input and output values.

`FewShotChatMessagePromptTemplate` works by taking in a list of `ChatPromptTemplate` for examples, and its output is a list of instances of `BaseMessage`.

On the other hand, `FewShotPromptTemplate` works by taking in a `PromptTemplate` for examples, and its output is a string.

## With Non Chat Models

LangChain also provides a class for few shot prompt formatting for non chat models: `FewShotPromptTemplate`. The API is largely the same, but the output is formatted differently (chat messages vs strings).

### Partials With Functions

```typescript
import {
  ChatPromptTemplate,
  FewShotChatMessagePromptTemplate,
} from "langchain/prompts";
```

```typescript
const examplePrompt = PromptTemplate.fromTemplate("{foo}{bar}");
const prompt = new FewShotPromptTemplate({
  prefix: "{foo}{bar}",
  examplePrompt,
  inputVariables: ["foo", "bar"],
});
const partialPrompt = await prompt.partial({
  foo: () => Promise.resolve("boo"),
});
const formatted = await partialPrompt.format({ bar: "baz" });
console.log(formatted);
```

```txt
boobaz\n
```

### With Functions and Example Selector

```typescript
import {
  ChatPromptTemplate,
  FewShotChatMessagePromptTemplate,
} from "langchain/prompts";
```

```typescript
const examplePrompt = PromptTemplate.fromTemplate("An example about {x}");
const exampleSelector = await LengthBasedExampleSelector.fromExamples(
  [{ x: "foo" }, { x: "bar" }],
  { examplePrompt, maxLength: 200 }
);
const prompt = new FewShotPromptTemplate({
  prefix: "{foo}{bar}",
  exampleSelector,
  examplePrompt,
  inputVariables: ["foo", "bar"],
});
const partialPrompt = await prompt.partial({
  foo: () => Promise.resolve("boo"),
});
const formatted = await partialPrompt.format({ bar: "baz" });
console.log(formatted);
```

```txt
boobaz
An example about foo
An example about bar
```



================================================
FILE: docs/core_docs/docs/how_to/few_shot_examples.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_position: 3
---
"""

"""
# How to use few shot examples

In this guide, we'll learn how to create a simple prompt template that provides the model with example inputs and outputs when generating. Providing the LLM with a few such examples is called few-shotting, and is a simple yet powerful way to guide generation and in some cases drastically improve model performance.

A few-shot prompt template can be constructed from either a set of examples, or from an [Example Selector](https://api.js.langchain.com/classes/langchain_core.example_selectors.BaseExampleSelector.html) class responsible for choosing a subset of examples from the defined set.

This guide will cover few-shotting with string prompt templates. For a guide on few-shotting with chat messages for chat models, see [here](/docs/how_to/few_shot_examples_chat/).

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Prompt templates](/docs/concepts/prompt_templates)
- [Example selectors](/docs/concepts/example_selectors)
- [LLMs](/docs/concepts/text_llms)
- [Vectorstores](/docs/concepts/#vectorstores)

:::

## Create a formatter for the few-shot examples

Configure a formatter that will format the few-shot examples into a string. This formatter should be a `PromptTemplate` object.
"""

import { PromptTemplate } from "@langchain/core/prompts";

const examplePrompt = PromptTemplate.fromTemplate("Question: {question}\n{answer}")

"""
## Creating the example set

Next, we'll create a list of few-shot examples. Each example should be a dictionary representing an example input to the formatter prompt we defined above.
"""

const examples = [
    {
      question: "Who lived longer, Muhammad Ali or Alan Turing?",
      answer: `
  Are follow up questions needed here: Yes.
  Follow up: How old was Muhammad Ali when he died?
  Intermediate answer: Muhammad Ali was 74 years old when he died.
  Follow up: How old was Alan Turing when he died?
  Intermediate answer: Alan Turing was 41 years old when he died.
  So the final answer is: Muhammad Ali
  `
    },
    {
      question: "When was the founder of craigslist born?",
      answer: `
  Are follow up questions needed here: Yes.
  Follow up: Who was the founder of craigslist?
  Intermediate answer: Craigslist was founded by Craig Newmark.
  Follow up: When was Craig Newmark born?
  Intermediate answer: Craig Newmark was born on December 6, 1952.
  So the final answer is: December 6, 1952
  `
    },
    {
      question: "Who was the maternal grandfather of George Washington?",
      answer: `
  Are follow up questions needed here: Yes.
  Follow up: Who was the mother of George Washington?
  Intermediate answer: The mother of George Washington was Mary Ball Washington.
  Follow up: Who was the father of Mary Ball Washington?
  Intermediate answer: The father of Mary Ball Washington was Joseph Ball.
  So the final answer is: Joseph Ball
  `
    },
    {
      question: "Are both the directors of Jaws and Casino Royale from the same country?",
      answer: `
  Are follow up questions needed here: Yes.
  Follow up: Who is the director of Jaws?
  Intermediate Answer: The director of Jaws is Steven Spielberg.
  Follow up: Where is Steven Spielberg from?
  Intermediate Answer: The United States.
  Follow up: Who is the director of Casino Royale?
  Intermediate Answer: The director of Casino Royale is Martin Campbell.
  Follow up: Where is Martin Campbell from?
  Intermediate Answer: New Zealand.
  So the final answer is: No
  `
    }
  ];

"""
### Pass the examples and formatter to `FewShotPromptTemplate`

Finally, create a [`FewShotPromptTemplate`](https://api.js.langchain.com/classes/langchain_core.prompts.FewShotPromptTemplate.html) object. This object takes in the few-shot examples and the formatter for the few-shot examples. When this `FewShotPromptTemplate` is formatted, it formats the passed examples using the `examplePrompt`, then and adds them to the final prompt before `suffix`:
"""

import { FewShotPromptTemplate } from "@langchain/core/prompts";

const prompt = new FewShotPromptTemplate({
    examples,
    examplePrompt,
    suffix: "Question: {input}",
    inputVariables: ["input"],
})

const formatted = await prompt.format({ input: "Who was the father of Mary Ball Washington?" })
console.log(formatted.toString())
# Output:
#   

#   

#   Question: Who lived longer, Muhammad Ali or Alan Turing?

#   

#     Are follow up questions needed here: Yes.

#     Follow up: How old was Muhammad Ali when he died?

#     Intermediate answer: Muhammad Ali was 74 years old when he died.

#     Follow up: How old was Alan Turing when he died?

#     Intermediate answer: Alan Turing was 41 years old when he died.

#     So the final answer is: Muhammad Ali

#     

#   

#   Question: When was the founder of craigslist born?

#   

#     Are follow up questions needed here: Yes.

#     Follow up: Who was the founder of craigslist?

#     Intermediate answer: Craigslist was founded by Craig Newmark.

#     Follow up: When was Craig Newmark born?

#     Intermediate answer: Craig Newmark was born on December 6, 1952.

#     So the final answer is: December 6, 1952

#     

#   

#   Question: Who was the maternal grandfather of George Washington?

#   

#     Are follow up questions needed here: Yes.

#     Follow up: Who was the mother of George Washington?

#     Intermediate answer: The mother of George Washington was Mary Ball Washington.

#     Follow up: Who was the father of Mary Ball Washington?

#     Intermediate answer: The father of Mary Ball Washington was Joseph Ball.

#     So the final answer is: Joseph Ball

#     

#   

#   Question: Are both the directors of Jaws and Casino Royale from the same country?

#   

#     Are follow up questions needed here: Yes.

#     Follow up: Who is the director of Jaws?

#     Intermediate Answer: The director of Jaws is Steven Spielberg.

#     Follow up: Where is Steven Spielberg from?

#     Intermediate Answer: The United States.

#     Follow up: Who is the director of Casino Royale?

#     Intermediate Answer: The director of Casino Royale is Martin Campbell.

#     Follow up: Where is Martin Campbell from?

#     Intermediate Answer: New Zealand.

#     So the final answer is: No

#     

#   

#   Question: Who was the father of Mary Ball Washington?


"""
By providing the model with examples like this, we can guide the model to a better response.
"""

"""
## Using an example selector

We will reuse the example set and the formatter from the previous section. However, instead of feeding the examples directly into the `FewShotPromptTemplate` object, we will feed them into an implementation of `ExampleSelector` called [`SemanticSimilarityExampleSelector`](https://api.js.langchain.com/classes/langchain_core.example_selectors.SemanticSimilarityExampleSelector.html) instance. This class selects few-shot examples from the initial set based on their similarity to the input. It uses an embedding model to compute the similarity between the input and the few-shot examples, as well as a vector store to perform the nearest neighbor search.

To show what it looks like, let's initialize an instance and call it in isolation:
"""

"""
Set your OpenAI API key for the embeddings model
```bash
export OPENAI_API_KEY="..."
```
"""

import { SemanticSimilarityExampleSelector } from "@langchain/core/example_selectors";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "@langchain/openai";

const exampleSelector = await SemanticSimilarityExampleSelector.fromExamples(
    // This is the list of examples available to select from.
    examples,
    // This is the embedding class used to produce embeddings which are used to measure semantic similarity.
    new OpenAIEmbeddings(),
    // This is the VectorStore class that is used to store the embeddings and do a similarity search over.
    MemoryVectorStore,
    {
        // This is the number of examples to produce.
        k: 1,
    }
)

// Select the most similar example to the input.
const question = "Who was the father of Mary Ball Washington?"
const selectedExamples = await exampleSelector.selectExamples({ question })
console.log(`Examples most similar to the input: ${question}`)
for (const example of selectedExamples) {
    console.log("\n");
    console.log(Object.entries(example).map(([k, v]) => `${k}: ${v}`).join("\n"))
}
# Output:
#   Examples most similar to the input: Who was the father of Mary Ball Washington?

#   

#   

#   question: Who was the maternal grandfather of George Washington?

#   answer: 

#     Are follow up questions needed here: Yes.

#     Follow up: Who was the mother of George Washington?

#     Intermediate answer: The mother of George Washington was Mary Ball Washington.

#     Follow up: Who was the father of Mary Ball Washington?

#     Intermediate answer: The father of Mary Ball Washington was Joseph Ball.

#     So the final answer is: Joseph Ball

#     


"""
Now, let's create a `FewShotPromptTemplate` object. This object takes in the example selector and the formatter prompt for the few-shot examples.
"""

const prompt = new FewShotPromptTemplate({
    exampleSelector,
    examplePrompt,
    suffix: "Question: {input}",
    inputVariables: ["input"],
})

const formatted = await prompt.invoke({ input: "Who was the father of Mary Ball Washington?" });
console.log(formatted.toString())
# Output:
#   

#   

#   Question: Who was the maternal grandfather of George Washington?

#   

#     Are follow up questions needed here: Yes.

#     Follow up: Who was the mother of George Washington?

#     Intermediate answer: The mother of George Washington was Mary Ball Washington.

#     Follow up: Who was the father of Mary Ball Washington?

#     Intermediate answer: The father of Mary Ball Washington was Joseph Ball.

#     So the final answer is: Joseph Ball

#     

#   

#   Question: Who was the father of Mary Ball Washington?


"""
## Next steps

You've now learned how to add few-shot examples to your prompts.

Next, check out the other how-to guides on prompt templates in this section, the related how-to guide on [few shotting with chat models](/docs/how_to/few_shot_examples_chat), or the other [example selector how-to guides](/docs/how_to/example_selectors/).
"""



================================================
FILE: docs/core_docs/docs/how_to/few_shot_examples_chat.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_position: 2
---
"""

"""
# How to use few shot examples in chat models

This guide covers how to prompt a chat model with example inputs and outputs. Providing the model with a few such examples is called few-shotting, and is a simple yet powerful way to guide generation and in some cases drastically improve model performance.

There does not appear to be solid consensus on how best to do few-shot prompting, and the optimal prompt compilation will likely vary by model. Because of this, we provide few-shot prompt templates like the [FewShotChatMessagePromptTemplate](https://api.js.langchain.com/classes/langchain_core.prompts.FewShotChatMessagePromptTemplate.html) as a flexible starting point, and you can modify or replace them as you see fit.

The goal of few-shot prompt templates are to dynamically select examples based on an input, and then format the examples in a final prompt to provide for the model.

**Note:** The following code examples are for chat models only, since `FewShotChatMessagePromptTemplates` are designed to output formatted [chat messages](/docs/concepts/messages) rather than pure strings. For similar few-shot prompt examples for pure string templates compatible with completion models (LLMs), see the [few-shot prompt templates](/docs/how_to/few_shot_examples/) guide.

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Prompt templates](/docs/concepts/prompt_templates)
- [Example selectors](/docs/concepts/example_selectors)
- [Chat models](/docs/concepts/chat_models)
- [Vectorstores](/docs/concepts/#vectorstores)

:::
"""

"""
## Fixed Examples

The most basic (and common) few-shot prompting technique is to use fixed prompt examples. This way you can select a chain, evaluate it, and avoid worrying about additional moving parts in production.

The basic components of the template are:
- `examples`: An array of object examples to include in the final prompt.
- `examplePrompt`: converts each example into 1 or more messages through its [`formatMessages`](https://api.js.langchain.com/classes/langchain_core.prompts.FewShotChatMessagePromptTemplate.html#formatMessages) method. A common example would be to convert each example into one human message and one AI message response, or a human message followed by a function call message.

Below is a simple demonstration. First, define the examples you'd like to include:
"""

import {
    ChatPromptTemplate,
    FewShotChatMessagePromptTemplate,
} from "@langchain/core/prompts"

const examples = [
    { input: "2+2", output: "4" },
    { input: "2+3", output: "5" },
]

"""
Next, assemble them into the few-shot prompt template.
"""

// This is a prompt template used to format each individual example.
const examplePrompt = ChatPromptTemplate.fromMessages(
    [
        ["human", "{input}"],
        ["ai", "{output}"],
    ]
)
const fewShotPrompt = new FewShotChatMessagePromptTemplate({
    examplePrompt,
    examples,
    inputVariables: [], // no input variables
})

const result = await fewShotPrompt.invoke({});
console.log(result.toChatMessages())
# Output:
#   [

#     HumanMessage {

#       lc_serializable: true,

#       lc_kwargs: { content: "2+2", additional_kwargs: {}, response_metadata: {} },

#       lc_namespace: [ "langchain_core", "messages" ],

#       content: "2+2",

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {}

#     },

#     AIMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: "4",

#         tool_calls: [],

#         invalid_tool_calls: [],

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ "langchain_core", "messages" ],

#       content: "4",

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {},

#       tool_calls: [],

#       invalid_tool_calls: []

#     },

#     HumanMessage {

#       lc_serializable: true,

#       lc_kwargs: { content: "2+3", additional_kwargs: {}, response_metadata: {} },

#       lc_namespace: [ "langchain_core", "messages" ],

#       content: "2+3",

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {}

#     },

#     AIMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: "5",

#         tool_calls: [],

#         invalid_tool_calls: [],

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ "langchain_core", "messages" ],

#       content: "5",

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {},

#       tool_calls: [],

#       invalid_tool_calls: []

#     }

#   ]


"""
Finally, we assemble the final prompt as shown below, passing `fewShotPrompt` directly into the `fromMessages` factory method, and use it with a model:
"""

const finalPrompt = ChatPromptTemplate.fromMessages(
    [
        ["system", "You are a wondrous wizard of math."],
        fewShotPrompt,
        ["human", "{input}"],
    ]
)

"""
```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs />
```
"""

const chain = finalPrompt.pipe(model);

await chain.invoke({ input: "What's the square of a triangle?" })
# Output:
#   AIMessage {

#     lc_serializable: [33mtrue[39m,

#     lc_kwargs: {

#       content: [32m"A triangle does not have a square. The square of a number is the result of multiplying the number by"[39m... 8 more characters,

#       tool_calls: [],

#       invalid_tool_calls: [],

#       additional_kwargs: { function_call: [90mundefined[39m, tool_calls: [90mundefined[39m },

#       response_metadata: {}

#     },

#     lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#     content: [32m"A triangle does not have a square. The square of a number is the result of multiplying the number by"[39m... 8 more characters,

#     name: [90mundefined[39m,

#     additional_kwargs: { function_call: [90mundefined[39m, tool_calls: [90mundefined[39m },

#     response_metadata: {

#       tokenUsage: { completionTokens: [33m23[39m, promptTokens: [33m52[39m, totalTokens: [33m75[39m },

#       finish_reason: [32m"stop"[39m

#     },

#     tool_calls: [],

#     invalid_tool_calls: []

#   }

"""
## Dynamic few-shot prompting

Sometimes you may want to select only a few examples from your overall set to show based on the input. For this, you can replace the `examples` passed into `FewShotChatMessagePromptTemplate` with an `exampleSelector`. The other components remain the same as above! Our dynamic few-shot prompt template would look like:

- `exampleSelector`: responsible for selecting few-shot examples (and the order in which they are returned) for a given input. These implement the [BaseExampleSelector](https://api.js.langchain.com/classes/langchain_core.example_selectors.BaseExampleSelector.html) interface. A common example is the vectorstore-backed [SemanticSimilarityExampleSelector](https://api.js.langchain.com/classes/langchain_core.example_selectors.SemanticSimilarityExampleSelector.html)
- `examplePrompt`: convert each example into 1 or more messages through its [`formatMessages`](https://api.js.langchain.com/classes/langchain_core.prompts.FewShotChatMessagePromptTemplate.html#formatMessages) method. A common example would be to convert each example into one human message and one AI message response, or a human message followed by a function call message.

These once again can be composed with other messages and chat templates to assemble your final prompt.

Let's walk through an example with the `SemanticSimilarityExampleSelector`. Since this implementation uses a vectorstore to select examples based on semantic similarity, we will want to first populate the store. Since the basic idea here is that we want to search for and return examples most similar to the text input, we embed the `values` of our prompt examples rather than considering the keys:
"""

import { SemanticSimilarityExampleSelector } from "@langchain/core/example_selectors";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from '@langchain/openai';

const examples = [
  { input: '2+2', output: '4' },
  { input: '2+3', output: '5' },
  { input: '2+4', output: '6' },
  { input: 'What did the cow say to the moon?', output: 'nothing at all' },
  {
    input: 'Write me a poem about the moon',
    output: 'One for the moon, and one for me, who are we to talk about the moon?',
  },
];

const toVectorize = examples.map((example) => `${example.input} ${example.output}`);
const embeddings = new OpenAIEmbeddings();
const vectorStore = await MemoryVectorStore.fromTexts(toVectorize, examples, embeddings);

"""
### Create the `exampleSelector`

With a vectorstore created, we can create the `exampleSelector`. Here we will call it in isolation, and set `k` on it to only fetch the two example closest to the input.
"""

const exampleSelector = new SemanticSimilarityExampleSelector(
    {
        vectorStore,
        k: 2
    }
)

// The prompt template will load examples by passing the input do the `select_examples` method
await exampleSelector.selectExamples({ input: "horse"})
# Output:
#   [

#     {

#       input: [32m"What did the cow say to the moon?"[39m,

#       output: [32m"nothing at all"[39m

#     },

#     { input: [32m"2+4"[39m, output: [32m"6"[39m }

#   ]

"""
### Create prompt template

We now assemble the prompt template, using the `exampleSelector` created above.
"""

import {
    ChatPromptTemplate,
    FewShotChatMessagePromptTemplate,
} from "@langchain/core/prompts"

// Define the few-shot prompt.
const fewShotPrompt = new FewShotChatMessagePromptTemplate({
    // The input variables select the values to pass to the example_selector
    inputVariables: ["input"],
    exampleSelector,
    // Define how ech example will be formatted.
    // In this case, each example will become 2 messages:
    // 1 human, and 1 AI
    examplePrompt: ChatPromptTemplate.fromMessages(
        [["human", "{input}"], ["ai", "{output}"]]
    ),
})

const results = await fewShotPrompt.invoke({ input: "What's 3+3?" });
console.log(results.toChatMessages())
# Output:
#   [

#     HumanMessage {

#       lc_serializable: true,

#       lc_kwargs: { content: "2+3", additional_kwargs: {}, response_metadata: {} },

#       lc_namespace: [ "langchain_core", "messages" ],

#       content: "2+3",

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {}

#     },

#     AIMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: "5",

#         tool_calls: [],

#         invalid_tool_calls: [],

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ "langchain_core", "messages" ],

#       content: "5",

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {},

#       tool_calls: [],

#       invalid_tool_calls: []

#     },

#     HumanMessage {

#       lc_serializable: true,

#       lc_kwargs: { content: "2+2", additional_kwargs: {}, response_metadata: {} },

#       lc_namespace: [ "langchain_core", "messages" ],

#       content: "2+2",

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {}

#     },

#     AIMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: "4",

#         tool_calls: [],

#         invalid_tool_calls: [],

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ "langchain_core", "messages" ],

#       content: "4",

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {},

#       tool_calls: [],

#       invalid_tool_calls: []

#     }

#   ]


"""
And we can pass this few-shot chat message prompt template into another chat prompt template:
"""

const finalPrompt = ChatPromptTemplate.fromMessages(
    [
        ["system", "You are a wondrous wizard of math."],
        fewShotPrompt,
        ["human", "{input}"],
    ]
)

const result = await fewShotPrompt.invoke({ input: "What's 3+3?" });
console.log(result)
# Output:
#   ChatPromptValue {

#     lc_serializable: true,

#     lc_kwargs: {

#       messages: [

#         HumanMessage {

#           lc_serializable: true,

#           lc_kwargs: {

#             content: "2+3",

#             additional_kwargs: {},

#             response_metadata: {}

#           },

#           lc_namespace: [ "langchain_core", "messages" ],

#           content: "2+3",

#           name: undefined,

#           additional_kwargs: {},

#           response_metadata: {}

#         },

#         AIMessage {

#           lc_serializable: true,

#           lc_kwargs: {

#             content: "5",

#             tool_calls: [],

#             invalid_tool_calls: [],

#             additional_kwargs: {},

#             response_metadata: {}

#           },

#           lc_namespace: [ "langchain_core", "messages" ],

#           content: "5",

#           name: undefined,

#           additional_kwargs: {},

#           response_metadata: {},

#           tool_calls: [],

#           invalid_tool_calls: []

#         },

#         HumanMessage {

#           lc_serializable: true,

#           lc_kwargs: {

#             content: "2+2",

#             additional_kwargs: {},

#             response_metadata: {}

#           },

#           lc_namespace: [ "langchain_core", "messages" ],

#           content: "2+2",

#           name: undefined,

#           additional_kwargs: {},

#           response_metadata: {}

#         },

#         AIMessage {

#           lc_serializable: true,

#           lc_kwargs: {

#             content: "4",

#             tool_calls: [],

#             invalid_tool_calls: [],

#             additional_kwargs: {},

#             response_metadata: {}

#           },

#           lc_namespace: [ "langchain_core", "messages" ],

#           content: "4",

#           name: undefined,

#           additional_kwargs: {},

#           response_metadata: {},

#           tool_calls: [],

#           invalid_tool_calls: []

#         }

#       ]

#     },

#     lc_namespace: [ "langchain_core", "prompt_values" ],

#     messages: [

#       HumanMessage {

#         lc_serializable: true,

#         lc_kwargs: { content: "2+3", additional_kwargs: {}, response_metadata: {} },

#         lc_namespace: [ "langchain_core", "messages" ],

#         content: "2+3",

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       AIMessage {

#         lc_serializable: true,

#         lc_kwargs: {

#           content: "5",

#           tool_calls: [],

#           invalid_tool_calls: [],

#           additional_kwargs: {},

#           response_metadata: {}

#         },

#         lc_namespace: [ "langchain_core", "messages" ],

#         content: "5",

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: {},

#         tool_calls: [],

#         invalid_tool_calls: []

#       },

#       HumanMessage {

#         lc_serializable: true,

#         lc_kwargs: { content: "2+2", additional_kwargs: {}, response_metadata: {} },

#         lc_namespace: [ "langchain_core", "messages" ],

#         content: "2+2",

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       AIMessage {

#         lc_serializable: true,

#         lc_kwargs: {

#           content: "4",

#           tool_calls: [],

#           invalid_tool_calls: [],

#           additional_kwargs: {},

#           response_metadata: {}

#         },

#         lc_namespace: [ "langchain_core", "messages" ],

#         content: "4",

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: {},

#         tool_calls: [],

#         invalid_tool_calls: []

#       }

#     ]

#   }


"""
### Use with an chat model

Finally, you can connect your model to the few-shot prompt.
"""

"""
```{=mdx}
<ChatModelTabs
  customVarName="model"
/>
```
"""

const chain = finalPrompt.pipe(model);

await chain.invoke({ input: "What's 3+3?" })
# Output:
#   AIMessage {

#     lc_serializable: [33mtrue[39m,

#     lc_kwargs: {

#       content: [32m"6"[39m,

#       tool_calls: [],

#       invalid_tool_calls: [],

#       additional_kwargs: { function_call: [90mundefined[39m, tool_calls: [90mundefined[39m },

#       response_metadata: {}

#     },

#     lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#     content: [32m"6"[39m,

#     name: [90mundefined[39m,

#     additional_kwargs: { function_call: [90mundefined[39m, tool_calls: [90mundefined[39m },

#     response_metadata: {

#       tokenUsage: { completionTokens: [33m1[39m, promptTokens: [33m51[39m, totalTokens: [33m52[39m },

#       finish_reason: [32m"stop"[39m

#     },

#     tool_calls: [],

#     invalid_tool_calls: []

#   }

"""
## Next steps

You've now learned how to add few-shot examples to your chat prompts.

Next, check out the other how-to guides on prompt templates in this section, the related how-to guide on [few shotting with text completion models](/docs/how_to/few_shot_examples), or the other [example selector how-to guides](/docs/how_to/example_selectors/).
"""



================================================
FILE: docs/core_docs/docs/how_to/filter_messages.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to filter messages

:::note
The `filterMessages` function is available in `@langchain/core` version `0.2.8` and above.
:::

In more complex chains and agents we might track state with a list of messages. This list can start to accumulate messages from multiple different models, speakers, sub-chains, etc., and we may only want to pass subsets of this full list of messages to each model call in the chain/agent.

The `filterMessages` utility makes it easy to filter messages by type, id, or name.

## Basic usage
"""

import { HumanMessage, SystemMessage, AIMessage, filterMessages } from "@langchain/core/messages"

const messages = [
    new SystemMessage({ content: "you are a good assistant", id: "1" }),
    new HumanMessage({ content: "example input", id: "2", name: "example_user" }),
    new AIMessage({ content: "example output", id: "3", name: "example_assistant" }),
    new HumanMessage({ content: "real input", id: "4", name: "bob" }),
    new AIMessage({ content: "real output", id: "5", name: "alice" }),
]

filterMessages(messages, { includeTypes: ["human"] })
# Output:
#   [

#     HumanMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: 'example input',

#         id: '2',

#         name: 'example_user',

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: 'example input',

#       name: 'example_user',

#       additional_kwargs: {},

#       response_metadata: {},

#       id: '2'

#     },

#     HumanMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: 'real input',

#         id: '4',

#         name: 'bob',

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: 'real input',

#       name: 'bob',

#       additional_kwargs: {},

#       response_metadata: {},

#       id: '4'

#     }

#   ]


filterMessages(messages, { excludeNames: ["example_user", "example_assistant"] })
# Output:
#   [

#     SystemMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: 'you are a good assistant',

#         id: '1',

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: 'you are a good assistant',

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {},

#       id: '1'

#     },

#     HumanMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: 'real input',

#         id: '4',

#         name: 'bob',

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: 'real input',

#       name: 'bob',

#       additional_kwargs: {},

#       response_metadata: {},

#       id: '4'

#     },

#     AIMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: 'real output',

#         id: '5',

#         name: 'alice',

#         tool_calls: [],

#         invalid_tool_calls: [],

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: 'real output',

#       name: 'alice',

#       additional_kwargs: {},

#       response_metadata: {},

#       id: '5',

#       tool_calls: [],

#       invalid_tool_calls: [],

#       usage_metadata: undefined

#     }

#   ]


filterMessages(messages, { includeTypes: [HumanMessage, AIMessage], excludeIds: ["3"] })

# Output:
#   [

#     HumanMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: 'example input',

#         id: '2',

#         name: 'example_user',

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: 'example input',

#       name: 'example_user',

#       additional_kwargs: {},

#       response_metadata: {},

#       id: '2'

#     },

#     HumanMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: 'real input',

#         id: '4',

#         name: 'bob',

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: 'real input',

#       name: 'bob',

#       additional_kwargs: {},

#       response_metadata: {},

#       id: '4'

#     },

#     AIMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: 'real output',

#         id: '5',

#         name: 'alice',

#         tool_calls: [],

#         invalid_tool_calls: [],

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: 'real output',

#       name: 'alice',

#       additional_kwargs: {},

#       response_metadata: {},

#       id: '5',

#       tool_calls: [],

#       invalid_tool_calls: [],

#       usage_metadata: undefined

#     }

#   ]


"""
## Chaining

`filterMessages` can be used in an imperatively (like above) or declaratively, making it easy to compose with other components in a chain:
"""

import { ChatAnthropic } from "@langchain/anthropic";

const llm = new ChatAnthropic({ model: "claude-3-sonnet-20240229", temperature: 0 })
// Notice we don't pass in messages. This creates
// a RunnableLambda that takes messages as input
const filter_ = filterMessages({ excludeNames: ["example_user", "example_assistant"], end })
const chain = filter_.pipe(llm);
await chain.invoke(messages)
# Output:
#   AIMessage {

#     lc_serializable: true,

#     lc_kwargs: {

#       content: [],

#       additional_kwargs: {

#         id: 'msg_01S2LQc1NLhtPHurW3jNRsCK',

#         type: 'message',

#         role: 'assistant',

#         model: 'claude-3-sonnet-20240229',

#         stop_reason: 'end_turn',

#         stop_sequence: null,

#         usage: [Object]

#       },

#       tool_calls: [],

#       usage_metadata: { input_tokens: 16, output_tokens: 3, total_tokens: 19 },

#       invalid_tool_calls: [],

#       response_metadata: {}

#     },

#     lc_namespace: [ 'langchain_core', 'messages' ],

#     content: [],

#     name: undefined,

#     additional_kwargs: {

#       id: 'msg_01S2LQc1NLhtPHurW3jNRsCK',

#       type: 'message',

#       role: 'assistant',

#       model: 'claude-3-sonnet-20240229',

#       stop_reason: 'end_turn',

#       stop_sequence: null,

#       usage: { input_tokens: 16, output_tokens: 3 }

#     },

#     response_metadata: {

#       id: 'msg_01S2LQc1NLhtPHurW3jNRsCK',

#       model: 'claude-3-sonnet-20240229',

#       stop_reason: 'end_turn',

#       stop_sequence: null,

#       usage: { input_tokens: 16, output_tokens: 3 }

#     },

#     id: undefined,

#     tool_calls: [],

#     invalid_tool_calls: [],

#     usage_metadata: { input_tokens: 16, output_tokens: 3, total_tokens: 19 }

#   }


"""
Looking at [the LangSmith trace](https://smith.langchain.com/public/a48c7935-04a8-4e87-9893-b14064ddbfc4/r) we can see that before the messages are passed to the model they are filtered.

Looking at just the filter_, we can see that it's a Runnable object that can be invoked like all Runnables:
"""

await filter_.invoke(messages)
# Output:
#   [

#     SystemMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: 'you are a good assistant',

#         id: '1',

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: 'you are a good assistant',

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {},

#       id: '1'

#     },

#     HumanMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: 'real input',

#         id: '4',

#         name: 'bob',

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: 'real input',

#       name: 'bob',

#       additional_kwargs: {},

#       response_metadata: {},

#       id: '4'

#     },

#     AIMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: 'real output',

#         id: '5',

#         name: 'alice',

#         tool_calls: [],

#         invalid_tool_calls: [],

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: 'real output',

#       name: 'alice',

#       additional_kwargs: {},

#       response_metadata: {},

#       id: '5',

#       tool_calls: [],

#       invalid_tool_calls: [],

#       usage_metadata: undefined

#     }

#   ]


"""
## API reference

For a complete description of all arguments head to the [API reference](https://api.js.langchain.com/functions/langchain_core.messages.filterMessages.html).
"""



================================================
FILE: docs/core_docs/docs/how_to/functions.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
keywords: [RunnableLambda, LCEL]
---
"""

"""
# How to run custom functions

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [LangChain Expression Language (LCEL)](/docs/concepts/lcel)
- [Chaining runnables](/docs/how_to/sequence/)

:::

You can use arbitrary functions as [Runnables](https://api.js.langchain.com/classes/langchain_core.runnables.Runnable.html). This is useful for formatting or when you need functionality not provided by other LangChain components, and custom functions used as Runnables are called [`RunnableLambdas`](https://api.js.langchain.com/classes/langchain_core.runnables.RunnableLambda.html).

Note that all inputs to these functions need to be a SINGLE argument. If you have a function that accepts multiple arguments, you should write a wrapper that accepts a single dict input and unpacks it into multiple argument.

This guide will cover:

- How to explicitly create a runnable from a custom function using the `RunnableLambda` constructor
- Coercion of custom functions into runnables when used in chains
- How to accept and use run metadata in your custom function
- How to stream with custom functions by having them return generators

## Using the constructor

Below, we explicitly wrap our custom logic using a `RunnableLambda` method:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/openai @langchain/core
</Npm2Yarn>
```
"""

import { StringOutputParser } from "@langchain/core/output_parsers";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnableLambda } from "@langchain/core/runnables";
import { ChatOpenAI } from "@langchain/openai";

const lengthFunction = (input: { foo: string }): { length: string } => {
  return {
    length: input.foo.length.toString(),
  };
};

const model = new ChatOpenAI({ model: "gpt-4o" });

const prompt = ChatPromptTemplate.fromTemplate("What is {length} squared?");

const chain = RunnableLambda.from(lengthFunction)
  .pipe(prompt)
  .pipe(model)
  .pipe(new StringOutputParser());

await chain.invoke({ "foo": "bar" });
# Output:
#   [32m"3 squared is \\(3^2\\), which means multiplying 3 by itself. \n"[39m +

#     [32m"\n"[39m +

#     [32m"\\[3^2 = 3 \\times 3 = 9\\]\n"[39m +

#     [32m"\n"[39m +

#     [32m"So, 3 squared"[39m... 6 more characters

"""
## Automatic coercion in chains

When using custom functions in chains with [`RunnableSequence.from`](https://api.js.langchain.com/classes/langchain_core.runnables.RunnableSequence.html#from) static method, you can omit the explicit `RunnableLambda` creation and rely on coercion.

Here's a simple example with a function that takes the output from the model and returns the first five letters of it:
"""

import { RunnableSequence } from "@langchain/core/runnables";

const storyPrompt = ChatPromptTemplate.fromTemplate("Tell me a short story about {topic}");

const storyModel = new ChatOpenAI({ model: "gpt-4o" });

const chainWithCoercedFunction = RunnableSequence.from([
  storyPrompt,
  storyModel,
  (input) => input.content.slice(0, 5),
]);

await chainWithCoercedFunction.invoke({ "topic": "bears" });
# Output:
#   [32m"Once "[39m

"""
Note that we didn't need to wrap the custom function `(input) => input.content.slice(0, 5)` in a `RunnableLambda` method. The custom function is **coerced** into a runnable. See [this section](/docs/how_to/sequence/#coercion) for more information.

## Passing run metadata

Runnable lambdas can optionally accept a [RunnableConfig](https://api.js.langchain.com/interfaces/langchain_core.runnables.RunnableConfig.html) parameter, which they can use to pass callbacks, tags, and other configuration information to nested runs.
"""

import { type RunnableConfig } from "@langchain/core/runnables";

const echo = (text: string, config: RunnableConfig) => {
  const prompt = ChatPromptTemplate.fromTemplate("Reverse the following text: {text}");
  const model = new ChatOpenAI({ model: "gpt-4o" });
  const chain = prompt.pipe(model).pipe(new StringOutputParser());
  return chain.invoke({ text }, config);
};

const output = await RunnableLambda.from(echo).invoke("foo", {
  tags: ["my-tag"],
  callbacks: [{
    handleLLMEnd: (output) => console.log(output),
  }],
});
# Output:
#   {

#     generations: [

#       [

#         {

#           text: "oof",

#           message: AIMessage {

#             lc_serializable: true,

#             lc_kwargs: [Object],

#             lc_namespace: [Array],

#             content: "oof",

#             name: undefined,

#             additional_kwargs: [Object],

#             response_metadata: [Object],

#             tool_calls: [],

#             invalid_tool_calls: []

#           },

#           generationInfo: { finish_reason: "stop" }

#         }

#       ]

#     ],

#     llmOutput: {

#       tokenUsage: { completionTokens: 2, promptTokens: 13, totalTokens: 15 }

#     }

#   }


"""
# Streaming

You can use generator functions (ie. functions that use the `yield` keyword, and behave like iterators) in a chain.

The signature of these generators should be `AsyncGenerator<Input> -> AsyncGenerator<Output>`.

These are useful for:
- implementing a custom output parser
- modifying the output of a previous step, while preserving streaming capabilities

Here's an example of a custom output parser for comma-separated lists. First, we create a chain that generates such a list as text:
"""

const streamingPrompt = ChatPromptTemplate.fromTemplate(
  "Write a comma-separated list of 5 animals similar to: {animal}. Do not include numbers"
);

const strChain = streamingPrompt.pipe(model).pipe(new StringOutputParser());

const stream = await strChain.stream({ animal: "bear" });

for await (const chunk of stream) {
  console.log(chunk);
}
# Output:
#   

#   Lion

#   ,

#    wolf

#   ,

#    tiger

#   ,

#    cougar

#   ,

#    leopard

#   


"""
Next, we define a custom function that will aggregate the currently streamed output and yield it when the model generates the next comma in the list:
"""

// This is a custom parser that splits an iterator of llm tokens
// into a list of strings separated by commas
async function* splitIntoList(input) {
  // hold partial input until we get a comma
  let buffer = "";
  for await (const chunk of input) {
    // add current chunk to buffer
    buffer += chunk;
    // while there are commas in the buffer
    while (buffer.includes(",")) {
      // split buffer on comma
      const commaIndex = buffer.indexOf(",");
      // yield everything before the comma
      yield [buffer.slice(0, commaIndex).trim()];
      // save the rest for the next iteration
      buffer = buffer.slice(commaIndex + 1);
    }
  }
  // yield the last chunk
  yield [buffer.trim()];
}

const listChain = strChain.pipe(splitIntoList);

const listChainStream = await listChain.stream({"animal": "bear"});

for await (const chunk of listChainStream) {
  console.log(chunk);
}
# Output:
#   [ "wolf" ]

#   [ "lion" ]

#   [ "tiger" ]

#   [ "cougar" ]

#   [ "cheetah" ]


"""
Invoking it gives a full array of values:
"""

await listChain.invoke({"animal": "bear"})
# Output:
#   [ [32m"lion"[39m, [32m"tiger"[39m, [32m"wolf"[39m, [32m"cougar"[39m, [32m"jaguar"[39m ]

"""
## Next steps

Now you've learned a few different ways to use custom logic within your chains, and how to implement streaming.

To learn more, see the other how-to guides on runnables in this section.
"""



================================================
FILE: docs/core_docs/docs/how_to/generative_ui.mdx
================================================
# How to build an LLM generated UI

This guide will walk through some high level concepts and code snippets for building generative UI's using LangChain.js. To see the full code for generative UI, [click here to visit our official LangChain Next.js template](https://github.com/langchain-ai/langchain-nextjs-template/blob/main/app/generative_ui/README.md).

The sample implements a tool calling agent, which outputs an interactive UI element when streaming intermediate outputs of tool calls to the client.

We introduce two utilities which wraps the AI SDK to make it easier to yield React elements inside runnables and tool calls: [`createRunnableUI`](https://github.com/langchain-ai/langchain-nextjs-template/blob/7f764d558682214d50b064f4293667123a31e6fe/app/generative_ui/utils/server.tsx#L89)
and [`streamRunnableUI`](https://github.com/langchain-ai/langchain-nextjs-template/blob/7f764d558682214d50b064f4293667123a31e6fe/app/generative_ui/utils/server.tsx#L126).

- The `streamRunnableUI` executes the provided Runnable with `streamEvents` method and sends every `stream` event to the client via the React Server Components stream.
- The `createRunnableUI` wraps the `createStreamableUI` function from AI SDK to properly hook into the Runnable event stream.

The usage is then as follows:

```tsx ai/chain.tsx
"use server";

const tool = tool(
  async (input, config) => {
    const stream = await createRunnableUI(config);
    stream.update(<div>Searching...</div>);

    const result = await images(input);
    stream.done(
      <Images
        images={result.images_results
          .map((image) => image.thumbnail)
          .slice(0, input.limit)}
      />
    );

    return `[Returned ${result.images_results.length} images]`;
  },
  {
    name: "Images",
    description: "A tool to search for images. input should be a search query.",
    schema: z.object({
      query: z.string().describe("The search query used to search for cats"),
      limit: z.number().describe("The number of pictures shown to the user"),
    }),
  }
);

// add LLM, prompt, etc...

const tools = [tool];

export const agentExecutor = new AgentExecutor({
  agent: createToolCallingAgent({ llm, tools, prompt }),
  tools,
});
```

:::tip
As of `langchain` version `0.2.8`, the `createToolCallingAgent` function now supports [OpenAI-formatted tools](https://api.js.langchain.com/interfaces/langchain_core.language_models_base.ToolDefinition.html).
:::

```tsx agent.tsx
async function agent(inputs: {
  input: string;
  chat_history: [role: string, content: string][];
}) {
  "use server";

  return streamRunnableUI(agentExecutor, {
    input: inputs.input,
    chat_history: inputs.chat_history.map(
      ([role, content]) => new ChatMessage(content, role)
    ),
  });
}

export const EndpointsContext = exposeEndpoints({ agent });
```

In order to ensure all of the client components are included in the bundle, we need to wrap all of the Server Actions into `exposeEndpoints` method. These endpoints will be accessible from the client via the Context API, seen in the `useActions` hook.

```tsx
"use client";
import type { EndpointsContext } from "./agent";

export default function Page() {
  const actions = useActions<typeof EndpointsContext>();
  const [node, setNode] = useState();

  return (
    <div>
      {node}

      <button
        onClick={async () => {
          setNode(await actions.agent({ input: "cats" }));
        }}
      >
        Get images of cats
      </button>
    </div>
  );
}
```



================================================
FILE: docs/core_docs/docs/how_to/graph_constructing.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to construct knowledge graphs

In this guide we'll go over the basic ways of constructing a knowledge graph based on unstructured text. The constructed graph can then be used as knowledge base in a RAG application. At a high-level, the steps of constructing a knowledge are from text are:

1. Extracting structured information from text: Model is used to extract structured graph information from text.
2. Storing into graph database: Storing the extracted structured graph information into a graph database enables downstream RAG applications
"""

"""
## Setup
#### Install dependencies

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  langchain @langchain/community @langchain/openai @langchain/core neo4j-driver zod
</Npm2Yarn>
```

#### Set environment variables

We'll use OpenAI in this example:

```env
OPENAI_API_KEY=your-api-key

# Optional, use LangSmith for best-in-class observability
LANGSMITH_API_KEY=your-api-key
LANGSMITH_TRACING=true

# Reduce tracing latency if you are not in a serverless environment
# LANGCHAIN_CALLBACKS_BACKGROUND=true
```

Next, we need to define Neo4j credentials.
Follow [these installation steps](https://neo4j.com/docs/operations-manual/current/installation/) to set up a Neo4j database.

```env
NEO4J_URI="bolt://localhost:7687"
NEO4J_USERNAME="neo4j"
NEO4J_PASSWORD="password"
```
"""

"""
The below example will create a connection with a Neo4j database.
"""

import "neo4j-driver";
import { Neo4jGraph } from "@langchain/community/graphs/neo4j_graph";

const url = process.env.NEO4J_URI;
const username = process.env.NEO4J_USER;
const password = process.env.NEO4J_PASSWORD;
const graph = await Neo4jGraph.initialize({ url, username, password });

"""
## LLM Graph Transformer
Extracting graph data from text enables the transformation of unstructured information into structured formats, facilitating deeper insights and more efficient navigation through complex relationships and patterns. The LLMGraphTransformer converts text documents into structured graph documents by leveraging a LLM to parse and categorize entities and their relationships. The selection of the LLM model significantly influences the output by determining the accuracy and nuance of the extracted graph data.
"""

import { ChatOpenAI } from "@langchain/openai";
import { LLMGraphTransformer } from "@langchain/community/experimental/graph_transformers/llm";

const model = new ChatOpenAI({
    temperature: 0,
    model: "gpt-4o-mini",
});

const llmGraphTransformer = new LLMGraphTransformer({
    llm: model
});


"""
Now we can pass in example text and examine the results.
"""

import { Document } from "@langchain/core/documents";

let text = `
Marie Curie, was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity.
She was the first woman to win a Nobel Prize, the first person to win a Nobel Prize twice, and the only person to win a Nobel Prize in two scientific fields.
Her husband, Pierre Curie, was a co-winner of her first Nobel Prize, making them the first-ever married couple to win the Nobel Prize and launching the Curie family legacy of five Nobel Prizes.
She was, in 1906, the first woman to become a professor at the University of Paris.
`

const result = await llmGraphTransformer.convertToGraphDocuments([
    new Document({ pageContent: text }),
]);

console.log(`Nodes: ${result[0].nodes.length}`);
console.log(`Relationships:${result[0].relationships.length}`);
# Output:
#   Nodes: 8

#   Relationships:7


"""
Note that the graph construction process is non-deterministic since we are using LLM. Therefore, you might get slightly different results on each execution.
Examine the following image to better grasp the structure of the generated knowledge graph.

![graph_construction1.png](../../static/img/graph_construction1.png)

Additionally, you have the flexibility to define specific types of nodes and relationships for extraction according to your requirements.
"""

const llmGraphTransformerFiltered = new LLMGraphTransformer({
    llm: model,
    allowedNodes: ["PERSON", "COUNTRY", "ORGANIZATION"],
    allowedRelationships:["NATIONALITY", "LOCATED_IN", "WORKED_AT", "SPOUSE"],
    strictMode:false
});

const result_filtered = await llmGraphTransformerFiltered.convertToGraphDocuments([
    new Document({ pageContent: text }),
]);

console.log(`Nodes: ${result_filtered[0].nodes.length}`);
console.log(`Relationships:${result_filtered[0].relationships.length}`);
# Output:
#   Nodes: 6

#   Relationships:4


"""
For a better understanding of the generated graph, we can again visualize it.

![graph_construction1.png](../../static/img/graph_construction2.png)

## Storing to graph database
The generated graph documents can be stored to a graph database using the `addGraphDocuments` method.
"""

await graph.addGraphDocuments(result_filtered)



================================================
FILE: docs/core_docs/docs/how_to/graph_mapping.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to map values to a database

In this guide we'll go over strategies to improve graph database query generation by mapping values from user inputs to database.
When using the built-in graph chains, the LLM is aware of the graph schema, but has no information about the values of properties stored in the database.
Therefore, we can introduce a new step in graph database QA system to accurately map values.
"""

"""
## Setup
#### Install dependencies

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  langchain @langchain/community @langchain/openai @langchain/core neo4j-driver zod
</Npm2Yarn>
```

#### Set environment variables

We'll use OpenAI in this example:

```env
OPENAI_API_KEY=your-api-key

# Optional, use LangSmith for best-in-class observability
LANGSMITH_API_KEY=your-api-key
LANGSMITH_TRACING=true

# Reduce tracing latency if you are not in a serverless environment
# LANGCHAIN_CALLBACKS_BACKGROUND=true
```

Next, we need to define Neo4j credentials.
Follow [these installation steps](https://neo4j.com/docs/operations-manual/current/installation/) to set up a Neo4j database.

```env
NEO4J_URI="bolt://localhost:7687"
NEO4J_USERNAME="neo4j"
NEO4J_PASSWORD="password"
```
"""

"""
The below example will create a connection with a Neo4j database and will populate it with example data about movies and their actors.
"""

import "neo4j-driver";
import { Neo4jGraph } from "@langchain/community/graphs/neo4j_graph";

const url = process.env.NEO4J_URI;
const username = process.env.NEO4J_USER;
const password = process.env.NEO4J_PASSWORD;
const graph = await Neo4jGraph.initialize({ url, username, password });

// Import movie information
const moviesQuery = `LOAD CSV WITH HEADERS FROM 
'https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/movies/movies_small.csv'
AS row
MERGE (m:Movie {id:row.movieId})
SET m.released = date(row.released),
    m.title = row.title,
    m.imdbRating = toFloat(row.imdbRating)
FOREACH (director in split(row.director, '|') | 
    MERGE (p:Person {name:trim(director)})
    MERGE (p)-[:DIRECTED]->(m))
FOREACH (actor in split(row.actors, '|') | 
    MERGE (p:Person {name:trim(actor)})
    MERGE (p)-[:ACTED_IN]->(m))
FOREACH (genre in split(row.genres, '|') | 
    MERGE (g:Genre {name:trim(genre)})
    MERGE (m)-[:IN_GENRE]->(g))`

await graph.query(moviesQuery);
# Output:
#   Schema refreshed successfully.

#   []

"""
## Detecting entities in the user input
We have to extract the types of entities/values we want to map to a graph database. In this example, we are dealing with a movie graph, so we can map movies and people to the database.
"""

import { ChatPromptTemplate } from "@langchain/core/prompts";
import { ChatOpenAI } from "@langchain/openai";
import { z } from "zod";

const llm = new ChatOpenAI({ model: "gpt-3.5-turbo", temperature: 0 })

const entitySchema = z.object({
    names: z.array(z.string()).describe("All the person or movies appearing in the text"),
}).describe("Identifying information about entities.");


const prompt = ChatPromptTemplate.fromMessages(
  [
    [
      "system",
      "You are extracting person and movies from the text."
    ],
    [
      "human",
      "Use the given format to extract information from the following\ninput: {question}"
    ]
  ]
);

const entityChain = prompt.pipe(llm.withStructuredOutput(entitySchema));

"""
We can test the entity extraction chain.
"""

const entities = await entityChain.invoke({ question: "Who played in Casino movie?" })
entities
# Output:
#   { names: [ [32m"Casino"[39m ] }

"""
We will utilize a simple `CONTAINS` clause to match entities to database. In practice, you might want to use a fuzzy search or a fulltext index to allow for minor misspellings.
"""

const matchQuery = `
MATCH (p:Person|Movie)
WHERE p.name CONTAINS $value OR p.title CONTAINS $value
RETURN coalesce(p.name, p.title) AS result, labels(p)[0] AS type
LIMIT 1`

const matchToDatabase = async (values) => {
    let result = ""
    for (const entity of values.names) {
        const response = await graph.query(matchQuery, {
            value: entity
        })
        if (response.length > 0) {
            result += `${entity} maps to ${response[0]["result"]} ${response[0]["type"]} in database\n`
        }
    }
    return result
}

await matchToDatabase(entities)
# Output:
#   [32m"Casino maps to Casino Movie in database\n"[39m

"""
## Custom Cypher generating chain

We need to define a custom Cypher prompt that takes the entity mapping information along with the schema and the user question to construct a Cypher statement.
We will be using the LangChain expression language to accomplish that.
"""

import { StringOutputParser } from "@langchain/core/output_parsers";
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";

// Generate Cypher statement based on natural language input
const cypherTemplate = `Based on the Neo4j graph schema below, write a Cypher query that would answer the user's question:
{schema}
Entities in the question map to the following database values:
{entities_list}
Question: {question}
Cypher query:`

const cypherPrompt = ChatPromptTemplate.fromMessages(
    [
        [
            "system",
            "Given an input question, convert it to a Cypher query. No pre-amble.",
        ],
        ["human", cypherTemplate]
    ]
)

const llmWithStop = llm.bind({ stop: ["\nCypherResult:"] })

const cypherResponse = RunnableSequence.from([
    RunnablePassthrough.assign({ names: entityChain }),
    RunnablePassthrough.assign({
        entities_list: async (x) => matchToDatabase(x.names),
        schema: async (_) => graph.getSchema(),
    }),
    cypherPrompt,
    llmWithStop,
    new StringOutputParser(),
])

const cypher = await cypherResponse.invoke({"question": "Who played in Casino movie?"})
cypher
# Output:
#   [32m'MATCH (:Movie {title: "Casino"})<-[:ACTED_IN]-(actor)\nRETURN actor.name'[39m



================================================
FILE: docs/core_docs/docs/how_to/graph_prompting.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to improve results with prompting

In this guide we’ll go over prompting strategies to improve graph database query generation. We’ll largely focus on methods for getting relevant database-specific information in your prompt.

```{=mdx}
:::warning

The `GraphCypherQAChain` used in this guide will execute Cypher statements against the provided database.
For production, make sure that the database connection uses credentials that are narrowly-scoped to only include necessary permissions.

Failure to do so may result in data corruption or loss, since the calling code
may attempt commands that would result in deletion, mutation of data
if appropriately prompted or reading sensitive data if such data is present in the database.

:::
```
"""

"""
## Setup
#### Install dependencies

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  langchain @langchain/community @langchain/openai @langchain/core neo4j-driver
</Npm2Yarn>
```

#### Set environment variables

We'll use OpenAI in this example:

```env
OPENAI_API_KEY=your-api-key

# Optional, use LangSmith for best-in-class observability
LANGSMITH_API_KEY=your-api-key
LANGSMITH_TRACING=true

# Reduce tracing latency if you are not in a serverless environment
# LANGCHAIN_CALLBACKS_BACKGROUND=true
```

Next, we need to define Neo4j credentials.
Follow [these installation steps](https://neo4j.com/docs/operations-manual/current/installation/) to set up a Neo4j database.

```env
NEO4J_URI="bolt://localhost:7687"
NEO4J_USERNAME="neo4j"
NEO4J_PASSWORD="password"
```
"""

"""
The below example will create a connection with a Neo4j database and will populate it with example data about movies and their actors.
"""

const url = process.env.NEO4J_URI;
const username = process.env.NEO4J_USER;
const password = process.env.NEO4J_PASSWORD;

import "neo4j-driver";
import { Neo4jGraph } from "@langchain/community/graphs/neo4j_graph";

const graph = await Neo4jGraph.initialize({ url, username, password });

// Import movie information
const moviesQuery = `LOAD CSV WITH HEADERS FROM 
'https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/movies/movies_small.csv'
AS row
MERGE (m:Movie {id:row.movieId})
SET m.released = date(row.released),
    m.title = row.title,
    m.imdbRating = toFloat(row.imdbRating)
FOREACH (director in split(row.director, '|') | 
    MERGE (p:Person {name:trim(director)})
    MERGE (p)-[:DIRECTED]->(m))
FOREACH (actor in split(row.actors, '|') | 
    MERGE (p:Person {name:trim(actor)})
    MERGE (p)-[:ACTED_IN]->(m))
FOREACH (genre in split(row.genres, '|') | 
    MERGE (g:Genre {name:trim(genre)})
    MERGE (m)-[:IN_GENRE]->(g))`

await graph.query(moviesQuery);
# Output:
#   Schema refreshed successfully.

#   []

"""
# Filtering graph schema

At times, you may need to focus on a specific subset of the graph schema while generating Cypher statements.
Let's say we are dealing with the following graph schema:
"""

await graph.refreshSchema()
console.log(graph.getSchema())
# Output:
#   Node properties are the following:

#   Movie {imdbRating: FLOAT, id: STRING, released: DATE, title: STRING}, Person {name: STRING}, Genre {name: STRING}, Chunk {embedding: LIST, id: STRING, text: STRING}

#   Relationship properties are the following:

#   

#   The relationships are the following:

#   (:Movie)-[:IN_GENRE]->(:Genre), (:Person)-[:DIRECTED]->(:Movie), (:Person)-[:ACTED_IN]->(:Movie)


"""
## Few-shot examples

Including examples of natural language questions being converted to valid Cypher queries against our database in the prompt will often improve model performance, especially for complex queries.

Let's say we have the following examples:
"""

const examples = [
    {
        "question": "How many artists are there?",
        "query": "MATCH (a:Person)-[:ACTED_IN]->(:Movie) RETURN count(DISTINCT a)",
    },
    {
        "question": "Which actors played in the movie Casino?",
        "query": "MATCH (m:Movie {{title: 'Casino'}})<-[:ACTED_IN]-(a) RETURN a.name",
    },
    {
        "question": "How many movies has Tom Hanks acted in?",
        "query": "MATCH (a:Person {{name: 'Tom Hanks'}})-[:ACTED_IN]->(m:Movie) RETURN count(m)",
    },
    {
        "question": "List all the genres of the movie Schindler's List",
        "query": "MATCH (m:Movie {{title: 'Schindler\\'s List'}})-[:IN_GENRE]->(g:Genre) RETURN g.name",
    },
    {
        "question": "Which actors have worked in movies from both the comedy and action genres?",
        "query": "MATCH (a:Person)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]->(g1:Genre), (a)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]->(g2:Genre) WHERE g1.name = 'Comedy' AND g2.name = 'Action' RETURN DISTINCT a.name",
    },
    {
        "question": "Which directors have made movies with at least three different actors named 'John'?",
        "query": "MATCH (d:Person)-[:DIRECTED]->(m:Movie)<-[:ACTED_IN]-(a:Person) WHERE a.name STARTS WITH 'John' WITH d, COUNT(DISTINCT a) AS JohnsCount WHERE JohnsCount >= 3 RETURN d.name",
    },
    {
        "question": "Identify movies where directors also played a role in the film.",
        "query": "MATCH (p:Person)-[:DIRECTED]->(m:Movie), (p)-[:ACTED_IN]->(m) RETURN m.title, p.name",
    },
    {
        "question": "Find the actor with the highest number of movies in the database.",
        "query": "MATCH (a:Actor)-[:ACTED_IN]->(m:Movie) RETURN a.name, COUNT(m) AS movieCount ORDER BY movieCount DESC LIMIT 1",
    },
]

"""
We can create a few-shot prompt with them like so:
"""

import { FewShotPromptTemplate, PromptTemplate } from "@langchain/core/prompts";

const examplePrompt = PromptTemplate.fromTemplate(
    "User input: {question}\nCypher query: {query}"
)
const prompt = new FewShotPromptTemplate({
    examples: examples.slice(0, 5),
    examplePrompt,
    prefix: "You are a Neo4j expert. Given an input question, create a syntactically correct Cypher query to run.\n\nHere is the schema information\n{schema}.\n\nBelow are a number of examples of questions and their corresponding Cypher queries.",
    suffix: "User input: {question}\nCypher query: ",
    inputVariables: ["question", "schema"],
})

console.log(await prompt.format({ question: "How many artists are there?", schema: "foo" }))
# Output:
#   You are a Neo4j expert. Given an input question, create a syntactically correct Cypher query to run.

#   

#   Here is the schema information

#   foo.

#   

#   Below are a number of examples of questions and their corresponding Cypher queries.

#   

#   User input: How many artists are there?

#   Cypher query: MATCH (a:Person)-[:ACTED_IN]->(:Movie) RETURN count(DISTINCT a)

#   

#   User input: Which actors played in the movie Casino?

#   Cypher query: MATCH (m:Movie {title: 'Casino'})<-[:ACTED_IN]-(a) RETURN a.name

#   

#   User input: How many movies has Tom Hanks acted in?

#   Cypher query: MATCH (a:Person {name: 'Tom Hanks'})-[:ACTED_IN]->(m:Movie) RETURN count(m)

#   

#   User input: List all the genres of the movie Schindler's List

#   Cypher query: MATCH (m:Movie {title: 'Schindler\'s List'})-[:IN_GENRE]->(g:Genre) RETURN g.name

#   

#   User input: Which actors have worked in movies from both the comedy and action genres?

#   Cypher query: MATCH (a:Person)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]->(g1:Genre), (a)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]->(g2:Genre) WHERE g1.name = 'Comedy' AND g2.name = 'Action' RETURN DISTINCT a.name

#   

#   User input: How many artists are there?

#   Cypher query: 


"""
## Dynamic few-shot examples

If we have enough examples, we may want to only include the most relevant ones in the prompt, either because they don't fit in the model's context window or because the long tail of examples distracts the model. And specifically, given any input we want to include the examples most relevant to that input.

We can do just this using an ExampleSelector. In this case we'll use a [SemanticSimilarityExampleSelector](https://api.js.langchain.com/classes/langchain_core.example_selectors.SemanticSimilarityExampleSelector.html), which will store the examples in the vector database of our choosing. At runtime it will perform a similarity search between the input and our examples, and return the most semantically similar ones: 
"""

import { OpenAIEmbeddings } from "@langchain/openai";
import { SemanticSimilarityExampleSelector } from "@langchain/core/example_selectors";
import { Neo4jVectorStore } from "@langchain/community/vectorstores/neo4j_vector";

const exampleSelector = await SemanticSimilarityExampleSelector.fromExamples(
    examples,
    new OpenAIEmbeddings(),
    Neo4jVectorStore,
    {
        k: 5,
        inputKeys: ["question"],
        preDeleteCollection: true,
        url,
        username,
        password
    }
)

await exampleSelector.selectExamples({ question: "how many artists are there?" })
# Output:
#   [

#     {

#       query: [32m"MATCH (a:Person)-[:ACTED_IN]->(:Movie) RETURN count(DISTINCT a)"[39m,

#       question: [32m"How many artists are there?"[39m

#     },

#     {

#       query: [32m"MATCH (a:Person {{name: 'Tom Hanks'}})-[:ACTED_IN]->(m:Movie) RETURN count(m)"[39m,

#       question: [32m"How many movies has Tom Hanks acted in?"[39m

#     },

#     {

#       query: [32m"MATCH (a:Person)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]->(g1:Genre), (a)-[:ACTED_IN]->(:Movie)-[:IN_GENRE"[39m... 84 more characters,

#       question: [32m"Which actors have worked in movies from both the comedy and action genres?"[39m

#     },

#     {

#       query: [32m"MATCH (d:Person)-[:DIRECTED]->(m:Movie)<-[:ACTED_IN]-(a:Person) WHERE a.name STARTS WITH 'John' WITH"[39m... 71 more characters,

#       question: [32m"Which directors have made movies with at least three different actors named 'John'?"[39m

#     },

#     {

#       query: [32m"MATCH (a:Actor)-[:ACTED_IN]->(m:Movie) RETURN a.name, COUNT(m) AS movieCount ORDER BY movieCount DES"[39m... 9 more characters,

#       question: [32m"Find the actor with the highest number of movies in the database."[39m

#     }

#   ]

"""
To use it, we can pass the ExampleSelector directly in to our FewShotPromptTemplate:
"""

const promptWithExampleSelector = new FewShotPromptTemplate({
  exampleSelector,
  examplePrompt,
  prefix: "You are a Neo4j expert. Given an input question, create a syntactically correct Cypher query to run.\n\nHere is the schema information\n{schema}.\n\nBelow are a number of examples of questions and their corresponding Cypher queries.",
  suffix: "User input: {question}\nCypher query: ",
  inputVariables: ["question", "schema"],
})

console.log(await promptWithExampleSelector.format({ question: "how many artists are there?", schema: "foo" }))
# Output:
#   You are a Neo4j expert. Given an input question, create a syntactically correct Cypher query to run.

#   

#   Here is the schema information

#   foo.

#   

#   Below are a number of examples of questions and their corresponding Cypher queries.

#   

#   User input: How many artists are there?

#   Cypher query: MATCH (a:Person)-[:ACTED_IN]->(:Movie) RETURN count(DISTINCT a)

#   

#   User input: How many movies has Tom Hanks acted in?

#   Cypher query: MATCH (a:Person {name: 'Tom Hanks'})-[:ACTED_IN]->(m:Movie) RETURN count(m)

#   

#   User input: Which actors have worked in movies from both the comedy and action genres?

#   Cypher query: MATCH (a:Person)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]->(g1:Genre), (a)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]->(g2:Genre) WHERE g1.name = 'Comedy' AND g2.name = 'Action' RETURN DISTINCT a.name

#   

#   User input: Which directors have made movies with at least three different actors named 'John'?

#   Cypher query: MATCH (d:Person)-[:DIRECTED]->(m:Movie)<-[:ACTED_IN]-(a:Person) WHERE a.name STARTS WITH 'John' WITH d, COUNT(DISTINCT a) AS JohnsCount WHERE JohnsCount >= 3 RETURN d.name

#   

#   User input: Find the actor with the highest number of movies in the database.

#   Cypher query: MATCH (a:Actor)-[:ACTED_IN]->(m:Movie) RETURN a.name, COUNT(m) AS movieCount ORDER BY movieCount DESC LIMIT 1

#   

#   User input: how many artists are there?

#   Cypher query: 


import { ChatOpenAI } from "@langchain/openai";
import { GraphCypherQAChain } from "langchain/chains/graph_qa/cypher";

const llm = new ChatOpenAI({
  model: "gpt-3.5-turbo",
  temperature: 0,
});
const chain = GraphCypherQAChain.fromLLM(
  {
    graph,
    llm,
    cypherPrompt: promptWithExampleSelector,
  }
)

await chain.invoke({
  query: "How many actors are in the graph?"
})
# Output:
#   { result: [32m"There are 967 actors in the graph."[39m }



================================================
FILE: docs/core_docs/docs/how_to/graph_semantic.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to add a semantic layer over the database

You can use database queries to retrieve information from a graph database like Neo4j.
One option is to use LLMs to generate Cypher statements.
While that option provides excellent flexibility, the solution could be brittle and not consistently generating precise Cypher statements.
Instead of generating Cypher statements, we can implement Cypher templates as tools in a semantic layer that an LLM agent can interact with.

![graph_semantic.png](../../static/img/graph_semantic.png)

```{=mdx}
:::warning

The code in this guide will execute Cypher statements against the provided database.
For production, make sure that the database connection uses credentials that are narrowly-scoped to only include necessary permissions.

Failure to do so may result in data corruption or loss, since the calling code
may attempt commands that would result in deletion, mutation of data
if appropriately prompted or reading sensitive data if such data is present in the database.

:::
```
"""

"""
## Setup
#### Install dependencies

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  langchain @langchain/community @langchain/openai @langchain/core neo4j-driver zod
</Npm2Yarn>
```

#### Set environment variables

We'll use OpenAI in this example:

```env
OPENAI_API_KEY=your-api-key

# Optional, use LangSmith for best-in-class observability
LANGSMITH_API_KEY=your-api-key
LANGSMITH_TRACING=true

# Reduce tracing latency if you are not in a serverless environment
# LANGCHAIN_CALLBACKS_BACKGROUND=true
```

Next, we need to define Neo4j credentials.
Follow [these installation steps](https://neo4j.com/docs/operations-manual/current/installation/) to set up a Neo4j database.

```env
NEO4J_URI="bolt://localhost:7687"
NEO4J_USERNAME="neo4j"
NEO4J_PASSWORD="password"
```
"""

"""
The below example will create a connection with a Neo4j database and will populate it with example data about movies and their actors.
"""

import "neo4j-driver";
import { Neo4jGraph } from "@langchain/community/graphs/neo4j_graph";

const url = process.env.NEO4J_URI;
const username = process.env.NEO4J_USER;
const password = process.env.NEO4J_PASSWORD;
const graph = await Neo4jGraph.initialize({ url, username, password });

// Import movie information
const moviesQuery = `LOAD CSV WITH HEADERS FROM 
'https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/movies/movies_small.csv'
AS row
MERGE (m:Movie {id:row.movieId})
SET m.released = date(row.released),
    m.title = row.title,
    m.imdbRating = toFloat(row.imdbRating)
FOREACH (director in split(row.director, '|') | 
    MERGE (p:Person {name:trim(director)})
    MERGE (p)-[:DIRECTED]->(m))
FOREACH (actor in split(row.actors, '|') | 
    MERGE (p:Person {name:trim(actor)})
    MERGE (p)-[:ACTED_IN]->(m))
FOREACH (genre in split(row.genres, '|') | 
    MERGE (g:Genre {name:trim(genre)})
    MERGE (m)-[:IN_GENRE]->(g))`

await graph.query(moviesQuery);
# Output:
#   Schema refreshed successfully.

#   []

"""
## Custom tools with Cypher templates

A semantic layer consists of various tools exposed to an LLM that it can use to interact with a knowledge graph.
They can be of various complexity. You can think of each tool in a semantic layer as a function.

The function we will implement is to retrieve information about movies or their cast.
"""

const descriptionQuery = `MATCH (m:Movie|Person)
WHERE m.title CONTAINS $candidate OR m.name CONTAINS $candidate
MATCH (m)-[r:ACTED_IN|HAS_GENRE]-(t)
WITH m, type(r) as type, collect(coalesce(t.name, t.title)) as names
WITH m, type+": "+reduce(s="", n IN names | s + n + ", ") as types
WITH m, collect(types) as contexts
WITH m, "type:" + labels(m)[0] + "\ntitle: "+ coalesce(m.title, m.name) 
       + "\nyear: "+coalesce(m.released,"") +"\n" +
       reduce(s="", c in contexts | s + substring(c, 0, size(c)-2) +"\n") as context
RETURN context LIMIT 1`

const getInformation = async (entity: string) => {
    try {
        const data = await graph.query(descriptionQuery, { candidate: entity });
        return data[0]["context"];
    } catch (error) {
        return "No information was found";
    }
    
}

"""
You can observe that we have defined the Cypher statement used to retrieve information.
Therefore, we can avoid generating Cypher statements and use the LLM agent to only populate the input parameters.
To provide additional information to an LLM agent about when to use the tool and their input parameters, we wrap the function as a tool.
"""

import { tool } from "@langchain/core/tools";
import { z } from "zod";

const informationTool = tool((input) => {
    return getInformation(input.entity);
}, {
    name: "Information",
    description: "useful for when you need to answer questions about various actors or movies",
    schema: z.object({
        entity: z.string().describe("movie or a person mentioned in the question"),
    }),
});

"""
## OpenAI Agent

LangChain expression language makes it very convenient to define an agent to interact with a graph database over the semantic layer.
"""

import { ChatOpenAI } from "@langchain/openai";
import { AgentExecutor } from "langchain/agents";
import { formatToOpenAIFunctionMessages } from "langchain/agents/format_scratchpad";
import { OpenAIFunctionsAgentOutputParser } from "langchain/agents/openai/output_parser";
import { convertToOpenAIFunction } from "@langchain/core/utils/function_calling";
import { ChatPromptTemplate, MessagesPlaceholder } from "@langchain/core/prompts";
import { AIMessage, BaseMessage, HumanMessage } from "@langchain/core/messages";
import { RunnableSequence } from "@langchain/core/runnables";

const llm = new ChatOpenAI({ model: "gpt-3.5-turbo", temperature: 0 })
const tools = [informationTool]

const llmWithTools = llm.bind({
    functions: tools.map(convertToOpenAIFunction),
})

const prompt = ChatPromptTemplate.fromMessages(
    [
        [
            "system",
            "You are a helpful assistant that finds information about movies and recommends them. If tools require follow up questions, make sure to ask the user for clarification. Make sure to include any available options that need to be clarified in the follow up questions Do only the things the user specifically requested."
        ],
        new MessagesPlaceholder("chat_history"),
        ["human", "{input}"],
        new MessagesPlaceholder("agent_scratchpad"),
    ]
)

const _formatChatHistory = (chatHistory) => {
    const buffer: Array<BaseMessage> = []
    for (const [human, ai] of chatHistory) {
        buffer.push(new HumanMessage({ content: human }))
        buffer.push(new AIMessage({ content: ai }))
    }
    return buffer
}

const agent = RunnableSequence.from([
    {
        input: (x) => x.input,
        chat_history: (x) => {
            if ("chat_history" in x) {
                return _formatChatHistory(x.chat_history);
            }
            return [];
        },
        agent_scratchpad: (x) => {
            if ("steps" in x) {
                return formatToOpenAIFunctionMessages(
                    x.steps
                );
            }
            return [];
        },
    },
    prompt,
    llmWithTools,
    new OpenAIFunctionsAgentOutputParser(),
])

const agentExecutor = new AgentExecutor({ agent, tools });

await agentExecutor.invoke({ input: "Who played in Casino?" })
# Output:
#   {

#     input: [32m"Who played in Casino?"[39m,

#     output: [32m'The movie "Casino" starred James Woods, Joe Pesci, Robert De Niro, and Sharon Stone.'[39m

#   }



================================================
FILE: docs/core_docs/docs/how_to/index.mdx
================================================
---
sidebar_position: 0
sidebar_class_name: hidden
---

# How-to guides

Here you'll find answers to “How do I….?” types of questions.
These guides are _goal-oriented_ and _concrete_; they're meant to help you complete a specific task.
For conceptual explanations see [Conceptual Guides](/docs/concepts/).
For end-to-end walkthroughs see [Tutorials](/docs/tutorials).
For comprehensive descriptions of every class and function see [API Reference](https://api.js.langchain.com/).

## Installation

- [How to: install LangChain packages](/docs/how_to/installation/)

## Key features

This highlights functionality that is core to using LangChain.

- [How to: return structured data from an LLM](/docs/how_to/structured_output/)
- [How to: use a chat model to call tools](/docs/how_to/tool_calling/)
- [How to: stream runnables](/docs/how_to/streaming)
- [How to: debug your LLM apps](/docs/how_to/debugging/)

## LangChain Expression Language (LCEL)

LangChain Expression Language is a way to create arbitrary custom chains. It is built on the [`Runnable`](https://api.js.langchain.com/classes/langchain_core.runnables.Runnable.html) protocol.

[**LCEL cheatsheet**](/docs/how_to/lcel_cheatsheet/): For a quick overview of how to use the main LCEL primitives.

- [How to: chain runnables](/docs/how_to/sequence)
- [How to: stream runnables](/docs/how_to/streaming)
- [How to: invoke runnables in parallel](/docs/how_to/parallel/)
- [How to: attach runtime arguments to a runnable](/docs/how_to/binding/)
- [How to: run custom functions](/docs/how_to/functions)
- [How to: pass through arguments from one step to the next](/docs/how_to/passthrough)
- [How to: add values to a chain's state](/docs/how_to/assign)
- [How to: add message history](/docs/how_to/message_history)
- [How to: route execution within a chain](/docs/how_to/routing)
- [How to: add fallbacks](/docs/how_to/fallbacks)
- [How to: cancel execution](/docs/how_to/cancel_execution/)

## Components

These are the core building blocks you can use when building applications.

### Prompt templates

[Prompt Templates](/docs/concepts/prompt_templates) are responsible for formatting user input into a format that can be passed to a language model.

- [How to: use few shot examples](/docs/how_to/few_shot_examples)
- [How to: use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)
- [How to: partially format prompt templates](/docs/how_to/prompts_partial)
- [How to: compose prompts together](/docs/how_to/prompts_composition)

### Example selectors

[Example Selectors](/docs/concepts/example_selectors) are responsible for selecting the correct few shot examples to pass to the prompt.

- [How to: use example selectors](/docs/how_to/example_selectors)
- [How to: select examples by length](/docs/how_to/example_selectors_length_based)
- [How to: select examples by semantic similarity](/docs/how_to/example_selectors_similarity)
- [How to: select examples from LangSmith few-shot datasets](/docs/how_to/example_selectors_langsmith)

### Chat models

[Chat Models](/docs/concepts/chat_models) are newer forms of language models that take messages in and output a message.

- [How to: do function/tool calling](/docs/how_to/tool_calling)
- [How to: get models to return structured output](/docs/how_to/structured_output)
- [How to: cache model responses](/docs/how_to/chat_model_caching)
- [How to: create a custom chat model class](/docs/how_to/custom_chat)
- [How to: get log probabilities](/docs/how_to/logprobs)
- [How to: stream a response back](/docs/how_to/chat_streaming)
- [How to: track token usage](/docs/how_to/chat_token_usage_tracking)
- [How to: pass tool outputs to chat models](/docs/how_to/tool_results_pass_to_model/)
- [How to: stream tool calls](/docs/how_to/tool_streaming)
- [How to: few shot prompt tool behavior](/docs/how_to/tools_few_shot)
- [How to: force a specific tool call](/docs/how_to/tool_choice)
- [How to: disable parallel tool calling](/docs/how_to/tool_calling_parallel/)
- [How to: init any model in one line](/docs/how_to/chat_models_universal_init/)

### Messages

[Messages](/docs/concepts/##message-types) are the input and output of chat models. They have some `content` and a `role`, which describes the source of the message.

- [How to: trim messages](/docs/how_to/trim_messages/)
- [How to: filter messages](/docs/how_to/filter_messages/)
- [How to: merge consecutive messages of the same type](/docs/how_to/merge_message_runs/)

### LLMs

What LangChain calls [LLMs](/docs/concepts/text_llms) are older forms of language models that take a string in and output a string.

- [How to: cache model responses](/docs/how_to/llm_caching)
- [How to: create a custom LLM class](/docs/how_to/custom_llm)
- [How to: stream a response back](/docs/how_to/streaming_llm)
- [How to: track token usage](/docs/how_to/llm_token_usage_tracking)

### Output parsers

[Output Parsers](/docs/concepts/output_parsers) are responsible for taking the output of an LLM and parsing into more structured format.

- [How to: use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured)
- [How to: parse JSON output](/docs/how_to/output_parser_json)
- [How to: parse XML output](/docs/how_to/output_parser_xml)
- [How to: try to fix errors in output parsing](/docs/how_to/output_parser_fixing/)

### Document loaders

[Document Loaders](/docs/concepts/document_loaders) are responsible for loading documents from a variety of sources.

- [How to: load CSV data](/docs/how_to/document_loader_csv)
- [How to: load data from a directory](/docs/how_to/document_loader_directory)
- [How to: load PDF files](/docs/how_to/document_loader_pdf)
- [How to: write a custom document loader](/docs/how_to/document_loader_custom)
- [How to: load HTML data](/docs/how_to/document_loader_html)
- [How to: load Markdown data](/docs/how_to/document_loader_markdown)

### Text splitters

[Text Splitters](/docs/concepts/text_splitters) take a document and split into chunks that can be used for retrieval.

- [How to: recursively split text](/docs/how_to/recursive_text_splitter)
- [How to: split by character](/docs/how_to/character_text_splitter)
- [How to: split code](/docs/how_to/code_splitter)
- [How to: split by tokens](/docs/how_to/split_by_token)

### Embedding models

[Embedding Models](/docs/concepts/embedding_models) take a piece of text and create a numerical representation of it.

- [How to: embed text data](/docs/how_to/embed_text)
- [How to: cache embedding results](/docs/how_to/caching_embeddings)

### Vector stores

[Vector stores](/docs/concepts/#vectorstores) are databases that can efficiently store and retrieve embeddings.

- [How to: create and query vector stores](/docs/how_to/vectorstores)

### Retrievers

[Retrievers](/docs/concepts/retrievers) are responsible for taking a query and returning relevant documents.

- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)
- [How to: generate multiple queries to retrieve data for](/docs/how_to/multiple_queries)
- [How to: use contextual compression to compress the data retrieved](/docs/how_to/contextual_compression)
- [How to: write a custom retriever class](/docs/how_to/custom_retriever)
- [How to: combine the results from multiple retrievers](/docs/how_to/ensemble_retriever)
- [How to: generate multiple embeddings per document](/docs/how_to/multi_vector)
- [How to: retrieve the whole document for a chunk](/docs/how_to/parent_document_retriever)
- [How to: generate metadata filters](/docs/how_to/self_query)
- [How to: create a time-weighted retriever](/docs/how_to/time_weighted_vectorstore)
- [How to: reduce retrieval latency](/docs/how_to/reduce_retrieval_latency)

### Indexing

Indexing is the process of keeping your vectorstore in-sync with the underlying data source.

- [How to: reindex data to keep your vectorstore in-sync with the underlying data source](/docs/how_to/indexing)

### Tools

LangChain [Tools](/docs/concepts/tools) contain a description of the tool (to pass to the language model) as well as the implementation of the function to call.

- [How to: create tools](/docs/how_to/custom_tools)
- [How to: use built-in tools and toolkits](/docs/how_to/tools_builtin)
- [How to: use chat models to call tools](/docs/how_to/tool_calling/)
- [How to: pass tool outputs to chat models](/docs/how_to/tool_results_pass_to_model/)
- [How to: few shot prompt tool behavior](/docs/how_to/tools_few_shot)
- [How to: pass run time values to tools](/docs/how_to/tool_runtime)
- [How to: handle tool errors](/docs/how_to/tools_error)
- [How to: force a specific tool call](/docs/how_to/tool_choice/)
- [How to: disable parallel tool calling](/docs/how_to/tool_calling_parallel/)
- [How to: access the `RunnableConfig` object within a custom tool](/docs/how_to/tool_configure)
- [How to: stream events from child runs within a custom tool](/docs/how_to/tool_stream_events)
- [How to: return artifacts from a tool](/docs/how_to/tool_artifacts)
- [How to: convert Runnables to tools](/docs/how_to/convert_runnable_to_tool)
- [How to: add ad-hoc tool calling capability to models](/docs/how_to/tools_prompting)

### Agents

:::note

For in depth how-to guides for agents, please check out [LangGraph](https://langchain-ai.github.io/langgraphjs/) documentation.

:::

- [How to: use legacy LangChain Agents (AgentExecutor)](/docs/how_to/agent_executor)
- [How to: migrate from legacy LangChain agents to LangGraph](/docs/how_to/migrate_agent)

### Callbacks

[Callbacks](/docs/concepts/callbacks) allow you to hook into the various stages of your LLM application's execution.

- [How to: pass in callbacks at runtime](/docs/how_to/callbacks_runtime)
- [How to: attach callbacks to a module](/docs/how_to/callbacks_attach)
- [How to: pass callbacks into a module constructor](/docs/how_to/callbacks_constructor)
- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)
- [How to: await callbacks in serverless environments](/docs/how_to/callbacks_serverless)
- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events)

### Custom

All of LangChain components can easily be extended to support your own versions.

- [How to: create a custom chat model class](/docs/how_to/custom_chat)
- [How to: create a custom LLM class](/docs/how_to/custom_llm)
- [How to: write a custom retriever class](/docs/how_to/custom_retriever)
- [How to: write a custom document loader](/docs/how_to/document_loader_custom)
- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)
- [How to: define a custom tool](/docs/how_to/custom_tools)
- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events)

### Generative UI

- [How to: build an LLM generated UI](/docs/how_to/generative_ui)
- [How to: stream agentic data to the client](/docs/how_to/stream_agent_client)
- [How to: stream structured output to the client](/docs/how_to/stream_tool_client)

### Multimodal

- [How to: pass multimodal data directly to models](/docs/how_to/multimodal_inputs/)
- [How to: use multimodal prompts](/docs/how_to/multimodal_prompts/)
- [How to: call tools with multimodal data](/docs/how_to/tool_calls_multimodal/)

## Use cases

These guides cover use-case specific details.

### Q&A with RAG

Retrieval Augmented Generation (RAG) is a way to connect LLMs to external sources of data.
For a high-level tutorial on RAG, check out [this guide](/docs/tutorials/rag/).

- [How to: add chat history](/docs/how_to/qa_chat_history_how_to/)
- [How to: stream](/docs/how_to/qa_streaming/)
- [How to: return sources](/docs/how_to/qa_sources/)
- [How to: return citations](/docs/how_to/qa_citations/)
- [How to: do per-user retrieval](/docs/how_to/qa_per_user/)

### Extraction

Extraction is when you use LLMs to extract structured information from unstructured text.
For a high level tutorial on extraction, check out [this guide](/docs/tutorials/extraction/).

- [How to: use reference examples](/docs/how_to/extraction_examples/)
- [How to: handle long text](/docs/how_to/extraction_long_text/)
- [How to: do extraction without using function calling](/docs/how_to/extraction_parse)

### Chatbots

Chatbots involve using an LLM to have a conversation.
For a high-level tutorial on building chatbots, check out [this guide](/docs/tutorials/chatbot/).

- [How to: manage memory](/docs/how_to/chatbots_memory)
- [How to: do retrieval](/docs/how_to/chatbots_retrieval)
- [How to: use tools](/docs/how_to/chatbots_tools)

### Query analysis

Query Analysis is the task of using an LLM to generate a query to send to a retriever.
For a high-level tutorial on query analysis, check out [this guide](/docs/tutorials/rag#query-analysis).

- [How to: add examples to the prompt](/docs/how_to/query_few_shot)
- [How to: handle cases where no queries are generated](/docs/how_to/query_no_queries)
- [How to: handle multiple queries](/docs/how_to/query_multiple_queries)
- [How to: handle multiple retrievers](/docs/how_to/query_multiple_retrievers)
- [How to: construct filters](/docs/how_to/query_constructing_filters)
- [How to: deal with high cardinality categorical variables](/docs/how_to/query_high_cardinality)

### Q&A over SQL + CSV

You can use LLMs to do question answering over tabular data.
For a high-level tutorial, check out [this guide](/docs/tutorials/sql_qa/).

- [How to: use prompting to improve results](/docs/how_to/sql_prompting)
- [How to: do query validation](/docs/how_to/sql_query_checking)
- [How to: deal with large databases](/docs/how_to/sql_large_db)

### Q&A over graph databases

You can use an LLM to do question answering over graph databases.
For a high-level tutorial, check out [this guide](/docs/tutorials/graph/).

- [How to: map values to a database](/docs/how_to/graph_mapping)
- [How to: add a semantic layer over the database](/docs/how_to/graph_semantic)
- [How to: improve results with prompting](/docs/how_to/graph_prompting)
- [How to: construct knowledge graphs](/docs/how_to/graph_constructing)

## [LangGraph.js](https://langchain-ai.github.io/langgraphjs)

LangGraph.js is an extension of LangChain aimed at
building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.

LangGraph.js documentation is currently hosted on a separate site.
You can peruse [LangGraph.js how-to guides here](https://langchain-ai.github.io/langgraphjs/how-tos/).

## [LangSmith](https://docs.smith.langchain.com/)

LangSmith allows you to closely trace, monitor and evaluate your LLM application.
It seamlessly integrates with LangChain and LangGraph.js, and you can use it to inspect and debug individual steps of your chains as you build.

LangSmith documentation is hosted on a separate site.
You can peruse [LangSmith how-to guides here](https://docs.smith.langchain.com/how_to_guides/), but we'll highlight a few sections that are particularly
relevant to LangChain below:

### Evaluation

<span data-heading-keywords="evaluation,evaluate"></span>

Evaluating performance is a vital part of building LLM-powered applications.
LangSmith helps with every step of the process from creating a dataset to defining metrics to running evaluators.

To learn more, check out the [LangSmith evaluation how-to guides](https://docs.smith.langchain.com/how_to_guides#evaluation).

### Tracing

<span data-heading-keywords="trace,tracing"></span>

Tracing gives you observability inside your chains and agents, and is vital in diagnosing issues.

- [How to: trace with LangChain](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain)
- [How to: add metadata and tags to traces](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces)

You can see general tracing-related how-tos [in this section of the LangSmith docs](https://docs.smith.langchain.com/how_to_guides/tracing).



================================================
FILE: docs/core_docs/docs/how_to/indexing.mdx
================================================
# How to reindex data to keep your vectorstore in-sync with the underlying data source

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Retrieval-augmented generation (RAG)](/docs/tutorials/rag/)
- [Vector stores](/docs/concepts/#vectorstores)

:::

Here, we will look at a basic indexing workflow using the LangChain indexing API.

The indexing API lets you load and keep in sync documents from any source into a vector store. Specifically, it helps:

- Avoid writing duplicated content into the vector store
- Avoid re-writing unchanged content
- Avoid re-computing embeddings over unchanged content

All of which should save you time and money, as well as improve your vector search results.

Crucially, the indexing API will work even with documents that have gone through several transformation steps (e.g., via text chunking) with respect to the original source documents.

## How it works

LangChain indexing makes use of a record manager (`RecordManager`) that keeps track of document writes into the vector store.

When indexing content, hashes are computed for each document, and the following information is stored in the record manager:

- the document hash (hash of both page content and metadata)
- write time
- the source ID - each document should include information in its metadata to allow us to determine the ultimate source of this document

## Deletion Modes

When indexing documents into a vector store, it's possible that some existing documents in the vector store should be deleted.
In certain situations you may want to remove any existing documents that are derived from the same sources as the new documents being indexed.
In others you may want to delete all existing documents wholesale. The indexing API deletion modes let you pick the behavior you want:

| Cleanup Mode | De-Duplicates Content | Parallelizable | Cleans Up Deleted Source Docs | Cleans Up Mutations of Source Docs and/or Derived Docs | Clean Up Timing    |
| ------------ | --------------------- | -------------- | ----------------------------- | ------------------------------------------------------ | ------------------ |
| None         | ✅                    | ✅             | ❌                            | ❌                                                     | -                  |
| Incremental  | ✅                    | ✅             | ❌                            | ✅                                                     | Continuously       |
| Full         | ✅                    | ❌             | ✅                            | ✅                                                     | At end of indexing |

`None` does not do any automatic clean up, allowing the user to manually do clean up of old content.

`incremental` and `full` offer the following automated clean up:

- If the content of the source document or derived documents has changed, both `incremental` or `full` modes will clean up (delete) previous versions of the content.
- If the source document has been deleted (meaning it is not included in the documents currently being indexed), the full cleanup mode will delete it from the vector store correctly, but the `incremental` mode will not.

When content is mutated (e.g., the source PDF file was revised) there will be a period of time during indexing when both the new and old versions may be returned to the user. This happens after the new content was written, but before the old version was deleted.

- `incremental` indexing minimizes this period of time as it is able to do clean up continuously, as it writes.
- `full` mode does the clean up after all batches have been written.

## Requirements

1. Do not use with a store that has been pre-populated with content independently of the indexing API, as the record manager will not know that records have been inserted previously.
2. Only works with LangChain `vectorstore`'s that support:
   a). document addition by id (`addDocuments` method with ids argument)
   b). delete by id (delete method with ids argument)

Compatible Vectorstores: [`PGVector`](/docs/integrations/vectorstores/pgvector), [`Chroma`](/docs/integrations/vectorstores/chroma), [`CloudflareVectorize`](/docs/integrations/vectorstores/cloudflare_vectorize),
[`ElasticVectorSearch`](/docs/integrations/vectorstores/elasticsearch), [`FAISS`](/docs/integrations/vectorstores/faiss), [`MariaDB`](/docs/integrations/vectorstores/mariadb), [`MomentoVectorIndex`](/docs/integrations/vectorstores/momento_vector_index),
[`Pinecone`](/docs/integrations/vectorstores/pinecone), [`SupabaseVectorStore`](/docs/integrations/vectorstores/supabase), [`VercelPostgresVectorStore`](/docs/integrations/vectorstores/vercel_postgres),
[`Weaviate`](/docs/integrations/vectorstores/weaviate), [`Xata`](/docs/integrations/vectorstores/xata)

## Caution

The record manager relies on a time-based mechanism to determine what content can be cleaned up (when using `full` or `incremental` cleanup modes).

If two tasks run back-to-back, and the first task finishes before the clock time changes, then the second task may not be able to clean up content.

This is unlikely to be an issue in actual settings for the following reasons:

1. The `RecordManager` uses higher resolution timestamps.
2. The data would need to change between the first and the second tasks runs, which becomes unlikely if the time interval between the tasks is small.
3. Indexing tasks typically take more than a few ms.

## Quickstart

import CodeBlock from "@theme/CodeBlock";
import QuickStartExample from "@examples/indexes/indexing_api/indexing.ts";

<CodeBlock language="typescript">{QuickStartExample}</CodeBlock>

## Next steps

You've now learned how to use indexing in your RAG pipelines.

Next, check out some of the other sections on retrieval.



================================================
FILE: docs/core_docs/docs/how_to/installation.mdx
================================================
---
sidebar_position: 1
---

# Installation

## Supported Environments

LangChain is written in TypeScript and can be used in:

- Node.js (ESM and CommonJS) - 18.x, 19.x, 20.x
- Cloudflare Workers
- Vercel / Next.js (Browser, Serverless and Edge functions)
- Supabase Edge Functions
- Browser
- Deno
- Bun

However, note that individual integrations may not be supported in all environments.

## Installation

To install the main `langchain` package, run:

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import CodeBlock from "@theme/CodeBlock";

```bash npm2yarn
npm install langchain @langchain/core
```

While this package acts as a sane starting point to using LangChain,
much of the value of LangChain comes when integrating it with various model providers, datastores, etc.
By default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately.
We'll show how to do that in the next sections of this guide.

Please also see the section on [installing integration packages](/docs/how_to/installation/#installing-integration-packages)
for some special considerations when installing LangChain packages.

## Ecosystem packages

With the exception of the `langsmith` SDK, all packages in the LangChain ecosystem depend on `@langchain/core`, which contains base
classes and abstractions that other packages use. The dependency graph below shows how the difference packages are related.
A directed arrow indicates that the source package depends on the target package:

![](/img/ecosystem_packages.png)

**Note:** It is important that your app only uses one version of `@langchain/core`. Common package managers may introduce additional versions
when resolving direct dependencies, even if you don't intend this. See [this section on installing integration packages](/docs/how_to/installation/#installing-integration-packages)
for more information and ways to remedy this.

### @langchain/community

The [@langchain/community](https://www.npmjs.com/package/@langchain/community) package contains a range of third-party integrations.
Install with:

```bash npm2yarn
npm install @langchain/community @langchain/core
```

There are also more granular packages containing LangChain integrations for individual providers.

### @langchain/core

The [@langchain/core](https://www.npmjs.com/package/@langchain/core) package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language.
It should be installed separately:

```bash npm2yarn
npm install @langchain/core
```

### LangGraph

[LangGraph.js](https://langchain-ai.github.io/langgraphjs/) is a library for building stateful, multi-actor applications with LLMs.
It integrates smoothly with LangChain, but can be used without it.

Install with:

```bash npm2yarn
npm install @langchain/langgraph @langchain/core
```

### LangSmith SDK

The LangSmith SDK is automatically installed by LangChain.
If you're not using it with LangChain, install with:

```bash npm2yarn
npm install langsmith
```

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

## Installing integration packages

LangChain supports packages that contain module integrations with individual third-party providers.
They can be as specific as [`@langchain/anthropic`](/docs/integrations/platforms/anthropic/), which contains integrations just for Anthropic models,
or as broad as [`@langchain/community`](https://www.npmjs.com/package/@langchain/community), which contains broader variety of community contributed integrations.

These packages, as well as the main LangChain package, all have [`@langchain/core`](https://www.npmjs.com/package/@langchain/core) as a peer dependency
to avoid package managers installing multiple versions of the same package. It contains the base abstractions that these integration packages extend.

To ensure that all integrations and their types interact with each other properly, it is important that they all use the same version of `@langchain/core`.
If you encounter type errors around base classes, you may need to guarantee that your package manager is resolving a single version of `@langchain/core`. To do so,
you can add a `"resolutions"` or `"overrides"` field like the following in your project's `package.json`. The name will depend on your package manager:

:::tip
The `resolutions` or `pnpm.overrides` fields for `yarn` or `pnpm` must be set in the root `package.json` file.
:::

If you are using `yarn`:

```json title="yarn package.json"
{
  "name": "your-project",
  "version": "0.0.0",
  "private": true,
  "engines": {
    "node": ">=18"
  },
  "dependencies": {
    "@langchain/anthropic": "^0.0.2",
    "@langchain/core": "^0.3.0",
    "langchain": "0.0.207"
  },
  "resolutions": {
    "@langchain/core": "0.3.0"
  }
}
```

You can also try running the [`yarn dedupe`](https://yarnpkg.com/cli/dedupe) command if you are on `yarn` version 2 or higher.

Or for `npm`:

```json title="npm package.json"
{
  "name": "your-project",
  "version": "0.0.0",
  "private": true,
  "engines": {
    "node": ">=18"
  },
  "dependencies": {
    "@langchain/anthropic": "^0.0.2",
    "@langchain/core": "^0.3.0",
    "langchain": "0.0.207"
  },
  "overrides": {
    "@langchain/core": "0.3.0"
  }
}
```

You can also try the [`npm dedupe`](https://docs.npmjs.com/cli/commands/npm-dedupe) command.

Or for `pnpm`:

```json title="pnpm package.json"
{
  "name": "your-project",
  "version": "0.0.0",
  "private": true,
  "engines": {
    "node": ">=18"
  },
  "dependencies": {
    "@langchain/anthropic": "^0.0.2",
    "@langchain/core": "^0.3.0",
    "langchain": "0.0.207"
  },
  "pnpm": {
    "overrides": {
      "@langchain/core": "0.3.0"
    }
  }
}
```

You can also try the [`pnpm dedupe`](https://pnpm.io/cli/dedupe) command.

## Loading the library

### TypeScript

LangChain is written in TypeScript and provides type definitions for all of its public APIs.

### ESM

LangChain provides an ESM build targeting Node.js environments. You can import it using the following syntax:

```bash npm2yarn
npm install @langchain/openai @langchain/core
```

```typescript
import { ChatOpenAI } from "@langchain/openai";
```

If you are using TypeScript in an ESM project we suggest updating your `tsconfig.json` to include the following:

```json title="tsconfig.json"
{
  "compilerOptions": {
    ...
    "target": "ES2020", // or higher
    "module": "nodenext",
  }
}
```

### CommonJS

LangChain provides a CommonJS build targeting Node.js environments. You can import it using the following syntax:

```typescript
const { ChatOpenAI } = require("@langchain/openai");
```

### Cloudflare Workers

LangChain can be used in Cloudflare Workers. You can import it using the following syntax:

```typescript
import { ChatOpenAI } from "@langchain/openai";
```

### Vercel / Next.js

LangChain can be used in Vercel / Next.js. We support using LangChain in frontend components, in Serverless functions and in Edge functions. You can import it using the following syntax:

```typescript
import { ChatOpenAI } from "@langchain/openai";
```

### Deno / Supabase Edge Functions

LangChain can be used in Deno / Supabase Edge Functions. You can import it using the following syntax:

```typescript
import { ChatOpenAI } from "https://esm.sh/@langchain/openai";
```

or

```typescript
import { ChatOpenAI } from "npm:@langchain/openai";
```

### Browser

LangChain can be used in the browser. In our CI we test bundling LangChain with Webpack and Vite, but other bundlers should work too. You can import it using the following syntax:

```typescript
import { ChatOpenAI } from "@langchain/openai";
```

## Unsupported: Node.js 16

We do not support Node.js 16, but if you still want to run LangChain on Node.js 16, you will need to follow the instructions in this section. We do not guarantee that these instructions will continue to work in the future.

You will have to make `fetch` available globally, either:

- run your application with `NODE_OPTIONS='--experimental-fetch' node ...`, or
- install `node-fetch` and follow the instructions [here](https://github.com/node-fetch/node-fetch#providing-global-access)

You'll also need to [polyfill `ReadableStream`](https://www.npmjs.com/package/web-streams-polyfill) by installing:

```bash npm2yarn
npm i web-streams-polyfill@4
```

And then adding it to the global namespace in your main entrypoint:

```typescript
import "web-streams-polyfill/polyfill";
```

Additionally you'll have to polyfill `structuredClone`, eg. by installing `core-js` and following the instructions [here](https://github.com/zloirock/core-js).

If you are running Node.js 18+, you do not need to do anything.



================================================
FILE: docs/core_docs/docs/how_to/lcel_cheatsheet.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# LangChain Expression Language Cheatsheet

This is a quick reference for all the most important LCEL primitives. For more advanced usage see the [LCEL how-to guides](/docs/how_to/#langchain-expression-language-lcel) and the [full API reference](https://api.js.langchain.com/classes/langchain_core.runnables.Runnable.html).

### Invoke a runnable
#### [runnable.invoke()](https://api.js.langchain.com/classes/langchain_core.runnables.Runnable.html#invoke)
"""

import { RunnableLambda } from "@langchain/core/runnables";

const runnable = RunnableLambda.from((x: number) => x.toString());

await runnable.invoke(5);
# Output:
#   [32m"5"[39m

"""
### Batch a runnable
#### [runnable.batch()](hhttps://api.js.langchain.com/classes/langchain_core.runnables.Runnable.html#batch)
"""

import { RunnableLambda } from "@langchain/core/runnables";

const runnable = RunnableLambda.from((x: number) => x.toString());

await runnable.batch([7, 8, 9]);
# Output:
#   [ [32m"7"[39m, [32m"8"[39m, [32m"9"[39m ]

"""
### Stream a runnable
#### [runnable.stream()](https://api.js.langchain.com/classes/langchain_core.runnables.Runnable.html#stream)
"""

import { RunnableLambda } from "@langchain/core/runnables";

async function* generatorFn(x: number[]) {
  for (const i of x) {
    yield i.toString();
  }
}

const runnable = RunnableLambda.from(generatorFn);

const stream = await runnable.stream([0, 1, 2, 3, 4]);

for await (const chunk of stream) {
  console.log(chunk);
  console.log("---")
}
# Output:
#   0

#   ---

#   1

#   ---

#   2

#   ---

#   3

#   ---

#   4

#   ---


"""
### Compose runnables
#### [runnable.pipe()](https://api.js.langchain.com/classes/langchain_core.runnables.Runnable.html#pipe)
"""

import { RunnableLambda } from "@langchain/core/runnables";

const runnable1 = RunnableLambda.from((x: any) => {
  return { foo: x };
});

const runnable2 = RunnableLambda.from((x: any) => [x].concat([x]));

const chain = runnable1.pipe(runnable2);

await chain.invoke(2);
# Output:
#   [ { foo: [33m2[39m }, { foo: [33m2[39m } ]

"""
#### [RunnableSequence.from()](https://api.js.langchain.com/classes/langchain_core.runnables.RunnableSequence.html#from)
"""

import { RunnableLambda, RunnableSequence } from "@langchain/core/runnables";

const runnable1 = RunnableLambda.from((x: any) => {
  return { foo: x };
});

const runnable2 = RunnableLambda.from((x: any) => [x].concat([x]));

const chain = RunnableSequence.from([
  runnable1,
  runnable2,
]);

await chain.invoke(2);
# Output:
#   [ { foo: [33m2[39m }, { foo: [33m2[39m } ]

"""
### Invoke runnables in parallel
#### [RunnableParallel](https://api.js.langchain.com/classes/langchain_core.runnables.RunnableParallel.html)
"""

import { RunnableLambda, RunnableParallel } from "@langchain/core/runnables";

const runnable1 = RunnableLambda.from((x: any) => {
  return { foo: x };
});

const runnable2 = RunnableLambda.from((x: any) => [x].concat([x]));

const chain = RunnableParallel.from({
  first: runnable1,
  second: runnable2,
});

await chain.invoke(2);
# Output:
#   { first: { foo: [33m2[39m }, second: [ [33m2[39m, [33m2[39m ] }

"""
### Turn a function into a runnable
#### [RunnableLambda](https://api.js.langchain.com/classes/langchain_core.runnables.RunnableLambda.html)
"""

import { RunnableLambda } from "@langchain/core/runnables";

const adder = (x: number) => {
  return x + 5;
};

const runnable = RunnableLambda.from(adder);

await runnable.invoke(5);
# Output:
#   [33m10[39m

"""
### Merge input and output dicts
#### [RunnablePassthrough.assign()](https://api.js.langchain.com/classes/langchain_core.runnables.RunnablePassthrough.html#assign)
"""

import { RunnableLambda, RunnablePassthrough } from "@langchain/core/runnables";

const runnable = RunnableLambda.from((x: { foo: number }) => {
  return x.foo + 7;
});

const chain = RunnablePassthrough.assign({
  bar: runnable,
});

await chain.invoke({ foo: 10 });
# Output:
#   { foo: [33m10[39m, bar: [33m17[39m }

"""
### Include input dict in output dict

#### [RunnablePassthrough](https://api.js.langchain.com/classes/langchain_core.runnables.RunnablePassthrough.html)
"""

import {
  RunnableLambda,
  RunnableParallel,
  RunnablePassthrough
} from "@langchain/core/runnables";

const runnable = RunnableLambda.from((x: { foo: number }) => {
  return x.foo + 7;
});

const chain = RunnableParallel.from({
  bar: runnable,
  baz: new RunnablePassthrough(),
});

await chain.invoke({ foo: 10 });
# Output:
#   { baz: { foo: [33m10[39m }, bar: [33m17[39m }

"""
### Add default invocation args

#### [runnable.bind()](https://api.js.langchain.com/classes/langchain_core.runnables.Runnable.html#bind)
"""

import { type RunnableConfig, RunnableLambda } from "@langchain/core/runnables";

const branchedFn = (mainArg: Record<string, any>, config?: RunnableConfig) => {
  if (config?.configurable?.boundKey !== undefined) {
    return { ...mainArg, boundKey: config?.configurable?.boundKey };
  }
  return mainArg;
}

const runnable = RunnableLambda.from(branchedFn);
const boundRunnable = runnable.bind({ configurable: { boundKey: "goodbye!" } });

await boundRunnable.invoke({ bar: "hello" });
# Output:
#   { bar: [32m"hello"[39m, boundKey: [32m"goodbye!"[39m }

"""
### Add fallbacks

#### [runnable.withFallbacks()](https://api.js.langchain.com/classes/langchain_core.runnables.Runnable.html#withFallbacks)
"""

import { RunnableLambda } from "@langchain/core/runnables";

const runnable = RunnableLambda.from((x: any) => {
  throw new Error("Error case")
});

const fallback = RunnableLambda.from((x: any) => x + x);

const chain = runnable.withFallbacks([fallback]);

await chain.invoke("foo");
# Output:
#   [32m"foofoo"[39m

"""
### Add retries
#### [runnable.withRetry()](https://api.js.langchain.com/classes/langchain_core.runnables.Runnable.html#withRetry)
"""

import { RunnableLambda } from "@langchain/core/runnables";

let counter = 0;

const retryFn = (_: any) => {
  counter++;
  console.log(`attempt with counter ${counter}`);
  throw new Error("Expected error");
};

const chain = RunnableLambda.from(retryFn).withRetry({
  stopAfterAttempt: 2,
});

await chain.invoke(2);
# Output:
#   attempt with counter 1

#   attempt with counter 2

#   Error: Error: Expected error

"""
### Configure runnable execution

#### [RunnableConfig](https://api.js.langchain.com/interfaces/langchain_core.runnables.RunnableConfig.html)
"""

import { RunnableLambda } from "@langchain/core/runnables";

const runnable1 = RunnableLambda.from(async (x: any) => {
  await new Promise((resolve) => setTimeout(resolve, 2000));
  return { foo: x };
});

// Takes 4 seconds
await runnable1.batch([1, 2, 3], { maxConcurrency: 2 });
# Output:
#   [ { foo: [33m1[39m }, { foo: [33m2[39m }, { foo: [33m3[39m } ]

"""
### Add default config to runnable

#### [runnable.withConfig()](https://api.js.langchain.com/classes/langchain_core.runnables.Runnable.html#withConfig)
"""

import { RunnableLambda } from "@langchain/core/runnables";

const runnable1 = RunnableLambda.from(async (x: any) => {
  await new Promise((resolve) => setTimeout(resolve, 2000));
  return { foo: x };
}).withConfig({
  maxConcurrency: 2,
});

// Takes 4 seconds
await runnable1.batch([1, 2, 3]);
# Output:
#   [ { foo: [33m1[39m }, { foo: [33m2[39m }, { foo: [33m3[39m } ]

"""
### Build a chain dynamically based on input
"""

import { RunnableLambda } from "@langchain/core/runnables";

const runnable1 = RunnableLambda.from((x: any) => {
  return { foo: x };
});

const runnable2 = RunnableLambda.from((x: any) => [x].concat([x]));

const chain = RunnableLambda.from((x: number): any => {
  if (x > 6) {
    return runnable1;
  }
  return runnable2;
});

await chain.invoke(7);
# Output:
#   { foo: [33m7[39m }

await chain.invoke(5);
# Output:
#   [ [33m5[39m, [33m5[39m ]

"""
### Generate a stream of internal events
#### [runnable.streamEvents()](https://api.js.langchain.com/classes/langchain_core.runnables.Runnable.html#streamEvents)
"""

import { RunnableLambda } from "@langchain/core/runnables";

const runnable1 = RunnableLambda.from((x: number) => {
  return {
    foo: x,
  };
}).withConfig({
  runName: "first",
});

async function* generatorFn(x: { foo: number }) {
  for (let i = 0; i < x.foo; i++) {
    yield i.toString();
  }
}

const runnable2 = RunnableLambda.from(generatorFn).withConfig({
  runName: "second",
});

const chain = runnable1.pipe(runnable2);

for await (const event of chain.streamEvents(2, { version: "v1" })) {
  console.log(`event=${event.event} | name=${event.name} | data=${JSON.stringify(event.data)}`);
}
# Output:
#   event=on_chain_start | name=RunnableSequence | data={"input":2}

#   event=on_chain_start | name=first | data={}

#   event=on_chain_stream | name=first | data={"chunk":{"foo":2}}

#   event=on_chain_start | name=second | data={}

#   event=on_chain_end | name=first | data={"input":2,"output":{"foo":2}}

#   event=on_chain_stream | name=second | data={"chunk":"0"}

#   event=on_chain_stream | name=RunnableSequence | data={"chunk":"0"}

#   event=on_chain_stream | name=second | data={"chunk":"1"}

#   event=on_chain_stream | name=RunnableSequence | data={"chunk":"1"}

#   event=on_chain_end | name=second | data={"output":"01"}

#   event=on_chain_end | name=RunnableSequence | data={"output":"01"}


"""
### Return a subset of keys from output object

#### [runnable.pick()](https://api.js.langchain.com/classes/langchain_core.runnables.Runnable.html#pick)
"""

import { RunnableLambda, RunnablePassthrough } from "@langchain/core/runnables";

const runnable = RunnableLambda.from((x: { baz: number }) => {
  return x.baz + 5;
});

const chain = RunnablePassthrough.assign({
  foo: runnable,
}).pick(["foo", "bar"]);

await chain.invoke({"bar": "hi", "baz": 2});
# Output:
#   { foo: [33m7[39m, bar: [32m"hi"[39m }

"""
### Declaratively make a batched version of a runnable

#### [`runnable.map()`](https://api.js.langchain.com/classes/langchain_core.runnables.Runnable.html#map)
"""

import { RunnableLambda } from "@langchain/core/runnables";

const runnable1 = RunnableLambda.from((x: number) => [...Array(x).keys()]);
const runnable2 = RunnableLambda.from((x: number) => x + 5);

const chain = runnable1.pipe(runnable2.map());

await chain.invoke(3);
# Output:
#   [ [33m5[39m, [33m6[39m, [33m7[39m ]

"""
### Get a graph representation of a runnable

#### [runnable.getGraph()](https://api.js.langchain.com/classes/langchain_core.runnables.Runnable.html#getGraph)
"""

import { RunnableLambda, RunnableSequence } from "@langchain/core/runnables";

const runnable1 = RunnableLambda.from((x: any) => {
  return { foo: x };
});

const runnable2 = RunnableLambda.from((x: any) => [x].concat([x]));

const runnable3 = RunnableLambda.from((x: any) => x.toString());

const chain = RunnableSequence.from([
  runnable1,
  {
    second: runnable2,
    third: runnable3,
  }
]);

await chain.getGraph();
# Output:
#   Graph {

#     nodes: {

#       [32m"935c67df-7ae3-4853-9d26-579003c08407"[39m: {

#         id: [32m"935c67df-7ae3-4853-9d26-579003c08407"[39m,

#         data: {

#           name: [32m"RunnableLambdaInput"[39m,

#           schema: ZodAny {

#             spa: [36m[Function: bound safeParseAsync] AsyncFunction[39m,

#             _def: [36m[Object][39m,

#             parse: [36m[Function: bound parse][39m,

#             safeParse: [36m[Function: bound safeParse][39m,

#             parseAsync: [36m[Function: bound parseAsync] AsyncFunction[39m,

#             safeParseAsync: [36m[Function: bound safeParseAsync] AsyncFunction[39m,

#             refine: [36m[Function: bound refine][39m,

#             refinement: [36m[Function: bound refinement][39m,

#             superRefine: [36m[Function: bound superRefine][39m,

#             optional: [36m[Function: bound optional][39m,

#             nullable: [36m[Function: bound nullable][39m,

#             nullish: [36m[Function: bound nullish][39m,

#             array: [36m[Function: bound array][39m,

#             promise: [36m[Function: bound promise][39m,

#             or: [36m[Function: bound or][39m,

#             and: [36m[Function: bound and][39m,

#             transform: [36m[Function: bound transform][39m,

#             brand: [36m[Function: bound brand][39m,

#             default: [36m[Function: bound default][39m,

#             catch: [36m[Function: bound catch][39m,

#             describe: [36m[Function: bound describe][39m,

#             pipe: [36m[Function: bound pipe][39m,

#             readonly: [36m[Function: bound readonly][39m,

#             isNullable: [36m[Function: bound isNullable][39m,

#             isOptional: [36m[Function: bound isOptional][39m,

#             _any: [33mtrue[39m

#           }

#         }

#       },

#       [32m"a73d7b3e-0ed7-46cf-b141-de64ea1e12de"[39m: {

#         id: [32m"a73d7b3e-0ed7-46cf-b141-de64ea1e12de"[39m,

#         data: RunnableLambda {

#           lc_serializable: [33mfalse[39m,

#           lc_kwargs: { func: [36m[Function (anonymous)][39m },

#           lc_runnable: [33mtrue[39m,

#           name: [90mundefined[39m,

#           lc_namespace: [ [32m"langchain_core"[39m, [32m"runnables"[39m ],

#           func: [36m[Function (anonymous)][39m

#         }

#       },

#       [32m"ff104b34-c13b-4677-8b82-af70d3548e12"[39m: {

#         id: [32m"ff104b34-c13b-4677-8b82-af70d3548e12"[39m,

#         data: RunnableMap {

#           lc_serializable: [33mtrue[39m,

#           lc_kwargs: { steps: [36m[Object][39m },

#           lc_runnable: [33mtrue[39m,

#           name: [90mundefined[39m,

#           lc_namespace: [ [32m"langchain_core"[39m, [32m"runnables"[39m ],

#           steps: { second: [36m[RunnableLambda][39m, third: [36m[RunnableLambda][39m }

#         }

#       },

#       [32m"2dc627dc-1c06-45b1-b14f-bb1f6e689f83"[39m: {

#         id: [32m"2dc627dc-1c06-45b1-b14f-bb1f6e689f83"[39m,

#         data: {

#           name: [32m"RunnableMapOutput"[39m,

#           schema: ZodAny {

#             spa: [36m[Function: bound safeParseAsync] AsyncFunction[39m,

#             _def: [36m[Object][39m,

#             parse: [36m[Function: bound parse][39m,

#             safeParse: [36m[Function: bound safeParse][39m,

#             parseAsync: [36m[Function: bound parseAsync] AsyncFunction[39m,

#             safeParseAsync: [36m[Function: bound safeParseAsync] AsyncFunction[39m,

#             refine: [36m[Function: bound refine][39m,

#             refinement: [36m[Function: bound refinement][39m,

#             superRefine: [36m[Function: bound superRefine][39m,

#             optional: [36m[Function: bound optional][39m,

#             nullable: [36m[Function: bound nullable][39m,

#             nullish: [36m[Function: bound nullish][39m,

#             array: [36m[Function: bound array][39m,

#             promise: [36m[Function: bound promise][39m,

#             or: [36m[Function: bound or][39m,

#             and: [36m[Function: bound and][39m,

#             transform: [36m[Function: bound transform][39m,

#             brand: [36m[Function: bound brand][39m,

#             default: [36m[Function: bound default][39m,

#             catch: [36m[Function: bound catch][39m,

#             describe: [36m[Function: bound describe][39m,

#             pipe: [36m[Function: bound pipe][39m,

#             readonly: [36m[Function: bound readonly][39m,

#             isNullable: [36m[Function: bound isNullable][39m,

#             isOptional: [36m[Function: bound isOptional][39m,

#             _any: [33mtrue[39m

#           }

#         }

#       }

#     },

#     edges: [

#       {

#         source: [32m"935c67df-7ae3-4853-9d26-579003c08407"[39m,

#         target: [32m"a73d7b3e-0ed7-46cf-b141-de64ea1e12de"[39m,

#         data: [90mundefined[39m

#       },

#       {

#         source: [32m"ff104b34-c13b-4677-8b82-af70d3548e12"[39m,

#         target: [32m"2dc627dc-1c06-45b1-b14f-bb1f6e689f83"[39m,

#         data: [90mundefined[39m

#       },

#       {

#         source: [32m"a73d7b3e-0ed7-46cf-b141-de64ea1e12de"[39m,

#         target: [32m"ff104b34-c13b-4677-8b82-af70d3548e12"[39m,

#         data: [90mundefined[39m

#       }

#     ]

#   }



================================================
FILE: docs/core_docs/docs/how_to/llm_caching.mdx
================================================
---
sidebar_position: 2
---

# How to cache model responses

LangChain provides an optional caching layer for LLMs. This is useful for two reasons:

It can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times.
It can speed up your application by reducing the number of API calls you make to the LLM provider.

import CodeBlock from "@theme/CodeBlock";

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core
```

```typescript
import { OpenAI } from "@langchain/openai";

const model = new OpenAI({
  model: "gpt-3.5-turbo-instruct",
  cache: true,
});
```

## In Memory Cache

The default cache is stored in-memory. This means that if you restart your application, the cache will be cleared.

```typescript
console.time();

// The first time, it is not yet in cache, so it should take longer
const res = await model.invoke("Tell me a long joke");

console.log(res);

console.timeEnd();

/*
  A man walks into a bar and sees a jar filled with money on the counter. Curious, he asks the bartender about it.

  The bartender explains, "We have a challenge for our customers. If you can complete three tasks, you win all the money in the jar."

  Intrigued, the man asks what the tasks are.

  The bartender replies, "First, you have to drink a whole bottle of tequila without making a face. Second, there's a pitbull out back with a sore tooth. You have to pull it out. And third, there's an old lady upstairs who has never had an orgasm. You have to give her one."

  The man thinks for a moment and then confidently says, "I'll do it."

  He grabs the bottle of tequila and downs it in one gulp, without flinching. He then heads to the back and after a few minutes of struggling, emerges with the pitbull's tooth in hand.

  The bar erupts in cheers and the bartender leads the man upstairs to the old lady's room. After a few minutes, the man walks out with a big smile on his face and the old lady is giggling with delight.

  The bartender hands the man the jar of money and asks, "How

  default: 4.187s
*/
```

```typescript
console.time();

// The second time it is, so it goes faster
const res2 = await model.invoke("Tell me a joke");

console.log(res2);

console.timeEnd();

/*
  A man walks into a bar and sees a jar filled with money on the counter. Curious, he asks the bartender about it.

  The bartender explains, "We have a challenge for our customers. If you can complete three tasks, you win all the money in the jar."

  Intrigued, the man asks what the tasks are.

  The bartender replies, "First, you have to drink a whole bottle of tequila without making a face. Second, there's a pitbull out back with a sore tooth. You have to pull it out. And third, there's an old lady upstairs who has never had an orgasm. You have to give her one."

  The man thinks for a moment and then confidently says, "I'll do it."

  He grabs the bottle of tequila and downs it in one gulp, without flinching. He then heads to the back and after a few minutes of struggling, emerges with the pitbull's tooth in hand.

  The bar erupts in cheers and the bartender leads the man upstairs to the old lady's room. After a few minutes, the man walks out with a big smile on his face and the old lady is giggling with delight.

  The bartender hands the man the jar of money and asks, "How

  default: 175.74ms
*/
```

## Caching with Momento

LangChain also provides a Momento-based cache. [Momento](https://gomomento.com) is a distributed, serverless cache that requires zero setup or infrastructure maintenance. Given Momento's compatibility with Node.js, browser, and edge environments, ensure you install the relevant package.

To install for **Node.js**:

```bash npm2yarn
npm install @gomomento/sdk
```

To install for **browser/edge workers**:

```bash npm2yarn
npm install @gomomento/sdk-web
```

Next you'll need to sign up and create an API key. Once you've done that, pass a `cache` option when you instantiate the LLM like this:

import MomentoCacheExample from "@examples/cache/momento.ts";

<CodeBlock language="typescript">{MomentoCacheExample}</CodeBlock>

## Caching with Redis

LangChain also provides a Redis-based cache. This is useful if you want to share the cache across multiple processes or servers. To use it, you'll need to install the `redis` package:

```bash npm2yarn
npm install ioredis
```

Then, you can pass a `cache` option when you instantiate the LLM. For example:

```typescript
import { OpenAI } from "@langchain/openai";
import { RedisCache } from "@langchain/community/caches/ioredis";
import { Redis } from "ioredis";

// See https://github.com/redis/ioredis for connection options
const client = new Redis({});

const cache = new RedisCache(client);

const model = new OpenAI({ cache });
```

## Caching with Upstash Redis

LangChain provides an Upstash Redis-based cache. Like the Redis-based cache, this cache is useful if you want to share the cache across multiple processes or servers. The Upstash Redis client uses HTTP and supports edge environments. To use it, you'll need to install the `@upstash/redis` package:

```bash npm2yarn
npm install @upstash/redis
```

You'll also need an [Upstash account](https://docs.upstash.com/redis#create-account) and a [Redis database](https://docs.upstash.com/redis#create-a-database) to connect to. Once you've done that, retrieve your REST URL and REST token.

Then, you can pass a `cache` option when you instantiate the LLM. For example:

import UpstashRedisCacheExample from "@examples/cache/upstash_redis.ts";

<CodeBlock language="typescript">{UpstashRedisCacheExample}</CodeBlock>

You can also directly pass in a previously created [@upstash/redis](https://docs.upstash.com/redis/sdks/javascriptsdk/overview) client instance:

import AdvancedUpstashRedisCacheExample from "@examples/cache/upstash_redis_advanced.ts";

<CodeBlock language="typescript">{AdvancedUpstashRedisCacheExample}</CodeBlock>

## Caching with Vercel KV

LangChain provides an Vercel KV-based cache. Like the Redis-based cache, this cache is useful if you want to share the cache across multiple processes or servers. The Vercel KV client uses HTTP and supports edge environments. To use it, you'll need to install the `@vercel/kv` package:

```bash npm2yarn
npm install @vercel/kv
```

You'll also need an Vercel account and a [KV database](https://vercel.com/docs/storage/vercel-kv/kv-reference) to connect to. Once you've done that, retrieve your REST URL and REST token.

Then, you can pass a `cache` option when you instantiate the LLM. For example:

import VercelKVCacheExample from "@examples/cache/vercel_kv.ts";

<CodeBlock language="typescript">{VercelKVCacheExample}</CodeBlock>

## Caching with Cloudflare KV

:::info
This integration is only supported in Cloudflare Workers.
:::

If you're deploying your project as a Cloudflare Worker, you can use LangChain's Cloudflare KV-powered LLM cache.

For information on how to set up KV in Cloudflare, see [the official documentation](https://developers.cloudflare.com/kv/).

**Note:** If you are using TypeScript, you may need to install types if they aren't already present:

```bash npm2yarn
npm install -S @cloudflare/workers-types
```

import CloudflareExample from "@examples/cache/cloudflare_kv.ts";

<CodeBlock language="typescript">{CloudflareExample}</CodeBlock>

## Caching on the File System

:::warning
This cache is not recommended for production use. It is only intended for local development.
:::

LangChain provides a simple file system cache.
By default the cache is stored a temporary directory, but you can specify a custom directory if you want.

```typescript
const cache = await LocalFileCache.create();
```

## Next steps

You've now learned how to cache model responses to save time and money.

Next, check out the other how-to guides on LLMs, like [how to create your own custom LLM class](/docs/how_to/custom_llm).



================================================
FILE: docs/core_docs/docs/how_to/llm_token_usage_tracking.mdx
================================================
---
sidebar_position: 5
---

# How to track token usage

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [LLMs](/docs/concepts/text_llms)

:::

This notebook goes over how to track your token usage for specific LLM calls. This is only implemented by some providers, including OpenAI.

Here's an example of tracking token usage for a single LLM call via a callback:

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/models/llm/token_usage_tracking.ts";

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core
```

<CodeBlock language="typescript">{Example}</CodeBlock>

If this model is passed to a chain or agent that calls it multiple times, it will log an output each time.

## Next steps

You've now seen how to get token usage for supported LLM providers.

Next, check out the other how-to guides in this section, like [how to implement your own custom LLM](/docs/how_to/custom_llm).



================================================
FILE: docs/core_docs/docs/how_to/logprobs.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to get log probabilities

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat models](/docs/concepts/chat_models)

:::

Certain chat models can be configured to return token-level log probabilities representing the likelihood of a given token. This guide walks through how to get this information in LangChain.
"""

"""
## OpenAI

Install the `@langchain/openai` package and set your API key:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/openai @langchain/core
</Npm2Yarn>
```
"""

"""
For the OpenAI API to return log probabilities, we need to set the `logprobs` param to `true`. Then, the logprobs are included on each output [`AIMessage`](https://api.python.langchain.com/en/latest/messages/langchain_core.messages.ai.AIMessage.html) as part of the `response_metadata`:
"""

import { ChatOpenAI } from "@langchain/openai";

const model = new ChatOpenAI({
  model: "gpt-4o",
  logprobs: true,
});

const responseMessage = await model.invoke("how are you today?");

responseMessage.response_metadata.logprobs.content.slice(0, 5);
# Output:
#   [

#     {

#       token: [32m"Thank"[39m,

#       logprob: [33m-0.70174205[39m,

#       bytes: [ [33m84[39m, [33m104[39m, [33m97[39m, [33m110[39m, [33m107[39m ],

#       top_logprobs: []

#     },

#     {

#       token: [32m" you"[39m,

#       logprob: [33m0[39m,

#       bytes: [ [33m32[39m, [33m121[39m, [33m111[39m, [33m117[39m ],

#       top_logprobs: []

#     },

#     {

#       token: [32m" for"[39m,

#       logprob: [33m-0.000004723352[39m,

#       bytes: [ [33m32[39m, [33m102[39m, [33m111[39m, [33m114[39m ],

#       top_logprobs: []

#     },

#     {

#       token: [32m" asking"[39m,

#       logprob: [33m-0.0000013856493[39m,

#       bytes: [

#          [33m32[39m,  [33m97[39m, [33m115[39m,

#         [33m107[39m, [33m105[39m, [33m110[39m,

#         [33m103[39m

#       ],

#       top_logprobs: []

#     },

#     {

#       token: [32m"!"[39m,

#       logprob: [33m-0.00030102333[39m,

#       bytes: [ [33m33[39m ],

#       top_logprobs: []

#     }

#   ]

"""
And are part of streamed Message chunks as well:
"""

let count = 0;
const stream = await model.stream("How are you today?");
let aggregateResponse;

for await (const chunk of stream) {
  if (count > 5) {
    break;
  }
  if (aggregateResponse === undefined) {
    aggregateResponse = chunk;
  } else {
    aggregateResponse = aggregateResponse.concat(chunk);
  }
  console.log(aggregateResponse.response_metadata.logprobs?.content);
  count++;
}
# Output:
#   []

#   [

#     {

#       token: "Thank",

#       logprob: -0.23375113,

#       bytes: [ 84, 104, 97, 110, 107 ],

#       top_logprobs: []

#     }

#   ]

#   [

#     {

#       token: "Thank",

#       logprob: -0.23375113,

#       bytes: [ 84, 104, 97, 110, 107 ],

#       top_logprobs: []

#     },

#     {

#       token: " you",

#       logprob: 0,

#       bytes: [ 32, 121, 111, 117 ],

#       top_logprobs: []

#     }

#   ]

#   [

#     {

#       token: "Thank",

#       logprob: -0.23375113,

#       bytes: [ 84, 104, 97, 110, 107 ],

#       top_logprobs: []

#     },

#     {

#       token: " you",

#       logprob: 0,

#       bytes: [ 32, 121, 111, 117 ],

#       top_logprobs: []

#     },

#     {

#       token: " for",

#       logprob: -0.000004723352,

#       bytes: [ 32, 102, 111, 114 ],

#       top_logprobs: []

#     }

#   ]

#   [

#     {

#       token: "Thank",

#       logprob: -0.23375113,

#       bytes: [ 84, 104, 97, 110, 107 ],

#       top_logprobs: []

#     },

#     {

#       token: " you",

#       logprob: 0,

#       bytes: [ 32, 121, 111, 117 ],

#       top_logprobs: []

#     },

#     {

#       token: " for",

#       logprob: -0.000004723352,

#       bytes: [ 32, 102, 111, 114 ],

#       top_logprobs: []

#     },

#     {

#       token: " asking",

#       logprob: -0.0000029352968,

#       bytes: [

#          32,  97, 115,

#         107, 105, 110,

#         103

#       ],

#       top_logprobs: []

#     }

#   ]

#   [

#     {

#       token: "Thank",

#       logprob: -0.23375113,

#       bytes: [ 84, 104, 97, 110, 107 ],

#       top_logprobs: []

#     },

#     {

#       token: " you",

#       logprob: 0,

#       bytes: [ 32, 121, 111, 117 ],

#       top_logprobs: []

#     },

#     {

#       token: " for",

#       logprob: -0.000004723352,

#       bytes: [ 32, 102, 111, 114 ],

#       top_logprobs: []

#     },

#     {

#       token: " asking",

#       logprob: -0.0000029352968,

#       bytes: [

#          32,  97, 115,

#         107, 105, 110,

#         103

#       ],

#       top_logprobs: []

#     },

#     {

#       token: "!",

#       logprob: -0.00039694557,

#       bytes: [ 33 ],

#       top_logprobs: []

#     }

#   ]


"""
## `topLogprobs`

To see alternate potential generations at each step, you can use the `topLogprobs` parameter:
"""

const modelWithTopLogprobs = new ChatOpenAI({
  model: "gpt-4o",
  logprobs: true,
  topLogprobs: 3,
});

const res = await modelWithTopLogprobs.invoke("how are you today?");

res.response_metadata.logprobs.content.slice(0, 5);
# Output:
#   [

#     {

#       token: [32m"I'm"[39m,

#       logprob: [33m-2.2864406[39m,

#       bytes: [ [33m73[39m, [33m39[39m, [33m109[39m ],

#       top_logprobs: [

#         {

#           token: [32m"Thank"[39m,

#           logprob: [33m-0.28644064[39m,

#           bytes: [ [33m84[39m, [33m104[39m, [33m97[39m, [33m110[39m, [33m107[39m ]

#         },

#         {

#           token: [32m"Hello"[39m,

#           logprob: [33m-2.0364406[39m,

#           bytes: [ [33m72[39m, [33m101[39m, [33m108[39m, [33m108[39m, [33m111[39m ]

#         },

#         { token: [32m"I'm"[39m, logprob: [33m-2.2864406[39m, bytes: [ [33m73[39m, [33m39[39m, [33m109[39m ] }

#       ]

#     },

#     {

#       token: [32m" just"[39m,

#       logprob: [33m-0.14442946[39m,

#       bytes: [ [33m32[39m, [33m106[39m, [33m117[39m, [33m115[39m, [33m116[39m ],

#       top_logprobs: [

#         {

#           token: [32m" just"[39m,

#           logprob: [33m-0.14442946[39m,

#           bytes: [ [33m32[39m, [33m106[39m, [33m117[39m, [33m115[39m, [33m116[39m ]

#         },

#         { token: [32m" an"[39m, logprob: [33m-2.2694294[39m, bytes: [ [33m32[39m, [33m97[39m, [33m110[39m ] },

#         {

#           token: [32m" here"[39m,

#           logprob: [33m-4.0194297[39m,

#           bytes: [ [33m32[39m, [33m104[39m, [33m101[39m, [33m114[39m, [33m101[39m ]

#         }

#       ]

#     },

#     {

#       token: [32m" a"[39m,

#       logprob: [33m-0.00066632946[39m,

#       bytes: [ [33m32[39m, [33m97[39m ],

#       top_logprobs: [

#         { token: [32m" a"[39m, logprob: [33m-0.00066632946[39m, bytes: [ [33m32[39m, [33m97[39m ] },

#         {

#           token: [32m" lines"[39m,

#           logprob: [33m-7.750666[39m,

#           bytes: [ [33m32[39m, [33m108[39m, [33m105[39m, [33m110[39m, [33m101[39m, [33m115[39m ]

#         },

#         { token: [32m" an"[39m, logprob: [33m-9.250667[39m, bytes: [ [33m32[39m, [33m97[39m, [33m110[39m ] }

#       ]

#     },

#     {

#       token: [32m" computer"[39m,

#       logprob: [33m-0.015423919[39m,

#       bytes: [

#          [33m32[39m,  [33m99[39m, [33m111[39m, [33m109[39m,

#         [33m112[39m, [33m117[39m, [33m116[39m, [33m101[39m,

#         [33m114[39m

#       ],

#       top_logprobs: [

#         {

#           token: [32m" computer"[39m,

#           logprob: [33m-0.015423919[39m,

#           bytes: [

#              [33m32[39m,  [33m99[39m, [33m111[39m, [33m109[39m,

#             [33m112[39m, [33m117[39m, [33m116[39m, [33m101[39m,

#             [33m114[39m

#           ]

#         },

#         {

#           token: [32m" program"[39m,

#           logprob: [33m-5.265424[39m,

#           bytes: [

#              [33m32[39m, [33m112[39m, [33m114[39m, [33m111[39m,

#             [33m103[39m, [33m114[39m,  [33m97[39m, [33m109[39m

#           ]

#         },

#         {

#           token: [32m" machine"[39m,

#           logprob: [33m-5.390424[39m,

#           bytes: [

#              [33m32[39m, [33m109[39m,  [33m97[39m,  [33m99[39m,

#             [33m104[39m, [33m105[39m, [33m110[39m, [33m101[39m

#           ]

#         }

#       ]

#     },

#     {

#       token: [32m" program"[39m,

#       logprob: [33m-0.0010724656[39m,

#       bytes: [

#          [33m32[39m, [33m112[39m, [33m114[39m, [33m111[39m,

#         [33m103[39m, [33m114[39m,  [33m97[39m, [33m109[39m

#       ],

#       top_logprobs: [

#         {

#           token: [32m" program"[39m,

#           logprob: [33m-0.0010724656[39m,

#           bytes: [

#              [33m32[39m, [33m112[39m, [33m114[39m, [33m111[39m,

#             [33m103[39m, [33m114[39m,  [33m97[39m, [33m109[39m

#           ]

#         },

#         {

#           token: [32m"-based"[39m,

#           logprob: [33m-6.8760724[39m,

#           bytes: [ [33m45[39m, [33m98[39m, [33m97[39m, [33m115[39m, [33m101[39m, [33m100[39m ]

#         },

#         {

#           token: [32m" algorithm"[39m,

#           logprob: [33m-10.626073[39m,

#           bytes: [

#              [33m32[39m,  [33m97[39m, [33m108[39m, [33m103[39m,

#             [33m111[39m, [33m114[39m, [33m105[39m, [33m116[39m,

#             [33m104[39m, [33m109[39m

#           ]

#         }

#       ]

#     }

#   ]

"""
## Next steps

You've now learned how to get logprobs from OpenAI models in LangChain.

Next, check out the other how-to guides chat models in this section, like [how to get a model to return structured output](/docs/how_to/structured_output) or [how to track token usage](/docs/how_to/chat_token_usage_tracking).
"""



================================================
FILE: docs/core_docs/docs/how_to/merge_message_runs.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to merge consecutive messages of the same type

:::note
The `mergeMessageRuns` function is available in `@langchain/core` version `0.2.8` and above.
:::

Certain models do not support passing in consecutive messages of the same type (a.k.a. "runs" of the same message type).

The `mergeMessageRuns` utility makes it easy to merge consecutive messages of the same type.

## Basic usage
"""

import { HumanMessage, SystemMessage, AIMessage, mergeMessageRuns } from "@langchain/core/messages";

const messages = [
    new SystemMessage("you're a good assistant."),
    new SystemMessage("you always respond with a joke."),
    new HumanMessage({ content: [{"type": "text", "text": "i wonder why it's called langchain"}] }),
    new HumanMessage("and who is harrison chasing anyways"),
    new AIMessage(
        'Well, I guess they thought "WordRope" and "SentenceString" just didn\'t have the same ring to it!'
    ),
    new AIMessage("Why, he's probably chasing after the last cup of coffee in the office!"),
];

const merged = mergeMessageRuns(messages);
console.log(merged.map((x) => JSON.stringify({
    role: x._getType(),
    content: x.content,
}, null, 2)).join("\n\n"));
# Output:
#   {

#     "role": "system",

#     "content": "you're a good assistant.\nyou always respond with a joke."

#   }

#   

#   {

#     "role": "human",

#     "content": [

#       {

#         "type": "text",

#         "text": "i wonder why it's called langchain"

#       },

#       {

#         "type": "text",

#         "text": "and who is harrison chasing anyways"

#       }

#     ]

#   }

#   

#   {

#     "role": "ai",

#     "content": "Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn't have the same ring to it!\nWhy, he's probably chasing after the last cup of coffee in the office!"

#   }


"""
Notice that if the contents of one of the messages to merge is a list of content blocks then the merged message will have a list of content blocks. And if both messages to merge have string contents then those are concatenated with a newline character.
"""

"""
## Chaining

`mergeMessageRuns` can be used in an imperatively (like above) or declaratively, making it easy to compose with other components in a chain:
"""

import { ChatAnthropic } from "@langchain/anthropic";
import { mergeMessageRuns } from "@langchain/core/messages";

const llm = new ChatAnthropic({ model: "claude-3-sonnet-20240229", temperature: 0 });
// Notice we don't pass in messages. This creates
// a RunnableLambda that takes messages as input
const merger = mergeMessageRuns();
const chain = merger.pipe(llm);
await chain.invoke(messages);
# Output:
#   AIMessage {

#     lc_serializable: true,

#     lc_kwargs: {

#       content: [],

#       additional_kwargs: {

#         id: 'msg_01LsdS4bjQ3EznH7Tj4xujV1',

#         type: 'message',

#         role: 'assistant',

#         model: 'claude-3-sonnet-20240229',

#         stop_reason: 'end_turn',

#         stop_sequence: null,

#         usage: [Object]

#       },

#       tool_calls: [],

#       usage_metadata: { input_tokens: 84, output_tokens: 3, total_tokens: 87 },

#       invalid_tool_calls: [],

#       response_metadata: {}

#     },

#     lc_namespace: [ 'langchain_core', 'messages' ],

#     content: [],

#     name: undefined,

#     additional_kwargs: {

#       id: 'msg_01LsdS4bjQ3EznH7Tj4xujV1',

#       type: 'message',

#       role: 'assistant',

#       model: 'claude-3-sonnet-20240229',

#       stop_reason: 'end_turn',

#       stop_sequence: null,

#       usage: { input_tokens: 84, output_tokens: 3 }

#     },

#     response_metadata: {

#       id: 'msg_01LsdS4bjQ3EznH7Tj4xujV1',

#       model: 'claude-3-sonnet-20240229',

#       stop_reason: 'end_turn',

#       stop_sequence: null,

#       usage: { input_tokens: 84, output_tokens: 3 }

#     },

#     id: undefined,

#     tool_calls: [],

#     invalid_tool_calls: [],

#     usage_metadata: { input_tokens: 84, output_tokens: 3, total_tokens: 87 }

#   }


"""
Looking at [the LangSmith trace](https://smith.langchain.com/public/48d256fb-fd7e-48a0-bdfd-217ab74ad01d/r) we can see that before the messages are passed to the model they are merged.

Looking at just the merger, we can see that it's a Runnable object that can be invoked like all Runnables:
"""

await merger.invoke(messages)
# Output:
#   [

#     SystemMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: "you're a good assistant.\nyou always respond with a joke.",

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: {},

#         id: undefined

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: "you're a good assistant.\nyou always respond with a joke.",

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {},

#       id: undefined

#     },

#     HumanMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: [Array],

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: {},

#         id: undefined

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: [ [Object], [Object] ],

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {},

#       id: undefined

#     },

#     AIMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: `Well, I guess they thought "WordRope" and "SentenceString" just didn't have the same ring to it!\n` +

#           "Why, he's probably chasing after the last cup of coffee in the office!",

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: {},

#         id: undefined,

#         tool_calls: [],

#         invalid_tool_calls: [],

#         usage_metadata: undefined

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: `Well, I guess they thought "WordRope" and "SentenceString" just didn't have the same ring to it!\n` +

#         "Why, he's probably chasing after the last cup of coffee in the office!",

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {},

#       id: undefined,

#       tool_calls: [],

#       invalid_tool_calls: [],

#       usage_metadata: undefined

#     }

#   ]


"""
## API reference

For a complete description of all arguments head to the [API reference](https://api.js.langchain.com/functions/langchain_core.messages.mergeMessageRuns.html).
"""



================================================
FILE: docs/core_docs/docs/how_to/message_history.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
keywords: [memory]
---
"""

"""
# How to add message history

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Chaining runnables](/docs/how_to/sequence/)
- [Prompt templates](/docs/concepts/prompt_templates)
- [Chat Messages](/docs/concepts/messages)

:::

```{=mdx}
:::note

This guide previously covered the [RunnableWithMessageHistory](https://api.js.langchain.com/classes/_langchain_core.runnables.RunnableWithMessageHistory.html) abstraction. You can access this version of the guide in the [v0.2 docs](https://js.langchain.com/v0.2/docs/how_to/message_history/).

The LangGraph implementation offers a number of advantages over `RunnableWithMessageHistory`, including the ability to persist arbitrary components of an application's state (instead of only messages).

:::
```


Passing conversation state into and out a chain is vital when building a chatbot. LangGraph implements a built-in persistence layer, allowing chain states to be automatically persisted in memory, or external backends such as SQLite, Postgres or Redis. Details can be found in the LangGraph persistence documentation.

In this guide we demonstrate how to add persistence to arbitrary LangChain runnables by wrapping them in a minimal LangGraph application. This lets us persist the message history and other elements of the chain's state, simplifying the development of multi-turn applications. It also supports multiple threads, enabling a single application to interact separately with multiple users.

## Setup

```{=mdx}
import Npm2Yarn from "@theme/Npm2Yarn";

<Npm2Yarn>
  @langchain/core @langchain/langgraph
</Npm2Yarn>
```

Let’s also set up a chat model that we’ll use for the below examples.

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```

"""

// @lc-docs-hide-cell

import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0,
});

"""
## Example: message inputs

Adding memory to a [chat model](/docs/concepts/chat_models) provides a simple example. Chat models accept a list of messages as input and output a message. LangGraph includes a built-in `MessagesState` that we can use for this purpose.

Below, we:
1. Define the graph state to be a list of messages;
2. Add a single node to the graph that calls a chat model;
3. Compile the graph with an in-memory checkpointer to store messages between runs.

:::info

The output of a LangGraph application is its [state](https://langchain-ai.github.io/langgraphjs/concepts/low_level/).

:::
"""

import { START, END, MessagesAnnotation, StateGraph, MemorySaver } from "@langchain/langgraph";

// Define the function that calls the model
const callModel = async (state: typeof MessagesAnnotation.State) => {
  const response = await llm.invoke(state.messages);
  // Update message history with response:
  return { messages: response };
};

// Define a new graph
const workflow = new StateGraph(MessagesAnnotation)
  // Define the (single) node in the graph
  .addNode("model", callModel)
  .addEdge(START, "model")
  .addEdge("model", END);

// Add memory
const memory = new MemorySaver();
const app = workflow.compile({ checkpointer: memory });

"""
When we run the application, we pass in a configuration object that specifies a `thread_id`. This ID is used to distinguish conversational threads (e.g., between different users).
"""

import { v4 as uuidv4 } from "uuid";

const config = { configurable: { thread_id: uuidv4() } }

"""
We can then invoke the application:
"""

const input = [
  {
    role: "user",
    content: "Hi! I'm Bob.",
  }
]
const output = await app.invoke({ messages: input }, config)
// The output contains all messages in the state.
// This will log the last message in the conversation.
console.log(output.messages[output.messages.length - 1]);
# Output:
#   AIMessage {

#     "id": "chatcmpl-ABTqCeKnMQmG9IH8dNF5vPjsgXtcM",

#     "content": "Hi Bob! How can I assist you today?",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "completionTokens": 10,

#         "promptTokens": 12,

#         "totalTokens": 22

#       },

#       "finish_reason": "stop",

#       "system_fingerprint": "fp_e375328146"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 12,

#       "output_tokens": 10,

#       "total_tokens": 22

#     }

#   }


const input2 = [
  {
    role: "user",
    content: "What's my name?",
  }
]
const output2 = await app.invoke({ messages: input2 }, config)
console.log(output2.messages[output2.messages.length - 1]);
# Output:
#   AIMessage {

#     "id": "chatcmpl-ABTqD5jrJXeKCpvoIDp47fvgw2OPn",

#     "content": "Your name is Bob. How can I help you today, Bob?",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "completionTokens": 14,

#         "promptTokens": 34,

#         "totalTokens": 48

#       },

#       "finish_reason": "stop",

#       "system_fingerprint": "fp_e375328146"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 34,

#       "output_tokens": 14,

#       "total_tokens": 48

#     }

#   }


"""
Note that states are separated for different threads. If we issue the same query to a thread with a new `thread_id`, the model indicates that it does not know the answer:
"""

const config2 = { configurable: { thread_id: uuidv4() } }
const input3 = [
  {
    role: "user",
    content: "What's my name?",
  }
]
const output3 = await app.invoke({ messages: input3 }, config2)
console.log(output3.messages[output3.messages.length - 1]);
# Output:
#   AIMessage {

#     "id": "chatcmpl-ABTqDkctxwmXjeGOZpK6Km8jdCqdl",

#     "content": "I'm sorry, but I don't have access to personal information about users. How can I assist you today?",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "completionTokens": 21,

#         "promptTokens": 11,

#         "totalTokens": 32

#       },

#       "finish_reason": "stop",

#       "system_fingerprint": "fp_52a7f40b0b"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 11,

#       "output_tokens": 21,

#       "total_tokens": 32

#     }

#   }


"""
## Example: object inputs

LangChain runnables often accept multiple inputs via separate keys in a single object argument. A common example is a prompt template with multiple parameters.

Whereas before our runnable was a chat model, here we chain together a prompt template and chat model.
"""

import { ChatPromptTemplate, MessagesPlaceholder } from "@langchain/core/prompts";

const prompt = ChatPromptTemplate.fromMessages([
  ["system", "Answer in {language}."],
  new MessagesPlaceholder("messages"),
])

const runnable = prompt.pipe(llm);

"""
For this scenario, we define the graph state to include these parameters (in addition to the message history). We then define a single-node graph in the same way as before.

Note that in the below state:
- Updates to the `messages` list will append messages;
- Updates to the `language` string will overwrite the string.
"""

import { START, END, StateGraph, MemorySaver, MessagesAnnotation, Annotation } from "@langchain/langgraph";

// Define the State
// highlight-next-line
const GraphAnnotation = Annotation.Root({
  // highlight-next-line
  language: Annotation<string>(),
  // Spread `MessagesAnnotation` into the state to add the `messages` field.
  // highlight-next-line
  ...MessagesAnnotation.spec,
})


// Define the function that calls the model
const callModel2 = async (state: typeof GraphAnnotation.State) => {
  const response = await runnable.invoke(state);
  // Update message history with response:
  return { messages: [response] };
};

const workflow2 = new StateGraph(GraphAnnotation)
  .addNode("model", callModel2)
  .addEdge(START, "model")
  .addEdge("model", END);

const app2 = workflow2.compile({ checkpointer: new MemorySaver() });

const config3 = { configurable: { thread_id: uuidv4() } }
const input4 = {
  messages: [
    {
      role: "user",
      content: "What's my name?",
    }
  ],
  language: "Spanish",
} 
const output4 = await app2.invoke(input4, config3)
console.log(output4.messages[output4.messages.length - 1]);
# Output:
#   AIMessage {

#     "id": "chatcmpl-ABTqFnCASRB5UhZ7XAbbf5T0Bva4U",

#     "content": "Lo siento, pero no tengo suficiente información para saber tu nombre. ¿Cómo te llamas?",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "completionTokens": 19,

#         "promptTokens": 19,

#         "totalTokens": 38

#       },

#       "finish_reason": "stop",

#       "system_fingerprint": "fp_e375328146"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 19,

#       "output_tokens": 19,

#       "total_tokens": 38

#     }

#   }


"""
## Managing message history

The message history (and other elements of the application state) can be accessed via `.getState`:
"""

const state = (await app2.getState(config3)).values

console.log(`Language: ${state.language}`);
console.log(state.messages)
# Output:
#   Language: Spanish

#   [

#     HumanMessage {

#       "content": "What's my name?",

#       "additional_kwargs": {},

#       "response_metadata": {}

#     },

#     AIMessage {

#       "id": "chatcmpl-ABTqFnCASRB5UhZ7XAbbf5T0Bva4U",

#       "content": "Lo siento, pero no tengo suficiente información para saber tu nombre. ¿Cómo te llamas?",

#       "additional_kwargs": {},

#       "response_metadata": {

#         "tokenUsage": {

#           "completionTokens": 19,

#           "promptTokens": 19,

#           "totalTokens": 38

#         },

#         "finish_reason": "stop",

#         "system_fingerprint": "fp_e375328146"

#       },

#       "tool_calls": [],

#       "invalid_tool_calls": []

#     }

#   ]


"""
We can also update the state via `.updateState`. For example, we can manually append a new message:
"""

const _ = await app2.updateState(config3, { messages: [{ role: "user", content: "test" }]})

const state2 = (await app2.getState(config3)).values

console.log(`Language: ${state2.language}`);
console.log(state2.messages)
# Output:
#   Language: Spanish

#   [

#     HumanMessage {

#       "content": "What's my name?",

#       "additional_kwargs": {},

#       "response_metadata": {}

#     },

#     AIMessage {

#       "id": "chatcmpl-ABTqFnCASRB5UhZ7XAbbf5T0Bva4U",

#       "content": "Lo siento, pero no tengo suficiente información para saber tu nombre. ¿Cómo te llamas?",

#       "additional_kwargs": {},

#       "response_metadata": {

#         "tokenUsage": {

#           "completionTokens": 19,

#           "promptTokens": 19,

#           "totalTokens": 38

#         },

#         "finish_reason": "stop",

#         "system_fingerprint": "fp_e375328146"

#       },

#       "tool_calls": [],

#       "invalid_tool_calls": []

#     },

#     HumanMessage {

#       "content": "test",

#       "additional_kwargs": {},

#       "response_metadata": {}

#     }

#   ]


"""
For details on managing state, including deleting messages, see the LangGraph documentation:

- [How to delete messages](https://langchain-ai.github.io/langgraphjs/how-tos/delete-messages/)
- [How to view and update past graph state](https://langchain-ai.github.io/langgraphjs/how-tos/time-travel/)
"""



================================================
FILE: docs/core_docs/docs/how_to/migrate_agent.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
keywords: [create_react_agent, create_react_agent()]
---
"""

"""
# How to migrate from legacy LangChain agents to LangGraph

:::info Prerequisites

This guide assumes familiarity with the following concepts:
- [Agents](/docs/concepts/agents)
- [LangGraph.js](https://langchain-ai.github.io/langgraphjs/)
- [Tool calling](/docs/how_to/tool_calling/)

:::

Here we focus on how to move from legacy LangChain agents to more flexible [LangGraph](https://langchain-ai.github.io/langgraphjs/) agents.
LangChain agents (the
[`AgentExecutor`](https://api.js.langchain.com/classes/langchain.agents.AgentExecutor.html)
in particular) have multiple configuration parameters. In this notebook we will
show how those parameters map to the LangGraph
react agent executor using the [create_react_agent](https://langchain-ai.github.io/langgraphjs/reference/functions/prebuilt.createReactAgent.html) prebuilt helper method.

For more information on how to build agentic workflows in LangGraph, check out
the [docs here](https://langchain-ai.github.io/langgraphjs/how-tos/).

#### Prerequisites

This how-to guide uses OpenAI's `"gpt-4o-mini"` as the LLM. If you are running this guide as a notebook, set your OpenAI API key as shown below:
"""

// process.env.OPENAI_API_KEY = "...";

// Optional, add tracing in LangSmith
// process.env.LANGSMITH_API_KEY = "ls...";
// process.env.LANGCHAIN_CALLBACKS_BACKGROUND = "true";
// process.env.LANGSMITH_TRACING = "true";
// process.env.LANGSMITH_PROJECT = "How to migrate: LangGraphJS";

// Reduce tracing latency if you are not in a serverless environment
// process.env.LANGCHAIN_CALLBACKS_BACKGROUND = "true";

"""
## Basic Usage

For basic creation and usage of a tool-calling ReAct-style agent, the
functionality is the same. First, let's define a model and tool(s), then we'll
use those to create an agent.

:::note
The `tool` function is available in `@langchain/core` version 0.2.7 and above.

If you are on an older version of core, you should use instantiate and use [`DynamicStructuredTool`](https://api.js.langchain.com/classes/langchain_core.tools.DynamicStructuredTool.html) instead.
:::
"""

import { tool } from "@langchain/core/tools";
import { z } from "zod";
import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({
  model: "gpt-4o-mini",
});

const magicTool = tool(async ({ input }: { input: number }) => {
  return `${input + 2}`;
}, {
  name: "magic_function",
  description: "Applies a magic function to an input.",
  schema: z.object({
    input: z.number(),
  }),
});

const tools = [magicTool];

const query = "what is the value of magic_function(3)?";

"""
For the LangChain
[`AgentExecutor`](https://api.js.langchain.com/classes/langchain_agents.AgentExecutor.html),
we define a prompt with a placeholder for the agent's scratchpad. The agent can
be invoked as follows:

"""

import {
  ChatPromptTemplate,
} from "@langchain/core/prompts";
import { createToolCallingAgent } from "langchain/agents";
import { AgentExecutor } from "langchain/agents";

const prompt = ChatPromptTemplate.fromMessages([
  ["system", "You are a helpful assistant"],
  ["placeholder", "{chat_history}"],
  ["human", "{input}"],
  ["placeholder", "{agent_scratchpad}"],
]);

const agent = createToolCallingAgent({
  llm,
  tools,
  prompt
});
const agentExecutor = new AgentExecutor({
  agent,
  tools,
});

await agentExecutor.invoke({ input: query });
# Output:
#   {

#     input: [32m"what is the value of magic_function(3)?"[39m,

#     output: [32m"The value of `magic_function(3)` is 5."[39m

#   }

"""
LangGraph's off-the-shelf
[react agent executor](https://langchain-ai.github.io/langgraphjs/reference/functions/prebuilt.createReactAgent.html)
manages a state that is defined by a list of messages. In a similar way to the `AgentExecutor`, it will continue to
process the list until there are no tool calls in the agent's output. To kick it
off, we input a list of messages. The output will contain the entire state of
the graph - in this case, the conversation history and messages representing intermediate tool calls:

"""

import { createReactAgent } from "@langchain/langgraph/prebuilt";

const app = createReactAgent({
  llm,
  tools,
});

let agentOutput = await app.invoke({
  messages: [
    {
      role: "user",
      content: query
    },
  ],
});

console.log(agentOutput);
# Output:
#   {

#     messages: [

#       HumanMessage {

#         "id": "eeef343c-80d1-4ccb-86af-c109343689cd",

#         "content": "what is the value of magic_function(3)?",

#         "additional_kwargs": {},

#         "response_metadata": {}

#       },

#       AIMessage {

#         "id": "chatcmpl-A7exs2uRqEipaZ7MtRbXnqu0vT0Da",

#         "content": "",

#         "additional_kwargs": {

#           "tool_calls": [

#             {

#               "id": "call_MtwWLn000BQHeSYQKsbxYNR0",

#               "type": "function",

#               "function": "[Object]"

#             }

#           ]

#         },

#         "response_metadata": {

#           "tokenUsage": {

#             "completionTokens": 14,

#             "promptTokens": 55,

#             "totalTokens": 69

#           },

#           "finish_reason": "tool_calls",

#           "system_fingerprint": "fp_483d39d857"

#         },

#         "tool_calls": [

#           {

#             "name": "magic_function",

#             "args": {

#               "input": 3

#             },

#             "type": "tool_call",

#             "id": "call_MtwWLn000BQHeSYQKsbxYNR0"

#           }

#         ],

#         "invalid_tool_calls": [],

#         "usage_metadata": {

#           "input_tokens": 55,

#           "output_tokens": 14,

#           "total_tokens": 69

#         }

#       },

#       ToolMessage {

#         "id": "1001bf20-7cde-4f8b-81f1-1faa654a8bb4",

#         "content": "5",

#         "name": "magic_function",

#         "additional_kwargs": {},

#         "response_metadata": {},

#         "tool_call_id": "call_MtwWLn000BQHeSYQKsbxYNR0"

#       },

#       AIMessage {

#         "id": "chatcmpl-A7exsTk3ilzGzC8DuY8GpnKOaGdvx",

#         "content": "The value of `magic_function(3)` is 5.",

#         "additional_kwargs": {},

#         "response_metadata": {

#           "tokenUsage": {

#             "completionTokens": 14,

#             "promptTokens": 78,

#             "totalTokens": 92

#           },

#           "finish_reason": "stop",

#           "system_fingerprint": "fp_54e2f484be"

#         },

#         "tool_calls": [],

#         "invalid_tool_calls": [],

#         "usage_metadata": {

#           "input_tokens": 78,

#           "output_tokens": 14,

#           "total_tokens": 92

#         }

#       }

#     ]

#   }


const messageHistory = agentOutput.messages;
const newQuery = "Pardon?";

agentOutput = await app.invoke({
  messages: [
    ...messageHistory,
    { role: "user", content: newQuery }
  ],
});

# Output:
#   {

#     messages: [

#       HumanMessage {

#         "id": "eeef343c-80d1-4ccb-86af-c109343689cd",

#         "content": "what is the value of magic_function(3)?",

#         "additional_kwargs": {},

#         "response_metadata": {}

#       },

#       AIMessage {

#         "id": "chatcmpl-A7exs2uRqEipaZ7MtRbXnqu0vT0Da",

#         "content": "",

#         "additional_kwargs": {

#           "tool_calls": [

#             {

#               "id": "call_MtwWLn000BQHeSYQKsbxYNR0",

#               "type": "function",

#               "function": "[Object]"

#             }

#           ]

#         },

#         "response_metadata": {

#           "tokenUsage": {

#             "completionTokens": 14,

#             "promptTokens": 55,

#             "totalTokens": 69

#           },

#           "finish_reason": "tool_calls",

#           "system_fingerprint": "fp_483d39d857"

#         },

#         "tool_calls": [

#           {

#             "name": "magic_function",

#             "args": {

#               "input": 3

#             },

#             "type": "tool_call",

#             "id": "call_MtwWLn000BQHeSYQKsbxYNR0"

#           }

#         ],

#         "invalid_tool_calls": [],

#         "usage_metadata": {

#           "input_tokens": 55,

#           "output_tokens": 14,

#           "total_tokens": 69

#         }

#       },

#       ToolMessage {

#         "id": "1001bf20-7cde-4f8b-81f1-1faa654a8bb4",

#         "content": "5",

#         "name": "magic_function",

#         "additional_kwargs": {},

#         "response_metadata": {},

#         "tool_call_id": "call_MtwWLn000BQHeSYQKsbxYNR0"

#       },

#       AIMessage {

#         "id": "chatcmpl-A7exsTk3ilzGzC8DuY8GpnKOaGdvx",

#         "content": "The value of `magic_function(3)` is 5.",

#         "additional_kwargs": {},

#         "response_metadata": {

#           "tokenUsage": {

#             "completionTokens": 14,

#             "promptTokens": 78,

#             "totalTokens": 92

#           },

#           "finish_reason": "stop",

#           "system_fingerprint": "fp_54e2f484be"

#         },

#         "tool_calls": [],

#         "invalid_tool_calls": [],

#         "usage_metadata": {

#           "input_tokens": 78,

#           "output_tokens": 14,

#           "total_tokens": 92

#         }

#       },

#       HumanMessage {

#         "id": "1f2a9f41-c8ff-48fe-9d93-e663ee9279ff",

#         "content": "Pardon?",

#         "additional_kwargs": {},

#         "response_metadata": {}

#       },

#       AIMessage {

#         "id": "chatcmpl-A7exyTe9Ofs63Ex3sKwRx3wWksNup",

#         "content": "The result of calling the `magic_function` with an input of 3 is 5.",

#         "additional_kwargs": {},

#         "response_metadata": {

#           "tokenUsage": {

#             "completionTokens": 20,

#             "promptTokens": 102,

#             "totalTokens": 122

#           },

#           "finish_reason": "stop",

#           "system_fingerprint": "fp_483d39d857"

#         },

#         "tool_calls": [],

#         "invalid_tool_calls": [],

#         "usage_metadata": {

#           "input_tokens": 102,

#           "output_tokens": 20,

#           "total_tokens": 122

#         }

#       }

#     ]

#   }

"""
## Prompt Templates

With legacy LangChain agents you have to pass in a prompt template. You can use
this to control the agent.

With LangGraph
[react agent executor](https://langchain-ai.github.io/langgraphjs/reference/functions/prebuilt.createReactAgent.html),
by default there is no prompt. You can achieve similar control over the agent in
a few ways:

1. Pass in a system message as input
2. Initialize the agent with a system message
3. Initialize the agent with a function to transform messages before passing to
   the model.

Let's take a look at all of these below. We will pass in custom instructions to
get the agent to respond in Spanish.

First up, using LangChain's `AgentExecutor`:

"""

const spanishPrompt = ChatPromptTemplate.fromMessages([
  ["system", "You are a helpful assistant. Respond only in Spanish."],
  ["placeholder", "{chat_history}"],
  ["human", "{input}"],
  ["placeholder", "{agent_scratchpad}"],
]);

const spanishAgent = createToolCallingAgent({
  llm,
  tools,
  prompt: spanishPrompt,
});
const spanishAgentExecutor = new AgentExecutor({
  agent: spanishAgent,
  tools,
});

await spanishAgentExecutor.invoke({ input: query });

# Output:
#   {

#     input: [32m"what is the value of magic_function(3)?"[39m,

#     output: [32m"El valor de `magic_function(3)` es 5."[39m

#   }

"""
Now, let's pass a custom system message to [react agent executor](https://langchain-ai.github.io/langgraphjs/reference/functions/prebuilt.createReactAgent.html).

LangGraph's prebuilt `create_react_agent` does not take a prompt template directly as a parameter, but instead takes a `messages_modifier` parameter. This modifies messages before they are passed into the model, and can be one of four values:

- A `SystemMessage`, which is added to the beginning of the list of messages.
- A `string`, which is converted to a `SystemMessage` and added to the beginning of the list of messages.
- A `Callable`, which should take in a list of messages. The output is then passed to the language model.
- Or a [`Runnable`](/docs/concepts/lcel), which should should take in a list of messages. The output is then passed to the language model.

Here's how it looks in action:

"""

const systemMessage = "You are a helpful assistant. Respond only in Spanish.";

// This could also be a SystemMessage object
// const systemMessage = new SystemMessage("You are a helpful assistant. Respond only in Spanish.");

const appWithSystemMessage = createReactAgent({
  llm,
  tools,
  messageModifier: systemMessage,
});

agentOutput = await appWithSystemMessage.invoke({
  messages: [
    { role: "user", content: query }
  ],
});
agentOutput.messages[agentOutput.messages.length - 1];
# Output:
#   AIMessage {

#     "id": "chatcmpl-A7ey8LGWAs8ldrRRcO5wlHM85w9T8",

#     "content": "El valor de `magic_function(3)` es 5.",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "completionTokens": 14,

#         "promptTokens": 89,

#         "totalTokens": 103

#       },

#       "finish_reason": "stop",

#       "system_fingerprint": "fp_483d39d857"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 89,

#       "output_tokens": 14,

#       "total_tokens": 103

#     }

#   }

"""
We can also pass in an arbitrary function. This function should take in a list
of messages and output a list of messages. We can do all types of arbitrary
formatting of messages here. In this cases, let's just add a `SystemMessage` to
the start of the list of messages.

"""

import { BaseMessage, SystemMessage, HumanMessage } from "@langchain/core/messages";

const modifyMessages = (messages: BaseMessage[]) => {
  return [
    new SystemMessage("You are a helpful assistant. Respond only in Spanish."),
    ...messages,
    new HumanMessage("Also say 'Pandemonium!' after the answer."),
  ];
};

const appWithMessagesModifier = createReactAgent({
  llm,
  tools,
  messageModifier: modifyMessages,
});

agentOutput = await appWithMessagesModifier.invoke({
  messages: [{ role: "user", content: query }],
});

console.log({
  input: query,
  output: agentOutput.messages[agentOutput.messages.length - 1].content,
});
# Output:
#   {

#     input: "what is the value of magic_function(3)?",

#     output: "El valor de magic_function(3) es 5. ¡Pandemonium!"

#   }


"""
## Memory

With LangChain's
[`AgentExecutor`](https://api.js.langchain.com/classes/langchain_agents.AgentExecutor.html), you could add chat memory classes so it can engage in a multi-turn conversation.

"""

import { ChatMessageHistory } from "@langchain/community/stores/message/in_memory";
import { RunnableWithMessageHistory } from "@langchain/core/runnables";

const memory = new ChatMessageHistory();
const agentExecutorWithMemory = new RunnableWithMessageHistory({
  runnable: agentExecutor,
  getMessageHistory: () => memory,
  inputMessagesKey: "input",
  historyMessagesKey: "chat_history",
});

const config = { configurable: { sessionId: "test-session" } };

agentOutput = await agentExecutorWithMemory.invoke(
  { input: "Hi, I'm polly! What's the output of magic_function of 3?" },
  config,
);

console.log(agentOutput.output);

agentOutput = await agentExecutorWithMemory.invoke(
  { input: "Remember my name?" },
  config,
);

console.log("---");
console.log(agentOutput.output);
console.log("---");

agentOutput = await agentExecutorWithMemory.invoke(
  { input: "what was that output again?" },
  config,
);

console.log(agentOutput.output);
# Output:
#   The output of the magic function for the input 3 is 5.

#   ---

#   Yes, your name is Polly! How can I assist you today?

#   ---

#   The output of the magic function for the input 3 is 5.


"""
#### In LangGraph

The equivalent to this type of memory in LangGraph is [persistence](https://langchain-ai.github.io/langgraphjs/how-tos/persistence/), and [checkpointing](https://langchain-ai.github.io/langgraphjs/reference/interfaces/index.Checkpoint.html).

Add a `checkpointer` to the agent and you get chat memory for free. You'll need to also pass a `thread_id` within the `configurable` field in the `config` parameter. Notice that we only pass one message into each request, but the model still has context from previous runs:
"""

import { MemorySaver } from "@langchain/langgraph";

const checkpointer = new MemorySaver();
const appWithMemory = createReactAgent({
  llm: llm,
  tools: tools,
  checkpointSaver: checkpointer
});

const langGraphConfig = {
  configurable: {
    thread_id: "test-thread",
  },
};

agentOutput = await appWithMemory.invoke(
  {
    messages: [
      {
        role: "user",
        content: "Hi, I'm polly! What's the output of magic_function of 3?",
      }
    ],
  },
  langGraphConfig,
);

console.log(agentOutput.messages[agentOutput.messages.length - 1].content);
console.log("---");

agentOutput = await appWithMemory.invoke(
  {
    messages: [
      { role: "user", content: "Remember my name?" }
    ]
  },
  langGraphConfig,
);

console.log(agentOutput.messages[agentOutput.messages.length - 1].content);
console.log("---");

agentOutput = await appWithMemory.invoke(
  {
    messages: [
      { role: "user", content: "what was that output again?" }
    ]
  },
  langGraphConfig,
);

console.log(agentOutput.messages[agentOutput.messages.length - 1].content);
# Output:
#   Hi Polly! The output of the magic function for the input 3 is 5.

#   ---

#   Yes, your name is Polly!

#   ---

#   The output of the magic function for the input 3 was 5.


"""
## Iterating through steps

With LangChain's
[`AgentExecutor`](https://api.js.langchain.com/classes/langchain_agents.AgentExecutor.html),
you could iterate over the steps using the
[`stream`](https://api.js.langchain.com/classes/langchain_core.runnables.Runnable.html#stream) method:

"""

const langChainStream = await agentExecutor.stream({ input: query });

for await (const step of langChainStream) {
  console.log(step);
}
# Output:
#   {

#     intermediateSteps: [

#       {

#         action: {

#           tool: "magic_function",

#           toolInput: { input: 3 },

#           toolCallId: "call_IQZr1yy2Ug6904VkQg6pWGgR",

#           log: 'Invoking "magic_function" with {"input":3}\n',

#           messageLog: [

#             AIMessageChunk {

#               "id": "chatcmpl-A7eziUrDmLSSMoiOskhrfbsHqx4Sd",

#               "content": "",

#               "additional_kwargs": {

#                 "tool_calls": [

#                   {

#                     "index": 0,

#                     "id": "call_IQZr1yy2Ug6904VkQg6pWGgR",

#                     "type": "function",

#                     "function": "[Object]"

#                   }

#                 ]

#               },

#               "response_metadata": {

#                 "prompt": 0,

#                 "completion": 0,

#                 "finish_reason": "tool_calls",

#                 "system_fingerprint": "fp_483d39d857"

#               },

#               "tool_calls": [

#                 {

#                   "name": "magic_function",

#                   "args": {

#                     "input": 3

#                   },

#                   "id": "call_IQZr1yy2Ug6904VkQg6pWGgR",

#                   "type": "tool_call"

#                 }

#               ],

#               "tool_call_chunks": [

#                 {

#                   "name": "magic_function",

#                   "args": "{\"input\":3}",

#                   "id": "call_IQZr1yy2Ug6904VkQg6pWGgR",

#                   "index": 0,

#                   "type": "tool_call_chunk"

#                 }

#               ],

#               "invalid_tool_calls": [],

#               "usage_metadata": {

#                 "input_tokens": 61,

#                 "output_tokens": 14,

#                 "total_tokens": 75

#               }

#             }

#           ]

#         },

#         observation: "5"

#       }

#     ]

#   }

#   { output: "The value of `magic_function(3)` is 5." }


"""
#### In LangGraph

In LangGraph, things are handled natively using the stream method.

"""

const langGraphStream = await app.stream(
  { messages: [{ role: "user", content: query }] },
  { streamMode: "updates" },
);

for await (const step of langGraphStream) {
  console.log(step);
}
# Output:
#   {

#     agent: {

#       messages: [

#         AIMessage {

#           "id": "chatcmpl-A7ezu8hirCENjdjR2GpLjkzXFTEmp",

#           "content": "",

#           "additional_kwargs": {

#             "tool_calls": [

#               {

#                 "id": "call_KhhNL0m3mlPoJiboFMoX8hzk",

#                 "type": "function",

#                 "function": "[Object]"

#               }

#             ]

#           },

#           "response_metadata": {

#             "tokenUsage": {

#               "completionTokens": 14,

#               "promptTokens": 55,

#               "totalTokens": 69

#             },

#             "finish_reason": "tool_calls",

#             "system_fingerprint": "fp_483d39d857"

#           },

#           "tool_calls": [

#             {

#               "name": "magic_function",

#               "args": {

#                 "input": 3

#               },

#               "type": "tool_call",

#               "id": "call_KhhNL0m3mlPoJiboFMoX8hzk"

#             }

#           ],

#           "invalid_tool_calls": [],

#           "usage_metadata": {

#             "input_tokens": 55,

#             "output_tokens": 14,

#             "total_tokens": 69

#           }

#         }

#       ]

#     }

#   }

#   {

#     tools: {

#       messages: [

#         ToolMessage {

#           "content": "5",

#           "name": "magic_function",

#           "additional_kwargs": {},

#           "response_metadata": {},

#           "tool_call_id": "call_KhhNL0m3mlPoJiboFMoX8hzk"

#         }

#       ]

#     }

#   }

#   {

#     agent: {

#       messages: [

#         AIMessage {

#           "id": "chatcmpl-A7ezuTrh8GC550eKa1ZqRZGjpY5zh",

#           "content": "The value of `magic_function(3)` is 5.",

#           "additional_kwargs": {},

#           "response_metadata": {

#             "tokenUsage": {

#               "completionTokens": 14,

#               "promptTokens": 78,

#               "totalTokens": 92

#             },

#             "finish_reason": "stop",

#             "system_fingerprint": "fp_483d39d857"

#           },

#           "tool_calls": [],

#           "invalid_tool_calls": [],

#           "usage_metadata": {

#             "input_tokens": 78,

#             "output_tokens": 14,

#             "total_tokens": 92

#           }

#         }

#       ]

#     }

#   }


"""
## `returnIntermediateSteps`

Setting this parameter on AgentExecutor allows users to access
intermediate_steps, which pairs agent actions (e.g., tool invocations) with
their outcomes.
"""

const agentExecutorWithIntermediateSteps = new AgentExecutor({
  agent,
  tools,
  returnIntermediateSteps: true,
});

const result = await agentExecutorWithIntermediateSteps.invoke({
  input: query,
});

console.log(result.intermediateSteps);

# Output:
#   [

#     {

#       action: {

#         tool: "magic_function",

#         toolInput: { input: 3 },

#         toolCallId: "call_mbg1xgLEYEEWClbEaDe7p5tK",

#         log: 'Invoking "magic_function" with {"input":3}\n',

#         messageLog: [

#           AIMessageChunk {

#             "id": "chatcmpl-A7f0NdSRSUJsBP6ENTpiQD4LzpBAH",

#             "content": "",

#             "additional_kwargs": {

#               "tool_calls": [

#                 {

#                   "index": 0,

#                   "id": "call_mbg1xgLEYEEWClbEaDe7p5tK",

#                   "type": "function",

#                   "function": "[Object]"

#                 }

#               ]

#             },

#             "response_metadata": {

#               "prompt": 0,

#               "completion": 0,

#               "finish_reason": "tool_calls",

#               "system_fingerprint": "fp_54e2f484be"

#             },

#             "tool_calls": [

#               {

#                 "name": "magic_function",

#                 "args": {

#                   "input": 3

#                 },

#                 "id": "call_mbg1xgLEYEEWClbEaDe7p5tK",

#                 "type": "tool_call"

#               }

#             ],

#             "tool_call_chunks": [

#               {

#                 "name": "magic_function",

#                 "args": "{\"input\":3}",

#                 "id": "call_mbg1xgLEYEEWClbEaDe7p5tK",

#                 "index": 0,

#                 "type": "tool_call_chunk"

#               }

#             ],

#             "invalid_tool_calls": [],

#             "usage_metadata": {

#               "input_tokens": 61,

#               "output_tokens": 14,

#               "total_tokens": 75

#             }

#           }

#         ]

#       },

#       observation: "5"

#     }

#   ]


"""
By default the
[react agent executor](https://langchain-ai.github.io/langgraphjs/reference/functions/prebuilt.createReactAgent.html)
in LangGraph appends all messages to the central state. Therefore, it is easy to
see any intermediate steps by just looking at the full state.

"""

agentOutput = await app.invoke({
  messages: [
    { role: "user", content: query },
  ]
});

console.log(agentOutput.messages);
# Output:
#   [

#     HumanMessage {

#       "id": "46a825b2-13a3-4f19-b1aa-7716c53eb247",

#       "content": "what is the value of magic_function(3)?",

#       "additional_kwargs": {},

#       "response_metadata": {}

#     },

#     AIMessage {

#       "id": "chatcmpl-A7f0iUuWktC8gXztWZCjofqyCozY2",

#       "content": "",

#       "additional_kwargs": {

#         "tool_calls": [

#           {

#             "id": "call_ndsPDU58wsMeGaqr41cSlLlF",

#             "type": "function",

#             "function": "[Object]"

#           }

#         ]

#       },

#       "response_metadata": {

#         "tokenUsage": {

#           "completionTokens": 14,

#           "promptTokens": 55,

#           "totalTokens": 69

#         },

#         "finish_reason": "tool_calls",

#         "system_fingerprint": "fp_483d39d857"

#       },

#       "tool_calls": [

#         {

#           "name": "magic_function",

#           "args": {

#             "input": 3

#           },

#           "type": "tool_call",

#           "id": "call_ndsPDU58wsMeGaqr41cSlLlF"

#         }

#       ],

#       "invalid_tool_calls": [],

#       "usage_metadata": {

#         "input_tokens": 55,

#         "output_tokens": 14,

#         "total_tokens": 69

#       }

#     },

#     ToolMessage {

#       "id": "ac6aa309-bbfb-46cd-ba27-cbdbfd848705",

#       "content": "5",

#       "name": "magic_function",

#       "additional_kwargs": {},

#       "response_metadata": {},

#       "tool_call_id": "call_ndsPDU58wsMeGaqr41cSlLlF"

#     },

#     AIMessage {

#       "id": "chatcmpl-A7f0i7iHyDUV6is6sgwtcXivmFZ1x",

#       "content": "The value of `magic_function(3)` is 5.",

#       "additional_kwargs": {},

#       "response_metadata": {

#         "tokenUsage": {

#           "completionTokens": 14,

#           "promptTokens": 78,

#           "totalTokens": 92

#         },

#         "finish_reason": "stop",

#         "system_fingerprint": "fp_54e2f484be"

#       },

#       "tool_calls": [],

#       "invalid_tool_calls": [],

#       "usage_metadata": {

#         "input_tokens": 78,

#         "output_tokens": 14,

#         "total_tokens": 92

#       }

#     }

#   ]


"""
## `maxIterations`

`AgentExecutor` implements a `maxIterations` parameter, whereas this is
controlled via `recursionLimit` in LangGraph.

Note that in the LangChain `AgentExecutor`, an "iteration" includes a full turn of tool
invocation and execution. In LangGraph, each step contributes to the recursion
limit, so we will need to multiply by two (and add one) to get equivalent
results.

Here's an example of how you'd set this parameter with the legacy `AgentExecutor`:
"""

const badMagicTool = tool(async ({ input: _input }) => {
  return "Sorry, there was a temporary error. Please try again with the same input.";
}, {
  name: "magic_function",
  description: "Applies a magic function to an input.",
  schema: z.object({
    input: z.string(),
  }),
});

const badTools = [badMagicTool];

const spanishAgentExecutorWithMaxIterations = new AgentExecutor({
  agent: createToolCallingAgent({
    llm,
    tools: badTools,
    prompt: spanishPrompt,
  }),
  tools: badTools,
  verbose: true,
  maxIterations: 2,
});

await spanishAgentExecutorWithMaxIterations.invoke({ input: query });

"""
If the recursion limit is reached in LangGraph.js, the framework will raise a specific exception type that we can catch and manage similarly to AgentExecutor.
"""

import { GraphRecursionError } from "@langchain/langgraph";

const RECURSION_LIMIT = 2 * 2 + 1;

const appWithBadTools = createReactAgent({ llm, tools: badTools });

try {
  await appWithBadTools.invoke({
    messages: [
      { role: "user", content: query }
    ]
  }, {
    recursionLimit: RECURSION_LIMIT,
  });
} catch (e) {
  if (e instanceof GraphRecursionError) {
    console.log("Recursion limit reached.");
  } else {
    throw e;
  }
}
# Output:
#   Recursion limit reached.


"""
## Next steps

You've now learned how to migrate your LangChain agent executors to LangGraph.

Next, check out other [LangGraph how-to guides](https://langchain-ai.github.io/langgraphjs/how-tos/).
"""



================================================
FILE: docs/core_docs/docs/how_to/multi_vector.mdx
================================================
# How to generate multiple embeddings per document

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Retrievers](/docs/concepts/retrievers)
- [Text splitters](/docs/concepts/text_splitters)
- [Retrieval-augmented generation (RAG)](/docs/tutorials/rag)

:::

Embedding different representations of an original document, then returning the original document when any of the representations result in a search hit, can allow you to
tune and improve your retrieval performance. LangChain has a base [`MultiVectorRetriever`](https://api.js.langchain.com/classes/langchain.retrievers_multi_vector.MultiVectorRetriever.html) designed to do just this!

A lot of the complexity lies in how to create the multiple vectors per document.
This guide covers some of the common ways to create those vectors and use the `MultiVectorRetriever`.

Some methods to create multiple vectors per document include:

- smaller chunks: split a document into smaller chunks, and embed those (e.g. the [`ParentDocumentRetriever`](/docs/how_to/parent_document_retriever))
- summary: create a summary for each document, embed that along with (or instead of) the document
- hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document

Note that this also enables another method of adding embeddings - manually. This is great because you can explicitly add questions or queries that should lead to a document being recovered, giving you more control.

## Smaller chunks

Often times it can be useful to retrieve larger chunks of information, but embed smaller chunks.
This allows for embeddings to capture the semantic meaning as closely as possible, but for as much context as possible to be passed downstream.
NOTE: this is what the ParentDocumentRetriever does. Here we show what is going on under the hood.

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

import CodeBlock from "@theme/CodeBlock";
import SmallChunksExample from "@examples/retrievers/multi_vector_small_chunks.ts";

<CodeBlock language="typescript">{SmallChunksExample}</CodeBlock>

## Summary

Oftentimes a summary may be able to distill more accurately what a chunk is about, leading to better retrieval.
Here we show how to create summaries, and then embed those.

import SummaryExample from "@examples/retrievers/multi_vector_summary.ts";

<CodeBlock language="typescript">{SummaryExample}</CodeBlock>

## Hypothetical queries

An LLM can also be used to generate a list of hypothetical questions that could be asked of a particular document.
These questions can then be embedded and used to retrieve the original document:

import HypotheticalExample from "@examples/retrievers/multi_vector_hypothetical.ts";

<CodeBlock language="typescript">{HypotheticalExample}</CodeBlock>

## Next steps

You've now learned a few ways to generate multiple embeddings per document.

Next, check out the individual sections for deeper dives on specific retrievers, the [broader tutorial on RAG](/docs/tutorials/rag), or this section to learn how to
[create your own custom retriever over any data source](/docs/how_to/custom_retriever/).



================================================
FILE: docs/core_docs/docs/how_to/multimodal_inputs.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to pass multimodal data directly to models

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat models](/docs/concepts/chat_models)

:::

Here we demonstrate how to pass multimodal input directly to models. 
We currently expect all input to be passed in the same format as [OpenAI expects](https://platform.openai.com/docs/guides/vision).
For other model providers that support multimodal input, we have added logic inside the class to convert to the expected format.

In this example we will ask a model to describe an image.
"""

import * as fs from "node:fs/promises";

import { ChatAnthropic } from "@langchain/anthropic";

const model = new ChatAnthropic({
  model: "claude-3-sonnet-20240229",
});

const imageData = await fs.readFile("../../../../examples/hotdog.jpg");

"""
The most commonly supported way to pass in images is to pass it in as a byte string within a message with a complex content type for models that support multimodal input. Here's an example:
"""

import { HumanMessage } from "@langchain/core/messages";

const message = new HumanMessage({
  content: [
    {
      type: "text",
      text: "what does this image contain?"},
    {
      type: "image_url",
      image_url: {
        url: `data:image/jpeg;base64,${imageData.toString("base64")}`},
    },
  ],
})
const response = await model.invoke([message]);
console.log(response.content);
# Output:
#   This image contains a hot dog. It shows a frankfurter or sausage encased in a soft, elongated bread bun. The sausage itself appears to be reddish in color, likely a smoked or cured variety. The bun is a golden-brown color, suggesting it has been lightly toasted or grilled. The hot dog is presented against a plain white background, allowing the details of the iconic American fast food item to be clearly visible.


"""
Some model providers support taking an HTTP URL to the image directly in a content block of type `"image_url"`:
"""

import { ChatOpenAI } from "@langchain/openai";

const openAIModel = new ChatOpenAI({
  model: "gpt-4o",
});

const imageUrl = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg";

const message = new HumanMessage({
  content: [
    {
      type: "text",
      text: "describe the weather in this image"},
    {
      type: "image_url",
      image_url: { url: imageUrl }
    },
  ],
});
const response = await openAIModel.invoke([message]);
console.log(response.content);
# Output:
#   The weather in the image appears to be pleasant and clear. The sky is mostly blue with a few scattered clouds, indicating good visibility and no immediate signs of rain. The lighting suggests it’s either morning or late afternoon, with sunlight creating a warm and bright atmosphere. There is no indication of strong winds, as the grass and foliage appear calm and undisturbed. Overall, it looks like a beautiful day, possibly spring or summer, ideal for outdoor activities.


"""
We can also pass in multiple images.
"""

const message = new HumanMessage({
  content: [
    {
      type: "text",
      text: "are these two images the same?"
    },
    {
      type: "image_url",
      image_url: {
        url: imageUrl
      }
    },
    {
      type: "image_url",
      image_url: {
        url: imageUrl
      }
    },
  ],
});
const response = await openAIModel.invoke([message]);
console.log(response.content);
# Output:
#   Yes, the two images are the same.


"""
## Next steps

You've now learned how to pass multimodal data to a modal.

Next, you can check out our guide on [multimodal tool calls](/docs/how_to/tool_calls_multimodal).
"""



================================================
FILE: docs/core_docs/docs/how_to/multimodal_prompts.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to use multimodal prompts

Here we demonstrate how to use prompt templates to format multimodal inputs to models. 

In this example we will ask a model to describe an image.

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat models](/docs/concepts/chat_models)
- [LangChain Tools](/docs/concepts/tools)

:::

```{=mdx}
import Npm2Yarn from "@theme/Npm2Yarn"

<Npm2Yarn>
  axios @langchain/openai @langchain/core
</Npm2Yarn>
```
"""

import axios from "axios";

const imageUrl = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg";
const axiosRes = await axios.get(imageUrl, { responseType: "arraybuffer" });
const base64 = btoa(
  new Uint8Array(axiosRes.data).reduce(
    (data, byte) => data + String.fromCharCode(byte),
    ''
  )
);

import { ChatPromptTemplate } from "@langchain/core/prompts";
import { ChatOpenAI } from "@langchain/openai";

const model = new ChatOpenAI({ model: "gpt-4o" })

const prompt = ChatPromptTemplate.fromMessages(
    [
        ["system", "Describe the image provided"],
        [
            "user",
            [{ type: "image_url", image_url: "data:image/jpeg;base64,{base64}" }],
        ]
    ]
)

const chain = prompt.pipe(model);

const response = await chain.invoke({ base64 })
console.log(response.content)
# Output:
#   The image depicts a scenic outdoor landscape featuring a wooden boardwalk path extending forward through a large field of green grass and vegetation. On either side of the path, the grass is lush and vibrant, with a variety of bushes and low shrubs visible as well. The sky overhead is expansive and mostly clear, adorned with soft, wispy clouds, illuminated by the light giving a warm and serene ambiance. In the distant background, there are clusters of trees and additional foliage, suggesting a natural and tranquil setting, ideal for a peaceful walk or nature exploration.


"""
We can also pass in multiple images.
"""

const promptWithMultipleImages = ChatPromptTemplate.fromMessages(
    [
        ["system", "compare the two pictures provided"],
        [
            "user",
            [
                {
                    "type": "image_url",
                    "image_url": "data:image/jpeg;base64,{imageData1}",
                },
                {
                    "type": "image_url",
                    "image_url": "data:image/jpeg;base64,{imageData2}",
                },
            ],
        ],
    ]
)

const chainWithMultipleImages = promptWithMultipleImages.pipe(model);

const res = await chainWithMultipleImages.invoke({ imageData1: base64, imageData2: base64 })
console.log(res.content)
# Output:
#   The two images provided are identical. Both show a wooden boardwalk path extending into a grassy field under a blue sky with scattered clouds. The scenery includes green shrubs and trees in the background, with a bright and clear sky above.




================================================
FILE: docs/core_docs/docs/how_to/multiple_queries.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to generate multiple queries to retrieve data for

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Vector stores](/docs/concepts/#vectorstores)
- [Retrievers](/docs/concepts/retrievers)
- [Retrieval-augmented generation (RAG)](/docs/tutorials/rag)

:::

Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on "distance".
But retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well.
Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious.

The [`MultiQueryRetriever`](https://api.js.langchain.com/classes/langchain.retrievers_multi_query.MultiQueryRetriever.html) automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query.
For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents.
By generating multiple perspectives on the same question, the `MultiQueryRetriever` can help overcome some of the limitations of the distance-based retrieval and get a richer set of results.

## Get started

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/anthropic @langchain/cohere
</Npm2Yarn>
```
"""

import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { CohereEmbeddings } from "@langchain/cohere";
import { MultiQueryRetriever } from "langchain/retrievers/multi_query";
import { ChatAnthropic } from "@langchain/anthropic";

const embeddings = new CohereEmbeddings();

const vectorstore = await MemoryVectorStore.fromTexts(
  [
    "Buildings are made out of brick",
    "Buildings are made out of wood",
    "Buildings are made out of stone",
    "Cars are made out of metal",
    "Cars are made out of plastic",
    "mitochondria is the powerhouse of the cell",
    "mitochondria is made of lipids",
  ],
  [{ id: 1 }, { id: 2 }, { id: 3 }, { id: 4 }, { id: 5 }],
  embeddings
);

const model = new ChatAnthropic({
  model: "claude-3-sonnet-20240229"
});

const retriever = MultiQueryRetriever.fromLLM({
  llm: model,
  retriever: vectorstore.asRetriever(),
});

const query = "What are mitochondria made of?";
const retrievedDocs = await retriever.invoke(query);

/*
  Generated queries: What are the components of mitochondria?,What substances comprise the mitochondria organelle?  ,What is the molecular composition of mitochondria?
*/

console.log(retrievedDocs);
# Output:
#   [

#     Document {

#       pageContent: "mitochondria is made of lipids",

#       metadata: {}

#     },

#     Document {

#       pageContent: "mitochondria is the powerhouse of the cell",

#       metadata: {}

#     },

#     Document {

#       pageContent: "Buildings are made out of brick",

#       metadata: { id: 1 }

#     },

#     Document {

#       pageContent: "Buildings are made out of wood",

#       metadata: { id: 2 }

#     }

#   ]


"""
## Customization

You can also supply a custom prompt to tune what types of questions are generated.
You can also pass a custom output parser to parse and split the results of the LLM call into a list of queries.
"""

import { LLMChain } from "langchain/chains";
import { pull } from "langchain/hub";
import { BaseOutputParser } from "@langchain/core/output_parsers";
import { PromptTemplate } from "@langchain/core/prompts";


type LineList = {
  lines: string[];
};

class LineListOutputParser extends BaseOutputParser<LineList> {
  static lc_name() {
    return "LineListOutputParser";
  }

  lc_namespace = ["langchain", "retrievers", "multiquery"];

  async parse(text: string): Promise<LineList> {
    const startKeyIndex = text.indexOf("<questions>");
    const endKeyIndex = text.indexOf("</questions>");
    const questionsStartIndex =
      startKeyIndex === -1 ? 0 : startKeyIndex + "<questions>".length;
    const questionsEndIndex = endKeyIndex === -1 ? text.length : endKeyIndex;
    const lines = text
      .slice(questionsStartIndex, questionsEndIndex)
      .trim()
      .split("\n")
      .filter((line) => line.trim() !== "");
    return { lines };
  }

  getFormatInstructions(): string {
    throw new Error("Not implemented.");
  }
}

// Default prompt is available at: https://smith.langchain.com/hub/jacob/multi-vector-retriever-german
const prompt: PromptTemplate = await pull(
  "jacob/multi-vector-retriever-german"
);

const vectorstore = await MemoryVectorStore.fromTexts(
  [
    "Gebäude werden aus Ziegelsteinen hergestellt",
    "Gebäude werden aus Holz hergestellt",
    "Gebäude werden aus Stein hergestellt",
    "Autos werden aus Metall hergestellt",
    "Autos werden aus Kunststoff hergestellt",
    "Mitochondrien sind die Energiekraftwerke der Zelle",
    "Mitochondrien bestehen aus Lipiden",
  ],
  [{ id: 1 }, { id: 2 }, { id: 3 }, { id: 4 }, { id: 5 }],
  embeddings
);
const model = new ChatAnthropic({});
const llmChain = new LLMChain({
  llm: model,
  prompt,
  outputParser: new LineListOutputParser(),
});
const retriever = new MultiQueryRetriever({
  retriever: vectorstore.asRetriever(),
  llmChain,
});

const query = "What are mitochondria made of?";
const retrievedDocs = await retriever.invoke(query);

/*
  Generated queries: Was besteht ein Mitochondrium?,Aus welchen Komponenten setzt sich ein Mitochondrium zusammen?  ,Welche Moleküle finden sich in einem Mitochondrium?
*/

console.log(retrievedDocs);
# Output:
#   [

#     Document {

#       pageContent: "Mitochondrien bestehen aus Lipiden",

#       metadata: {}

#     },

#     Document {

#       pageContent: "Mitochondrien sind die Energiekraftwerke der Zelle",

#       metadata: {}

#     },

#     Document {

#       pageContent: "Gebäude werden aus Stein hergestellt",

#       metadata: { id: 3 }

#     },

#     Document {

#       pageContent: "Autos werden aus Metall hergestellt",

#       metadata: { id: 4 }

#     },

#     Document {

#       pageContent: "Gebäude werden aus Holz hergestellt",

#       metadata: { id: 2 }

#     },

#     Document {

#       pageContent: "Gebäude werden aus Ziegelsteinen hergestellt",

#       metadata: { id: 1 }

#     }

#   ]


"""
## Next steps

You've now learned how to use the `MultiQueryRetriever` to query a vector store with automatically generated queries.

See the individual sections for deeper dives on specific retrievers, the [broader tutorial on RAG](/docs/tutorials/rag), or this section to learn how to
[create your own custom retriever over any data source](/docs/how_to/custom_retriever/).
"""



================================================
FILE: docs/core_docs/docs/how_to/output_parser_fixing.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to try to fix errors in output parsing

:::info Prerequisites

This guide assumes familiarity with the following concepts:
- [Chat models](/docs/concepts/chat_models)
- [Output parsers](/docs/concepts/output_parsers)
- [Prompt templates](/docs/concepts/prompt_templates)
- [Chaining runnables together](/docs/how_to/sequence/)

:::

LLMs aren't perfect, and sometimes fail to produce output that perfectly matches a the desired format. To help handle errors, we can use the [`OutputFixingParser`](https://api.js.langchain.com/classes/langchain.output_parsers.OutputFixingParser.html) This output parser wraps another output parser, and in the event that the first one fails, it calls out to another LLM in an attempt to fix any errors.

Specifically, we can pass the misformatted output, along with the formatted instructions, to the model and ask it to fix it.

For this example, we'll use the [`StructuredOutputParser`](https://api.js.langchain.com/classes/langchain_core.output_parsers.StructuredOutputParser.html), which can validate output according to a Zod schema. Here's what happens if we pass it a result that does not comply with the schema:
"""

import { z } from "zod";
import { RunnableSequence } from "@langchain/core/runnables";
import { StructuredOutputParser } from "@langchain/core/output_parsers";
import { ChatPromptTemplate } from "@langchain/core/prompts";

const zodSchema = z.object({
  name: z.string().describe("name of an actor"),
  film_names: z.array(z.string()).describe("list of names of films they starred in"),
});

const parser = StructuredOutputParser.fromZodSchema(zodSchema);

const misformatted = "{'name': 'Tom Hanks', 'film_names': ['Forrest Gump']}";

await parser.parse(misformatted);
# Output:
#   Error: Error: Failed to parse. Text: "{'name': 'Tom Hanks', 'film_names': ['Forrest Gump']}". Error: SyntaxError: Expected property name or '}' in JSON at position 1 (line 1 column 2)

"""
Now we can construct and use a `OutputFixingParser`. This output parser takes as an argument another output parser but also an LLM with which to try to correct any formatting mistakes.
"""

import { ChatAnthropic } from "@langchain/anthropic";

import { OutputFixingParser } from "langchain/output_parsers";

const model = new ChatAnthropic({
  model: "claude-3-sonnet-20240229",
  maxTokens: 512,
  temperature: 0.1,
});

const parserWithFix = OutputFixingParser.fromLLM(model, parser);

await parserWithFix.parse(misformatted);
# Output:
#   {

#     name: [32m"Tom Hanks"[39m,

#     film_names: [

#       [32m"Forrest Gump"[39m,

#       [32m"Saving Private Ryan"[39m,

#       [32m"Cast Away"[39m,

#       [32m"Catch Me If You Can"[39m

#     ]

#   }

"""
For more about different parameters and options, check out our [API reference docs](https://api.js.langchain.com/classes/langchain.output_parsers.OutputFixingParser.html).
"""



================================================
FILE: docs/core_docs/docs/how_to/output_parser_json.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to parse JSON output

While some model providers support [built-in ways to return structured output](/docs/how_to/structured_output), not all do. We can use an output parser to help users to specify an arbitrary JSON schema via the prompt, query a model for outputs that conform to that schema, and finally parse that schema as JSON.

:::{.callout-note}
Keep in mind that large language models are leaky abstractions! You'll have to use an LLM with sufficient capacity to generate well-formed JSON.
:::

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat models](/docs/concepts/chat_models)
- [Output parsers](/docs/concepts/output_parsers)
- [Prompt templates](/docs/concepts/prompt_templates)
- [Structured output](/docs/how_to/structured_output)
- [Chaining runnables together](/docs/how_to/sequence/)

:::
"""

"""
The [`JsonOutputParser`](https://api.js.langchain.com/classes/langchain_core.output_parsers.JsonOutputParser.html) is one built-in option for prompting for and then parsing JSON output.
"""

"""
```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs />
```
"""

import { ChatOpenAI } from "@langchain/openai";
const model = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0,
})

import { JsonOutputParser } from "@langchain/core/output_parsers"
import { ChatPromptTemplate } from "@langchain/core/prompts"

// Define your desired data structure. Only used for typing the parser output.
interface Joke {
  setup: string
  punchline: string
}

// A query and format instructions used to prompt a language model.
const jokeQuery = "Tell me a joke.";
const formatInstructions = "Respond with a valid JSON object, containing two fields: 'setup' and 'punchline'."

// Set up a parser + inject instructions into the prompt template.
const parser = new JsonOutputParser<Joke>()

const prompt = ChatPromptTemplate.fromTemplate(
  "Answer the user query.\n{format_instructions}\n{query}\n"
);

const partialedPrompt = await prompt.partial({
  format_instructions: formatInstructions
});

const chain = partialedPrompt.pipe(model).pipe(parser);

await chain.invoke({ query: jokeQuery });
# Output:
#   {

#     setup: [32m"Why don't scientists trust atoms?"[39m,

#     punchline: [32m"Because they make up everything!"[39m

#   }

"""
## Streaming

The `JsonOutputParser` also supports streaming partial chunks. This is useful when the model returns partial JSON output in multiple chunks. The parser will keep track of the partial chunks and return the final JSON output when the model finishes generating the output.
"""

for await (const s of await chain.stream({ query: jokeQuery })) {
    console.log(s)
}
# Output:
#   {}

#   { setup: "" }

#   { setup: "Why" }

#   { setup: "Why don't" }

#   { setup: "Why don't scientists" }

#   { setup: "Why don't scientists trust" }

#   { setup: "Why don't scientists trust atoms" }

#   { setup: "Why don't scientists trust atoms?", punchline: "" }

#   { setup: "Why don't scientists trust atoms?", punchline: "Because" }

#   {

#     setup: "Why don't scientists trust atoms?",

#     punchline: "Because they"

#   }

#   {

#     setup: "Why don't scientists trust atoms?",

#     punchline: "Because they make"

#   }

#   {

#     setup: "Why don't scientists trust atoms?",

#     punchline: "Because they make up"

#   }

#   {

#     setup: "Why don't scientists trust atoms?",

#     punchline: "Because they make up everything"

#   }

#   {

#     setup: "Why don't scientists trust atoms?",

#     punchline: "Because they make up everything!"

#   }


"""
## Next steps

You've now learned one way to prompt a model to return structured JSON. Next, check out the [broader guide on obtaining structured output](/docs/how_to/structured_output) for other techniques.
"""



================================================
FILE: docs/core_docs/docs/how_to/output_parser_structured.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_position: 3
---
"""

"""
# How to use output parsers to parse an LLM response into structured format

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Output parsers](/docs/concepts/output_parsers)
- [Chat models](/docs/concepts/chat_models)

:::

Language models output text. But there are times where you want to get more structured information than just text back. While some model providers support [built-in ways to return structured output](/docs/how_to/structured_output), not all do. For these providers, you must use prompting to encourage the model to return structured data in the desired format.

LangChain has [output parsers](/docs/concepts/output_parsers) which can help parse model outputs into usable objects. We'll go over a few examples below.

## Get started

The primary type of output parser for working with structured data in model responses is the [`StructuredOutputParser`](https://api.js.langchain.com/classes/langchain_core.output_parsers.StructuredOutputParser.html). In the below example, we define a schema for the type of output we expect from the model using [`zod`](https://zod.dev).

First, let's see the default formatting instructions we'll plug into the prompt:
"""

"""
```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs />
```
"""

import { z } from "zod";
import { RunnableSequence } from "@langchain/core/runnables";
import { StructuredOutputParser } from "@langchain/core/output_parsers";
import { ChatPromptTemplate } from "@langchain/core/prompts";

const zodSchema = z.object({
  answer: z.string().describe("answer to the user's question"),
  source: z.string().describe("source used to answer the user's question, should be a website."),
})

const parser = StructuredOutputParser.fromZodSchema(zodSchema);

const chain = RunnableSequence.from([
  ChatPromptTemplate.fromTemplate(
    "Answer the users question as best as possible.\n{format_instructions}\n{question}"
  ),
  model,
  parser,
]);

console.log(parser.getFormatInstructions());

# Output:
#   You must format your output as a JSON value that adheres to a given "JSON Schema" instance.

#   

#   "JSON Schema" is a declarative language that allows you to annotate and validate JSON documents.

#   

#   For example, the example "JSON Schema" instance {{"properties": {{"foo": {{"description": "a list of test words", "type": "array", "items": {{"type": "string"}}}}}}, "required": ["foo"]}}}}

#   would match an object with one required property, "foo". The "type" property specifies "foo" must be an "array", and the "description" property semantically describes it as "a list of test words". The items within "foo" must be strings.

#   Thus, the object {{"foo": ["bar", "baz"]}} is a well-formatted instance of this example "JSON Schema". The object {{"properties": {{"foo": ["bar", "baz"]}}}} is not well-formatted.

#   

#   Your output will be parsed and type-checked according to the provided schema instance, so make sure all fields in your output match the schema exactly and there are no trailing commas!

#   

#   Here is the JSON Schema instance your output must adhere to. Include the enclosing markdown codeblock:

#   ```json

#   {"type":"object","properties":{"answer":{"type":"string","description":"answer to the user's question"},"source":{"type":"string","description":"source used to answer the user's question, should be a website."}},"required":["answer","source"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}

#   ```

#   


"""
Next, let's invoke the chain:
"""

const response = await chain.invoke({
  question: "What is the capital of France?",
  format_instructions: parser.getFormatInstructions(),
});

console.log(response);
# Output:
#   {

#     answer: "The capital of France is Paris.",

#     source: "https://en.wikipedia.org/wiki/Paris"

#   }


"""
Output parsers implement the [Runnable interface](/docs/how_to/#langchain-expression-language-lcel), the basic building block of [LangChain Expression Language (LCEL)](/docs/how_to/#langchain-expression-language-lcel). This means they support `invoke`, `stream`, `batch`, `streamLog` calls.

## Validation

One feature of the `StructuredOutputParser` is that it supports stricter Zod validations. For example, if you pass a simulated model output that does not conform to the schema, we get a detailed type error:
"""

import { AIMessage } from "@langchain/core/messages";

await parser.invoke(new AIMessage(`{"badfield": "foo"}`));
# Output:
#   Error: Error: Failed to parse. Text: "{"badfield": "foo"}". Error: [
  {
    "code": "invalid_type",
    "expected": "string",
    "received": "undefined",
    "path": [
      "answer"
    ],
    "message": "Required"
  },
  {
    "code": "invalid_type",
    "expected": "string",
    "received": "undefined",
    "path": [
      "source"
    ],
    "message": "Required"
  }
]

"""
Compared to:
"""

await parser.invoke(new AIMessage(`{"answer": "Paris", "source": "I made it up"}`));
# Output:
#   { answer: [32m"Paris"[39m, source: [32m"I made it up"[39m }

"""
More advanced Zod validations are supported as well. To learn more, check out the [Zod documentation](https://zod.dev).
"""

"""
## Streaming

While all parsers are runnables and support the streaming interface, only certain parsers can stream through partially parsed objects, since this is highly dependent on the output type. The `StructuredOutputParser` does not support partial streaming because it validates the output at each step. If you try to stream using a chain with this output parser, the chain will simply yield the fully parsed output:
"""

const stream = await chain.stream({
  question: "What is the capital of France?",
  format_instructions: parser.getFormatInstructions(),
});

for await (const s of stream) {
  console.log(s)
}
# Output:
#   {

#     answer: "The capital of France is Paris.",

#     source: "https://en.wikipedia.org/wiki/Paris"

#   }


"""
The simpler [`JsonOutputParser`](https://api.js.langchain.com/classes/langchain_core.output_parsers.JsonOutputParser.html), however, supports streaming through partial outputs:
"""

import { JsonOutputParser } from "@langchain/core/output_parsers";

const template = `Return a JSON object with a single key named "answer" that answers the following question: {question}.
Do not wrap the JSON output in markdown blocks.`

const jsonPrompt = ChatPromptTemplate.fromTemplate(template);
const jsonParser = new JsonOutputParser();
const jsonChain = jsonPrompt.pipe(model).pipe(jsonParser);

const stream = await jsonChain.stream({
  question: "Who invented the microscope?",
});

for await (const s of stream) {
  console.log(s)
}
# Output:
#   {}

#   { answer: "" }

#   { answer: "The" }

#   { answer: "The invention" }

#   { answer: "The invention of" }

#   { answer: "The invention of the" }

#   { answer: "The invention of the microscope" }

#   { answer: "The invention of the microscope is" }

#   { answer: "The invention of the microscope is attributed" }

#   { answer: "The invention of the microscope is attributed to" }

#   { answer: "The invention of the microscope is attributed to Hans" }

#   { answer: "The invention of the microscope is attributed to Hans L" }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippers"

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey"

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey,"

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zach"

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias"

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Jans"

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen"

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen,"

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and"

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Anton"

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie"

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 4 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 8 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 12 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 13 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 18 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 20 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 26 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 29 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 33 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 38 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 43 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 48 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 51 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 52 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 57 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 63 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 73 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 80 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 81 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 85 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 94 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 99 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 108 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 112 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 118 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 127 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 138 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 145 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 149 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 150 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 151 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 157 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 159 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 163 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 167 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 171 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 175 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 176 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 181 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 186 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 190 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 202 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 203 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 209 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 214 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 226 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 239 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 242 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 246 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 253 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 257 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 262 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 265 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 268 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 273 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 288 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 300 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 303 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 311 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 316 more characters

#   }

#   {

#     answer: "The invention of the microscope is attributed to Hans Lippershey, Zacharias Janssen, and Antonie van"... 317 more characters

#   }


"""
## Next steps

You've learned about using output parsers to parse structured outputs from prompted model outputs.

Next, check out the [guide on tool calling](/docs/how_to/tool_calling), a more built-in way of obtaining structured output that some model providers support, or read more about output parsers for other types of structured data like [XML](/docs/how_to/output_parser_xml).
"""



================================================
FILE: docs/core_docs/docs/how_to/output_parser_xml.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to parse XML output

:::info Prerequisites

This guide assumes familiarity with the following concepts:
- [Chat models](/docs/concepts/chat_models)
- [Output parsers](/docs/concepts/output_parsers)
- [Prompt templates](/docs/concepts/prompt_templates)
- [Structured output](/docs/how_to/structured_output)
- [Chaining runnables together](/docs/how_to/sequence/)

:::

LLMs from different providers often have different strengths depending on the specific data they are trianed on. This also means that some may be "better" and more reliable at generating output in formats other than JSON.

This guide shows you how to use the [`XMLOutputParser`](https://api.js.langchain.com/classes/langchain_core.output_parsers.XMLOutputParser.html) to prompt models for XML output, then and parse that output into a usable format.

:::{.callout-note}
Keep in mind that large language models are leaky abstractions! You'll have to use an LLM with sufficient capacity to generate well-formed XML.
:::

In the following examples, we use Anthropic's Claude (https://docs.anthropic.com/claude/docs), which is one such model that is optimized for XML tags.

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/anthropic @langchain/core
</Npm2Yarn>
```
"""

"""
Let's start with a simple request to the model.
"""

import { ChatAnthropic } from "@langchain/anthropic";

const model = new ChatAnthropic({
  model: "claude-3-sonnet-20240229",
  maxTokens: 512,
  temperature: 0.1,
});

const query = `Generate the shortened filmograph for Tom Hanks.`;

const result = await model.invoke(query + ` Please enclose the movies in "movie" tags.`);

console.log(result.content);
# Output:
#   Here is the shortened filmography for Tom Hanks, with movies enclosed in "movie" tags:

#   

#   <movie>Forrest Gump</movie>

#   <movie>Saving Private Ryan</movie>

#   <movie>Cast Away</movie>

#   <movie>Apollo 13</movie>

#   <movie>Catch Me If You Can</movie>

#   <movie>The Green Mile</movie>

#   <movie>Toy Story</movie>

#   <movie>Toy Story 2</movie>

#   <movie>Toy Story 3</movie>

#   <movie>Toy Story 4</movie>

#   <movie>Philadelphia</movie>

#   <movie>Big</movie>

#   <movie>Sleepless in Seattle</movie>

#   <movie>You've Got Mail</movie>

#   <movie>The Terminal</movie>


"""
This actually worked pretty well! But it would be nice to parse that XML into a more easily usable format. We can use the `XMLOutputParser` to both add default format instructions to the prompt and parse outputted XML into a dict:
"""

import { XMLOutputParser } from "@langchain/core/output_parsers";

// We will add these instructions to the prompt below
const parser = new XMLOutputParser();

parser.getFormatInstructions();
# Output:
#   [32m"The output should be formatted as a XML file.\n"[39m +

#     [32m"1. Output should conform to the tags below. \n"[39m +

#     [32m"2. If tag"[39m... 434 more characters

import { ChatPromptTemplate } from "@langchain/core/prompts";

const prompt = ChatPromptTemplate.fromTemplate(`{query}\n{format_instructions}`);
const partialedPrompt = await prompt.partial({
  format_instructions: parser.getFormatInstructions(),
});

const chain = partialedPrompt.pipe(model).pipe(parser);

const output = await chain.invoke({
  query: "Generate the shortened filmograph for Tom Hanks.",
});

console.log(JSON.stringify(output, null, 2));
# Output:
#   {

#     "filmography": [

#       {

#         "actor": [

#           {

#             "name": "Tom Hanks"

#           },

#           {

#             "films": [

#               {

#                 "film": [

#                   {

#                     "title": "Forrest Gump"

#                   },

#                   {

#                     "year": "1994"

#                   },

#                   {

#                     "role": "Forrest Gump"

#                   }

#                 ]

#               },

#               {

#                 "film": [

#                   {

#                     "title": "Saving Private Ryan"

#                   },

#                   {

#                     "year": "1998"

#                   },

#                   {

#                     "role": "Captain Miller"

#                   }

#                 ]

#               },

#               {

#                 "film": [

#                   {

#                     "title": "Cast Away"

#                   },

#                   {

#                     "year": "2000"

#                   },

#                   {

#                     "role": "Chuck Noland"

#                   }

#                 ]

#               },

#               {

#                 "film": [

#                   {

#                     "title": "Catch Me If You Can"

#                   },

#                   {

#                     "year": "2002"

#                   },

#                   {

#                     "role": "Carl Hanratty"

#                   }

#                 ]

#               },

#               {

#                 "film": [

#                   {

#                     "title": "The Terminal"

#                   },

#                   {

#                     "year": "2004"

#                   },

#                   {

#                     "role": "Viktor Navorski"

#                   }

#                 ]

#               }

#             ]

#           }

#         ]

#       }

#     ]

#   }


"""
You'll notice above that our output is no longer just between `movie` tags. We can also add some tags to tailor the output to our needs:
"""

const parserWithTags = new XMLOutputParser({ tags: ["movies", "actor", "film", "name", "genre"] });

// We will add these instructions to the prompt below
parserWithTags.getFormatInstructions();
# Output:
#   [32m"The output should be formatted as a XML file.\n"[39m +

#     [32m"1. Output should conform to the tags below. \n"[39m +

#     [32m"2. If tag"[39m... 460 more characters

"""
You can and should experiment with adding your own formatting hints in the other parts of your prompt to either augment or replace the default instructions.

Here's the result when we invoke it:
"""

import { ChatPromptTemplate } from "@langchain/core/prompts";

const promptWithTags = ChatPromptTemplate.fromTemplate(`{query}\n{format_instructions}`);
const partialedPromptWithTags = await promptWithTags.partial({
  format_instructions: parserWithTags.getFormatInstructions(),
});

const chainWithTags = partialedPromptWithTags.pipe(model).pipe(parserWithTags);

const outputWithTags = await chainWithTags.invoke({
  query: "Generate the shortened filmograph for Tom Hanks.",
});

console.log(JSON.stringify(outputWithTags, null, 2));
# Output:
#   {

#     "movies": [

#       {

#         "actor": [

#           {

#             "film": [

#               {

#                 "name": "Forrest Gump"

#               },

#               {

#                 "genre": "Drama"

#               }

#             ]

#           },

#           {

#             "film": [

#               {

#                 "name": "Saving Private Ryan"

#               },

#               {

#                 "genre": "War"

#               }

#             ]

#           },

#           {

#             "film": [

#               {

#                 "name": "Cast Away"

#               },

#               {

#                 "genre": "Drama"

#               }

#             ]

#           },

#           {

#             "film": [

#               {

#                 "name": "Catch Me If You Can"

#               },

#               {

#                 "genre": "Biography"

#               }

#             ]

#           },

#           {

#             "film": [

#               {

#                 "name": "The Terminal"

#               },

#               {

#                 "genre": "Comedy-drama"

#               }

#             ]

#           }

#         ]

#       }

#     ]

#   }


"""
## Next steps

You've now learned how to prompt a model to return XML. Next, check out the [broader guide on obtaining structured output](/docs/how_to/structured_output) for other related techniques.
"""



================================================
FILE: docs/core_docs/docs/how_to/parallel.mdx
================================================
# How to invoke runnables in parallel

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [LangChain Expression Language (LCEL)](/docs/concepts/lcel)
- [Chaining runnables](/docs/how_to/sequence/)

:::

The [`RunnableParallel`](https://api.js.langchain.com/classes/langchain_core.runnables.RunnableParallel.html) (also known as a `RunnableMap`) primitive is an object whose values are runnables (or things that can be coerced to runnables, like functions).
It runs all of its values in parallel, and each value is called with the initial input to the `RunnableParallel`. The final return value is an object with the results of each value under its appropriate key.

## Formatting with `RunnableParallels`

`RunnableParallels` are useful for parallelizing operations, but can also be useful for manipulating the output of one Runnable to match the input format of the next Runnable in a sequence. You can use them to split or fork the chain so that multiple components can process the input in parallel. Later, other components can join or merge the results to synthesize a final response. This type of chain creates a computation graph that looks like the following:

```text
     Input
      / \
     /   \
 Branch1 Branch2
     \   /
      \ /
      Combine
```

Below, the input to each chain in the `RunnableParallel` is expected to be an object with a key for `"topic"`.
We can satisfy that requirement by invoking our chain with an object matching that structure.

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/anthropic @langchain/cohere @langchain/core
```

import CodeBlock from "@theme/CodeBlock";
import BasicExample from "@examples/guides/expression_language/runnable_maps_basic.ts";

<CodeBlock language="typescript">{BasicExample}</CodeBlock>

## Manipulating outputs/inputs

Maps can be useful for manipulating the output of one Runnable to match the input format of the next Runnable in a sequence.

Note below that the object within the `RunnableSequence.from()` call is automatically coerced into a runnable map. All keys of the object must
have values that are runnables or can be themselves coerced to runnables (functions to `RunnableLambda`s or objects to `RunnableMap`s).
This coercion will also occur when composing chains via the `.pipe()` method.

import SequenceExample from "@examples/guides/expression_language/runnable_maps_sequence.ts";

<CodeBlock language="typescript">{SequenceExample}</CodeBlock>

Here the input to prompt is expected to be a map with keys "context" and "question". The user input is just the question. So we need to get the context using our retriever and passthrough the user input under the "question" key.

## Next steps

You now know some ways to format and parallelize chain steps with `RunnableParallel`.

Next, you might be interested in [using custom logic](/docs/how_to/functions/) in your chains.



================================================
FILE: docs/core_docs/docs/how_to/parent_document_retriever.mdx
================================================
import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/retrievers/parent_document_retriever.ts";
import ExampleWithScoreThreshold from "@examples/retrievers/parent_document_retriever_score_threshold.ts";
import ExampleWithChunkHeader from "@examples/retrievers/parent_document_retriever_chunk_header.ts";
import ExampleWithRerank from "@examples/retrievers/parent_document_retriever_rerank.ts";

# How to retrieve the whole document for a chunk

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Retrievers](/docs/concepts/retrievers)
- [Text splitters](/docs/concepts/text_splitters)
- [Retrieval-augmented generation (RAG)](/docs/tutorials/rag)

:::

When splitting documents for retrieval, there are often conflicting desires:

1. You may want to have small documents, so that their embeddings can most accurately reflect their meaning. If documents are too long, then the embeddings can lose meaning.
2. You want to have long enough documents that the context of each chunk is retained.

The [`ParentDocumentRetriever`](https://api.js.langchain.com/classes/langchain.retrievers_parent_document.ParentDocumentRetriever.html) strikes that balance by splitting and storing small chunks of data. During retrieval, it first fetches the small chunks but then looks up the parent ids for those chunks and returns those larger documents.

Note that "parent document" refers to the document that a small chunk originated from. This can either be the whole raw document OR a larger chunk.

This is a more specific form of [generating multiple embeddings per document](/docs/how_to/multi_vector).

## Usage

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core
```

<CodeBlock language="typescript">{Example}</CodeBlock>

## With Score Threshold

By setting the options in `scoreThresholdOptions` we can force the `ParentDocumentRetriever` to use the `ScoreThresholdRetriever` under the hood.
This sets the vector store inside `ScoreThresholdRetriever` as the one we passed when initializing `ParentDocumentRetriever`, while also allowing us to also set a score threshold for the retriever.

This can be helpful when you're not sure how many documents you want (or if you are sure, just set the `maxK` option), but you want to make sure that the documents you do get are within a certain relevancy threshold.

Note: if a retriever is passed, `ParentDocumentRetriever` will default to use it for retrieving small chunks, as well as adding documents via the `addDocuments` method.

<CodeBlock language="typescript">{ExampleWithScoreThreshold}</CodeBlock>

## With Contextual chunk headers

Consider a scenario where you want to store collection of documents in a vector store and perform Q&A tasks on them. Simply splitting documents with overlapping text may not provide sufficient context for LLMs to determine if multiple chunks are referencing the same information, or how to resolve information from contradictory sources.

Tagging each document with metadata is a solution if you know what to filter against, but you may not know ahead of time exactly what kind of queries your vector store will be expected to handle. Including additional contextual information directly in each chunk in the form of headers can help deal with arbitrary queries.

This is particularly important if you have several fine-grained child chunks that need to be correctly retrieved from the vector store.

<CodeBlock language="typescript">{ExampleWithChunkHeader}</CodeBlock>

## With Reranking

With many documents from the vector store that are passed to LLM, final answers sometimes consist of information from
irrelevant chunks, making it less precise and sometimes incorrect. Also, passing multiple irrelevant documents makes it
more expensive.
So there are two reasons to use rerank - precision and costs.

<CodeBlock language="typescript">{ExampleWithRerank}</CodeBlock>

## Next steps

You've now learned how to use the `ParentDocumentRetriever`.

Next, check out the more general form of [generating multiple embeddings per document](/docs/how_to/multi_vector), the [broader tutorial on RAG](/docs/tutorials/rag), or this section to learn how to
[create your own custom retriever over any data source](/docs/how_to/custom_retriever/).



================================================
FILE: docs/core_docs/docs/how_to/passthrough.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_position: 5
keywords: [RunnablePassthrough, LCEL]
---
"""

"""
# How to pass through arguments from one step to the next

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [LangChain Expression Language (LCEL)](/docs/concepts/lcel)
- [Chaining runnables](/docs/how_to/sequence/)
- [Calling runnables in parallel](/docs/how_to/parallel/)
- [Custom functions](/docs/how_to/functions/)

:::


When composing chains with several steps, sometimes you will want to pass data from previous steps unchanged for use as input to a later step. The [`RunnablePassthrough`](https://api.js.langchain.com/classes/langchain_core.runnables.RunnablePassthrough.html) class allows you to do just this, and is typically is used in conjuction with a [RunnableParallel](/docs/how_to/parallel/) to pass data through to a later step in your constructed chains.

Let's look at an example:
"""

import { RunnableParallel, RunnablePassthrough } from "@langchain/core/runnables";

const runnable = RunnableParallel.from({
  passed: new RunnablePassthrough<{ num: number }>(),
  modified: (input: { num: number }) => input.num + 1,
});

await runnable.invoke({ num: 1 });
# Output:
#   { passed: { num: [33m1[39m }, modified: [33m2[39m }

"""
As seen above, `passed` key was called with `RunnablePassthrough()` and so it simply passed on `{'num': 1}`. 

We also set a second key in the map with `modified`. This uses a lambda to set a single value adding 1 to the num, which resulted in `modified` key with the value of `2`.
"""

"""
## Retrieval Example

In the example below, we see a more real-world use case where we use `RunnablePassthrough` along with `RunnableParallel` in a chain to properly format inputs to a prompt:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/openai @langchain/core
</Npm2Yarn>
```
"""

import { StringOutputParser } from "@langchain/core/output_parsers";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";
import { ChatOpenAI, OpenAIEmbeddings } from "@langchain/openai";
import { MemoryVectorStore } from "langchain/vectorstores/memory";

const vectorstore = await MemoryVectorStore.fromDocuments([
  { pageContent: "harrison worked at kensho", metadata: {} }
], new OpenAIEmbeddings());

const retriever = vectorstore.asRetriever();

const template = `Answer the question based only on the following context:
{context}

Question: {question}
`;

const prompt = ChatPromptTemplate.fromTemplate(template);

const model = new ChatOpenAI({ model: "gpt-4o" });

const retrievalChain = RunnableSequence.from([
  {
    context: retriever.pipe((docs) => docs[0].pageContent),
    question: new RunnablePassthrough()
  },
  prompt,
  model,
  new StringOutputParser(),
]);

await retrievalChain.invoke("where did harrison work?");
# Output:
#   [32m"Harrison worked at Kensho."[39m

"""
Here the input to prompt is expected to be a map with keys `"context"` and `"question"`. The user input is just the question. So we need to get the context using our retriever and passthrough the user input under the `"question"` key. The `RunnablePassthrough` allows us to pass on the user's question to the prompt and model.

## Next steps

Now you've learned how to pass data through your chains to help to help format the data flowing through your chains.

To learn more, see the other how-to guides on runnables in this section.
"""



================================================
FILE: docs/core_docs/docs/how_to/prompts_composition.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_position: 5
---
"""

"""
# How to compose prompts together

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Prompt templates](/docs/concepts/prompt_templates)

:::

LangChain provides a user friendly interface for composing different parts of prompts together. You can do this with either string prompts or chat prompts. Constructing prompts this way allows for easy reuse of components.
"""

"""
## String prompt composition

When working with string prompts, each template is joined together. You can work with either prompts directly or strings (the first element in the list needs to be a prompt).
"""

import { PromptTemplate } from "@langchain/core/prompts";

const prompt = PromptTemplate.fromTemplate(`Tell me a joke about {topic}, make it funny and in {language}`)

prompt
# Output:
#   PromptTemplate {

#     lc_serializable: [33mtrue[39m,

#     lc_kwargs: {

#       inputVariables: [ [32m"topic"[39m, [32m"language"[39m ],

#       templateFormat: [32m"f-string"[39m,

#       template: [32m"Tell me a joke about {topic}, make it funny and in {language}"[39m

#     },

#     lc_runnable: [33mtrue[39m,

#     name: [90mundefined[39m,

#     lc_namespace: [ [32m"langchain_core"[39m, [32m"prompts"[39m, [32m"prompt"[39m ],

#     inputVariables: [ [32m"topic"[39m, [32m"language"[39m ],

#     outputParser: [90mundefined[39m,

#     partialVariables: [90mundefined[39m,

#     templateFormat: [32m"f-string"[39m,

#     template: [32m"Tell me a joke about {topic}, make it funny and in {language}"[39m,

#     validateTemplate: [33mtrue[39m

#   }

await prompt.format({ topic: "sports", language: "spanish" })
# Output:
#   [32m"Tell me a joke about sports, make it funny and in spanish"[39m

"""
## Chat prompt composition
"""

"""
A chat prompt is made up a of a list of messages. Similarly to the above example, we can concatenate chat prompt templates. Each new element is a new message in the final prompt.

First, let's initialize the a [`ChatPromptTemplate`](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) with a [`SystemMessage`](https://api.python.langchain.com/en/latest/messages/langchain_core.messages.system.SystemMessage.html).
"""

import { AIMessage, HumanMessage, SystemMessage} from "@langchain/core/messages"

const prompt = new SystemMessage("You are a nice pirate")

"""
You can then easily create a pipeline combining it with other messages *or* message templates.
Use a `BaseMessage` when there are no variables to be formatted, use a `MessageTemplate` when there are variables to be formatted. You can also use just a string (note: this will automatically get inferred as a [`HumanMessagePromptTemplate`](https://api.js.langchain.com/classes/langchain_core.prompts.HumanMessagePromptTemplate.html).)
"""

import { HumanMessagePromptTemplate } from "@langchain/core/prompts"

const newPrompt = HumanMessagePromptTemplate.fromTemplate([prompt, new HumanMessage("Hi"), new AIMessage("what?"), "{input}"])

"""
Under the hood, this creates an instance of the ChatPromptTemplate class, so you can use it just as you did before!
"""

await newPrompt.formatMessages({ input: "i said hi" })
# Output:
#   [

#     HumanMessage {

#       lc_serializable: [33mtrue[39m,

#       lc_kwargs: {

#         content: [

#           { type: [32m"text"[39m, text: [32m"You are a nice pirate"[39m },

#           { type: [32m"text"[39m, text: [32m"Hi"[39m },

#           { type: [32m"text"[39m, text: [32m"what?"[39m },

#           { type: [32m"text"[39m, text: [32m"i said hi"[39m }

#         ],

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#       content: [

#         { type: [32m"text"[39m, text: [32m"You are a nice pirate"[39m },

#         { type: [32m"text"[39m, text: [32m"Hi"[39m },

#         { type: [32m"text"[39m, text: [32m"what?"[39m },

#         { type: [32m"text"[39m, text: [32m"i said hi"[39m }

#       ],

#       name: [90mundefined[39m,

#       additional_kwargs: {},

#       response_metadata: {}

#     }

#   ]

"""
## Using PipelinePrompt
"""

"""
LangChain includes a class called [`PipelinePromptTemplate`](https://api.js.langchain.com/classes/_langchain_core.prompts.PipelinePromptTemplate.html), which can be useful when you want to reuse parts of prompts. A PipelinePrompt consists of two main parts:

- Final prompt: The final prompt that is returned
- Pipeline prompts: A list of tuples, consisting of a string name and a prompt template. Each prompt template will be formatted and then passed to future prompt templates as a variable with the same name.
"""

import {
    PromptTemplate,
    PipelinePromptTemplate,
  } from "@langchain/core/prompts";
  
const fullPrompt = PromptTemplate.fromTemplate(`{introduction}

{example}

{start}`);

const introductionPrompt = PromptTemplate.fromTemplate(
`You are impersonating {person}.`
);

const examplePrompt =
PromptTemplate.fromTemplate(`Here's an example of an interaction:
Q: {example_q}
A: {example_a}`);

const startPrompt = PromptTemplate.fromTemplate(`Now, do this for real!
Q: {input}
A:`);

const composedPrompt = new PipelinePromptTemplate({
pipelinePrompts: [
    {
    name: "introduction",
    prompt: introductionPrompt,
    },
    {
    name: "example",
    prompt: examplePrompt,
    },
    {
    name: "start",
    prompt: startPrompt,
    },
],
finalPrompt: fullPrompt,
});
  

  

const formattedPrompt = await composedPrompt.format({
    person: "Elon Musk",
    example_q: `What's your favorite car?`,
    example_a: "Telsa",
    input: `What's your favorite social media site?`,
  });
  
  
console.log(formattedPrompt);

# Output:
#   You are impersonating Elon Musk.

#   

#   Here's an example of an interaction:

#   Q: What's your favorite car?

#   A: Telsa

#   

#   Now, do this for real!

#   Q: What's your favorite social media site?

#   A:


"""
## Next steps

You've now learned how to compose prompts together.

Next, check out the other how-to guides on prompt templates in this section, like [adding few-shot examples to your prompt templates](/docs/how_to/few_shot_examples_chat).
"""



================================================
FILE: docs/core_docs/docs/how_to/prompts_partial.mdx
================================================
# How to partially format prompt templates

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Prompt templates](/docs/concepts/prompt_templates)

:::

Like partially binding arguments to a function, it can make sense to "partial" a prompt template - e.g. pass in
a subset of the required values, as to create a new prompt template which expects only the remaining subset of values.

LangChain supports this in two ways:

1. Partial formatting with string values.
2. Partial formatting with functions that return string values.

In the examples below, we go over the motivations for both use cases as well as how to do it in LangChain.

## Partial with strings

One common use case for wanting to partial a prompt template is if you get access to some of the variables in a
prompt before others. For example, suppose you have a prompt template that requires two variables, `foo` and `baz`.
If you get the `foo` value early on in your chain, but the `baz` value later, it can be inconvenient to pass both variables all the way through the chain.
Instead, you can partial the prompt template with the `foo` value, and then pass the partialed prompt template along and just use that.
Below is an example of doing this:

```typescript
import { PromptTemplate } from "langchain/prompts";

const prompt = new PromptTemplate({
  template: "{foo}{bar}",
  inputVariables: ["foo", "bar"],
});

const partialPrompt = await prompt.partial({
  foo: "foo",
});

const formattedPrompt = await partialPrompt.format({
  bar: "baz",
});

console.log(formattedPrompt);

// foobaz
```

You can also just initialize the prompt with the partialed variables.

```typescript
const prompt = new PromptTemplate({
  template: "{foo}{bar}",
  inputVariables: ["bar"],
  partialVariables: {
    foo: "foo",
  },
});

const formattedPrompt = await prompt.format({
  bar: "baz",
});

console.log(formattedPrompt);

// foobaz
```

## Partial With Functions

You can also partial with a function. The use case for this is when you have a variable you know that you always want to fetch in a common way. A prime example of this is with date or time. Imagine you have a prompt which you always want to have the current date. You can't hard code it in the prompt, and passing it along with the other input variables can be tedious. In this case, it's very handy to be able to partial the prompt with a function that always returns the current date.

```typescript
const getCurrentDate = () => {
  return new Date().toISOString();
};

const prompt = new PromptTemplate({
  template: "Tell me a {adjective} joke about the day {date}",
  inputVariables: ["adjective", "date"],
});

const partialPrompt = await prompt.partial({
  date: getCurrentDate,
});

const formattedPrompt = await partialPrompt.format({
  adjective: "funny",
});

console.log(formattedPrompt);

// Tell me a funny joke about the day 2023-07-13T00:54:59.287Z
```

You can also just initialize the prompt with the partialed variables:

```typescript
const prompt = new PromptTemplate({
  template: "Tell me a {adjective} joke about the day {date}",
  inputVariables: ["adjective"],
  partialVariables: {
    date: getCurrentDate,
  },
});

const formattedPrompt = await prompt.format({
  adjective: "funny",
});

console.log(formattedPrompt);

// Tell me a funny joke about the day 2023-07-13T00:54:59.287Z
```

## Next steps

You've now learned how to partially apply variables to your prompt templates.

Next, check out the other how-to guides on prompt templates in this section, like [adding few-shot examples to your prompt templates](/docs/how_to/few_shot_examples_chat).



================================================
FILE: docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to add chat history


:::note

This tutorial previously built a chatbot using [RunnableWithMessageHistory](https://api.js.langchain.com/classes/_langchain_core.runnables.RunnableWithMessageHistory.html). You can access this version of the tutorial in the [v0.2 docs](https://js.langchain.com/v0.2/docs/how_to/qa_chat_history_how_to/).

The LangGraph implementation offers a number of advantages over `RunnableWithMessageHistory`, including the ability to persist arbitrary components of an application's state (instead of only messages).

:::

In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of "memory" of past questions and answers, and some logic for incorporating those into its current thinking.

In this guide we focus on **adding logic for incorporating historical messages.**

This is largely a condensed version of the [Conversational RAG tutorial](/docs/tutorials/qa_chat_history).

We will cover two approaches:

1. [Chains](/docs/how_to/qa_chat_history_how_to#chains), in which we always execute a retrieval step;
2. [Agents](/docs/how_to/qa_chat_history_how_to#agents), in which we give an LLM discretion over whether and how to execute a retrieval step (or multiple steps).

For the external knowledge source, we will use the same [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng from the [RAG tutorial](/docs/tutorials/rag).
"""

"""
## Setup
### Dependencies

We’ll use an OpenAI chat model and embeddings and a Memory vector store in this walkthrough, but everything shown here works with any [ChatModel](/docs/concepts/chat_models) or [LLM](/docs/concepts/text_llms), [Embeddings](/docs/concepts/embedding_models), and [VectorStore](/docs/concepts/vectorstores) or [Retriever](/docs/concepts/retrievers).

We’ll use the following packages:

```bash
npm install --save langchain @langchain/openai langchain cheerio uuid
```

We need to set environment variable `OPENAI_API_KEY`:

```bash
export OPENAI_API_KEY=YOUR_KEY
```
"""

"""
### LangSmith

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://docs.smith.langchain.com).

Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:


```bash
export LANGSMITH_TRACING=true
export LANGSMITH_API_KEY=YOUR_KEY

# Reduce tracing latency if you are not in a serverless environment
# export LANGCHAIN_CALLBACKS_BACKGROUND=true
```
"""

"""
## Chains {#chains}

In a conversational RAG application, queries issued to the retriever should be informed by the context of the conversation. LangChain provides a [createHistoryAwareRetriever](https://api.js.langchain.com/functions/langchain.chains_history_aware_retriever.createHistoryAwareRetriever.html) constructor to simplify this. It constructs a chain that accepts keys `input` and `chat_history` as input, and has the same output schema as a retriever. `createHistoryAwareRetriever` requires as inputs:  

1. LLM;
2. Retriever;
3. Prompt.

First we obtain these objects:

### LLM

We can use any supported chat model:

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs"

<ChatModelTabs customVarName="llm" />
```
"""

// @lc-docs-hide-cell
import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({ model: "gpt-4o" });

"""
### Initial setup
"""

import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { MemoryVectorStore } from "langchain/vectorstores/memory"
import { OpenAIEmbeddings } from "@langchain/openai";

const loader = new CheerioWebBaseLoader(
  "https://lilianweng.github.io/posts/2023-06-23-agent/"
);

const docs = await loader.load();

const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000, chunkOverlap: 200 });
const splits = await textSplitter.splitDocuments(docs);
const vectorStore = await MemoryVectorStore.fromDocuments(splits, new OpenAIEmbeddings());

// Retrieve and generate using the relevant snippets of the blog.
const retriever = vectorStore.asRetriever();

"""
### Prompt

We'll use a prompt that includes a `MessagesPlaceholder` variable under the name "chat_history". This allows us to pass in a list of Messages to the prompt using the "chat_history" input key, and these messages will be inserted after the system message and before the human message containing the latest question.
"""

import { ChatPromptTemplate, MessagesPlaceholder } from "@langchain/core/prompts";

const contextualizeQSystemPrompt = (
  "Given a chat history and the latest user question " +
  "which might reference context in the chat history, " +
  "formulate a standalone question which can be understood " +
  "without the chat history. Do NOT answer the question, " +
  "just reformulate it if needed and otherwise return it as is."
)

const contextualizeQPrompt = ChatPromptTemplate.fromMessages(
  [
    ["system", contextualizeQSystemPrompt],
    new MessagesPlaceholder("chat_history"),
    ["human", "{input}"],
  ]
)

"""
### Assembling the chain

We can then instantiate the history-aware retriever:
"""

import { createHistoryAwareRetriever } from "langchain/chains/history_aware_retriever";

const historyAwareRetriever = await createHistoryAwareRetriever({
  llm,
  retriever,
  rephrasePrompt: contextualizeQPrompt
});


"""
This chain prepends a rephrasing of the input query to our retriever, so that the retrieval incorporates the context of the conversation.

Now we can build our full QA chain.

As in the [RAG tutorial](/docs/tutorials/rag), we will use [createStuffDocumentsChain](https://api.js.langchain.com/functions/langchain.chains_combine_documents.createStuffDocumentsChain.html) to generate a `questionAnswerChain`, with input keys `context`, `chat_history`, and `input`-- it accepts the retrieved context alongside the conversation history and query to generate an answer.

We build our final `ragChain` with [createRetrievalChain](https://api.js.langchain.com/functions/langchain.chains_retrieval.createRetrievalChain.html). This chain applies the `historyAwareRetriever` and `questionAnswerChain` in sequence, retaining intermediate outputs such as the retrieved context for convenience. It has input keys `input` and `chat_history`, and includes `input`, `chat_history`, `context`, and `answer` in its output.
"""

import { createStuffDocumentsChain } from "langchain/chains/combine_documents";
import { createRetrievalChain } from "langchain/chains/retrieval";

const systemPrompt = 
  "You are an assistant for question-answering tasks. " +
  "Use the following pieces of retrieved context to answer " +
  "the question. If you don't know the answer, say that you " +
  "don't know. Use three sentences maximum and keep the " +
  "answer concise." +
  "\n\n" +
  "{context}";

const qaPrompt = ChatPromptTemplate.fromMessages([
  ["system", systemPrompt],
  new MessagesPlaceholder("chat_history"),
  ["human", "{input}"],
]);

const questionAnswerChain = await createStuffDocumentsChain({
  llm,
  prompt: qaPrompt,
});

const ragChain = await createRetrievalChain({
  retriever: historyAwareRetriever,
  combineDocsChain: questionAnswerChain,
});

"""
### Stateful Management of chat history

We have added application logic for incorporating chat history, but we are still manually plumbing it through our application. In production, the Q&A application we usually persist the chat history into a database, and be able to read and update it appropriately.

[LangGraph](https://langchain-ai.github.io/langgraphjs/) implements a built-in [persistence layer](https://langchain-ai.github.io/langgraphjs/concepts/persistence/), making it ideal for chat applications that support multiple conversational turns.

Wrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications.

LangGraph comes with a simple [in-memory checkpointer](https://langchain-ai.github.io/langgraphjs/reference/classes/checkpoint.MemorySaver.html), which we use below. See its documentation for more detail, including how to use different persistence backends (e.g., SQLite or Postgres).

For a detailed walkthrough of how to manage message history, head to the How to add message history (memory) guide.
"""

import { AIMessage, BaseMessage, HumanMessage } from "@langchain/core/messages";
import { StateGraph, START, END, MemorySaver, messagesStateReducer, Annotation } from "@langchain/langgraph";

// Define the State interface
const GraphAnnotation = Annotation.Root({
  input: Annotation<string>(),
  chat_history: Annotation<BaseMessage[]>({
    reducer: messagesStateReducer,
    default: () => [],
  }),
  context: Annotation<string>(),
  answer: Annotation<string>(),
})

// Define the call_model function
async function callModel(state: typeof GraphAnnotation.State) {
  const response = await ragChain.invoke(state);
  return {
    chat_history: [
      new HumanMessage(state.input),
      new AIMessage(response.answer),
    ],
    context: response.context,
    answer: response.answer,
  };
}

// Create the workflow
const workflow = new StateGraph(GraphAnnotation)
  .addNode("model", callModel)
  .addEdge(START, "model")
  .addEdge("model", END);

// Compile the graph with a checkpointer object
const memory = new MemorySaver();
const app = workflow.compile({ checkpointer: memory });

import { v4 as uuidv4 } from "uuid";

const threadId = uuidv4();
const config = { configurable: { thread_id: threadId } };

const result = await app.invoke(
  { input: "What is Task Decomposition?" },
  config,
)
console.log(result.answer);
# Output:
#   Task Decomposition is the process of breaking down a complicated task into smaller, simpler, and more manageable steps. Techniques like Chain of Thought (CoT) and Tree of Thoughts expand on this by enabling agents to think step by step or explore multiple reasoning possibilities at each step. This allows for a more structured and interpretable approach to handling complex tasks.


const result2 = await app.invoke(
  { input: "What is one way of doing it?" },
  config,
)
console.log(result2.answer);
# Output:
#   One way of doing task decomposition is by using an LLM with simple prompting, such as asking "Steps for XYZ.\n1." or "What are the subgoals for achieving XYZ?" This method leverages direct prompts to guide the model in breaking down tasks.


"""
The conversation history can be inspected via the state of the application:
"""

const chatHistory = (await app.getState(config)).values.chat_history;
for (const message of chatHistory) {
  console.log(message);
}
# Output:
#   HumanMessage {

#     "content": "What is Task Decomposition?",

#     "additional_kwargs": {},

#     "response_metadata": {}

#   }

#   AIMessage {

#     "content": "Task Decomposition is the process of breaking down a complicated task into smaller, simpler, and more manageable steps. Techniques like Chain of Thought (CoT) and Tree of Thoughts expand on this by enabling agents to think step by step or explore multiple reasoning possibilities at each step. This allows for a more structured and interpretable approach to handling complex tasks.",

#     "additional_kwargs": {},

#     "response_metadata": {},

#     "tool_calls": [],

#     "invalid_tool_calls": []

#   }

#   HumanMessage {

#     "content": "What is one way of doing it?",

#     "additional_kwargs": {},

#     "response_metadata": {}

#   }

#   AIMessage {

#     "content": "One way of doing task decomposition is by using an LLM with simple prompting, such as asking \"Steps for XYZ.\\n1.\" or \"What are the subgoals for achieving XYZ?\" This method leverages direct prompts to guide the model in breaking down tasks.",

#     "additional_kwargs": {},

#     "response_metadata": {},

#     "tool_calls": [],

#     "invalid_tool_calls": []

#   }


"""
### Tying it together

![](../../static/img/conversational_retrieval_chain.png)

For convenience, we tie together all of the necessary steps in a single code cell:
"""

import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { MemoryVectorStore } from "langchain/vectorstores/memory"
import { OpenAIEmbeddings, ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate, MessagesPlaceholder } from "@langchain/core/prompts";
import { createHistoryAwareRetriever } from "langchain/chains/history_aware_retriever";
import { createStuffDocumentsChain } from "langchain/chains/combine_documents";
import { createRetrievalChain } from "langchain/chains/retrieval";
import { AIMessage, BaseMessage, HumanMessage } from "@langchain/core/messages";
import { StateGraph, START, END, MemorySaver, messagesStateReducer, Annotation } from "@langchain/langgraph";
import { v4 as uuidv4 } from "uuid";

const llm2 = new ChatOpenAI({ model: "gpt-4o" });

const loader2 = new CheerioWebBaseLoader(
  "https://lilianweng.github.io/posts/2023-06-23-agent/"
);

const docs2 = await loader2.load();

const textSplitter2 = new RecursiveCharacterTextSplitter({ chunkSize: 1000, chunkOverlap: 200 });
const splits2 = await textSplitter2.splitDocuments(docs2);
const vectorStore2 = await MemoryVectorStore.fromDocuments(splits2, new OpenAIEmbeddings());

// Retrieve and generate using the relevant snippets of the blog.
const retriever2 = vectorStore2.asRetriever();

const contextualizeQSystemPrompt2 =
  "Given a chat history and the latest user question " +
  "which might reference context in the chat history, " +
  "formulate a standalone question which can be understood " +
  "without the chat history. Do NOT answer the question, " +
  "just reformulate it if needed and otherwise return it as is.";

const contextualizeQPrompt2 = ChatPromptTemplate.fromMessages(
  [
    ["system", contextualizeQSystemPrompt2],
    new MessagesPlaceholder("chat_history"),
    ["human", "{input}"],
  ]
)

const historyAwareRetriever2 = await createHistoryAwareRetriever({
  llm: llm2,
  retriever: retriever2,
  rephrasePrompt: contextualizeQPrompt2
});

const systemPrompt2 = 
  "You are an assistant for question-answering tasks. " +
  "Use the following pieces of retrieved context to answer " +
  "the question. If you don't know the answer, say that you " +
  "don't know. Use three sentences maximum and keep the " +
  "answer concise." +
  "\n\n" +
  "{context}";

const qaPrompt2 = ChatPromptTemplate.fromMessages([
  ["system", systemPrompt2],
  new MessagesPlaceholder("chat_history"),
  ["human", "{input}"],
]);

const questionAnswerChain2 = await createStuffDocumentsChain({
  llm: llm2,
  prompt: qaPrompt2,
});

const ragChain2 = await createRetrievalChain({
  retriever: historyAwareRetriever2,
  combineDocsChain: questionAnswerChain2,
});

// Define the State interface
const GraphAnnotation2 = Annotation.Root({
  input: Annotation<string>(),
  chat_history: Annotation<BaseMessage[]>({
    reducer: messagesStateReducer,
    default: () => [],
  }),
  context: Annotation<string>(),
  answer: Annotation<string>(),
})

// Define the call_model function
async function callModel2(state: typeof GraphAnnotation2.State) {
  const response = await ragChain2.invoke(state);
  return {
    chat_history: [
      new HumanMessage(state.input),
      new AIMessage(response.answer),
    ],
    context: response.context,
    answer: response.answer,
  };
}

// Create the workflow
const workflow2 = new StateGraph(GraphAnnotation2)
  .addNode("model", callModel2)
  .addEdge(START, "model")
  .addEdge("model", END);

// Compile the graph with a checkpointer object
const memory2 = new MemorySaver();
const app2 = workflow2.compile({ checkpointer: memory2 });

const threadId2 = uuidv4();
const config2 = { configurable: { thread_id: threadId2 } };

const result3 = await app2.invoke(
  { input: "What is Task Decomposition?" },
  config2,
)
console.log(result3.answer);

const result4 = await app2.invoke(
  { input: "What is one way of doing it?" },
  config2,
)
console.log(result4.answer);
# Output:
#   Task Decomposition is the process of breaking a complicated task into smaller, simpler steps to enhance model performance on complex tasks. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) are used for this, with CoT focusing on step-by-step thinking and ToT exploring multiple reasoning possibilities at each step. Decomposition can be carried out by the LLM itself, using task-specific instructions, or through human inputs.

#   One way of doing task decomposition is by prompting the LLM with simple instructions such as "Steps for XYZ.\n1." or "What are the subgoals for achieving XYZ?" This encourages the model to break down the task into smaller, manageable steps on its own.


"""
## Agents {#agents}

Agents leverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allow you to offload some discretion over the retrieval process. Although their behavior is less predictable than chains, they offer some advantages in this context:
- Agents generate the input to the retriever directly, without necessarily needing us to explicitly build in contextualization, as we did above;
- Agents can execute multiple retrieval steps in service of a query, or refrain from executing a retrieval step altogether (e.g., in response to a generic greeting from a user).

### Retrieval tool

Agents can access "tools" and manage their execution. In this case, we will convert our retriever into a LangChain tool to be wielded by the agent:
"""

import { createRetrieverTool } from "langchain/tools/retriever";

const tool =  createRetrieverTool(
    retriever,
    {
      name: "blog_post_retriever",
      description: "Searches and returns excerpts from the Autonomous Agents blog post.",
    }
)
const tools = [tool]

"""
### Agent constructor

Now that we have defined the tools and the LLM, we can create the agent. We will be using [LangGraph](https://langchain-ai.github.io/langgraphjs) to construct the agent. 
Currently we are using a high level interface to construct the agent, but the nice thing about LangGraph is that this high-level interface is backed by a low-level, highly controllable API in case you want to modify the agent logic.
"""

import { createReactAgent } from "@langchain/langgraph/prebuilt";

const agentExecutor = createReactAgent({ llm, tools })

"""
We can now try it out. Note that so far it is not stateful (we still need to add in memory)
"""

const query = "What is Task Decomposition?"

for await (const s of await agentExecutor.stream(
  { messages: [{ role: "user", content: query }] },
)){
  console.log(s)
  console.log("----")
}
# Output:
#   {

#     agent: {

#       messages: [

#         AIMessage {

#           "id": "chatcmpl-AB7xlcJBGSKSp1GvgDY9FP8KvXxwB",

#           "content": "",

#           "additional_kwargs": {

#             "tool_calls": [

#               {

#                 "id": "call_Ev0nA6nzGwOeMC5upJUUxTuw",

#                 "type": "function",

#                 "function": "[Object]"

#               }

#             ]

#           },

#           "response_metadata": {

#             "tokenUsage": {

#               "completionTokens": 19,

#               "promptTokens": 66,

#               "totalTokens": 85

#             },

#             "finish_reason": "tool_calls",

#             "system_fingerprint": "fp_52a7f40b0b"

#           },

#           "tool_calls": [

#             {

#               "name": "blog_post_retriever",

#               "args": {

#                 "query": "Task Decomposition"

#               },

#               "type": "tool_call",

#               "id": "call_Ev0nA6nzGwOeMC5upJUUxTuw"

#             }

#           ],

#           "invalid_tool_calls": [],

#           "usage_metadata": {

#             "input_tokens": 66,

#             "output_tokens": 19,

#             "total_tokens": 85

#           }

#         }

#       ]

#     }

#   }

#   ----

#   {

#     tools: {

#       messages: [

#         ToolMessage {

#           "content": "Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\nSelf-Reflection#\n\nAgent System Overview\n                \n                    Component One: Planning\n                        \n                \n                    Task Decomposition\n                \n                    Self-Reflection\n                \n                \n                    Component Two: Memory\n                        \n                \n                    Types of Memory\n                \n                    Maximum Inner Product Search (MIPS)\n                \n                \n                    Component Three: Tool Use\n                \n                    Case Studies\n                        \n                \n                    Scientific Discovery Agent\n                \n                    Generative Agents Simulation\n                \n                    Proof-of-Concept Examples\n                \n                \n                    Challenges\n                \n                    Citation\n                \n                    References\n\n(3) Task execution: Expert models execute on the specific tasks and log results.\nInstruction:\n\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.",

#           "name": "blog_post_retriever",

#           "additional_kwargs": {},

#           "response_metadata": {},

#           "tool_call_id": "call_Ev0nA6nzGwOeMC5upJUUxTuw"

#         }

#       ]

#     }

#   }

#   ----

#   {

#     agent: {

#       messages: [

#         AIMessage {

#           "id": "chatcmpl-AB7xmiPNPbMX2KvZKHM2oPfcoFMnY",

#           "content": "**Task Decomposition** involves breaking down a complicated or large task into smaller, more manageable subtasks. Here are some insights based on current techniques and research:\n\n1. **Chain of Thought (CoT)**:\n   - Introduced by Wei et al. (2022), this technique prompts the model to \"think step by step\".\n   - It helps decompose hard tasks into several simpler steps.\n   - Enhances the interpretability of the model's thought process.\n\n2. **Tree of Thoughts (ToT)**:\n   - An extension of CoT by Yao et al. (2023).\n   - Decomposes problems into multiple thought steps and generates several possibilities at each step.\n   - Utilizes tree structures through BFS (Breadth-First Search) or DFS (Depth-First Search) with evaluation by a classifier or majority vote.\n\n3. **Methods of Task Decomposition**:\n   - **Simple Prompting**: Asking the model directly, e.g., \"Steps for XYZ.\\n1.\" or \"What are the subgoals for achieving XYZ?\".\n   - **Task-Specific Instructions**: Tailoring instructions to the task, such as \"Write a story outline\" for writing a novel.\n   - **Human Inputs**: Receiving inputs from humans to refine the process.\n\n4. **LLM+P Approach**:\n   - Suggested by Liu et al. (2023), combines language models with an external classical planner.\n   - Uses Planning Domain Definition Language (PDDL) for long-horizon planning:\n     1. Translates the problem into a PDDL problem.\n     2. Requests an external planner to generate a PDDL plan.\n     3. Translates the PDDL plan back into natural language.\n   - This method offloads the planning complexity to a specialized tool, especially relevant for domains utilizing robotic setups.\n\nTask Decomposition is a fundamental component of planning in autonomous agent systems, aiding in the efficient accomplishment of complex tasks by breaking them into smaller, actionable steps.",

#           "additional_kwargs": {},

#           "response_metadata": {

#             "tokenUsage": {

#               "completionTokens": 411,

#               "promptTokens": 732,

#               "totalTokens": 1143

#             },

#             "finish_reason": "stop",

#             "system_fingerprint": "fp_e375328146"

#           },

#           "tool_calls": [],

#           "invalid_tool_calls": [],

#           "usage_metadata": {

#             "input_tokens": 732,

#             "output_tokens": 411,

#             "total_tokens": 1143

#           }

#         }

#       ]

#     }

#   }

#   ----


"""
LangGraph comes with built in persistence, so we don't need to use `ChatMessageHistory`! Rather, we can pass in a checkpointer to our LangGraph agent directly.

Distinct conversations are managed by specifying a key for a conversation thread in the config object, as shown below.
"""

import { MemorySaver } from "@langchain/langgraph";

const memory3 = new MemorySaver();

const agentExecutor2 = createReactAgent({ llm, tools, checkpointSaver: memory3 })

"""
This is all we need to construct a conversational RAG agent.

Let's observe its behavior. Note that if we input a query that does not require a retrieval step, the agent does not execute one:
"""

const threadId3 = uuidv4();
const config3 = { configurable: { thread_id: threadId3 } };

for await (const s of await agentExecutor2.stream({ messages: [{ role: "user", content: "Hi! I'm bob" }] }, config3)) {
  console.log(s)
  console.log("----")
}
# Output:
#   {

#     agent: {

#       messages: [

#         AIMessage {

#           "id": "chatcmpl-AB7y8P8AGHkxOwKpwMc3qj6r0skYr",

#           "content": "Hello, Bob! How can I assist you today?",

#           "additional_kwargs": {},

#           "response_metadata": {

#             "tokenUsage": {

#               "completionTokens": 12,

#               "promptTokens": 64,

#               "totalTokens": 76

#             },

#             "finish_reason": "stop",

#             "system_fingerprint": "fp_e375328146"

#           },

#           "tool_calls": [],

#           "invalid_tool_calls": [],

#           "usage_metadata": {

#             "input_tokens": 64,

#             "output_tokens": 12,

#             "total_tokens": 76

#           }

#         }

#       ]

#     }

#   }

#   ----


"""
Further, if we input a query that does require a retrieval step, the agent generates the input to the tool:
"""

const query2 = "What is Task Decomposition?"

for await (const s of await agentExecutor2.stream({ messages: [{ role: "user", content: query2 }] }, config3)) {
  console.log(s)
  console.log("----")
}
# Output:
#   {

#     agent: {

#       messages: [

#         AIMessage {

#           "id": "chatcmpl-AB7y8Do2IHJ2rnUvvMU3pTggmuZud",

#           "content": "",

#           "additional_kwargs": {

#             "tool_calls": [

#               {

#                 "id": "call_3tSaOZ3xdKY4miIJdvBMR80V",

#                 "type": "function",

#                 "function": "[Object]"

#               }

#             ]

#           },

#           "response_metadata": {

#             "tokenUsage": {

#               "completionTokens": 19,

#               "promptTokens": 89,

#               "totalTokens": 108

#             },

#             "finish_reason": "tool_calls",

#             "system_fingerprint": "fp_e375328146"

#           },

#           "tool_calls": [

#             {

#               "name": "blog_post_retriever",

#               "args": {

#                 "query": "Task Decomposition"

#               },

#               "type": "tool_call",

#               "id": "call_3tSaOZ3xdKY4miIJdvBMR80V"

#             }

#           ],

#           "invalid_tool_calls": [],

#           "usage_metadata": {

#             "input_tokens": 89,

#             "output_tokens": 19,

#             "total_tokens": 108

#           }

#         }

#       ]

#     }

#   }

#   ----

#   {

#     tools: {

#       messages: [

#         ToolMessage {

#           "content": "Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\nSelf-Reflection#\n\nAgent System Overview\n                \n                    Component One: Planning\n                        \n                \n                    Task Decomposition\n                \n                    Self-Reflection\n                \n                \n                    Component Two: Memory\n                        \n                \n                    Types of Memory\n                \n                    Maximum Inner Product Search (MIPS)\n                \n                \n                    Component Three: Tool Use\n                \n                    Case Studies\n                        \n                \n                    Scientific Discovery Agent\n                \n                    Generative Agents Simulation\n                \n                    Proof-of-Concept Examples\n                \n                \n                    Challenges\n                \n                    Citation\n                \n                    References\n\n(3) Task execution: Expert models execute on the specific tasks and log results.\nInstruction:\n\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.",

#           "name": "blog_post_retriever",

#           "additional_kwargs": {},

#           "response_metadata": {},

#           "tool_call_id": "call_3tSaOZ3xdKY4miIJdvBMR80V"

#         }

#       ]

#     }

#   }

#   ----

#   {

#     agent: {

#       messages: [

#         AIMessage {

#           "id": "chatcmpl-AB7y9tpoTvM3lsrhoxCWkkerk9fb2",

#           "content": "Task decomposition is a methodology used to break down complex tasks into smaller, more manageable steps. Here’s an overview of various approaches to task decomposition:\n\n1. **Chain of Thought (CoT)**: This technique prompts a model to \"think step by step,\" which aids in transforming big tasks into multiple smaller tasks. This method enhances the model’s performance on complex tasks by making the problem more manageable and interpretable.\n\n2. **Tree of Thoughts (ToT)**: An extension of Chain of Thought, this approach explores multiple reasoning possibilities at each step, effectively creating a tree structure. The search process can be carried out using Breadth-First Search (BFS) or Depth-First Search (DFS), with each state evaluated by either a classifier or a majority vote.\n\n3. **Simple Prompting**: Involves straightforward instructions to decompose a task, such as starting with \"Steps for XYZ. 1.\" or asking \"What are the subgoals for achieving XYZ?\". This can also include task-specific instructions like \"Write a story outline\" for writing a novel.\n\n4. **LLM+P**: Combines Large Language Models (LLMs) with an external classical planner. The problem is translated into a Planning Domain Definition Language (PDDL) format, an external planner generates a plan, and then the plan is translated back into natural language. This approach highlights a synergy between modern AI techniques and traditional planning strategies.\n\nThese approaches allow complex problems to be approached and solved more efficiently by focusing on manageable sub-tasks.",

#           "additional_kwargs": {},

#           "response_metadata": {

#             "tokenUsage": {

#               "completionTokens": 311,

#               "promptTokens": 755,

#               "totalTokens": 1066

#             },

#             "finish_reason": "stop",

#             "system_fingerprint": "fp_52a7f40b0b"

#           },

#           "tool_calls": [],

#           "invalid_tool_calls": [],

#           "usage_metadata": {

#             "input_tokens": 755,

#             "output_tokens": 311,

#             "total_tokens": 1066

#           }

#         }

#       ]

#     }

#   }

#   ----


"""
Above, instead of inserting our query verbatim into the tool, the agent stripped unnecessary words like "what" and "is".

This same principle allows the agent to use the context of the conversation when necessary:
"""

const query3 = "What according to the blog post are common ways of doing it? redo the search"

for await (const s of await agentExecutor2.stream({ messages: [{ role: "user", content: query3 }] }, config3)) {
  console.log(s)
  console.log("----")
}
# Output:
#   {

#     agent: {

#       messages: [

#         AIMessage {

#           "id": "chatcmpl-AB7yDE4rCOXTPZ3595GknUgVzASmt",

#           "content": "",

#           "additional_kwargs": {

#             "tool_calls": [

#               {

#                 "id": "call_cWnDZq2aloVtMB4KjZlTxHmZ",

#                 "type": "function",

#                 "function": "[Object]"

#               }

#             ]

#           },

#           "response_metadata": {

#             "tokenUsage": {

#               "completionTokens": 21,

#               "promptTokens": 1089,

#               "totalTokens": 1110

#             },

#             "finish_reason": "tool_calls",

#             "system_fingerprint": "fp_52a7f40b0b"

#           },

#           "tool_calls": [

#             {

#               "name": "blog_post_retriever",

#               "args": {

#                 "query": "common ways of task decomposition"

#               },

#               "type": "tool_call",

#               "id": "call_cWnDZq2aloVtMB4KjZlTxHmZ"

#             }

#           ],

#           "invalid_tool_calls": [],

#           "usage_metadata": {

#             "input_tokens": 1089,

#             "output_tokens": 21,

#             "total_tokens": 1110

#           }

#         }

#       ]

#     }

#   }

#   ----

#   {

#     tools: {

#       messages: [

#         ToolMessage {

#           "content": "Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\nSelf-Reflection#\n\nAgent System Overview\n                \n                    Component One: Planning\n                        \n                \n                    Task Decomposition\n                \n                    Self-Reflection\n                \n                \n                    Component Two: Memory\n                        \n                \n                    Types of Memory\n                \n                    Maximum Inner Product Search (MIPS)\n                \n                \n                    Component Three: Tool Use\n                \n                    Case Studies\n                        \n                \n                    Scientific Discovery Agent\n                \n                    Generative Agents Simulation\n                \n                    Proof-of-Concept Examples\n                \n                \n                    Challenges\n                \n                    Citation\n                \n                    References\n\nResources:\n1. Internet access for searches and information gathering.\n2. Long Term memory management.\n3. GPT-3.5 powered Agents for delegation of simple tasks.\n4. File output.\n\nPerformance Evaluation:\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n2. Constructively self-criticize your big-picture behavior constantly.\n3. Reflect on past decisions and strategies to refine your approach.\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.",

#           "name": "blog_post_retriever",

#           "additional_kwargs": {},

#           "response_metadata": {},

#           "tool_call_id": "call_cWnDZq2aloVtMB4KjZlTxHmZ"

#         }

#       ]

#     }

#   }

#   ----

#   {

#     agent: {

#       messages: [

#         AIMessage {

#           "id": "chatcmpl-AB7yGASxz0Z0g2jiCxwx4gYHYJTi4",

#           "content": "According to the blog post, there are several common methods of task decomposition:\n\n1. **Simple Prompting by LLMs**: This involves straightforward instructions to decompose a task. Examples include:\n   - \"Steps for XYZ. 1.\"\n   - \"What are the subgoals for achieving XYZ?\"\n   - Task-specific instructions like \"Write a story outline\" for writing a novel.\n\n2. **Human Inputs**: Decomposition can be guided by human insights and instructions.\n\n3. **Chain of Thought (CoT)**: This technique prompts a model to think step-by-step, enabling it to break down complex tasks into smaller, more manageable tasks. CoT has become a standard method to enhance model performance on intricate tasks.\n\n4. **Tree of Thoughts (ToT)**: An extension of CoT, this approach decomposes the problem into multiple thought steps and generates several thoughts per step, forming a tree structure. The search process can be performed using Breadth-First Search (BFS) or Depth-First Search (DFS), with each state evaluated by a classifier or through a majority vote.\n\n5. **LLM+P (Large Language Model plus Planner)**: This method integrates LLMs with an external classical planner. It involves:\n   - Translating the problem into “Problem PDDL” (Planning Domain Definition Language).\n   - Using an external planner to generate a PDDL plan based on an existing “Domain PDDL”.\n   - Translating the PDDL plan back into natural language.\n  \nBy utilizing these methods, tasks can be effectively decomposed into more manageable parts, allowing for more efficient problem-solving and planning.",

#           "additional_kwargs": {},

#           "response_metadata": {

#             "tokenUsage": {

#               "completionTokens": 334,

#               "promptTokens": 1746,

#               "totalTokens": 2080

#             },

#             "finish_reason": "stop",

#             "system_fingerprint": "fp_52a7f40b0b"

#           },

#           "tool_calls": [],

#           "invalid_tool_calls": [],

#           "usage_metadata": {

#             "input_tokens": 1746,

#             "output_tokens": 334,

#             "total_tokens": 2080

#           }

#         }

#       ]

#     }

#   }

#   ----


"""
Note that the agent was able to infer that "it" in our query refers to "task decomposition", and generated a reasonable search query as a result-- in this case, "common ways of task decomposition".
"""

"""
### Tying it together

For convenience, we tie together all of the necessary steps in a single code cell:
"""

import { createRetrieverTool } from "langchain/tools/retriever";
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { MemorySaver } from "@langchain/langgraph";
import { ChatOpenAI } from "@langchain/openai";
import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { MemoryVectorStore } from "langchain/vectorstores/memory"
import { OpenAIEmbeddings } from "@langchain/openai";

const llm3 = new ChatOpenAI({ model: "gpt-4o" });

const loader3 = new CheerioWebBaseLoader(
  "https://lilianweng.github.io/posts/2023-06-23-agent/"
);

const docs3 = await loader3.load();

const textSplitter3 = new RecursiveCharacterTextSplitter({ chunkSize: 1000, chunkOverlap: 200 });
const splits3 = await textSplitter3.splitDocuments(docs3);
const vectorStore3 = await MemoryVectorStore.fromDocuments(splits3, new OpenAIEmbeddings());

// Retrieve and generate using the relevant snippets of the blog.
const retriever3 = vectorStore3.asRetriever();

const tool2 = createRetrieverTool(
    retriever3,
    {
      name: "blog_post_retriever",
      description: "Searches and returns excerpts from the Autonomous Agents blog post.",
    }
)
const tools2 = [tool2]
const memory4 = new MemorySaver();

const agentExecutor3 = createReactAgent({ llm: llm3, tools: tools2, checkpointSaver: memory4 })

"""
## Next steps

We've covered the steps to build a basic conversational Q&A application:

- We used chains to build a predictable application that generates search queries for each user input;
- We used agents to build an application that "decides" when and how to generate search queries.

To explore different types of retrievers and retrieval strategies, visit the [retrievers](/docs/how_to#retrievers) section of the how-to guides.

For a detailed walkthrough of LangChain's conversation memory abstractions, visit the [How to add message history (memory)](/docs/how_to/message_history) LCEL page.

"""



================================================
FILE: docs/core_docs/docs/how_to/qa_citations.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to return citations

:::info Prerequisites

This guide assumes familiarity with the following:

- [Retrieval-augmented generation](/docs/tutorials/rag/)
- [Returning structured data from a model](/docs/how_to/structured_output/)

:::

How can we get a model to cite which parts of the source documents it referenced in its response?

To explore some techniques for extracting citations, let's first create a simple RAG chain. To start we'll just retrieve from the web using the [`TavilySearchAPIRetriever`](https://api.js.langchain.com/classes/langchain_community_retrievers_tavily_search_api.TavilySearchAPIRetriever.html).
"""

"""
## Setup
### Dependencies

We’ll use an OpenAI chat model and embeddings and a Memory vector store in this walkthrough, but everything shown here works with any [ChatModel](/docs/concepts/chat_models) or [LLM](/docs/concepts/text_llms), [Embeddings](/docs/concepts/embedding_models/), and [VectorStore](/docs/concepts/vectorstores/) or [Retriever](/docs/concepts/retrievers).

We’ll use the following packages:

```bash
npm install --save langchain @langchain/community @langchain/openai
```

We need to set environment variables for Tavily Search & OpenAI:

```bash
export OPENAI_API_KEY=YOUR_KEY
export TAVILY_API_KEY=YOUR_KEY
```
"""

"""
### LangSmith

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://smith.langchain.com/).

Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:


```bash
export LANGSMITH_TRACING=true
export LANGSMITH_API_KEY=YOUR_KEY

# Reduce tracing latency if you are not in a serverless environment
# export LANGCHAIN_CALLBACKS_BACKGROUND=true
```
"""

"""
### Initial setup
"""

import { TavilySearchAPIRetriever } from "@langchain/community/retrievers/tavily_search_api";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({
  model: "gpt-3.5-turbo",
  temperature: 0,
});

const retriever = new TavilySearchAPIRetriever({
  k: 6,
});

const prompt = ChatPromptTemplate.fromMessages([
  ["system", "You're a helpful AI assistant. Given a user question and some web article snippets, answer the user question. If none of the articles answer the question, just say you don't know.\n\nHere are the web articles:{context}"],
  ["human", "{question}"],
]);

"""
Now that we've got a model, retriever and prompt, let's chain them all together. We'll need to add some logic for formatting our retrieved `Document`s to a string that can be passed to our prompt. We'll make it so our chain returns both the answer and the retrieved Documents.
"""

import { Document } from "@langchain/core/documents";
import { StringOutputParser } from "@langchain/core/output_parsers";
import { RunnableMap, RunnablePassthrough } from "@langchain/core/runnables";

/**
 * Format the documents into a readable string.
 */
const formatDocs = (input: Record<string, any>): string => {
  const { docs } = input;
  return "\n\n" + docs.map((doc: Document) => `Article title: ${doc.metadata.title}\nArticle Snippet: ${doc.pageContent}`).join("\n\n");
}
// subchain for generating an answer once we've done retrieval
const answerChain = prompt.pipe(llm).pipe(new StringOutputParser());
const map = RunnableMap.from({
  question: new RunnablePassthrough(),
  docs: retriever,
})
// complete chain that calls the retriever -> formats docs to string -> runs answer subchain -> returns just the answer and retrieved docs.
const chain = map.assign({ context: formatDocs }).assign({ answer: answerChain }).pick(["answer", "docs"])

await chain.invoke("How fast are cheetahs?")
# Output:
#   {

#     answer: [32m"Cheetahs are the fastest land animals on Earth. They can reach speeds as high as 75 mph or 120 km/h."[39m... 124 more characters,

#     docs: [

#       Document {

#         pageContent: [32m"Contact Us − +\n"[39m +

#           [32m"Address\n"[39m +

#           [32m"Smithsonian's National Zoo & Conservation Biology Institute  3001 Connecticut"[39m... 1343 more characters,

#         metadata: {

#           title: [32m"Cheetah | Smithsonian's National Zoo and Conservation Biology Institute"[39m,

#           source: [32m"https://nationalzoo.si.edu/animals/cheetah"[39m,

#           score: [33m0.96283[39m,

#           images: [1mnull[22m

#         }

#       },

#       Document {

#         pageContent: [32m"Now, their only hope lies in the hands of human conservationists, working tirelessly to save the che"[39m... 880 more characters,

#         metadata: {

#           title: [32m"How Fast Are Cheetahs, and Other Fascinating Facts About the World's ..."[39m,

#           source: [32m"https://www.discovermagazine.com/planet-earth/how-fast-are-cheetahs-and-other-fascinating-facts-abou"[39m... 21 more characters,

#           score: [33m0.96052[39m,

#           images: [1mnull[22m

#         }

#       },

#       Document {

#         pageContent: [32m"The maximum speed cheetahs have been measured at is 114 km (71 miles) per hour, and they routinely r"[39m... 1048 more characters,

#         metadata: {

#           title: [32m"Cheetah | Description, Speed, Habitat, Diet, Cubs, & Facts"[39m,

#           source: [32m"https://www.britannica.com/animal/cheetah-mammal"[39m,

#           score: [33m0.93137[39m,

#           images: [1mnull[22m

#         }

#       },

#       Document {

#         pageContent: [32m"The science of cheetah speed\n"[39m +

#           [32m"The cheetah (Acinonyx jubatus) is the fastest land animal on Earth, cap"[39m... 738 more characters,

#         metadata: {

#           title: [32m"How Fast Can a Cheetah Run? - ThoughtCo"[39m,

#           source: [32m"https://www.thoughtco.com/how-fast-can-a-cheetah-run-4587031"[39m,

#           score: [33m0.91385[39m,

#           images: [1mnull[22m

#         }

#       },

#       Document {

#         pageContent: [32m"One of two videos from National Geographic's award-winning multimedia coverage of cheetahs in the ma"[39m... 60 more characters,

#         metadata: {

#           title: [32m"The Science of a Cheetah's Speed | National Geographic"[39m,

#           source: [32m"https://www.youtube.com/watch?v=icFMTB0Pi0g"[39m,

#           score: [33m0.90358[39m,

#           images: [1mnull[22m

#         }

#       },

#       Document {

#         pageContent: [32m"If a lion comes along, the cheetah will abandon its catch -- it can't fight off a lion, and chances "[39m... 911 more characters,

#         metadata: {

#           title: [32m"What makes a cheetah run so fast? | HowStuffWorks"[39m,

#           source: [32m"https://animals.howstuffworks.com/mammals/cheetah-speed.htm"[39m,

#           score: [33m0.87824[39m,

#           images: [1mnull[22m

#         }

#       }

#     ]

#   }

"""
See a LangSmith trace [here](https://smith.langchain.com/public/bb0ed37e-b2be-4ae9-8b0d-ce2aff0b4b5e/r) that shows off the internals.
"""

"""
## Tool calling

### Cite documents
Let's try using [tool calling](/docs/how_to/tool_calling) to make the model specify which of the provided documents it's actually referencing when answering. LangChain has some utils for converting objects or [Zod](https://zod.dev) objects to the JSONSchema format expected by providers like OpenAI. We'll use the [`.withStructuredOutput()`](/docs/how_to/structured_output/) method to get the model to output data matching our desired schema:
"""

import { z } from "zod";

const llmWithTool1 = llm.withStructuredOutput(
  z.object({
    answer: z.string().describe("The answer to the user question, which is based only on the given sources."),
    citations: z.array(z.number()).describe("The integer IDs of the SPECIFIC sources which justify the answer.")
  }).describe("A cited source from the given text"),
  {
    name: "cited_answers"
  }
);

const exampleQ = `What is Brian's height?

Source: 1
Information: Suzy is 6'2"

Source: 2
Information: Jeremiah is blonde

Source: 3
Information: Brian is 3 inches shorter than Suzy`;

await llmWithTool1.invoke(exampleQ);
# Output:
#   {

#     answer: [32m`Brian is 6'2" - 3 inches = 5'11" tall.`[39m,

#     citations: [ [33m1[39m, [33m3[39m ]

#   }

"""
See a LangSmith trace [here](https://smith.langchain.com/public/28736c75-122e-4deb-9916-55c73eea3167/r) that shows off the internals
"""

"""
Now we're ready to put together our chain
"""

import { Document } from "@langchain/core/documents";

const formatDocsWithId = (docs: Array<Document>): string => {
  return "\n\n" + docs.map((doc: Document, idx: number) => `Source ID: ${idx}\nArticle title: ${doc.metadata.title}\nArticle Snippet: ${doc.pageContent}`).join("\n\n");
}
// subchain for generating an answer once we've done retrieval
const answerChain1 = prompt.pipe(llmWithTool1);
const map1 = RunnableMap.from({
  question: new RunnablePassthrough(),
  docs: retriever,
})
// complete chain that calls the retriever -> formats docs to string -> runs answer subchain -> returns just the answer and retrieved docs.
const chain1 = map1
  .assign({ context: (input: { docs: Array<Document> }) => formatDocsWithId(input.docs) })
  .assign({ cited_answer: answerChain1 })
  .pick(["cited_answer", "docs"])
  
await chain1.invoke("How fast are cheetahs?")
# Output:
#   {

#     cited_answer: {

#       answer: [32m"Cheetahs can reach speeds as high as 75 mph or 120 km/h."[39m,

#       citations: [ [33m1[39m, [33m2[39m, [33m5[39m ]

#     },

#     docs: [

#       Document {

#         pageContent: [32m"One of two videos from National Geographic's award-winning multimedia coverage of cheetahs in the ma"[39m... 60 more characters,

#         metadata: {

#           title: [32m"The Science of a Cheetah's Speed | National Geographic"[39m,

#           source: [32m"https://www.youtube.com/watch?v=icFMTB0Pi0g"[39m,

#           score: [33m0.97858[39m,

#           images: [1mnull[22m

#         }

#       },

#       Document {

#         pageContent: [32m"The maximum speed cheetahs have been measured at is 114 km (71 miles) per hour, and they routinely r"[39m... 1048 more characters,

#         metadata: {

#           title: [32m"Cheetah | Description, Speed, Habitat, Diet, Cubs, & Facts"[39m,

#           source: [32m"https://www.britannica.com/animal/cheetah-mammal"[39m,

#           score: [33m0.97213[39m,

#           images: [1mnull[22m

#         }

#       },

#       Document {

#         pageContent: [32m"The science of cheetah speed\n"[39m +

#           [32m"The cheetah (Acinonyx jubatus) is the fastest land animal on Earth, cap"[39m... 738 more characters,

#         metadata: {

#           title: [32m"How Fast Can a Cheetah Run? - ThoughtCo"[39m,

#           source: [32m"https://www.thoughtco.com/how-fast-can-a-cheetah-run-4587031"[39m,

#           score: [33m0.95759[39m,

#           images: [1mnull[22m

#         }

#       },

#       Document {

#         pageContent: [32m"Contact Us − +\n"[39m +

#           [32m"Address\n"[39m +

#           [32m"Smithsonian's National Zoo & Conservation Biology Institute  3001 Connecticut"[39m... 1343 more characters,

#         metadata: {

#           title: [32m"Cheetah | Smithsonian's National Zoo and Conservation Biology Institute"[39m,

#           source: [32m"https://nationalzoo.si.edu/animals/cheetah"[39m,

#           score: [33m0.92422[39m,

#           images: [1mnull[22m

#         }

#       },

#       Document {

#         pageContent: [32m"Now, their only hope lies in the hands of human conservationists, working tirelessly to save the che"[39m... 880 more characters,

#         metadata: {

#           title: [32m"How Fast Are Cheetahs, and Other Fascinating Facts About the World's ..."[39m,

#           source: [32m"https://www.discovermagazine.com/planet-earth/how-fast-are-cheetahs-and-other-fascinating-facts-abou"[39m... 21 more characters,

#           score: [33m0.91867[39m,

#           images: [1mnull[22m

#         }

#       },

#       Document {

#         pageContent: [32m"The speeds attained by the cheetah may be only slightly greater than those achieved by the pronghorn"[39m... 2527 more characters,

#         metadata: {

#           title: [32m"Cheetah - Wikipedia"[39m,

#           source: [32m"https://en.wikipedia.org/wiki/Cheetah"[39m,

#           score: [33m0.81617[39m,

#           images: [1mnull[22m

#         }

#       }

#     ]

#   }

"""
See a LangSmith trace [here](https://smith.langchain.com/public/86814255-b9b0-4c4f-9463-e795c9961451/r) that shows off the internals.
"""

"""
### Cite snippets

What if we want to cite actual text spans? We can try to get our model to return these, too.

**Note**: Note that if we break up our documents so that we have many documents with only a sentence or two instead of a few long documents, citing documents becomes roughly equivalent to citing snippets, and may be easier for the model because the model just needs to return an identifier for each snippet instead of the actual text. We recommend trying both approaches and evaluating.
"""

import { Document } from "@langchain/core/documents";

const citationSchema = z.object({
  sourceId: z.number().describe("The integer ID of a SPECIFIC source which justifies the answer."),
  quote: z.string().describe("The VERBATIM quote from the specified source that justifies the answer.")
});

const llmWithTool2 = llm.withStructuredOutput(
  z.object({
    answer: z.string().describe("The answer to the user question, which is based only on the given sources."),
    citations: z.array(citationSchema).describe("Citations from the given sources that justify the answer.")
  }), {
    name: "quoted_answer",
  })

const answerChain2 = prompt.pipe(llmWithTool2);
const map2 = RunnableMap.from({
  question: new RunnablePassthrough(),
  docs: retriever,
})
// complete chain that calls the retriever -> formats docs to string -> runs answer subchain -> returns just the answer and retrieved docs.
const chain2 = map2
  .assign({ context: (input: { docs: Array<Document> }) => formatDocsWithId(input.docs) })
  .assign({ quoted_answer: answerChain2 })
  .pick(["quoted_answer", "docs"]);
  
await chain2.invoke("How fast are cheetahs?")
# Output:
#   {

#     quoted_answer: {

#       answer: [32m"Cheetahs can reach speeds of up to 120kph or 75mph, making them the world’s fastest land animals."[39m,

#       citations: [

#         {

#           sourceId: [33m5[39m,

#           quote: [32m"Cheetahs can reach speeds of up to 120kph or 75mph, making them the world’s fastest land animals."[39m

#         },

#         {

#           sourceId: [33m1[39m,

#           quote: [32m"The cheetah (Acinonyx jubatus) is the fastest land animal on Earth, capable of reaching speeds as hi"[39m... 25 more characters

#         },

#         {

#           sourceId: [33m3[39m,

#           quote: [32m"The maximum speed cheetahs have been measured at is 114 km (71 miles) per hour, and they routinely r"[39m... 72 more characters

#         }

#       ]

#     },

#     docs: [

#       Document {

#         pageContent: [32m"Contact Us − +\n"[39m +

#           [32m"Address\n"[39m +

#           [32m"Smithsonian's National Zoo & Conservation Biology Institute  3001 Connecticut"[39m... 1343 more characters,

#         metadata: {

#           title: [32m"Cheetah | Smithsonian's National Zoo and Conservation Biology Institute"[39m,

#           source: [32m"https://nationalzoo.si.edu/animals/cheetah"[39m,

#           score: [33m0.95973[39m,

#           images: [1mnull[22m

#         }

#       },

#       Document {

#         pageContent: [32m"The science of cheetah speed\n"[39m +

#           [32m"The cheetah (Acinonyx jubatus) is the fastest land animal on Earth, cap"[39m... 738 more characters,

#         metadata: {

#           title: [32m"How Fast Can a Cheetah Run? - ThoughtCo"[39m,

#           source: [32m"https://www.thoughtco.com/how-fast-can-a-cheetah-run-4587031"[39m,

#           score: [33m0.92749[39m,

#           images: [1mnull[22m

#         }

#       },

#       Document {

#         pageContent: [32m"Now, their only hope lies in the hands of human conservationists, working tirelessly to save the che"[39m... 880 more characters,

#         metadata: {

#           title: [32m"How Fast Are Cheetahs, and Other Fascinating Facts About the World's ..."[39m,

#           source: [32m"https://www.discovermagazine.com/planet-earth/how-fast-are-cheetahs-and-other-fascinating-facts-abou"[39m... 21 more characters,

#           score: [33m0.92417[39m,

#           images: [1mnull[22m

#         }

#       },

#       Document {

#         pageContent: [32m"The maximum speed cheetahs have been measured at is 114 km (71 miles) per hour, and they routinely r"[39m... 1048 more characters,

#         metadata: {

#           title: [32m"Cheetah | Description, Speed, Habitat, Diet, Cubs, & Facts"[39m,

#           source: [32m"https://www.britannica.com/animal/cheetah-mammal"[39m,

#           score: [33m0.92341[39m,

#           images: [1mnull[22m

#         }

#       },

#       Document {

#         pageContent: [32m"One of two videos from National Geographic's award-winning multimedia coverage of cheetahs in the ma"[39m... 60 more characters,

#         metadata: {

#           title: [32m"The Science of a Cheetah's Speed | National Geographic"[39m,

#           source: [32m"https://www.youtube.com/watch?v=icFMTB0Pi0g"[39m,

#           score: [33m0.90025[39m,

#           images: [1mnull[22m

#         }

#       },

#       Document {

#         pageContent: [32m"In fact, they are more closely related to kangaroos…\n"[39m +

#           [32m"Read more\n"[39m +

#           [32m"Animals on the Galapagos Islands: A G"[39m... 987 more characters,

#         metadata: {

#           title: [32m"How fast can cheetahs run, and what enables their incredible speed?"[39m,

#           source: [32m"https://wildlifefaq.com/cheetah-speed/"[39m,

#           score: [33m0.87121[39m,

#           images: [1mnull[22m

#         }

#       }

#     ]

#   }

"""
You can check out a LangSmith trace [here](https://smith.langchain.com/public/f0588adc-1914-45e8-a2ed-4fa028cea0e1/r) that shows off the internals.
"""

"""
## Direct prompting

Not all models support tool-calling. We can achieve similar results with direct prompting. Let's see what this looks like using an older Anthropic chat model that is particularly proficient in working with XML:
"""

"""
### Setup

Install the LangChain Anthropic integration package:

```bash
npm install @langchain/anthropic
```

Add your Anthropic API key to your environment:

```bash
export ANTHROPIC_API_KEY=YOUR_KEY
```
"""

import { ChatAnthropic } from "@langchain/anthropic";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { XMLOutputParser } from "@langchain/core/output_parsers";
import { Document } from "@langchain/core/documents";
import { RunnableLambda, RunnablePassthrough, RunnableMap } from "@langchain/core/runnables";

const anthropic = new ChatAnthropic({
  model: "claude-instant-1.2",
  temperature: 0,
});
const system = `You're a helpful AI assistant. Given a user question and some web article snippets,
answer the user question and provide citations. If none of the articles answer the question, just say you don't know.

Remember, you must return both an answer and citations. A citation consists of a VERBATIM quote that
justifies the answer and the ID of the quote article. Return a citation for every quote across all articles
that justify the answer. Use the following format for your final output:

<cited_answer>
    <answer></answer>
    <citations>
        <citation><source_id></source_id><quote></quote></citation>
        <citation><source_id></source_id><quote></quote></citation>
        ...
    </citations>
</cited_answer>

Here are the web articles:{context}`;

const anthropicPrompt = ChatPromptTemplate.fromMessages([
  ["system", system],
  ["human", "{question}"]
]);

const formatDocsToXML = (docs: Array<Document>): string => {
  const formatted: Array<string> = [];
  docs.forEach((doc, idx) => {
    const docStr = `<source id="${idx}">
  <title>${doc.metadata.title}</title>
  <article_snippet>${doc.pageContent}</article_snippet>
</source>`
    formatted.push(docStr);
  });
  return `\n\n<sources>${formatted.join("\n")}</sources>`;
}

const format3 = new RunnableLambda({
  func: (input: { docs: Array<Document> }) => formatDocsToXML(input.docs)
})
const answerChain = anthropicPrompt
  .pipe(anthropic)
  .pipe(new XMLOutputParser())
  .pipe(
    new RunnableLambda({ func: (input: { cited_answer: any }) => input.cited_answer })
  );
const map3 = RunnableMap.from({
  question: new RunnablePassthrough(),
  docs: retriever,
});
const chain3 = map3.assign({ context: format3 }).assign({ cited_answer: answerChain }).pick(["cited_answer", "docs"])

const res = await chain3.invoke("How fast are cheetahs?");

console.log(JSON.stringify(res, null, 2));
# Output:
#   {

#     "cited_answer": [

#       {

#         "answer": "Cheetahs can reach top speeds of around 75 mph, but can only maintain bursts of speed for short distances before tiring."

#       },

#       {

#         "citations": [

#           {

#             "citation": [

#               {

#                 "source_id": "1"

#               },

#               {

#                 "quote": "Scientists calculate a cheetah's top speed is 75 mph, but the fastest recorded speed is somewhat slower."

#               }

#             ]

#           },

#           {

#             "citation": [

#               {

#                 "source_id": "3"

#               },

#               {

#                 "quote": "The maximum speed cheetahs have been measured at is 114 km (71 miles) per hour, and they routinely reach velocities of 80–100 km (50–62 miles) per hour while pursuing prey."

#               }

#             ]

#           }

#         ]

#       }

#     ],

#     "docs": [

#       {

#         "pageContent": "One of two videos from National Geographic's award-winning multimedia coverage of cheetahs in the magazine's November 2012 iPad edition. See the other: http:...",

#         "metadata": {

#           "title": "The Science of a Cheetah's Speed | National Geographic",

#           "source": "https://www.youtube.com/watch?v=icFMTB0Pi0g",

#           "score": 0.96603,

#           "images": null

#         }

#       },

#       {

#         "pageContent": "The science of cheetah speed\nThe cheetah (Acinonyx jubatus) is the fastest land animal on Earth, capable of reaching speeds as high as 75 mph or 120 km/h. Cheetahs are predators that sneak up on their prey and sprint a short distance to chase and attack.\n Key Takeaways: How Fast Can a Cheetah Run?\nFastest Cheetah on Earth\nScientists calculate a cheetah's top speed is 75 mph, but the fastest recorded speed is somewhat slower. The top 10 fastest animals are:\nThe pronghorn, an American animal resembling an antelope, is the fastest land animal in the Western Hemisphere. While a cheetah's top speed ranges from 65 to 75 mph (104 to 120 km/h), its average speed is only 40 mph (64 km/hr), punctuated by short bursts at its top speed. Basically, if a predator threatens to take a cheetah's kill or attack its young, a cheetah has to run.\n",

#         "metadata": {

#           "title": "How Fast Can a Cheetah Run? - ThoughtCo",

#           "source": "https://www.thoughtco.com/how-fast-can-a-cheetah-run-4587031",

#           "score": 0.96212,

#           "images": null

#         }

#       },

#       {

#         "pageContent": "Now, their only hope lies in the hands of human conservationists, working tirelessly to save the cheetahs, the leopards and all the other wildlife of the scattered savannas and other habitats of Africa and Asia.\n Their tough paw pads and grippy claws are made to grab at the ground, and their large nasal passages and lungs facilitate the flow of oxygen and allow their rapid intake of air as they reach their top speeds.\n And though the two cats share a similar coloration, a cheetah's spots are circular while a leopard's spots are rose-shaped \"rosettes,\" with the centers of their spots showing off the tan color of their coats.\n Also classified as \"vulnerable\" are two of the cheetah's foremost foes, the lion and the leopard, the latter of which is commonly confused for the cheetah thanks to its own flecked fur.\n The cats are also consumers of the smallest of the bigger, bulkier antelopes, such as sables and kudus, and are known to gnaw on the occasional rabbit or bird.\n",

#         "metadata": {

#           "title": "How Fast Are Cheetahs, and Other Fascinating Facts About the World's ...",

#           "source": "https://www.discovermagazine.com/planet-earth/how-fast-are-cheetahs-and-other-fascinating-facts-about-the-worlds-quickest",

#           "score": 0.95688,

#           "images": null

#         }

#       },

#       {

#         "pageContent": "The maximum speed cheetahs have been measured at is 114 km (71 miles) per hour, and they routinely reach velocities of 80–100 km (50–62 miles) per hour while pursuing prey.\ncheetah,\n(Acinonyx jubatus),\none of the world’s most-recognizable cats, known especially for its speed. Their fur is dark and includes a thick yellowish gray mane along the back, a trait that presumably offers better camouflage and increased protection from high temperatures during the day and low temperatures at night during the first few months of life. Cheetahs eat a variety of small animals, including game birds, rabbits, small antelopes (including the springbok, impala, and gazelle), young warthogs, and larger antelopes (such as the kudu, hartebeest, oryx, and roan).\n A cheetah eats a variety of small animals, including game birds, rabbits, small antelopes (including the springbok, impala, and gazelle), young warthogs, and larger antelopes (such as the kudu, hartebeest, oryx, and roan). Their faces are distinguished by prominent black lines that curve from the inner corner of each eye to the outer corners of the mouth, like a well-worn trail of inky tears.",

#         "metadata": {

#           "title": "Cheetah | Description, Speed, Habitat, Diet, Cubs, & Facts",

#           "source": "https://www.britannica.com/animal/cheetah-mammal",

#           "score": 0.95589,

#           "images": null

#         }

#       },

#       {

#         "pageContent": "Contact Us − +\nAddress\nSmithsonian's National Zoo & Conservation Biology Institute  3001 Connecticut Ave., NW  Washington, DC 20008\nAbout the Zoo\n−\n+\nCareers\n−\n+\nNews & Media\n−\n+\nFooter Donate\n−\n+\nShop\n−\n+\nFollow us on social media\nSign Up for Emails\nFooter - SI logo, privacy, terms Conservation Efforts\nHistorically, cheetahs ranged widely throughout Africa and Asia, from the Cape of Good Hope to the Mediterranean, throughout the Arabian Peninsula and the Middle East, from Israel, India and Pakistan north to the northern shores of the Caspian and Aral Seas, and west through Uzbekistan, Turkmenistan, Afghanistan, and Pakistan into central India. Header Links\nToday's hours: 8 a.m. to 4 p.m. (last entry 3 p.m.)\nMega menu\nAnimals Global Nav Links\nElephant Cam\nSee the Smithsonian's National Zoo's Asian elephants — Spike, Bozie, Kamala, Swarna and Maharani — both inside the Elephant Community Center and outside in their yards.\n Conservation Global Nav Links\nAbout the Smithsonian Conservation Biology Institute\nCheetah\nAcinonyx jubatus\nBuilt for speed, the cheetah can accelerate from zero to 45 in just 2.5 seconds and reach top speeds of 60 to 70 mph, making it the fastest land mammal! Fun Facts\nConservation Status\nCheetah News\nTaxonomic Information\nAnimal News\nNZCBI staff in Front Royal, Virginia, are mourning the loss of Walnut, a white-naped crane who became an internet sensation for choosing one of her keepers as her mate.\n",

#         "metadata": {

#           "title": "Cheetah | Smithsonian's National Zoo and Conservation Biology Institute",

#           "source": "https://nationalzoo.si.edu/animals/cheetah",

#           "score": 0.94744,

#           "images": null

#         }

#       },

#       {

#         "pageContent": "The speeds attained by the cheetah may be only slightly greater than those achieved by the pronghorn at 88.5 km/h (55.0 mph)[96] and the springbok at 88 km/h (55 mph),[97] but the cheetah additionally has an exceptional acceleration.[98]\nOne stride of a galloping cheetah measures 4 to 7 m (13 to 23 ft); the stride length and the number of jumps increases with speed.[60] During more than half the duration of the sprint, the cheetah has all four limbs in the air, increasing the stride length.[99] Running cheetahs can retain up to 90% of the heat generated during the chase. In December 2016 the results of an extensive survey detailing the distribution and demography of cheetahs throughout the range were published; the researchers recommended listing the cheetah as Endangered on the IUCN Red List.[25]\nThe cheetah was reintroduced in Malawi in 2017.[160]\nIn Asia\nIn 2001, the Iranian government collaborated with the CCF, the IUCN, Panthera Corporation, UNDP and the Wildlife Conservation Society on the Conservation of Asiatic Cheetah Project (CACP) to protect the natural habitat of the Asiatic cheetah and its prey.[161][162] Individuals on the periphery of the prey herd are common targets; vigilant prey which would react quickly on seeing the cheetah are not preferred.[47][60][122]\nCheetahs are one of the most iconic pursuit predators, hunting primarily throughout the day, sometimes with peaks at dawn and dusk; they tend to avoid larger predators like the primarily nocturnal lion.[66] Cheetahs in the Sahara and Maasai Mara in Kenya hunt after sunset to escape the high temperatures of the day.[123] Cheetahs use their vision to hunt instead of their sense of smell; they keep a lookout for prey from resting sites or low branches. This significantly sharpens the vision and enables the cheetah to swiftly locate prey against the horizon.[61][86] The cheetah is unable to roar due to the presence of a sharp-edged vocal fold within the larynx.[2][87]\nSpeed and acceleration\nThe cheetah is the world's fastest land animal.[88][89][90][91][92] Estimates of the maximum speed attained range from 80 to 128 km/h (50 to 80 mph).[60][63] A commonly quoted value is 112 km/h (70 mph), recorded in 1957, but this measurement is disputed.[93] The mouth can not be opened as widely as in other cats given the shorter length of muscles between the jaw and the skull.[60][65] A study suggested that the limited retraction of the cheetah's claws may result from the earlier truncation of the development of the middle phalanx bone in cheetahs.[77]\nThe cheetah has a total of 30 teeth; the dental formula is 3.1.3.13.1.2.1.",

#         "metadata": {

#           "title": "Cheetah - Wikipedia",

#           "source": "https://en.wikipedia.org/wiki/Cheetah",

#           "score": 0.81312,

#           "images": null

#         }

#       }

#     ]

#   }


"""
Check out this LangSmith trace [here](https://smith.langchain.com/public/e2e938e8-f847-4ea8-bc84-43d4eaf8e524/r) for more on the internals.
"""

"""
## Retrieval post-processing

Another approach is to post-process our retrieved documents to compress the content, so that the source content is already minimal enough that we don't need the model to cite specific sources or spans. For example, we could break up each document into a sentence or two, embed those and keep only the most relevant ones. LangChain has some built-in components for this. Here we'll use a [`RecursiveCharacterTextSplitter`](/docs/how_to/recursive_text_splitter), which creates chunks of a specified size by splitting on separator substrings, and an [`EmbeddingsFilter`](/docs/how_to/contextual_compression), which keeps only the texts with the most relevant embeddings.
"""

import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { EmbeddingsFilter } from "langchain/retrievers/document_compressors/embeddings_filter";
import { OpenAIEmbeddings } from "@langchain/openai";
import { DocumentInterface } from "@langchain/core/documents";
import { RunnableMap, RunnablePassthrough } from "@langchain/core/runnables";

const splitter = new RecursiveCharacterTextSplitter({
  chunkSize: 400,
  chunkOverlap: 0,
  separators: ["\n\n", "\n", ".", " "],
  keepSeparator: false,
});

const compressor = new EmbeddingsFilter({
  embeddings: new OpenAIEmbeddings(),
  k: 10,
});

const splitAndFilter = async (input): Promise<Array<DocumentInterface>> => {
  const { docs, question } = input;
  const splitDocs = await splitter.splitDocuments(docs);
  const statefulDocs = await compressor.compressDocuments(splitDocs, question);
  return statefulDocs;
};

const retrieveMap = RunnableMap.from({
  question: new RunnablePassthrough(),
  docs: retriever,
});

const retriever = retrieveMap.pipe(splitAndFilter);
const docs = await retriever.invoke("How fast are cheetahs?");
for (const doc of docs) {
  console.log(doc.pageContent, "\n\n");
}
# Output:
#   The maximum speed cheetahs have been measured at is 114 km (71 miles) per hour, and they routinely reach velocities of 80–100 km (50–62 miles) per hour while pursuing prey.

#   cheetah,

#   (Acinonyx jubatus), 

#   

#   

#   The science of cheetah speed

#   The cheetah (Acinonyx jubatus) is the fastest land animal on Earth, capable of reaching speeds as high as 75 mph or 120 km/h. Cheetahs are predators that sneak up on their prey and sprint a short distance to chase and attack.

#    Key Takeaways: How Fast Can a Cheetah Run?

#   Fastest Cheetah on Earth 

#   

#   

#   Built for speed, the cheetah can accelerate from zero to 45 in just 2.5 seconds and reach top speeds of 60 to 70 mph, making it the fastest land mammal! Fun Facts

#   Conservation Status

#   Cheetah News

#   Taxonomic Information

#   Animal News

#   NZCBI staff in Front Royal, Virginia, are mourning the loss of Walnut, a white-naped crane who became an internet sensation for choosing one of her keepers as her mate. 

#   

#   

#   The speeds attained by the cheetah may be only slightly greater than those achieved by the pronghorn at 88.5 km/h (55.0 mph)[96] and the springbok at 88 km/h (55 mph),[97] but the cheetah additionally has an exceptional acceleration.[98] 

#   

#   

#   The cheetah is the world's fastest land animal.[88][89][90][91][92] Estimates of the maximum speed attained range from 80 to 128 km/h (50 to 80 mph).[60][63] A commonly quoted value is 112 km/h (70 mph), recorded in 1957, but this measurement is disputed.[93] The mouth can not be opened as widely as in other cats given the shorter length of muscles between the jaw and the skull 

#   

#   

#   Scientists calculate a cheetah's top speed is 75 mph, but the fastest recorded speed is somewhat slower. The top 10 fastest animals are: 

#   

#   

#   One stride of a galloping cheetah measures 4 to 7 m (13 to 23 ft); the stride length and the number of jumps increases with speed.[60] During more than half the duration of the sprint, the cheetah has all four limbs in the air, increasing the stride length.[99] Running cheetahs can retain up to 90% of the heat generated during the chase 

#   

#   

#   The pronghorn, an American animal resembling an antelope, is the fastest land animal in the Western Hemisphere. While a cheetah's top speed ranges from 65 to 75 mph (104 to 120 km/h), its average speed is only 40 mph (64 km/hr), punctuated by short bursts at its top speed. Basically, if a predator threatens to take a cheetah's kill or attack its young, a cheetah has to run. 

#   

#   

#   A cheetah eats a variety of small animals, including game birds, rabbits, small antelopes (including the springbok, impala, and gazelle), young warthogs, and larger antelopes (such as the kudu, hartebeest, oryx, and roan). Their faces are distinguished by prominent black lines that curve from the inner corner of each eye to the outer corners of the mouth, like a well-worn trail of inky tears. 

#   

#   

#   Cheetahs are one of the most iconic pursuit predators, hunting primarily throughout the day, sometimes with peaks at dawn and dusk; they tend to avoid larger predators like the primarily nocturnal lion.[66] Cheetahs in the Sahara and Maasai Mara in Kenya hunt after sunset to escape the high temperatures of the day 

#   

#   


"""
See the LangSmith trace [here](https://smith.langchain.com/public/ae6b1f52-c1fe-49ec-843c-92edf2104652/r) to see the internals.
"""

const chain4 = retrieveMap
  .assign({ context: formatDocs })
  .assign({ answer: answerChain })
  .pick(["answer", "docs"]);
  
// Note the documents have an article "summary" in the metadata that is now much longer than the
// actual document page content. This summary isn't actually passed to the model.
const res = await chain4.invoke("How fast are cheetahs?");

console.log(JSON.stringify(res, null, 2))
# Output:
#   {

#     "answer": [

#       {

#         "answer": "\nCheetahs are the fastest land animals. They can reach top speeds between 75-81 mph (120-130 km/h). \n"

#       },

#       {

#         "citations": [

#           {

#             "citation": [

#               {

#                 "source_id": "Article title: How Fast Can a Cheetah Run? - ThoughtCo"

#               },

#               {

#                 "quote": "The science of cheetah speed\nThe cheetah (Acinonyx jubatus) is the fastest land animal on Earth, capable of reaching speeds as high as 75 mph or 120 km/h."

#               }

#             ]

#           },

#           {

#             "citation": [

#               {

#                 "source_id": "Article title: Cheetah - Wikipedia"

#               },

#               {

#                 "quote": "Scientists calculate a cheetah's top speed is 75 mph, but the fastest recorded speed is somewhat slower."

#               }

#             ]

#           }

#         ]

#       }

#     ],

#     "docs": [

#       {

#         "pageContent": "The science of cheetah speed\nThe cheetah (Acinonyx jubatus) is the fastest land animal on Earth, capable of reaching speeds as high as 75 mph or 120 km/h. Cheetahs are predators that sneak up on their prey and sprint a short distance to chase and attack.\n Key Takeaways: How Fast Can a Cheetah Run?\nFastest Cheetah on Earth\nScientists calculate a cheetah's top speed is 75 mph, but the fastest recorded speed is somewhat slower. The top 10 fastest animals are:\nThe pronghorn, an American animal resembling an antelope, is the fastest land animal in the Western Hemisphere. While a cheetah's top speed ranges from 65 to 75 mph (104 to 120 km/h), its average speed is only 40 mph (64 km/hr), punctuated by short bursts at its top speed. Basically, if a predator threatens to take a cheetah's kill or attack its young, a cheetah has to run.\n",

#         "metadata": {

#           "title": "How Fast Can a Cheetah Run? - ThoughtCo",

#           "source": "https://www.thoughtco.com/how-fast-can-a-cheetah-run-4587031",

#           "score": 0.96949,

#           "images": null

#         }

#       },

#       {

#         "pageContent": "The speeds attained by the cheetah may be only slightly greater than those achieved by the pronghorn at 88.5 km/h (55.0 mph)[96] and the springbok at 88 km/h (55 mph),[97] but the cheetah additionally has an exceptional acceleration.[98]\nOne stride of a galloping cheetah measures 4 to 7 m (13 to 23 ft); the stride length and the number of jumps increases with speed.[60] During more than half the duration of the sprint, the cheetah has all four limbs in the air, increasing the stride length.[99] Running cheetahs can retain up to 90% of the heat generated during the chase. In December 2016 the results of an extensive survey detailing the distribution and demography of cheetahs throughout the range were published; the researchers recommended listing the cheetah as Endangered on the IUCN Red List.[25]\nThe cheetah was reintroduced in Malawi in 2017.[160]\nIn Asia\nIn 2001, the Iranian government collaborated with the CCF, the IUCN, Panthera Corporation, UNDP and the Wildlife Conservation Society on the Conservation of Asiatic Cheetah Project (CACP) to protect the natural habitat of the Asiatic cheetah and its prey.[161][162] Individuals on the periphery of the prey herd are common targets; vigilant prey which would react quickly on seeing the cheetah are not preferred.[47][60][122]\nCheetahs are one of the most iconic pursuit predators, hunting primarily throughout the day, sometimes with peaks at dawn and dusk; they tend to avoid larger predators like the primarily nocturnal lion.[66] Cheetahs in the Sahara and Maasai Mara in Kenya hunt after sunset to escape the high temperatures of the day.[123] Cheetahs use their vision to hunt instead of their sense of smell; they keep a lookout for prey from resting sites or low branches. This significantly sharpens the vision and enables the cheetah to swiftly locate prey against the horizon.[61][86] The cheetah is unable to roar due to the presence of a sharp-edged vocal fold within the larynx.[2][87]\nSpeed and acceleration\nThe cheetah is the world's fastest land animal.[88][89][90][91][92] Estimates of the maximum speed attained range from 80 to 128 km/h (50 to 80 mph).[60][63] A commonly quoted value is 112 km/h (70 mph), recorded in 1957, but this measurement is disputed.[93] The mouth can not be opened as widely as in other cats given the shorter length of muscles between the jaw and the skull.[60][65] A study suggested that the limited retraction of the cheetah's claws may result from the earlier truncation of the development of the middle phalanx bone in cheetahs.[77]\nThe cheetah has a total of 30 teeth; the dental formula is 3.1.3.13.1.2.1.",

#         "metadata": {

#           "title": "Cheetah - Wikipedia",

#           "source": "https://en.wikipedia.org/wiki/Cheetah",

#           "score": 0.96423,

#           "images": null

#         }

#       },

#       {

#         "pageContent": "One of two videos from National Geographic's award-winning multimedia coverage of cheetahs in the magazine's November 2012 iPad edition. See the other: http:...",

#         "metadata": {

#           "title": "The Science of a Cheetah's Speed | National Geographic",

#           "source": "https://www.youtube.com/watch?v=icFMTB0Pi0g",

#           "score": 0.96071,

#           "images": null

#         }

#       },

#       {

#         "pageContent": "Contact Us − +\nAddress\nSmithsonian's National Zoo & Conservation Biology Institute  3001 Connecticut Ave., NW  Washington, DC 20008\nAbout the Zoo\n−\n+\nCareers\n−\n+\nNews & Media\n−\n+\nFooter Donate\n−\n+\nShop\n−\n+\nFollow us on social media\nSign Up for Emails\nFooter - SI logo, privacy, terms Conservation Efforts\nHistorically, cheetahs ranged widely throughout Africa and Asia, from the Cape of Good Hope to the Mediterranean, throughout the Arabian Peninsula and the Middle East, from Israel, India and Pakistan north to the northern shores of the Caspian and Aral Seas, and west through Uzbekistan, Turkmenistan, Afghanistan, and Pakistan into central India. Header Links\nToday's hours: 8 a.m. to 4 p.m. (last entry 3 p.m.)\nMega menu\nAnimals Global Nav Links\nElephant Cam\nSee the Smithsonian's National Zoo's Asian elephants — Spike, Bozie, Kamala, Swarna and Maharani — both inside the Elephant Community Center and outside in their yards.\n Conservation Global Nav Links\nAbout the Smithsonian Conservation Biology Institute\nCheetah\nAcinonyx jubatus\nBuilt for speed, the cheetah can accelerate from zero to 45 in just 2.5 seconds and reach top speeds of 60 to 70 mph, making it the fastest land mammal! Fun Facts\nConservation Status\nCheetah News\nTaxonomic Information\nAnimal News\nNZCBI staff in Front Royal, Virginia, are mourning the loss of Walnut, a white-naped crane who became an internet sensation for choosing one of her keepers as her mate.\n",

#         "metadata": {

#           "title": "Cheetah | Smithsonian's National Zoo and Conservation Biology Institute",

#           "source": "https://nationalzoo.si.edu/animals/cheetah",

#           "score": 0.91577,

#           "images": null

#         }

#       },

#       {

#         "pageContent": "The maximum speed cheetahs have been measured at is 114 km (71 miles) per hour, and they routinely reach velocities of 80–100 km (50–62 miles) per hour while pursuing prey.\ncheetah,\n(Acinonyx jubatus),\none of the world’s most-recognizable cats, known especially for its speed. Their fur is dark and includes a thick yellowish gray mane along the back, a trait that presumably offers better camouflage and increased protection from high temperatures during the day and low temperatures at night during the first few months of life. Cheetahs eat a variety of small animals, including game birds, rabbits, small antelopes (including the springbok, impala, and gazelle), young warthogs, and larger antelopes (such as the kudu, hartebeest, oryx, and roan).\n A cheetah eats a variety of small animals, including game birds, rabbits, small antelopes (including the springbok, impala, and gazelle), young warthogs, and larger antelopes (such as the kudu, hartebeest, oryx, and roan). Their faces are distinguished by prominent black lines that curve from the inner corner of each eye to the outer corners of the mouth, like a well-worn trail of inky tears.",

#         "metadata": {

#           "title": "Cheetah | Description, Speed, Habitat, Diet, Cubs, & Facts",

#           "source": "https://www.britannica.com/animal/cheetah-mammal",

#           "score": 0.91163,

#           "images": null

#         }

#       },

#       {

#         "pageContent": "If a lion comes along, the cheetah will abandon its catch -- it can't fight off a lion, and chances are, the cheetah will lose its life along with its prey if it doesn't get out of there fast enough.\n Advertisement\nLots More Information\nMore Great Links\nSources\nPlease copy/paste the following text to properly cite this HowStuffWorks.com article:\nAdvertisement\nAdvertisement\nAdvertisement\nAdvertisement\nAdvertisement If confronted, a roughly 125-pound cheetah will always run rather than fight -- it's too weak, light and thin to have any chance against something like a lion, which can be twice as long as a cheetah and weigh more than 400 pounds (181.4 kg) Cheetah moms spend a lot of time teaching their cubs to chase, sometimes dragging live animals back to the den so the cubs can practice the chase-and-catch process.\n It's more like a bound at that speed, completing up to three strides per second, with only one foot on the ground at any time and several stages when feet don't touch the ground at all.",

#         "metadata": {

#           "title": "What makes a cheetah run so fast? | HowStuffWorks",

#           "source": "https://animals.howstuffworks.com/mammals/cheetah-speed.htm",

#           "score": 0.89019,

#           "images": null

#         }

#       }

#     ]

#   }


"""
Check out the LangSmith trace [here](https://smith.langchain.com/public/b767cca0-6061-4208-99f2-7f522b94a587/r) to see the internals.

## Next steps

You've now learned a few ways to return citations from your QA chains.

Next, check out some of the other guides in this section, such as [how to add chat history](/docs/how_to/qa_chat_history_how_to).
"""



================================================
FILE: docs/core_docs/docs/how_to/qa_per_user.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_position: 4
---
"""

"""
# How to do per-user retrieval

:::info Prerequisites

This guide assumes familiarity with the following:

- [Retrieval-augmented generation](/docs/tutorials/rag/)

:::

When building a retrieval app, you often have to build it with multiple users in
mind. This means that you may be storing data not just for one user, but for
many different users, and they should not be able to see each other's data. This
means that you need to be able to configure your retrieval chain to only
retrieve certain information. This generally involves two steps.

**Step 1: Make sure the retriever you are using supports multiple users**

At the moment, there is no unified flag or filter for this in LangChain. Rather,
each vectorstore and retriever may have their own, and may be called different
things (namespaces, multi-tenancy, etc). For vectorstores, this is generally
exposed as a keyword argument that is passed in during `similaritySearch`. By
reading the documentation or source code, figure out whether the retriever you
are using supports multiple users, and, if so, how to use it.

**Step 2: Add that parameter as a configurable field for the chain**

The LangChain `config` object is passed through to every Runnable. Here you can
add any fields you'd like to the `configurable` object. Later, inside the chain
we can extract these fields.

**Step 3: Call the chain with that configurable field**

Now, at runtime you can call this chain with configurable field.

## Code Example

Let's see a concrete example of what this looks like in code. We will use
Pinecone for this example.
"""

"""
## Setup

### Install dependencies

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/pinecone @langchain/openai @langchain/core @pinecone-database/pinecone
</Npm2Yarn>
```

### Set environment variables

We'll use OpenAI and Pinecone in this example:

```env
OPENAI_API_KEY=your-api-key

PINECONE_API_KEY=your-api-key
PINECONE_INDEX=your-index-name

# Optional, use LangSmith for best-in-class observability
LANGSMITH_API_KEY=your-api-key
LANGSMITH_TRACING=true

# Reduce tracing latency if you are not in a serverless environment
# LANGCHAIN_CALLBACKS_BACKGROUND=true
```
"""

import { OpenAIEmbeddings } from "@langchain/openai";
import { PineconeStore } from "@langchain/pinecone";
import { Pinecone } from "@pinecone-database/pinecone";
import { Document } from "@langchain/core/documents";

const embeddings = new OpenAIEmbeddings();

const pinecone = new Pinecone();

const pineconeIndex = pinecone.Index(process.env.PINECONE_INDEX);

/**
 * Pinecone allows you to partition the records in an index into namespaces. 
 * Queries and other operations are then limited to one namespace, 
 * so different requests can search different subsets of your index.
 * Read more about namespaces here: https://docs.pinecone.io/guides/indexes/use-namespaces
 * 
 * NOTE: If you have namespace enabled in your Pinecone index, you must provide the namespace when creating the PineconeStore.
 */
const namespace = "pinecone";

const vectorStore = await PineconeStore.fromExistingIndex(
  new OpenAIEmbeddings(),
  { pineconeIndex, namespace },
);

await vectorStore.addDocuments(
  [new Document({ pageContent: "i worked at kensho" })],
  { namespace: "harrison" },
);

await vectorStore.addDocuments(
  [new Document({ pageContent: "i worked at facebook" })],
  { namespace: "ankush" },
);
# Output:
#   [ [32m"77b8f174-9d89-4c6c-b2ab-607fe3913b2d"[39m ]

"""
The pinecone kwarg for `namespace` can be used to separate documents
"""

// This will only get documents for Ankush
const ankushRetriever = vectorStore.asRetriever({
  filter: {
    namespace: "ankush",
  },
});

await ankushRetriever.invoke(
  "where did i work?",
);
# Output:
#   [ Document { pageContent: [32m"i worked at facebook"[39m, metadata: {} } ]

// This will only get documents for Harrison
const harrisonRetriever = vectorStore.asRetriever({
  filter: {
    namespace: "harrison",
  },
});

await harrisonRetriever.invoke(
  "where did i work?",
);
# Output:
#   [ Document { pageContent: [32m"i worked at kensho"[39m, metadata: {} } ]

"""
We can now create the chain that we will use to perform question-answering.
"""

import { StringOutputParser } from "@langchain/core/output_parsers";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import {
  RunnableBinding,
  RunnableLambda,
  RunnablePassthrough,
} from "@langchain/core/runnables";
import { ChatOpenAI, OpenAIEmbeddings } from "@langchain/openai";

const template = `Answer the question based only on the following context:
{context}
Question: {question}`;

const prompt = ChatPromptTemplate.fromTemplate(template);

const model = new ChatOpenAI({
  model: "gpt-3.5-turbo-0125",
  temperature: 0,
});

"""
We can now create the chain using our configurable retriever. It is configurable
because we can define any object which will be passed to the chain. From there,
we extract the configurable object and pass it to the vectorstore.
"""

import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";

const chain = RunnableSequence.from([
  RunnablePassthrough.assign({
    context: async (input: { question: string }, config) => {
      if (!config || !("configurable" in config)) {
        throw new Error("No config");
      }
      const { configurable } = config;
      const documents = await vectorStore.asRetriever(configurable).invoke(
        input.question,
        config,
      );
      return documents.map((doc) => doc.pageContent).join("\n\n");
    },
  }),
  prompt,
  model,
  new StringOutputParser(),
]);

"""
We can now invoke the chain with configurable options. `search_kwargs` is the id
of the configurable field. The value is the search kwargs to use for Pinecone
"""

await chain.invoke(
  { question: "where did the user work?"},
  { configurable: { filter: { namespace: "harrison" } } },
);
# Output:
#   [32m"The user worked at Kensho."[39m

await chain.invoke(
  { question: "where did the user work?"},
  { configurable: { filter: { namespace: "ankush" } } },
);
# Output:
#   [32m"The user worked at Facebook."[39m

"""
For more vector store implementations that can support multiple users, please refer to specific
pages, such as [Milvus](/docs/integrations/vectorstores/milvus).

## Next steps

You've now seen one approach for supporting retrieval with data from multiple users.

Next, check out some of the other how-to guides on RAG, such as [returning sources](/docs/how_to/qa_sources).
"""



================================================
FILE: docs/core_docs/docs/how_to/qa_sources.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to return sources

:::info Prerequisites

This guide assumes familiarity with the following:

- [Retrieval-augmented generation](/docs/tutorials/rag/)

:::

Often in Q&A applications it’s important to show users the sources that were used to generate the answer. The simplest way to do this is for the chain to return the Documents that were retrieved in each generation.

We'll be using the [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng for retrieval content this notebook.
"""

"""
## Setup
### Dependencies

We’ll use an OpenAI chat model and embeddings and a Memory vector store in this walkthrough, but everything shown here works with any [ChatModel](/docs/concepts/chat_models) or [LLM](/docs/concepts/text_llms), [Embeddings](/docs/concepts/embedding_models), and [VectorStore](/docs/concepts/vectorstores) or [Retriever](/docs/concepts/retrievers).

We’ll use the following packages:

```bash
npm install --save langchain @langchain/openai cheerio
```

We need to set environment variable `OPENAI_API_KEY`:

```bash
export OPENAI_API_KEY=YOUR_KEY
```

"""

"""
### LangSmith

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://smith.langchain.com/).

Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:


```bash
export LANGSMITH_TRACING=true
export LANGSMITH_API_KEY=YOUR_KEY

# Reduce tracing latency if you are not in a serverless environment
# export LANGCHAIN_CALLBACKS_BACKGROUND=true
```
"""

"""
## Chain without sources

Here is the Q&A app we built over the [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng in the [Quickstart](/docs/tutorials/qa_chat_history/).
"""

import "cheerio";
import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { MemoryVectorStore } from "langchain/vectorstores/memory"
import { OpenAIEmbeddings, ChatOpenAI } from "@langchain/openai";
import { pull } from "langchain/hub";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { formatDocumentsAsString } from "langchain/util/document";
import { RunnableSequence, RunnablePassthrough } from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers";

const loader = new CheerioWebBaseLoader(
  "https://lilianweng.github.io/posts/2023-06-23-agent/"
);

const docs = await loader.load();

const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000, chunkOverlap: 200 });
const splits = await textSplitter.splitDocuments(docs);
const vectorStore = await MemoryVectorStore.fromDocuments(splits, new OpenAIEmbeddings());

// Retrieve and generate using the relevant snippets of the blog.
const retriever = vectorStore.asRetriever();
const prompt = await pull<ChatPromptTemplate>("rlm/rag-prompt");
const llm = new ChatOpenAI({ model: "gpt-3.5-turbo", temperature: 0 });

const ragChain = RunnableSequence.from([
  {
    context: retriever.pipe(formatDocumentsAsString),
    question: new RunnablePassthrough(),
  },
  prompt,
  llm,
  new StringOutputParser()
]);

"""
Let's see what this prompt actually looks like:
"""

console.log(prompt.promptMessages.map((msg) => msg.prompt.template).join("\n"));
# Output:
#   You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.

#   Question: {question} 

#   Context: {context} 

#   Answer:


await ragChain.invoke("What is task decomposition?")
# Output:
#   [32m"Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. T"[39m... 254 more characters

"""
## Adding sources

With LCEL, we can easily pass the retrieved documents through the chain and return them in the final response:
"""

import {
  RunnableMap,
  RunnablePassthrough,
  RunnableSequence
} from "@langchain/core/runnables";
import { formatDocumentsAsString } from "langchain/util/document";

const ragChainWithSources = RunnableMap.from({
  // Return raw documents here for now since we want to return them at
  // the end - we'll format in the next step of the chain
  context: retriever,
  question: new RunnablePassthrough(),
}).assign({
  answer: RunnableSequence.from([
    (input) => {
      return {
        // Now we format the documents as strings for the prompt
        context: formatDocumentsAsString(input.context),
        question: input.question
      };
    },
    prompt,
    llm,
    new StringOutputParser()
  ]),
})

await ragChainWithSources.invoke("What is Task Decomposition")
# Output:
#   {

#     question: [32m"What is Task Decomposition"[39m,

#     context: [

#       Document {

#         pageContent: [32m"Fig. 1. Overview of a LLM-powered autonomous agent system.\n"[39m +

#           [32m"Component One: Planning#\n"[39m +

#           [32m"A complicated ta"[39m... 898 more characters,

#         metadata: {

#           source: [32m"https://lilianweng.github.io/posts/2023-06-23-agent/"[39m,

#           loc: { lines: [36m[Object][39m }

#         }

#       },

#       Document {

#         pageContent: [32m'Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\\n1.", "What are'[39m... 887 more characters,

#         metadata: {

#           source: [32m"https://lilianweng.github.io/posts/2023-06-23-agent/"[39m,

#           loc: { lines: [36m[Object][39m }

#         }

#       },

#       Document {

#         pageContent: [32m"Agent System Overview\n"[39m +

#           [32m"                \n"[39m +

#           [32m"                    Component One: Planning\n"[39m +

#           [32m"                 "[39m... 850 more characters,

#         metadata: {

#           source: [32m"https://lilianweng.github.io/posts/2023-06-23-agent/"[39m,

#           loc: { lines: [36m[Object][39m }

#         }

#       },

#       Document {

#         pageContent: [32m"Resources:\n"[39m +

#           [32m"1. Internet access for searches and information gathering.\n"[39m +

#           [32m"2. Long Term memory management"[39m... 456 more characters,

#         metadata: {

#           source: [32m"https://lilianweng.github.io/posts/2023-06-23-agent/"[39m,

#           loc: { lines: [36m[Object][39m }

#         }

#       }

#     ],

#     answer: [32m"Task decomposition is a technique used to break down complex tasks into smaller and simpler steps fo"[39m... 230 more characters

#   }

"""
Check out the [LangSmith trace](https://smith.langchain.com/public/c3753531-563c-40d4-a6bf-21bfe8741d10/r) here to see the internals of the chain.

## Next steps

You've now learned how to return sources from your QA chains.

Next, check out some of the other guides around RAG, such as [how to stream responses](/docs/how_to/qa_streaming).
"""



================================================
FILE: docs/core_docs/docs/how_to/qa_streaming.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to stream from a question-answering chain

:::info Prerequisites

This guide assumes familiarity with the following:

- [Retrieval-augmented generation](/docs/tutorials/rag/)

:::

Often in Q&A applications it's important to show users the sources that were used to generate the answer. The simplest way to do this is for the chain to return the Documents that were retrieved in each generation.

We'll be using the [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng for retrieval content this notebook.
"""

"""
## Setup
### Dependencies

We’ll use an OpenAI chat model and embeddings and a Memory vector store in this walkthrough, but everything shown here works with any [ChatModel](/docs/concepts/chat_models) or [LLM](/docs/concepts/text_llms), [Embeddings](/docs/concepts/embedding_models), and [VectorStore](/docs/concepts/vectorstores) or [Retriever](/docs/concepts/retrievers).

We’ll use the following packages:

```bash
npm install --save langchain @langchain/openai cheerio
```

We need to set environment variable `OPENAI_API_KEY`:

```bash
export OPENAI_API_KEY=YOUR_KEY
```

"""

"""
### LangSmith

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://smith.langchain.com/).

Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:


```bash
export LANGSMITH_TRACING=true
export LANGSMITH_API_KEY=YOUR_KEY

# Reduce tracing latency if you are not in a serverless environment
# export LANGCHAIN_CALLBACKS_BACKGROUND=true
```
"""

"""
## Chain with sources

Here is Q&A app with sources we built over the [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng in the [Returning sources](/docs/how_to/qa_sources/) guide:
"""

import "cheerio";
import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { MemoryVectorStore } from "langchain/vectorstores/memory"
import { OpenAIEmbeddings, ChatOpenAI } from "@langchain/openai";
import { pull } from "langchain/hub";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { formatDocumentsAsString } from "langchain/util/document";
import { RunnableSequence, RunnablePassthrough, RunnableMap } from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers";

const loader = new CheerioWebBaseLoader(
  "https://lilianweng.github.io/posts/2023-06-23-agent/"
);

const docs = await loader.load();

const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000, chunkOverlap: 200 });
const splits = await textSplitter.splitDocuments(docs);
const vectorStore = await MemoryVectorStore.fromDocuments(splits, new OpenAIEmbeddings());

// Retrieve and generate using the relevant snippets of the blog.
const retriever = vectorStore.asRetriever();
const prompt = await pull<ChatPromptTemplate>("rlm/rag-prompt");
const llm = new ChatOpenAI({ model: "gpt-3.5-turbo", temperature: 0 });

const ragChainFromDocs = RunnableSequence.from([
  RunnablePassthrough.assign({ context: (input) => formatDocumentsAsString(input.context) }),
  prompt,
  llm,
  new StringOutputParser()
]);

let ragChainWithSource = new RunnableMap({ steps: { context: retriever, question: new RunnablePassthrough() }})
ragChainWithSource = ragChainWithSource.assign({ answer: ragChainFromDocs });

await ragChainWithSource.invoke("What is Task Decomposition")
# Output:
#   {

#     question: [32m"What is Task Decomposition"[39m,

#     context: [

#       Document {

#         pageContent: [32m"Fig. 1. Overview of a LLM-powered autonomous agent system.\n"[39m +

#           [32m"Component One: Planning#\n"[39m +

#           [32m"A complicated ta"[39m... 898 more characters,

#         metadata: {

#           source: [32m"https://lilianweng.github.io/posts/2023-06-23-agent/"[39m,

#           loc: { lines: [36m[Object][39m }

#         }

#       },

#       Document {

#         pageContent: [32m'Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\\n1.", "What are'[39m... 887 more characters,

#         metadata: {

#           source: [32m"https://lilianweng.github.io/posts/2023-06-23-agent/"[39m,

#           loc: { lines: [36m[Object][39m }

#         }

#       },

#       Document {

#         pageContent: [32m"Agent System Overview\n"[39m +

#           [32m"                \n"[39m +

#           [32m"                    Component One: Planning\n"[39m +

#           [32m"                 "[39m... 850 more characters,

#         metadata: {

#           source: [32m"https://lilianweng.github.io/posts/2023-06-23-agent/"[39m,

#           loc: { lines: [36m[Object][39m }

#         }

#       },

#       Document {

#         pageContent: [32m"Resources:\n"[39m +

#           [32m"1. Internet access for searches and information gathering.\n"[39m +

#           [32m"2. Long Term memory management"[39m... 456 more characters,

#         metadata: {

#           source: [32m"https://lilianweng.github.io/posts/2023-06-23-agent/"[39m,

#           loc: { lines: [36m[Object][39m }

#         }

#       }

#     ],

#     answer: [32m"Task decomposition is a technique used to break down complex tasks into smaller and simpler steps fo"[39m... 230 more characters

#   }

"""
Let's see what this prompt actually looks like. You can also view it [in the LangChain prompt hub](https://smith.langchain.com/hub/rlm/rag-prompt):
"""

console.log(prompt.promptMessages.map((msg) => msg.prompt.template).join("\n"));
# Output:
#   You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.

#   Question: {question} 

#   Context: {context} 

#   Answer:


"""
## Streaming final outputs

With [LCEL](/docs/concepts/lcel), we can stream outputs as they are generated:
"""

for await (const chunk of await ragChainWithSource.stream("What is task decomposition?")) {
  console.log(chunk)
}
# Output:
#   { question: "What is task decomposition?" }

#   {

#     context: [

#       Document {

#         pageContent: "Fig. 1. Overview of a LLM-powered autonomous agent system.\n" +

#           "Component One: Planning#\n" +

#           "A complicated ta"... 898 more characters,

#         metadata: {

#           source: "https://lilianweng.github.io/posts/2023-06-23-agent/",

#           loc: { lines: [Object] }

#         }

#       },

#       Document {

#         pageContent: 'Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\\n1.", "What are'... 887 more characters,

#         metadata: {

#           source: "https://lilianweng.github.io/posts/2023-06-23-agent/",

#           loc: { lines: [Object] }

#         }

#       },

#       Document {

#         pageContent: "Agent System Overview\n" +

#           "                \n" +

#           "                    Component One: Planning\n" +

#           "                 "... 850 more characters,

#         metadata: {

#           source: "https://lilianweng.github.io/posts/2023-06-23-agent/",

#           loc: { lines: [Object] }

#         }

#       },

#       Document {

#         pageContent: "(3) Task execution: Expert models execute on the specific tasks and log results.\n" +

#           "Instruction:\n" +

#           "\n" +

#           "With "... 539 more characters,

#         metadata: {

#           source: "https://lilianweng.github.io/posts/2023-06-23-agent/",

#           loc: { lines: [Object] }

#         }

#       }

#     ]

#   }

#   { answer: "" }

#   { answer: "Task" }

#   { answer: " decomposition" }

#   { answer: " is" }

#   { answer: " a" }

#   { answer: " technique" }

#   { answer: " used" }

#   { answer: " to" }

#   { answer: " break" }

#   { answer: " down" }

#   { answer: " complex" }

#   { answer: " tasks" }

#   { answer: " into" }

#   { answer: " smaller" }

#   { answer: " and" }

#   { answer: " simpler" }

#   { answer: " steps" }

#   { answer: "." }

#   { answer: " It" }

#   { answer: " can" }

#   { answer: " be" }

#   { answer: " done" }

#   { answer: " through" }

#   { answer: " various" }

#   { answer: " methods" }

#   { answer: " such" }

#   { answer: " as" }

#   { answer: " using" }

#   { answer: " prompting" }

#   { answer: " techniques" }

#   { answer: "," }

#   { answer: " task" }

#   { answer: "-specific" }

#   { answer: " instructions" }

#   { answer: "," }

#   { answer: " or" }

#   { answer: " human" }

#   { answer: " inputs" }

#   { answer: "." }

#   { answer: " Another" }

#   { answer: " approach" }

#   { answer: " involves" }

#   { answer: " outsourcing" }

#   { answer: " the" }

#   { answer: " planning" }

#   { answer: " step" }

#   { answer: " to" }

#   { answer: " an" }

#   { answer: " external" }

#   { answer: " classical" }

#   { answer: " planner" }

#   { answer: "." }

#   { answer: "" }


"""
We can add some logic to compile our stream as it's being returned:
"""

const output = {};
let currentKey: string | null = null;

for await (const chunk of await ragChainWithSource.stream("What is task decomposition?")) {
  for (const key of Object.keys(chunk)) {
    if (output[key] === undefined) {
      output[key] = chunk[key];
    } else {
      output[key] += chunk[key];
    }

    if (key !== currentKey) {
      console.log(`\n\n${key}: ${JSON.stringify(chunk[key])}`);
    } else {
      console.log(chunk[key]);
    }
    currentKey = key;
  }
}
# Output:
#   

#   

#   question: "What is task decomposition?"

#   

#   

#   context: [{"pageContent":"Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.","metadata":{"source":"https://lilianweng.github.io/posts/2023-06-23-agent/","loc":{"lines":{"from":176,"to":181}}}},{"pageContent":"Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\nSelf-Reflection#","metadata":{"source":"https://lilianweng.github.io/posts/2023-06-23-agent/","loc":{"lines":{"from":182,"to":184}}}},{"pageContent":"Agent System Overview\n                \n                    Component One: Planning\n                        \n                \n                    Task Decomposition\n                \n                    Self-Reflection\n                \n                \n                    Component Two: Memory\n                        \n                \n                    Types of Memory\n                \n                    Maximum Inner Product Search (MIPS)\n                \n                \n                    Component Three: Tool Use\n                \n                    Case Studies\n                        \n                \n                    Scientific Discovery Agent\n                \n                    Generative Agents Simulation\n                \n                    Proof-of-Concept Examples\n                \n                \n                    Challenges\n                \n                    Citation\n                \n                    References","metadata":{"source":"https://lilianweng.github.io/posts/2023-06-23-agent/","loc":{"lines":{"from":112,"to":146}}}},{"pageContent":"(3) Task execution: Expert models execute on the specific tasks and log results.\nInstruction:\n\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.","metadata":{"source":"https://lilianweng.github.io/posts/2023-06-23-agent/","loc":{"lines":{"from":277,"to":280}}}}]

#   

#   

#   answer: ""

#   Task

#    decomposition

#    is

#    a

#    technique

#    used

#    to

#    break

#    down

#    complex

#    tasks

#    into

#    smaller

#    and

#    simpler

#    steps

#   .

#    It

#    can

#    be

#    done

#    through

#    various

#    methods

#    such

#    as

#    using

#    prompting

#    techniques

#   ,

#    task

#   -specific

#    instructions

#   ,

#    or

#    human

#    inputs

#   .

#    Another

#    approach

#    involves

#    outsourcing

#    the

#    planning

#    step

#    to

#    an

#    external

#    classical

#    planner

#   .

#   

#   [32m"answer"[39m

"""
## Next steps

You've now learned how to stream responses from a QA chain.

Next, check out some of the other how-to guides around RAG, such as [how to add chat history](/docs/how_to/qa_chat_history_how_to).
"""



================================================
FILE: docs/core_docs/docs/how_to/query_constructing_filters.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
keywords: [self-query]
---
"""

"""
# How to construct filters

:::info Prerequisites

This guide assumes familiarity with the following:

- [Query analysis](/docs/tutorials/rag#query-analysis)

:::

We may want to do query analysis to extract filters to pass into retrievers. One way we ask the LLM to represent these filters is as a Zod schema. There is then the issue of converting that Zod schema into a filter that can be passed into a retriever. 

This can be done manually, but LangChain also provides some "Translators" that are able to translate from a common syntax into filters specific to each retriever. Here, we will cover how to use those translators.
"""

"""
## Setup

### Install dependencies

```{=mdx}
import Npm2Yarn from "@theme/Npm2Yarn";

<Npm2Yarn>
  @langchain/core zod
</Npm2Yarn>
```

"""

"""

In this example, `year` and `author` are both attributes to filter on.
"""

import { z } from "zod";

const searchSchema = z.object({
  query: z.string(),
  startYear: z.number().optional(),
  author: z.string().optional(),
})

const searchQuery: z.infer<typeof searchSchema> = {
  query: "RAG",
  startYear: 2022,
  author: "LangChain"
}

import { Comparison, Comparator } from "langchain/chains/query_constructor/ir";

function constructComparisons(query: z.infer<typeof searchSchema>): Comparison[] {
  const comparisons: Comparison[] = [];
  if (query.startYear !== undefined) {
    comparisons.push(
      new Comparison(
        "gt" as Comparator,
        "start_year",
        query.startYear,
      )
    );
  }
  if (query.author !== undefined) {
    comparisons.push(
      new Comparison(
        "eq" as Comparator,
        "author",
        query.author,
      )
    );
  }
  return comparisons;
}

const comparisons = constructComparisons(searchQuery);

import {
  Operation,
  Operator,
} from "langchain/chains/query_constructor/ir";

const _filter = new Operation("and" as Operator, comparisons)

import { ChromaTranslator } from "@langchain/community/structured_query/chroma";

new ChromaTranslator().visitOperation(_filter)
# Output:
#   {

#     [32m"$and"[39m: [

#       { start_year: { [32m"$gt"[39m: [33m2022[39m } },

#       { author: { [32m"$eq"[39m: [32m"LangChain"[39m } }

#     ]

#   }

"""
## Next steps

You've now learned how to create a specific filter from an arbitrary query.

Next, check out some of the other query analysis guides in this section, like [how to use few-shotting to improve performance](/docs/how_to/query_no_queries).
"""



================================================
FILE: docs/core_docs/docs/how_to/query_few_shot.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to add examples to the prompt

:::info Prerequisites

This guide assumes familiarity with the following:

- [Query analysis](/docs/tutorials/rag#query-analysis)

:::

As our query analysis becomes more complex, the LLM may struggle to understand how exactly it should respond in certain scenarios. In order to improve performance here, we can add examples to the prompt to guide the LLM.

Let's take a look at how we can add examples for the LangChain YouTube video query analyzer we built in the [query analysis tutorial](/docs/tutorials/rag#query-analysis).
"""

"""
## Setup

### Install dependencies

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/core zod uuid
</Npm2Yarn>
```

### Set environment variables

```
# Optional, use LangSmith for best-in-class observability
LANGSMITH_API_KEY=your-api-key
LANGSMITH_TRACING=true

# Reduce tracing latency if you are not in a serverless environment
# LANGCHAIN_CALLBACKS_BACKGROUND=true
```
"""

// @lc-docs-hide-cell
import { ChatOpenAI } from '@langchain/openai';

const llm = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0,
})

"""
## Query schema

We'll define a query schema that we want our model to output. To make our query analysis a bit more interesting, we'll add a `subQueries` field that contains more narrow questions derived from the top level question.
"""

import { z } from "zod";

const subQueriesDescription = `
If the original question contains multiple distinct sub-questions,
or if there are more generic questions that would be helpful to answer in
order to answer the original question, write a list of all relevant sub-questions.
Make sure this list is comprehensive and covers all parts of the original question.
It's ok if there's redundancy in the sub-questions, it's better to cover all the bases than to miss some.
Make sure the sub-questions are as narrowly focused as possible in order to get the most relevant results.`

const searchSchema = z.object({
    query: z.string().describe("Primary similarity search query applied to video transcripts."),
    subQueries: z.array(z.string()).optional().describe(subQueriesDescription),
    publishYear: z.number().optional().describe("Year video was published")
})

"""
## Query generation
"""

"""
```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

import { ChatPromptTemplate } from "@langchain/core/prompts"
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables"

const system = `You are an expert at converting user questions into database queries.
You have access to a database of tutorial videos about a software library for building LLM-powered applications.
Given a question, return a list of database queries optimized to retrieve the most relevant results.

If there are acronyms or words you are not familiar with, do not try to rephrase them.`

const prompt = ChatPromptTemplate.fromMessages(
[
    ["system", system],
    ["placeholder", "{examples}"],
    ["human", "{question}"],
]
)
const llmWithTools = llm.withStructuredOutput(searchSchema, {
  name: "Search",
})
const queryAnalyzer = RunnableSequence.from([
  {
    question: new RunnablePassthrough(),
  },
  prompt,
  llmWithTools
]);

"""
Let's try out our query analyzer without any examples in the prompt:
"""

await queryAnalyzer.invoke(
  "what's the difference between web voyager and reflection agents? do both use langgraph?"
)
# Output:
#   {

#     query: [32m"difference between Web Voyager and Reflection Agents"[39m,

#     subQueries: [ [32m"Do Web Voyager and Reflection Agents use LangGraph?"[39m ]

#   }

"""
## Adding examples and tuning the prompt

This works pretty well, but we probably want it to decompose the question even further to separate the queries about Web Voyager and Reflection Agents.

To tune our query generation results, we can add some examples of inputs questions and gold standard output queries to our prompt.
"""

const examples = []

const question = "What's chat langchain, is it a langchain template?"
const query = {
  query: "What is chat langchain and is it a langchain template?",
  subQueries: ["What is chat langchain", "What is a langchain template"],
}
examples.push({ "input": question, "toolCalls": [query] })
# Output:
#   [33m1[39m

const question2 = "How to build multi-agent system and stream intermediate steps from it"
const query2 = {
  query: "How to build multi-agent system and stream intermediate steps from it",
  subQueries: [
    "How to build multi-agent system",
    "How to stream intermediate steps from multi-agent system",
    "How to stream intermediate steps",
  ],
}

examples.push({ "input": question2, "toolCalls": [query2] })
# Output:
#   [33m2[39m

const question3 = "LangChain agents vs LangGraph?"
const query3 = {
  query: "What's the difference between LangChain agents and LangGraph? How do you deploy them?",
  subQueries: [
    "What are LangChain agents",
    "What is LangGraph",
    "How do you deploy LangChain agents",
    "How do you deploy LangGraph",
  ],
}
examples.push({ "input": question3, "toolCalls": [query3] });
# Output:
#   [33m3[39m

"""
Now we need to update our prompt template and chain so that the examples are included in each prompt. Since we're working with LLM model function-calling, we'll need to do a bit of extra structuring to send example inputs and outputs to the model. We'll create a `toolExampleToMessages` helper function to handle this for us:
"""

import {
    AIMessage,
    BaseMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage,
  } from "@langchain/core/messages";
  import { v4 as uuidV4 } from "uuid";
  
  const toolExampleToMessages = (example: Record<string, any>): Array<BaseMessage> => {
    const messages: Array<BaseMessage> = [new HumanMessage({ content: example.input })];
    const openaiToolCalls = example.toolCalls.map((toolCall) => {
      return {
        id: uuidV4(),
        type: "function" as const,
        function: {
          name: "search",
          arguments: JSON.stringify(toolCall),
        },
      };
    });
  
    messages.push(new AIMessage({ content: "", additional_kwargs: { tool_calls: openaiToolCalls } }));
  
    const toolOutputs = "toolOutputs" in example ? example.toolOutputs : Array(openaiToolCalls.length).fill("You have correctly called this tool.");
    toolOutputs.forEach((output, index) => {
      messages.push(new ToolMessage({ content: output, tool_call_id: openaiToolCalls[index].id }));
    });
  
    return messages;
  }
  
  const exampleMessages = examples.map((ex) => toolExampleToMessages(ex)).flat();

import { ChatPromptTemplate, MessagesPlaceholder } from "@langchain/core/prompts";
import { RunnableSequence } from "@langchain/core/runnables";

const queryAnalyzerWithExamples = RunnableSequence.from([
  {
    question: new RunnablePassthrough(),
    examples: () => exampleMessages,
  },
  prompt,
  llmWithTools
]);

await queryAnalyzerWithExamples.invoke(
    "what's the difference between web voyager and reflection agents? do both use langgraph?"
)
# Output:
#   {

#     query: [32m"Difference between Web Voyager and Reflection agents, do they both use LangGraph?"[39m,

#     subQueries: [

#       [32m"Difference between Web Voyager and Reflection agents"[39m,

#       [32m"Do Web Voyager and Reflection agents use LangGraph"[39m

#     ]

#   }

"""
Thanks to our examples we get a slightly more decomposed search query. With some more prompt engineering and tuning of our examples we could improve query generation even more.

You can see that the examples are passed to the model as messages in the [LangSmith trace](https://smith.langchain.com/public/102829c3-69fc-4cb7-b28b-399ae2c9c008/r).

## Next steps

You've now learned some techniques for combining few-shotting with query analysis.

Next, check out some of the other query analysis guides in this section, like [how to deal with high cardinality data](/docs/how_to/query_high_cardinality).
"""



================================================
FILE: docs/core_docs/docs/how_to/query_high_cardinality.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to deal with high cardinality categorical variables

:::info Prerequisites

This guide assumes familiarity with the following:

- [Query analysis](/docs/tutorials/rag#query-analysis)

:::

High cardinality data refers to columns in a dataset that contain a large number of unique values. This guide demonstrates some techniques for dealing with these inputs.

For example, you may want to do query analysis to create a filter on a categorical column. One of the difficulties here is that you usually need to specify the EXACT categorical value. The issue is you need to make sure the LLM generates that categorical value exactly. This can be done relatively easy with prompting when there are only a few values that are valid. When there are a high number of valid values then it becomes more difficult, as those values may not fit in the LLM context, or (if they do) there may be too many for the LLM to properly attend to.

In this notebook we take a look at how to approach this.
"""

"""
## Setup

### Install dependencies

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core zod @faker-js/faker
</Npm2Yarn>
```

### Set environment variables

```
# Optional, use LangSmith for best-in-class observability
LANGSMITH_API_KEY=your-api-key
LANGSMITH_TRACING=true

# Reduce tracing latency if you are not in a serverless environment
# LANGCHAIN_CALLBACKS_BACKGROUND=true
```
"""

"""
#### Set up data

We will generate a bunch of fake names
"""

import { faker } from "@faker-js/faker";

const names = Array.from({ length: 10000 }, () => (faker as any).person.fullName());

"""
Let's look at some of the names
"""

names[0]
# Output:
#   [32m"Rolando Wilkinson"[39m

names[567]
# Output:
#   [32m"Homer Harber"[39m

"""
## Query Analysis

We can now set up a baseline query analysis
"""

import { z } from "zod";

const searchSchema = z.object({
    query: z.string(),
    author: z.string(),
})

"""
```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

// @lc-docs-hide-cell
import { ChatOpenAI } from '@langchain/openai';

const llm = new ChatOpenAI({
  model: "gpt-3.5-turbo",
  temperature: 0,
})

import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";

const system = `Generate a relevant search query for a library system`;
const prompt = ChatPromptTemplate.fromMessages(
    [
      ["system", system],
      ["human", "{question}"],
    ]
)
const llmWithTools = llm.withStructuredOutput(searchSchema, {
  name: "Search"
})
const queryAnalyzer = RunnableSequence.from([
  {
    question: new RunnablePassthrough(),
  },
  prompt,
  llmWithTools
]);

"""
We can see that if we spell the name exactly correctly, it knows how to handle it
"""

await queryAnalyzer.invoke("what are books about aliens by Jesse Knight")
# Output:
#   { query: [32m"aliens"[39m, author: [32m"Jesse Knight"[39m }

"""
The issue is that the values you want to filter on may NOT be spelled exactly correctly
"""

await queryAnalyzer.invoke("what are books about aliens by jess knight")
# Output:
#   { query: [32m"books about aliens"[39m, author: [32m"jess knight"[39m }

"""
### Add in all values

One way around this is to add ALL possible values to the prompt. That will generally guide the query in the right direction
"""

const systemTemplate = `Generate a relevant search query for a library system using the 'search' tool.

The 'author' you return to the user MUST be one of the following authors:

{authors}

Do NOT hallucinate author name!`
const basePrompt = ChatPromptTemplate.fromMessages(
    [
      ["system", systemTemplate],
      ["human", "{question}"],
    ]
)
const promptWithAuthors = await basePrompt.partial({ authors: names.join(", ") })

const queryAnalyzerAll = RunnableSequence.from([
  {
    question: new RunnablePassthrough(),
  },
  promptWithAuthors,
  llmWithTools
])

"""
However... if the list of categoricals is long enough, it may error!
"""

try {
    const res = await queryAnalyzerAll.invoke("what are books about aliens by jess knight")
} catch (e) {
    console.error(e)
}
# Output:
#   Error: 400 This model's maximum context length is 16385 tokens. However, your messages resulted in 50197 tokens (50167 in the messages, 30 in the functions). Please reduce the length of the messages or functions.

#       at Function.generate (file:///Users/jacoblee/Library/Caches/deno/npm/registry.npmjs.org/openai/4.47.1/error.mjs:41:20)

#       at OpenAI.makeStatusError (file:///Users/jacoblee/Library/Caches/deno/npm/registry.npmjs.org/openai/4.47.1/core.mjs:256:25)

#       at OpenAI.makeRequest (file:///Users/jacoblee/Library/Caches/deno/npm/registry.npmjs.org/openai/4.47.1/core.mjs:299:30)

#       at eventLoopTick (ext:core/01_core.js:63:7)

#       at async file:///Users/jacoblee/Library/Caches/deno/npm/registry.npmjs.org/@langchain/openai/0.0.31/dist/chat_models.js:756:29

#       at async RetryOperation._fn (file:///Users/jacoblee/Library/Caches/deno/npm/registry.npmjs.org/p-retry/4.6.2/index.js:50:12) {

#     status: 400,

#     headers: {

#       "alt-svc": 'h3=":443"; ma=86400',

#       "cf-cache-status": "DYNAMIC",

#       "cf-ray": "885f794b3df4fa52-SJC",

#       "content-length": "340",

#       "content-type": "application/json",

#       date: "Sat, 18 May 2024 23:02:16 GMT",

#       "openai-organization": "langchain",

#       "openai-processing-ms": "230",

#       "openai-version": "2020-10-01",

#       server: "cloudflare",

#       "set-cookie": "_cfuvid=F_c9lnRuQDUhKiUE2eR2PlsxHPldf1OAVMonLlHTjzM-1716073336256-0.0.1.1-604800000; path=/; domain="... 48 more characters,

#       "strict-transport-security": "max-age=15724800; includeSubDomains",

#       "x-ratelimit-limit-requests": "10000",

#       "x-ratelimit-limit-tokens": "2000000",

#       "x-ratelimit-remaining-requests": "9999",

#       "x-ratelimit-remaining-tokens": "1958402",

#       "x-ratelimit-reset-requests": "6ms",

#       "x-ratelimit-reset-tokens": "1.247s",

#       "x-request-id": "req_7b88677d6883fac1520e44543f68c839"

#     },

#     request_id: "req_7b88677d6883fac1520e44543f68c839",

#     error: {

#       message: "This model's maximum context length is 16385 tokens. However, your messages resulted in 50197 tokens"... 101 more characters,

#       type: "invalid_request_error",

#       param: "messages",

#       code: "context_length_exceeded"

#     },

#     code: "context_length_exceeded",

#     param: "messages",

#     type: "invalid_request_error",

#     attemptNumber: 1,

#     retriesLeft: 6

#   }


"""
We can try to use a longer context window... but with so much information in there, it is not guaranteed to pick it up reliably
"""

"""
```{=mdx}
<ChatModelTabs customVarName="llmLong" openaiParams={`{ model: "gpt-4o-mini" }`} />
```
"""

// @lc-docs-hide-cell
import { ChatOpenAI } from '@langchain/openai';

const llmLong = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0,
})

const structuredLlmLong = llmLong.withStructuredOutput(searchSchema, {
  name: "Search"
});
const queryAnalyzerAllLong = RunnableSequence.from([
  {
    question: new RunnablePassthrough(),
  },
  prompt,
  structuredLlmLong
]);

await queryAnalyzerAllLong.invoke("what are books about aliens by jess knight")
# Output:
#   { query: [32m"aliens"[39m, author: [32m"jess knight"[39m }

"""
### Find and all relevant values

Instead, what we can do is create a [vector store index](/docs/concepts/vectorstores) over the relevant values and then query that for the N most relevant values,
"""

import { OpenAIEmbeddings } from "@langchain/openai";
import { MemoryVectorStore } from "langchain/vectorstores/memory";

const embeddings = new OpenAIEmbeddings({
  model: "text-embedding-3-small",
})
const vectorstore = await MemoryVectorStore.fromTexts(names, {}, embeddings);

const selectNames = async (question: string) => {
  const _docs = await vectorstore.similaritySearch(question, 10);
  const _names = _docs.map(d => d.pageContent);
  return _names.join(", ");
}

const createPrompt = RunnableSequence.from([
  {
      question: new RunnablePassthrough(),
      authors: selectNames,
  },
  basePrompt
])

await createPrompt.invoke("what are books by jess knight")
# Output:
#   ChatPromptValue {

#     lc_serializable: [33mtrue[39m,

#     lc_kwargs: {

#       messages: [

#         SystemMessage {

#           lc_serializable: [33mtrue[39m,

#           lc_kwargs: {

#             content: [32m"Generate a relevant search query for a library system using the 'search' tool.\n"[39m +

#               [32m"\n"[39m +

#               [32m"The 'author' you ret"[39m... 243 more characters,

#             additional_kwargs: {},

#             response_metadata: {}

#           },

#           lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#           content: [32m"Generate a relevant search query for a library system using the 'search' tool.\n"[39m +

#             [32m"\n"[39m +

#             [32m"The 'author' you ret"[39m... 243 more characters,

#           name: [90mundefined[39m,

#           additional_kwargs: {},

#           response_metadata: {}

#         },

#         HumanMessage {

#           lc_serializable: [33mtrue[39m,

#           lc_kwargs: {

#             content: [32m"what are books by jess knight"[39m,

#             additional_kwargs: {},

#             response_metadata: {}

#           },

#           lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#           content: [32m"what are books by jess knight"[39m,

#           name: [90mundefined[39m,

#           additional_kwargs: {},

#           response_metadata: {}

#         }

#       ]

#     },

#     lc_namespace: [ [32m"langchain_core"[39m, [32m"prompt_values"[39m ],

#     messages: [

#       SystemMessage {

#         lc_serializable: [33mtrue[39m,

#         lc_kwargs: {

#           content: [32m"Generate a relevant search query for a library system using the 'search' tool.\n"[39m +

#             [32m"\n"[39m +

#             [32m"The 'author' you ret"[39m... 243 more characters,

#           additional_kwargs: {},

#           response_metadata: {}

#         },

#         lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#         content: [32m"Generate a relevant search query for a library system using the 'search' tool.\n"[39m +

#           [32m"\n"[39m +

#           [32m"The 'author' you ret"[39m... 243 more characters,

#         name: [90mundefined[39m,

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       HumanMessage {

#         lc_serializable: [33mtrue[39m,

#         lc_kwargs: {

#           content: [32m"what are books by jess knight"[39m,

#           additional_kwargs: {},

#           response_metadata: {}

#         },

#         lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#         content: [32m"what are books by jess knight"[39m,

#         name: [90mundefined[39m,

#         additional_kwargs: {},

#         response_metadata: {}

#       }

#     ]

#   }

const queryAnalyzerSelect = createPrompt.pipe(llmWithTools);

await queryAnalyzerSelect.invoke("what are books about aliens by jess knight")
# Output:
#   { query: [32m"aliens"[39m, author: [32m"Jess Knight"[39m }

"""
## Next steps

You've now learned how to deal with high cardinality data when constructing queries.

Next, check out some of the other query analysis guides in this section, like [how to use few-shotting to improve performance](/docs/how_to/query_no_queries).
"""



================================================
FILE: docs/core_docs/docs/how_to/query_multiple_queries.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to handle multiple queries

:::info Prerequisites

This guide assumes familiarity with the following:

- [Query analysis](/docs/tutorials/rag#query-analysis)

:::

Sometimes, a query analysis technique may allow for multiple queries to be generated. In these cases, we need to remember to run all queries and then to combine the results. We will show a simple example (using mock data) of how to do that.
"""

"""
## Setup

### Install dependencies

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/openai @langchain/core zod chromadb
</Npm2Yarn>
```

### Set environment variables

```
OPENAI_API_KEY=your-api-key

# Optional, use LangSmith for best-in-class observability
LANGSMITH_API_KEY=your-api-key
LANGSMITH_TRACING=true

# Reduce tracing latency if you are not in a serverless environment
# LANGCHAIN_CALLBACKS_BACKGROUND=true
```
"""

"""
### Create Index

We will create a vectorstore over fake information.
"""

import { Chroma } from "@langchain/community/vectorstores/chroma"
import { OpenAIEmbeddings } from "@langchain/openai"
import "chromadb";

const texts = ["Harrison worked at Kensho", "Ankush worked at Facebook"]
const embeddings = new OpenAIEmbeddings({ model: "text-embedding-3-small" })
const vectorstore = await Chroma.fromTexts(
    texts,
    {},
    embeddings,
    {
        collectionName: "multi_query"
    }
)
const retriever = vectorstore.asRetriever(1);

"""
## Query analysis

We will use function calling to structure the output. We will let it return multiple queries.
"""

import { z } from "zod";

const searchSchema = z.object({
    queries: z.array(z.string()).describe("Distinct queries to search for")
}).describe("Search over a database of job records.");

"""
```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

// @lc-docs-hide-cell
import { ChatOpenAI } from '@langchain/openai';

const llm = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0,
})

import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnableSequence, RunnablePassthrough } from "@langchain/core/runnables";

const system = `You have the ability to issue search queries to get information to help answer user information.

If you need to look up two distinct pieces of information, you are allowed to do that!`;

const prompt = ChatPromptTemplate.fromMessages([
    ["system", system],
    ["human", "{question}"],
])
const llmWithTools = llm.withStructuredOutput(searchSchema, {
  name: "Search"
});
const queryAnalyzer = RunnableSequence.from([
  {
      question: new RunnablePassthrough(),
  },
  prompt,
  llmWithTools
]);

"""
We can see that this allows for creating multiple queries
"""

await queryAnalyzer.invoke("where did Harrison Work")
# Output:
#   { queries: [ [32m"Harrison"[39m ] }

await queryAnalyzer.invoke("where did Harrison and ankush Work")
# Output:
#   { queries: [ [32m"Harrison work"[39m, [32m"Ankush work"[39m ] }

"""
## Retrieval with query analysis

So how would we include this in a chain? One thing that will make this a lot easier is if we call our retriever asyncronously - this will let us loop over the queries and not get blocked on the response time.
"""

import { RunnableConfig, RunnableLambda } from "@langchain/core/runnables";

const chain = async (question: string, config?: RunnableConfig) => {
    const response = await queryAnalyzer.invoke(question, config);
    const docs = [];
    for (const query of response.queries) {
        const newDocs = await retriever.invoke(query, config);
        docs.push(...newDocs);
    }
    // You probably want to think about reranking or deduplicating documents here
    // But that is a separate topic
    return docs;
}

const customChain = new RunnableLambda({ func: chain });

await customChain.invoke("where did Harrison Work")
# Output:
#   [ Document { pageContent: [32m"Harrison worked at Kensho"[39m, metadata: {} } ]

await customChain.invoke("where did Harrison and ankush Work")
# Output:
#   [

#     Document { pageContent: [32m"Harrison worked at Kensho"[39m, metadata: {} },

#     Document { pageContent: [32m"Ankush worked at Facebook"[39m, metadata: {} }

#   ]

"""
## Next steps

You've now learned some techniques for handling multiple queries in a query analysis system.

Next, check out some of the other query analysis guides in this section, like [how to deal with cases where no query is generated](/docs/how_to/query_no_queries).
"""



================================================
FILE: docs/core_docs/docs/how_to/query_multiple_retrievers.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to handle multiple retrievers

:::info Prerequisites

This guide assumes familiarity with the following:

- [Query analysis](/docs/tutorials/rag#query-analysis)

:::

Sometimes, a query analysis technique may allow for selection of which retriever to use. To use this, you will need to add some logic to select the retriever to do. We will show a simple example (using mock data) of how to do that.
"""

"""
## Setup

### Install dependencies

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/openai @langchain/core zod chromadb
</Npm2Yarn>
```

### Set environment variables

```
OPENAI_API_KEY=your-api-key

# Optional, use LangSmith for best-in-class observability
LANGSMITH_API_KEY=your-api-key
LANGSMITH_TRACING=true

# Reduce tracing latency if you are not in a serverless environment
# LANGCHAIN_CALLBACKS_BACKGROUND=true
```
"""

"""
### Create Index

We will create a vectorstore over fake information.
"""

import { Chroma } from "@langchain/community/vectorstores/chroma"
import { OpenAIEmbeddings } from "@langchain/openai"
import "chromadb";

const texts = ["Harrison worked at Kensho"]
const embeddings = new OpenAIEmbeddings({ model: "text-embedding-3-small" })
const vectorstore = await Chroma.fromTexts(texts, {}, embeddings, {
  collectionName: "harrison"
})
const retrieverHarrison = vectorstore.asRetriever(1)

const textsAnkush = ["Ankush worked at Facebook"]
const embeddingsAnkush = new OpenAIEmbeddings({ model: "text-embedding-3-small" })
const vectorstoreAnkush = await Chroma.fromTexts(textsAnkush, {}, embeddingsAnkush, {
  collectionName: "ankush"
})
const retrieverAnkush = vectorstoreAnkush.asRetriever(1)

"""
## Query analysis

We will use function calling to structure the output. We will let it return multiple queries.
"""

import { z } from "zod";

const searchSchema = z.object({
    query: z.string().describe("Query to look up"),
    person: z.string().describe("Person to look things up for. Should be `HARRISON` or `ANKUSH`.")
})

"""
```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

// @lc-docs-hide-cell
import { ChatOpenAI } from '@langchain/openai';

const llm = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0,
})

import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnableSequence, RunnablePassthrough } from "@langchain/core/runnables";

const system = `You have the ability to issue search queries to get information to help answer user information.`
const prompt = ChatPromptTemplate.fromMessages(
[
    ["system", system],
    ["human", "{question}"],
]
)
const llmWithTools = llm.withStructuredOutput(searchSchema, {
name: "Search"
})
const queryAnalyzer = RunnableSequence.from([
    {
        question: new RunnablePassthrough(),
    },
    prompt,
    llmWithTools
])

"""
We can see that this allows for routing between retrievers
"""

await queryAnalyzer.invoke("where did Harrison Work")
# Output:
#   { query: [32m"workplace of Harrison"[39m, person: [32m"HARRISON"[39m }

await queryAnalyzer.invoke("where did ankush Work")
# Output:
#   { query: [32m"Workplace of Ankush"[39m, person: [32m"ANKUSH"[39m }

"""
## Retrieval with query analysis

So how would we include this in a chain? We just need some simple logic to select the retriever and pass in the search query
"""

const retrievers = {
    HARRISON: retrieverHarrison,
    ANKUSH: retrieverAnkush,
}

import { RunnableConfig, RunnableLambda } from "@langchain/core/runnables";

const chain = async (question: string, config?: RunnableConfig) => {
    const response = await queryAnalyzer.invoke(question, config);
    const retriever = retrievers[response.person];
    return retriever.invoke(response.query, config);
}

const customChain = new RunnableLambda({ func: chain });

await customChain.invoke("where did Harrison Work")
# Output:
#   [ Document { pageContent: [32m"Harrison worked at Kensho"[39m, metadata: {} } ]

await customChain.invoke("where did ankush Work")
# Output:
#   [ Document { pageContent: [32m"Ankush worked at Facebook"[39m, metadata: {} } ]

"""
## Next steps

You've now learned some techniques for handling multiple retrievers in a query analysis system.

Next, check out some of the other query analysis guides in this section, like [how to deal with cases where no query is generated](/docs/how_to/query_no_queries).
"""



================================================
FILE: docs/core_docs/docs/how_to/query_no_queries.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to handle cases where no queries are generated

:::info Prerequisites

This guide assumes familiarity with the following:

- [Query analysis](/docs/tutorials/rag#query-analysis)

:::

Sometimes, a query analysis technique may allow for any number of queries to be generated - including no queries! In this case, our overall chain will need to inspect the result of the query analysis before deciding whether to call the retriever or not.

We will use mock data for this example.
"""

"""
## Setup

### Install dependencies

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/openai @langchain/core zod chromadb
</Npm2Yarn>
```

### Set environment variables

```
OPENAI_API_KEY=your-api-key

# Optional, use LangSmith for best-in-class observability
LANGSMITH_API_KEY=your-api-key
LANGSMITH_TRACING=true

# Reduce tracing latency if you are not in a serverless environment
# LANGCHAIN_CALLBACKS_BACKGROUND=true
```
"""

"""
### Create Index

We will create a vectorstore over fake information.
"""

import { Chroma } from "@langchain/community/vectorstores/chroma"
import { OpenAIEmbeddings } from "@langchain/openai"
import "chromadb";

const texts = ["Harrison worked at Kensho"]
const embeddings = new OpenAIEmbeddings({ model: "text-embedding-3-small" })
const vectorstore = await Chroma.fromTexts(texts, {}, embeddings, {
  collectionName: "harrison"
})
const retriever = vectorstore.asRetriever(1);

"""
## Query analysis

We will use function calling to structure the output. However, we will configure the LLM such that is doesn't NEED to call the function representing a search query (should it decide not to). We will also then use a prompt to do query analysis that explicitly lays when it should and shouldn't make a search.
"""

import { z } from "zod";

const searchSchema = z.object({
    query: z.string().describe("Similarity search query applied to job record."),
});

"""
```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

// @lc-docs-hide-cell
import { ChatOpenAI } from '@langchain/openai';

const llm = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0,
})

import { zodToJsonSchema } from "zod-to-json-schema";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnableSequence, RunnablePassthrough } from "@langchain/core/runnables";

const system = `You have the ability to issue search queries to get information to help answer user information.

You do not NEED to look things up. If you don't need to, then just respond normally.`;
const prompt = ChatPromptTemplate.fromMessages(
  [
    ["system", system],
    ["human", "{question}"],
  ]
)
const llmWithTools = llm.bind({
  tools: [{
    type: "function" as const,
    function: {
      name: "search",
      description: "Search over a database of job records.",
      parameters: zodToJsonSchema(searchSchema),
    }
  }]
})
const queryAnalyzer = RunnableSequence.from([
  {
    question: new RunnablePassthrough(),
  },
  prompt,
  llmWithTools
])

"""
We can see that by invoking this we get an message that sometimes - but not always - returns a tool call.
"""

await queryAnalyzer.invoke("where did Harrison work")
# Output:
#   AIMessage {

#     lc_serializable: [33mtrue[39m,

#     lc_kwargs: {

#       content: [32m""[39m,

#       additional_kwargs: {

#         function_call: [90mundefined[39m,

#         tool_calls: [

#           {

#             id: [32m"call_uqHm5OMbXBkmqDr7Xzj8EMmd"[39m,

#             type: [32m"function"[39m,

#             function: [36m[Object][39m

#           }

#         ]

#       }

#     },

#     lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#     content: [32m""[39m,

#     name: [90mundefined[39m,

#     additional_kwargs: {

#       function_call: [90mundefined[39m,

#       tool_calls: [

#         {

#           id: [32m"call_uqHm5OMbXBkmqDr7Xzj8EMmd"[39m,

#           type: [32m"function"[39m,

#           function: { name: [32m"search"[39m, arguments: [32m'{"query":"Harrison"}'[39m }

#         }

#       ]

#     }

#   }

await queryAnalyzer.invoke("hi!")
# Output:
#   AIMessage {

#     lc_serializable: [33mtrue[39m,

#     lc_kwargs: {

#       content: [32m"Hello! How can I assist you today?"[39m,

#       additional_kwargs: { function_call: [90mundefined[39m, tool_calls: [90mundefined[39m }

#     },

#     lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#     content: [32m"Hello! How can I assist you today?"[39m,

#     name: [90mundefined[39m,

#     additional_kwargs: { function_call: [90mundefined[39m, tool_calls: [90mundefined[39m }

#   }

"""
## Retrieval with query analysis

So how would we include this in a chain? Let's look at an example below.
"""

import { JsonOutputKeyToolsParser } from "@langchain/core/output_parsers/openai_tools";

const outputParser = new JsonOutputKeyToolsParser({
  keyName: "search",
})

import { RunnableConfig, RunnableLambda } from "@langchain/core/runnables";

const chain = async (question: string, config?: RunnableConfig) => {
  const response = await queryAnalyzer.invoke(question, config);
  if ("tool_calls" in response.additional_kwargs && response.additional_kwargs.tool_calls !== undefined) {
    const query = await outputParser.invoke(response, config);
    return retriever.invoke(query[0].query, config);
  } else {
    return response;
  }
}

const customChain = new RunnableLambda({ func: chain });

await customChain.invoke("where did Harrison Work")
# Output:
#   [ Document { pageContent: [32m"Harrison worked at Kensho"[39m, metadata: {} } ]

await customChain.invoke("hi!")
# Output:
#   AIMessage {

#     lc_serializable: [33mtrue[39m,

#     lc_kwargs: {

#       content: [32m"Hello! How can I assist you today?"[39m,

#       additional_kwargs: { function_call: [90mundefined[39m, tool_calls: [90mundefined[39m }

#     },

#     lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#     content: [32m"Hello! How can I assist you today?"[39m,

#     name: [90mundefined[39m,

#     additional_kwargs: { function_call: [90mundefined[39m, tool_calls: [90mundefined[39m }

#   }

"""
## Next steps

You've now learned some techniques for handling irrelevant questions in query analysis systems.

Next, check out some of the other query analysis guides in this section, like [how to use few-shot examples](/docs/how_to/query_few_shot).
"""



================================================
FILE: docs/core_docs/docs/how_to/recursive_text_splitter.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
keywords: [recursivecharactertextsplitter]
---
"""

"""
# How to recursively split text by characters

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Text splitters](/docs/concepts/text_splitters)

:::

This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is `["\n\n", "\n", " ", ""]`. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.

1. How the text is split: by list of characters.
2. How the chunk size is measured: by number of characters.

Below we show example usage.

To obtain the string content directly, use `.splitText`.

To create LangChain [Document](https://api.js.langchain.com/classes/langchain_core.documents.Document.html) objects (e.g., for use in downstream tasks), use `.createDocuments`.
"""

import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";

const text = `Hi.\n\nI'm Harrison.\n\nHow? Are? You?\nOkay then f f f f.
This is a weird text to write, but gotta test the splittingggg some how.\n\n
Bye!\n\n-H.`;
const splitter = new RecursiveCharacterTextSplitter({
  chunkSize: 10,
  chunkOverlap: 1,
});

const output = await splitter.createDocuments([text]);

console.log(output.slice(0, 3));
# Output:
#   [

#     Document {

#       pageContent: "Hi.",

#       metadata: { loc: { lines: { from: 1, to: 1 } } }

#     },

#     Document {

#       pageContent: "I'm",

#       metadata: { loc: { lines: { from: 3, to: 3 } } }

#     },

#     Document {

#       pageContent: "Harrison.",

#       metadata: { loc: { lines: { from: 3, to: 3 } } }

#     }

#   ]


"""
You'll note that in the above example we are splitting a raw text string and getting back a list of documents. We can also split documents directly.
"""

import { Document } from "@langchain/core/documents";
import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";

const text = `Hi.\n\nI'm Harrison.\n\nHow? Are? You?\nOkay then f f f f.
This is a weird text to write, but gotta test the splittingggg some how.\n\n
Bye!\n\n-H.`;
const splitter = new RecursiveCharacterTextSplitter({
  chunkSize: 10,
  chunkOverlap: 1,
});

const docOutput = await splitter.splitDocuments([
  new Document({ pageContent: text }),
]);

console.log(docOutput.slice(0, 3));
# Output:
#   [

#     Document {

#       pageContent: "Hi.",

#       metadata: { loc: { lines: { from: 1, to: 1 } } }

#     },

#     Document {

#       pageContent: "I'm",

#       metadata: { loc: { lines: { from: 3, to: 3 } } }

#     },

#     Document {

#       pageContent: "Harrison.",

#       metadata: { loc: { lines: { from: 3, to: 3 } } }

#     }

#   ]


"""
You can customize the `RecursiveCharacterTextSplitter` with arbitrary separators by passing a `separators` parameter like this:
"""

import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { Document } from "@langchain/core/documents";

const text = `Some other considerations include:

- Do you deploy your backend and frontend together, or separately?
- Do you deploy your backend co-located with your database, or separately?

**Production Support:** As you move your LangChains into production, we'd love to offer more hands-on support.
Fill out [this form](https://airtable.com/appwQzlErAS2qiP0L/shrGtGaVBVAz7NcV2) to share more about what you're building, and our team will get in touch.

## Deployment Options

See below for a list of deployment options for your LangChain app. If you don't see your preferred option, please get in touch and we can add it to this list.`;

const splitter = new RecursiveCharacterTextSplitter({
  chunkSize: 50,
  chunkOverlap: 1,
  separators: ["|", "##", ">", "-"],
});

const docOutput = await splitter.splitDocuments([
  new Document({ pageContent: text }),
]);

console.log(docOutput.slice(0, 3));
# Output:
#   [

#     Document {

#       pageContent: "Some other considerations include:",

#       metadata: { loc: { lines: { from: 1, to: 1 } } }

#     },

#     Document {

#       pageContent: "- Do you deploy your backend and frontend together",

#       metadata: { loc: { lines: { from: 3, to: 3 } } }

#     },

#     Document {

#       pageContent: "r, or separately?",

#       metadata: { loc: { lines: { from: 3, to: 3 } } }

#     }

#   ]


"""
## Next steps

You've now learned a method for splitting text by character.

Next, check out [specific techinques for splitting on code](/docs/how_to/code_splitter) or the [full tutorial on retrieval-augmented generation](/docs/tutorials/rag).
"""



================================================
FILE: docs/core_docs/docs/how_to/reduce_retrieval_latency.mdx
================================================
# How to reduce retrieval latency

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Retrievers](/docs/concepts/retrievers)
- [Embeddings](/docs/concepts/embedding_models)
- [Vector stores](/docs/concepts/#vectorstores)
- [Retrieval-augmented generation (RAG)](/docs/tutorials/rag)

:::

One way to reduce retrieval latency is through a technique called "Adaptive Retrieval".
The [`MatryoshkaRetriever`](https://api.js.langchain.com/classes/langchain.retrievers_matryoshka_retriever.MatryoshkaRetriever.html) uses the
Matryoshka Representation Learning (MRL) technique to retrieve documents for a given query in two steps:

- **First-pass**: Uses a lower dimensional sub-vector from the MRL embedding for an initial, fast,
  but less accurate search.

- **Second-pass**: Re-ranks the top results from the first pass using the full, high-dimensional
  embedding for higher accuracy.

![Matryoshka Retriever](/img/adaptive_retrieval.png)

It is based on this [Supabase](https://supabase.com/) blog post
["Matryoshka embeddings: faster OpenAI vector search using Adaptive Retrieval"](https://supabase.com/blog/matryoshka-embeddings).

### Setup

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

To follow the example below, you need an OpenAI API key:

```bash
export OPENAI_API_KEY=your-api-key
```

We'll also be using `chroma` for our vector store. Follow the instructions [here](/docs/integrations/vectorstores/chroma) to setup.

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/retrievers/matryoshka_retriever.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

:::note
Due to the constraints of some vector stores, the large embedding metadata field is stringified (`JSON.stringify`) before being stored. This means that the metadata field will need to be parsed (`JSON.parse`) when retrieved from the vector store.
:::

## Next steps

You've now learned a technique that can help speed up your retrieval queries.

Next, check out the [broader tutorial on RAG](/docs/tutorials/rag), or this section to learn how to
[create your own custom retriever over any data source](/docs/how_to/custom_retriever/).



================================================
FILE: docs/core_docs/docs/how_to/routing.mdx
================================================
# How to route execution within a chain

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [LangChain Expression Language (LCEL)](/docs/concepts/lcel)
- [Chaining runnables](/docs/how_to/sequence/)
- [Configuring chain parameters at runtime](/docs/how_to/binding)
- [Prompt templates](/docs/concepts/prompt_templates)
- [Chat Messages](/docs/concepts/messages)

:::

This guide covers how to do routing in the LangChain Expression Language.

Routing allows you to create non-deterministic chains where the output of a previous step defines the next step. Routing helps provide structure and consistency around interactions with LLMs.

There are two ways to perform routing:

1. Conditionally return runnables from a [`RunnableLambda`](/docs/how_to/functions) (recommended)
2. Using a `RunnableBranch` (legacy)

We'll illustrate both methods using a two step sequence where the first step classifies an input question as being about LangChain, Anthropic, or Other, then routes to a corresponding prompt chain.

## Using a custom function

You can use a custom function to route between different outputs. Here's an example:

import CodeBlock from "@theme/CodeBlock";

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/anthropic @langchain/core
```

import FactoryFunctionExample from "@examples/guides/expression_language/how_to_routing_custom_function.ts";

<CodeBlock language="typescript">{FactoryFunctionExample}</CodeBlock>

## Routing by semantic similarity

One especially useful technique is to use embeddings to route a query to the most relevant prompt. Here's an example:

import SemanticSimilarityExample from "@examples/guides/expression_language/how_to_routing_semantic_similarity.ts";

<CodeBlock language="typescript">{SemanticSimilarityExample}</CodeBlock>

## Using a RunnableBranch

A `RunnableBranch` is initialized with a list of (condition, runnable) pairs and a default runnable. It selects which branch by passing each condition the input it's invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input.

If no provided conditions match, it runs the default runnable.

Here's an example of what it looks like in action:

import BranchExample from "@examples/guides/expression_language/how_to_routing_runnable_branch.ts";

<CodeBlock language="typescript">{BranchExample}</CodeBlock>

## Next steps

You've now learned how to add routing to your composed LCEL chains.

Next, check out the other [how-to guides on runnables](/docs/how_to/#langchain-expression-language) in this section.



================================================
FILE: docs/core_docs/docs/how_to/self_query.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to do "self-querying" retrieval

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Retrievers](/docs/concepts/retrievers)
- [Vector stores](/docs/concepts/vectorstores)

:::

A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses an LLM to write a structured query and then applies that structured query to its underlying vector store. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.

![](../../static/img/self_querying.jpeg)

:::info

Head to [Integrations](/docs/integrations/retrievers/self_query) for documentation on vector stores with built-in support for self-querying.

:::

## Get started

For demonstration purposes, we'll use an in-memory, unoptimized vector store. You should swap it out for a supported production-ready vector store when seriously building.

The self-query retriever requires you to have the [`peggy`](https://www.npmjs.com/package/peggy) package installed as a peer dep, and we'll also use OpenAI for this example:

```{=mdx}
import Npm2Yarn from '@theme/Npm2Yarn';

<Npm2Yarn>
  peggy @langchain/openai @langchain/core
</Npm2Yarn>
```

We've created a small demo set of documents that contain summaries of movies:
"""

import "peggy";
import { Document } from "@langchain/core/documents";

/**
 * First, we create a bunch of documents. You can load your own documents here instead.
 * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.
 */
const docs = [
  new Document({
    pageContent:
      "A bunch of scientists bring back dinosaurs and mayhem breaks loose",
    metadata: { year: 1993, rating: 7.7, genre: "science fiction", length: 122 },
  }),
  new Document({
    pageContent:
      "Leo DiCaprio gets lost in a dream within a dream within a dream within a ...",
    metadata: { year: 2010, director: "Christopher Nolan", rating: 8.2, length: 148 },
  }),
  new Document({
    pageContent:
      "A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea",
    metadata: { year: 2006, director: "Satoshi Kon", rating: 8.6 },
  }),
  new Document({
    pageContent:
      "A bunch of normal-sized women are supremely wholesome and some men pine after them",
    metadata: { year: 2019, director: "Greta Gerwig", rating: 8.3, length: 135 },
  }),
  new Document({
    pageContent: "Toys come alive and have a blast doing so",
    metadata: { year: 1995, genre: "animated", length: 77 },
  }),
  new Document({
    pageContent: "Three men walk into the Zone, three men walk out of the Zone",
    metadata: {
      year: 1979,
      director: "Andrei Tarkovsky",
      genre: "science fiction",
      rating: 9.9,
    },
  }),
];

"""
### Creating our self-querying retriever

Now we can instantiate our retriever. To do this we'll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.
"""

import { OpenAIEmbeddings, OpenAI } from "@langchain/openai";
import { FunctionalTranslator } from "@langchain/core/structured_query";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { SelfQueryRetriever } from "langchain/retrievers/self_query";
import type { AttributeInfo } from "langchain/chains/query_constructor";

/**
 * We define the attributes we want to be able to query on.
 * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.
 * We also provide a description of each attribute and the type of the attribute.
 * This is used to generate the query prompts.
 */
const attributeInfo: AttributeInfo[] = [
  {
    name: "genre",
    description: "The genre of the movie",
    type: "string or array of strings",
  },
  {
    name: "year",
    description: "The year the movie was released",
    type: "number",
  },
  {
    name: "director",
    description: "The director of the movie",
    type: "string",
  },
  {
    name: "rating",
    description: "The rating of the movie (1-10)",
    type: "number",
  },
  {
    name: "length",
    description: "The length of the movie in minutes",
    type: "number",
  },
];



/**
 * Next, we instantiate a vector store. This is where we store the embeddings of the documents.
 * We also need to provide an embeddings object. This is used to embed the documents.
 */
const embeddings = new OpenAIEmbeddings();
const llm = new OpenAI();
const documentContents = "Brief summary of a movie";
const vectorStore = await MemoryVectorStore.fromDocuments(docs, embeddings);
const selfQueryRetriever = SelfQueryRetriever.fromLLM({
  llm,
  vectorStore,
  documentContents,
  attributeInfo,
  /**
   * We need to use a translator that translates the queries into a
   * filter format that the vector store can understand. We provide a basic translator
   * translator here, but you can create your own translator by extending BaseTranslator
   * abstract class. Note that the vector store needs to support filtering on the metadata
   * attributes you want to query on.
   */
  structuredQueryTranslator: new FunctionalTranslator(),
});

"""
### Testing it out

And now we can actually try using our retriever!

We can ask questions like "Which movies are less than 90 minutes?" or "Which movies are rated higher than 8.5?".
We can also ask questions like "Which movies are either comedy or drama and are less than 90 minutes?".
The translator within the retriever will automatically convert these questions into vector store filters that can be used to retrieve documents.
"""

await selfQueryRetriever.invoke(
  "Which movies are less than 90 minutes?"
);
# Output:
#   [

#     Document {

#       pageContent: [32m"Toys come alive and have a blast doing so"[39m,

#       metadata: { year: [33m1995[39m, genre: [32m"animated"[39m, length: [33m77[39m }

#     }

#   ]

await selfQueryRetriever.invoke(
  "Which movies are rated higher than 8.5?"
);
# Output:
#   [

#     Document {

#       pageContent: [32m"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception"[39m... 16 more characters,

#       metadata: { year: [33m2006[39m, director: [32m"Satoshi Kon"[39m, rating: [33m8.6[39m }

#     },

#     Document {

#       pageContent: [32m"Three men walk into the Zone, three men walk out of the Zone"[39m,

#       metadata: {

#         year: [33m1979[39m,

#         director: [32m"Andrei Tarkovsky"[39m,

#         genre: [32m"science fiction"[39m,

#         rating: [33m9.9[39m

#       }

#     }

#   ]

await selfQueryRetriever.invoke(
  "Which movies are directed by Greta Gerwig?"
);
# Output:
#   [

#     Document {

#       pageContent: [32m"A bunch of normal-sized women are supremely wholesome and some men pine after them"[39m,

#       metadata: { year: [33m2019[39m, director: [32m"Greta Gerwig"[39m, rating: [33m8.3[39m, length: [33m135[39m }

#     }

#   ]

await selfQueryRetriever.invoke(
  "Which movies are either comedy or drama and are less than 90 minutes?"
);
# Output:
#   [

#     Document {

#       pageContent: [32m"Toys come alive and have a blast doing so"[39m,

#       metadata: { year: [33m1995[39m, genre: [32m"animated"[39m, length: [33m77[39m }

#     }

#   ]

"""
## Next steps

You've now seen how to use the `SelfQueryRetriever` to to generate vector store filters based on an original question.

Next, you can check out the list of [vector stores that currently support self-querying](/docs/integrations/retrievers/self_query/).
"""



================================================
FILE: docs/core_docs/docs/how_to/sequence.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
keywords: [chain, chaining, runnablesequence]
---
"""

"""
# How to chain runnables

One point about [LangChain Expression Language](/docs/concepts/lcel) is that any two runnables can be "chained" together into sequences. The output of the previous runnable's `.invoke()` call is passed as input to the next runnable. This can be done using the `.pipe()` method.

The resulting [`RunnableSequence`](https://api.js.langchain.com/classes/langchain_core.runnables.RunnableSequence.html) is itself a runnable, which means it can be invoked, streamed, or further chained just like any other runnable. Advantages of chaining runnables in this way are efficient streaming (the sequence will stream output as soon as it is available), and debugging and tracing with tools like [LangSmith](/docs/how_to/debugging).

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [LangChain Expression Language (LCEL)](/docs/concepts/lcel)
- [Prompt templates](/docs/concepts/prompt_templates)
- [Chat models](/docs/concepts/chat_models)
- [Output parser](/docs/concepts/output_parsers)

:::

## The pipe method

To show off how this works, let's go through an example. We'll walk through a common pattern in LangChain: using a [prompt template](/docs/concepts/prompt_templates) to format input into a [chat model](/docs/concepts/chat_models), and finally converting the chat message output into a string with an [output parser](/docs/concepts/output_parsers).

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs
  customVarName="model"
/>
```
"""

"""
```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  langchain @langchain/core
</Npm2Yarn>
```
"""

// @lc-docs-hide-cell
import { ChatOpenAI } from '@langchain/openai';

const model = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0,
})

import { StringOutputParser } from "@langchain/core/output_parsers";
import { ChatPromptTemplate } from "@langchain/core/prompts";

const prompt = ChatPromptTemplate.fromTemplate("tell me a joke about {topic}")

const chain = prompt.pipe(model).pipe(new StringOutputParser())

"""
Prompts and models are both runnable, and the output type from the prompt call is the same as the input type of the chat model, so we can chain them together. We can then invoke the resulting sequence like any other runnable:
"""

await chain.invoke({ topic: "bears" })
# Output:
#   "Here's a bear joke for you:\n\nWhy did the bear dissolve in water?\nBecause it was a polar bear!"

"""
### Coercion

We can even combine this chain with more runnables to create another chain. This may involve some input/output formatting using other types of runnables, depending on the required inputs and outputs of the chain components.

For example, let's say we wanted to compose the joke generating chain with another chain that evaluates whether or not the generated joke was funny.

We would need to be careful with how we format the input into the next chain. In the below example, the dict in the chain is automatically parsed and converted into a [`RunnableParallel`](/docs/how_to/parallel), which runs all of its values in parallel and returns a dict with the results.

This happens to be the same format the next prompt template expects. Here it is in action:
"""

import { RunnableLambda } from "@langchain/core/runnables";

const analysisPrompt = ChatPromptTemplate.fromTemplate("is this a funny joke? {joke}")

const composedChain = new RunnableLambda({
  func: async (input: { topic: string }) => {
    const result = await chain.invoke(input);
    return { joke: result };
  }
}).pipe(analysisPrompt).pipe(model).pipe(new StringOutputParser())

await composedChain.invoke({ topic: "bears" })
# Output:
#   'Haha, that\'s a clever play on words! Using "polar" to imply the bear dissolved or became polar/polarized when put in water. Not the most hilarious joke ever, but it has a cute, groan-worthy pun that makes it mildly amusing. I appreciate a good pun or wordplay joke.'

"""
Functions will also be coerced into runnables, so you can add custom logic to your chains too. The below chain results in the same logical flow as before:
"""

import { RunnableSequence } from "@langchain/core/runnables";

const composedChainWithLambda = RunnableSequence.from([
    chain,
    (input) => ({ joke: input }),
    analysisPrompt,
    model,
    new StringOutputParser()
])

await composedChainWithLambda.invoke({ topic: "beets" })
# Output:
#   "Haha, that's a cute and punny joke! I like how it plays on the idea of beets blushing or turning red like someone blushing. Food puns can be quite amusing. While not a total knee-slapper, it's a light-hearted, groan-worthy dad joke that would make me chuckle and shake my head. Simple vegetable humor!"

"""
> See the LangSmith trace for the run above [here](https://smith.langchain.com/public/ef1bf347-a243-4da6-9be6-54f5d73e6da2/r)
"""

"""
However, keep in mind that using functions like this may interfere with operations like streaming. See [this section](/docs/how_to/functions) for more information.
"""

"""
## Next steps

You now know some ways to chain two runnables together.

To learn more, see the other how-to guides on runnables in [this section](/docs/how_to/#langchain-expression-language-lcel).
"""



================================================
FILE: docs/core_docs/docs/how_to/split_by_token.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to split text by tokens 

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Text splitters](/docs/concepts/text_splitters)

:::

Language models have a token limit. You should not exceed the token limit. When you split your text into chunks it is therefore a good idea to count the number of tokens. There are many tokenizers. When you count tokens in your text you should use the same tokenizer as used in the language model.
"""

"""
## `js-tiktoken`

:::{.callout-note}
[js-tiktoken](https://github.com/openai/js-tiktoken) is a JavaScript version of the `BPE` tokenizer created by OpenAI.
:::


We can use `js-tiktoken` to estimate tokens used. It is tuned to OpenAI models.

1. How the text is split: by character passed in.
2. How the chunk size is measured: by the `js-tiktoken` tokenizer.

You can use the [`TokenTextSplitter`](https://api.js.langchain.com/classes/langchain_textsplitters.TokenTextSplitter.html) like this:
"""

import { TokenTextSplitter } from "@langchain/textsplitters";
import * as fs from "node:fs";

// Load an example document
const rawData = await fs.readFileSync("../../../../examples/state_of_the_union.txt");
const stateOfTheUnion = rawData.toString();

const textSplitter = new TokenTextSplitter({
  chunkSize: 10,
  chunkOverlap: 0,
});

const texts = await textSplitter.splitText(stateOfTheUnion);

console.log(texts[0]);
# Output:
#   Madam Speaker, Madam Vice President, our


"""
**Note:** Some written languages (e.g. Chinese and Japanese) have characters which encode to 2 or more tokens. Using the `TokenTextSplitter` directly can split the tokens for a character between two chunks causing malformed Unicode characters.

## Next steps

You've now learned a method for splitting text based on token count.

Next, check out the [full tutorial on retrieval-augmented generation](/docs/tutorials/rag).
"""



================================================
FILE: docs/core_docs/docs/how_to/sql_large_db.mdx
================================================
# How to deal with large databases

:::info Prerequisites

This guide assumes familiarity with the following:

- [Question answering over SQL data](/docs/tutorials/sql_qa)

:::

In order to write valid queries against a database, we need to feed the model the table names, table schemas, and feature values for it to query over.
When there are many tables, columns, and/or high-cardinality columns, it becomes impossible for us to dump the full information about our database in every prompt.
Instead, we must find ways to dynamically insert into the prompt only the most relevant information. Let's take a look at some techniques for doing this.

## Setup

First, install the required packages and set your environment variables. This example will use OpenAI as the LLM.

```bash
npm install langchain @langchain/community @langchain/openai typeorm sqlite3
```

```bash
export OPENAI_API_KEY="your api key"
# Uncomment the below to use LangSmith. Not required.
# export LANGSMITH_API_KEY="your api key"
# export LANGSMITH_TRACING=true

# Reduce tracing latency if you are not in a serverless environment
# export LANGCHAIN_CALLBACKS_BACKGROUND=true
```

The below example will use a SQLite connection with Chinook database. Follow these [installation steps](https://database.guide/2-sample-databases-sqlite/) to create `Chinook.db` in the same directory as this notebook:

- Save [this](https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql) file as `Chinook_Sqlite.sql`
- Run sqlite3 `Chinook.db`
- Run `.read Chinook_Sqlite.sql`
- Test `SELECT * FROM Artist LIMIT 10;`

Now, `Chinhook.db` is in our directory and we can interface with it using the Typeorm-driven `SqlDatabase` class:

import CodeBlock from "@theme/CodeBlock";
import DbCheck from "@examples/use_cases/sql/db_check.ts";

<CodeBlock language="typescript">{DbCheck}</CodeBlock>

## Many tables

One of the main pieces of information we need to include in our prompt is the schemas of the relevant tables.
When we have very many tables, we can't fit all of the schemas in a single prompt.
What we can do in such cases is first extract the names of the tables related to the user input, and then include only their schemas.

One easy and reliable way to do this is using OpenAI function-calling and Zod models. LangChain comes with a built-in `createExtractionChainZod` chain that lets us do just this:

import LargeDbExample from "@examples/use_cases/sql/large_db.ts";

<CodeBlock language="typescript">{LargeDbExample}</CodeBlock>

We've seen how to dynamically include a subset of table schemas in a prompt within a chain.
Another possible approach to this problem is to let an Agent decide for itself when to look up tables by giving it a Tool to do so.

## High-cardinality columns

High-cardinality refers to columns in a database that have a vast range of unique values.
These columns are characterized by a high level of uniqueness in their data entries, such as individual names, addresses, or product serial numbers.
High-cardinality data can pose challenges for indexing and querying, as it requires more sophisticated strategies to efficiently filter and retrieve specific entries.

In order to filter columns that contain proper nouns such as addresses, song names or artists, we first need to double-check the spelling in order to filter the data correctly.

One naive strategy it to create a vector store with all the distinct proper nouns that exist in the database.
We can then query that vector store each user input and inject the most relevant proper nouns into the prompt.

First we need the unique values for each entity we want, for which we define a function that parses the result into a list of elements:

import HighCardinalityExample from "@examples/use_cases/sql/large_db_high_cardinality.ts";

<CodeBlock language="typescript">{HighCardinalityExample}</CodeBlock>

We can see that with retrieval we're able to correct the spelling and get back a valid result.

Another possible approach to this problem is to let an Agent decide for itself when to look up proper nouns.

## Next steps

You've now learned about some prompting strategies to improve SQL generation.

Next, check out some of the other guides in this section, like [how to validate queries](/docs/how_to/sql_query_checking).
You might also be interested in the query analysis guide [on handling high cardinality](/docs/how_to/query_high_cardinality).



================================================
FILE: docs/core_docs/docs/how_to/sql_prompting.mdx
================================================
# How to use prompting to improve results

:::info Prerequisites

This guide assumes familiarity with the following:

- [Question answering over SQL data](/docs/tutorials/sql_qa)

:::

In this guide we'll go over prompting strategies to improve SQL query generation.
We'll largely focus on methods for getting relevant database-specific information in your prompt.

## Setup

First, install the required packages and set your environment variables. This example will use OpenAI as the LLM.

```bash
npm install @langchain/community @langchain/openai typeorm sqlite3
```

```bash
export OPENAI_API_KEY="your api key"
# Uncomment the below to use LangSmith. Not required.
# export LANGSMITH_API_KEY="your api key"
# export LANGSMITH_TRACING=true

# Reduce tracing latency if you are not in a serverless environment
# export LANGCHAIN_CALLBACKS_BACKGROUND=true
```

The below example will use a SQLite connection with Chinook database. Follow these [installation steps](https://database.guide/2-sample-databases-sqlite/) to create `Chinook.db` in the same directory as this notebook:

- Save [this](https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql) file as `Chinook_Sqlite.sql`
- Run sqlite3 `Chinook.db`
- Run `.read Chinook_Sqlite.sql`
- Test `SELECT * FROM Artist LIMIT 10;`

Now, `Chinhook.db` is in our directory and we can interface with it using the Typeorm-driven `SqlDatabase` class:

import CodeBlock from "@theme/CodeBlock";
import DbCheck from "@examples/use_cases/sql/db_check.ts";

<CodeBlock language="typescript">{DbCheck}</CodeBlock>

## Dialect-specific prompting

One of the simplest things we can do is make our prompt specific to the SQL dialect we're using.
When using the built-in [`createSqlQueryChain`](https://api.js.langchain.com/functions/langchain.chains_sql_db.createSqlQueryChain.html) and [`SqlDatabase`](https://api.js.langchain.com/classes/langchain.sql_db.SqlDatabase.html), this is handled for you for any of the following dialects:

import DialectExample from "@examples/use_cases/sql/prompting/list_dialects.ts";

<CodeBlock language="typescript">{DialectExample}</CodeBlock>

## Table definitions and example rows

In basically any SQL chain, we'll need to feed the model at least part of the database schema.
Without this it won't be able to write valid queries. Our database comes with some convenience methods to give us the relevant context.
Specifically, we can get the table names, their schemas, and a sample of rows from each table:

import TableDefinitionsExample from "@examples/use_cases/sql/prompting/table_definitions.ts";

<CodeBlock language="typescript">{TableDefinitionsExample}</CodeBlock>

## Few-shot examples

Including examples of natural language questions being converted to valid SQL queries against our database in the prompt will often improve model performance, especially for complex queries.

Let's say we have the following examples:

import ExampleList from "@examples/use_cases/sql/prompting/examples.ts";

<CodeBlock language="typescript">{ExampleList}</CodeBlock>

We can create a few-shot prompt with them like so:

import FewShotExample from "@examples/use_cases/sql/prompting/few_shot.ts";

<CodeBlock language="typescript">{FewShotExample}</CodeBlock>

## Dynamic few-shot examples

If we have enough examples, we may want to only include the most relevant ones in the prompt, either because they don't fit in the model's context window or because the long tail of examples distracts the model.
And specifically, given any input we want to include the examples most relevant to that input.

We can do just this using an ExampleSelector. In this case we'll use a [`SemanticSimilarityExampleSelector`](https://api.js.langchain.com/classes/langchain_core.example_selectors.SemanticSimilarityExampleSelector.html),
which will store the examples in the vector database of our choosing.
At runtime it will perform a similarity search between the input and our examples, and return the most semantically similar ones:

import DynamicFewShotExample from "@examples/use_cases/sql/prompting/dynamic_few_shot.ts";

<CodeBlock language="typescript">{DynamicFewShotExample}</CodeBlock>

## Next steps

You've now learned about some prompting strategies to improve SQL generation.

Next, check out some of the other guides in this section, like [how to query over large databases](/docs/how_to/sql_large_db).



================================================
FILE: docs/core_docs/docs/how_to/sql_query_checking.mdx
================================================
# How to do query validation

:::info Prerequisites

This guide assumes familiarity with the following:

- [Question answering over SQL data](/docs/tutorials/sql_qa)

:::

Perhaps the most error-prone part of any SQL chain or agent is writing valid and safe SQL queries.
In this guide we'll go over some strategies for validating our queries and handling invalid queries.

## Setup

First, get required packages and set environment variables:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash
npm install @langchain/community @langchain/openai typeorm sqlite3
```

```bash
export OPENAI_API_KEY="your api key"
# Uncomment the below to use LangSmith. Not required.
# export LANGSMITH_API_KEY="your api key"
# export LANGSMITH_TRACING=true

# Reduce tracing latency if you are not in a serverless environment
# export LANGCHAIN_CALLBACKS_BACKGROUND=true
```

The below example will use a SQLite connection with Chinook database. Follow these [installation steps](https://database.guide/2-sample-databases-sqlite/) to create `Chinook.db` in the same directory as this notebook:

- Save [this](https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql) file as `Chinook_Sqlite.sql`
- Run sqlite3 `Chinook.db`
- Run `.read Chinook_Sqlite.sql`
- Test `SELECT * FROM Artist LIMIT 10;`

Now, `Chinhook.db` is in our directory and we can interface with it using the Typeorm-driven `SqlDatabase` class:

import CodeBlock from "@theme/CodeBlock";
import DbCheck from "@examples/use_cases/sql/db_check.ts";

<CodeBlock language="typescript">{DbCheck}</CodeBlock>

## Query checker

Perhaps the simplest strategy is to ask the model itself to check the original query for common mistakes.
Suppose we have the following SQL query chain:

import FullExample from "@examples/use_cases/sql/query_checking.ts";

<CodeBlock language="typescript">{FullExample}</CodeBlock>

## Next steps

You've now learned about some strategies to validate generated SQL queries.

Next, check out some of the other guides in this section, like [how to query over large databases](/docs/how_to/sql_large_db).



================================================
FILE: docs/core_docs/docs/how_to/stream_agent_client.mdx
================================================
# How to stream agent data to the client

This guide will walk you through how we stream agent data to the client using [React Server Components](https://react.dev/reference/rsc/server-components) inside this directory.
The code in this doc is taken from the `page.tsx` and `action.ts` files in this directory. To view the full, uninterrupted code, click [here for the actions file](https://github.com/langchain-ai/langchain-nextjs-template/blob/main/app/ai_sdk/agent/action.ts)
and [here for the client file](https://github.com/langchain-ai/langchain-nextjs-template/blob/main/app/ai_sdk/agent/page.tsx).

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [LangChain Expression Language](/docs/concepts/lcel)
- [Chat models](/docs/concepts/chat_models)
- [Tool calling](/docs/concepts/tool_calling)
- [Agents](/docs/concepts/agents)

:::

## Setup

First, install the necessary LangChain & AI SDK packages:

```bash npm2yarn
npm install langchain @langchain/core @langchain/community ai
```

In this demo we'll be using the `TavilySearchResults` tool, which requires an API key. You can get one [here](https://app.tavily.com/), or you can swap it out for another tool of your choice, like
[`WikipediaQueryRun`](/docs/integrations/tools/wikipedia) which doesn't require an API key.

If you choose to use `TavilySearchResults`, set your API key like so:

```bash
export TAVILY_API_KEY=your_api_key
```

## Get started

The first step is to create a new RSC file, and add the imports which we'll use for running our agent. In this demo, we'll name it `action.ts`:

```typescript action.ts
"use server";

import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { TavilySearchResults } from "@langchain/community/tools/tavily_search";
import { AgentExecutor, createToolCallingAgent } from "langchain/agents";
import { pull } from "langchain/hub";
import { createStreamableValue } from "ai/rsc";
```

Next, we'll define a `runAgent` function. This function takes in a single input of `string`, and contains all the logic for our agent and streaming data back to the client:

```typescript action.ts
export async function runAgent(input: string) {
  "use server";
}
```

Next, inside our function we'll define our chat model of choice:

```typescript action.ts
const llm = new ChatOpenAI({
  model: "gpt-4o-2024-05-13",
  temperature: 0,
});
```

Next, we'll use the `createStreamableValue` helper function provided by the `ai` package to create a streamable value:

```typescript action.ts
const stream = createStreamableValue();
```

This will be very important later on when we start streaming data back to the client.

Next, lets define our async function inside which contains the agent logic:

```typescript action.ts
  (async () => {
    const tools = [new TavilySearchResults({ maxResults: 1 })];

    const prompt = await pull<ChatPromptTemplate>(
      "hwchase17/openai-tools-agent",
    );

    const agent = createToolCallingAgent({
      llm,
      tools,
      prompt,
    });

    const agentExecutor = new AgentExecutor({
      agent,
      tools,
    });
```

:::tip
As of `langchain` version `0.2.8`, the `createToolCallingAgent` function now supports [OpenAI-formatted tools](https://api.js.langchain.com/interfaces/langchain_core.language_models_base.ToolDefinition.html).
:::

Here you can see we're doing a few things:

The first is we're defining our list of tools (in this case we're only using a single tool) and pulling in our prompt from the LangChain prompt hub.

After that, we're passing our LLM, tools and prompt to the `createToolCallingAgent` function, which will construct and return a runnable agent.
This is then passed into the `AgentExecutor` class, which will handle the execution & streaming of our agent.

Finally, we'll call `.streamEvents` and pass our streamed data back to the `stream` variable we defined above,

```typescript action.ts
    const streamingEvents = agentExecutor.streamEvents(
      { input },
      { version: "v2" },
    );

    for await (const item of streamingEvents) {
      stream.update(JSON.parse(JSON.stringify(item, null, 2)));
    }

    stream.done();
  })();
```

As you can see above, we're doing something a little wacky by stringifying and parsing our data. This is due to a bug in the RSC streaming code,
however if you stringify and parse like we are above, you shouldn't experience this.

Finally, at the bottom of the function return the stream value:

```typescript action.ts
return { streamData: stream.value };
```

Once we've implemented our server action, we can add a couple lines of code in our client function to request and stream this data:

First, add the necessary imports:

```typescript page.tsx
"use client";

import { useState } from "react";
import { readStreamableValue } from "ai/rsc";
import { runAgent } from "./action";
```

Then inside our `Page` function, calling the `runAgent` function is straightforward:

```typescript page.tsx
export default function Page() {
  const [input, setInput] = useState("");
  const [data, setData] = useState<StreamEvent[]>([]);

  async function handleSubmit(e: React.FormEvent) {
    e.preventDefault();

    const { streamData } = await runAgent(input);
    for await (const item of readStreamableValue(streamData)) {
      setData((prev) => [...prev, item]);
    }
  }
}
```

That's it! You've successfully built an agent that streams data back to the client. You can now run your application and see the data streaming in real-time.



================================================
FILE: docs/core_docs/docs/how_to/stream_tool_client.mdx
================================================
# How to stream structured output to the client

This guide will walk you through how we stream agent data to the client using [React Server Components](https://react.dev/reference/rsc/server-components) inside this directory.
The code in this doc is taken from the `page.tsx` and `action.ts` files in this directory. To view the full, uninterrupted code, click [here for the actions file](https://github.com/langchain-ai/langchain-nextjs-template/blob/main/app/ai_sdk/tools/action.ts)
and [here for the client file](https://github.com/langchain-ai/langchain-nextjs-template/blob/main/app/ai_sdk/tools/page.tsx).

:::info Prerequisites

This guide assumes familiarity with the following concepts:

> - [LangChain Expression Language](/docs/concepts/lcel)
> - [Chat models](/docs/concepts/chat_models)
> - [Tool calling](/docs/concepts/tool_calling)

:::

## Setup

First, install the necessary LangChain & AI SDK packages:

```bash npm2yarn
npm install @langchain/openai @langchain/core ai zod zod-to-json-schema
```

Next, we'll create our server file.
This will contain all the logic for making tool calls and sending the data back to the client.

Start by adding the necessary imports & the `"use server"` directive:

```typescript
"use server";

import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { createStreamableValue } from "ai/rsc";
import { z } from "zod";
import { zodToJsonSchema } from "zod-to-json-schema";
import { JsonOutputKeyToolsParser } from "@langchain/core/output_parsers/openai_tools";
```

After that, we'll define our tool schema. For this example we'll use a simple demo weather schema:

```typescript
const Weather = z
  .object({
    city: z.string().describe("City to search for weather"),
    state: z.string().describe("State abbreviation to search for weather"),
  })
  .describe("Weather search parameters");
```

Once our schema is defined, we can implement our `executeTool` function.
This function takes in a single input of `string`, and contains all the logic for our tool and streaming data back to the client:

```typescript
export async function executeTool(
  input: string,
) {
  "use server";

  const stream = createStreamableValue();
```

The `createStreamableValue` function is important as this is what we'll use for actually streaming all the data back to the client.

For the main logic, we'll wrap it in an async function. Start by defining our prompt and chat model:

```typescript
  (async () => {
    const prompt = ChatPromptTemplate.fromMessages([
      [
        "system",
        `You are a helpful assistant. Use the tools provided to best assist the user.`,
      ],
      ["human", "{input}"],
    ]);

    const llm = new ChatOpenAI({
      model: "gpt-4o-2024-05-13",
      temperature: 0,
    });

```

After defining our chat model, we'll define our runnable chain using LCEL.

We start binding our `weather` tool we defined earlier to the model:

```typescript
const modelWithTools = llm.bind({
  tools: [
    {
      type: "function" as const,
      function: {
        name: "get_weather",
        description: Weather.description,
        parameters: zodToJsonSchema(Weather),
      },
    },
  ],
});
```

Next, we'll use LCEL to pipe each component together, starting with the prompt, then the model with tools, and finally the output parser:

```typescript
const chain = prompt.pipe(modelWithTools).pipe(
  new JsonOutputKeyToolsParser<z.infer<typeof Weather>>({
    keyName: "get_weather",
    zodSchema: Weather,
  })
);
```

Finally, we'll call `.stream` on our chain, and similarly to the [streaming agent](/docs/how_to/stream_agent_client)
example, we'll iterate over the stream and stringify + parse the data before updating the stream value:

```typescript
    const streamResult = await chain.stream({
      input,
    });

    for await (const item of streamResult) {
      stream.update(JSON.parse(JSON.stringify(item, null, 2)));
    }

    stream.done();
  })();

  return { streamData: stream.value };
}
```



================================================
FILE: docs/core_docs/docs/how_to/streaming.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to stream

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat models](/docs/concepts/chat_models)
- [LangChain Expression Language](/docs/concepts/lcel)
- [Output parsers](/docs/concepts/output_parsers)

:::

Streaming is critical in making applications based on LLMs feel responsive to end-users.

Important LangChain primitives like LLMs, parsers, prompts, retrievers, and agents implement the LangChain Runnable Interface.

This interface provides two general approaches to stream content:

- `.stream()`: a default implementation of streaming that streams the final output from the chain.
- `streamEvents()` and `streamLog()`: these provide a way to stream both intermediate steps and final output from the chain.

Let’s take a look at both approaches!

:::info
For a higher-level overview of streaming techniques in LangChain, see [this section of the conceptual guide](/docs/concepts/streaming).
:::

# Using Stream

All `Runnable` objects implement a method called stream.

These methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.

Streaming is only possible if all steps in the program know how to process an **input stream**; i.e., process an input chunk one at a time, and yield a corresponding output chunk.

The complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.

The best place to start exploring streaming is with the single most important components in LLM apps – the models themselves!

## LLMs and Chat Models

Large language models can take several seconds to generate a complete response to a query. This is far slower than the **~200-300 ms** threshold at which an application feels responsive to an end user.

The key strategy to make the application feel more responsive is to show intermediate progress; e.g., to stream the output from the model token by token.
"""

import "dotenv/config";

"""
```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs />
```
"""

// @lc-docs-hide-cell
import { ChatOpenAI } from "@langchain/openai";

const model = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0,
});

const stream = await model.stream("Hello! Tell me about yourself.");
const chunks = [];
for await (const chunk of stream) {
  chunks.push(chunk);
  console.log(`${chunk.content}|`)
}
# Output:
#   |

#   Hello|

#   !|

#    I'm|

#    a|

#    large|

#    language|

#    model|

#    developed|

#    by|

#    Open|

#   AI|

#    called|

#    GPT|

#   -|

#   4|

#   ,|

#    based|

#    on|

#    the|

#    Gener|

#   ative|

#    Pre|

#   -trained|

#    Transformer|

#    architecture|

#   .|

#    I'm|

#    designed|

#    to|

#    understand|

#    and|

#    generate|

#    human|

#   -like|

#    text|

#    based|

#    on|

#    the|

#    input|

#    I|

#    receive|

#   .|

#    My|

#    primary|

#    function|

#    is|

#    to|

#    assist|

#    with|

#    answering|

#    questions|

#   ,|

#    providing|

#    information|

#   ,|

#    and|

#    engaging|

#    in|

#    various|

#    types|

#    of|

#    conversations|

#   .|

#    While|

#    I|

#    don't|

#    have|

#    personal|

#    experiences|

#    or|

#    emotions|

#   ,|

#    I'm|

#    trained|

#    on|

#    diverse|

#    datasets|

#    that|

#    enable|

#    me|

#    to|

#    provide|

#    useful|

#    and|

#    relevant|

#    information|

#    across|

#    a|

#    wide|

#    array|

#    of|

#    topics|

#   .|

#    How|

#    can|

#    I|

#    assist|

#    you|

#    today|

#   ?|

#   |

#   |


"""
Let's have a look at one of the raw chunks:
"""

chunks[0]
# Output:
#   AIMessageChunk {

#     lc_serializable: true,

#     lc_kwargs: {

#       content: '',

#       tool_call_chunks: [],

#       additional_kwargs: {},

#       id: 'chatcmpl-9lO8YUEcX7rqaxxevelHBtl1GaWoo',

#       tool_calls: [],

#       invalid_tool_calls: [],

#       response_metadata: {}

#     },

#     lc_namespace: [ 'langchain_core', 'messages' ],

#     content: '',

#     name: undefined,

#     additional_kwargs: {},

#     response_metadata: { prompt: 0, completion: 0, finish_reason: null },

#     id: 'chatcmpl-9lO8YUEcX7rqaxxevelHBtl1GaWoo',

#     tool_calls: [],

#     invalid_tool_calls: [],

#     tool_call_chunks: [],

#     usage_metadata: undefined

#   }


"""
We got back something called an `AIMessageChunk`. This chunk represents a part of an `AIMessage`.

Message chunks are additive by design – one can simply add them up using the `.concat()` method to get the state of the response so far!
"""

let finalChunk = chunks[0];

for (const chunk of chunks.slice(1, 5)) {
  finalChunk = finalChunk.concat(chunk);
}

finalChunk
# Output:
#   AIMessageChunk {

#     lc_serializable: true,

#     lc_kwargs: {

#       content: "Hello! I'm a",

#       additional_kwargs: {},

#       response_metadata: { prompt: 0, completion: 0, finish_reason: null },

#       tool_call_chunks: [],

#       id: 'chatcmpl-9lO8YUEcX7rqaxxevelHBtl1GaWoo',

#       tool_calls: [],

#       invalid_tool_calls: []

#     },

#     lc_namespace: [ 'langchain_core', 'messages' ],

#     content: "Hello! I'm a",

#     name: undefined,

#     additional_kwargs: {},

#     response_metadata: { prompt: 0, completion: 0, finish_reason: null },

#     id: 'chatcmpl-9lO8YUEcX7rqaxxevelHBtl1GaWoo',

#     tool_calls: [],

#     invalid_tool_calls: [],

#     tool_call_chunks: [],

#     usage_metadata: undefined

#   }


"""
## Chains

Virtually all LLM applications involve more steps than just a call to a language model.

Let’s build a simple chain using `LangChain Expression Language` (`LCEL`) that combines a prompt, model and a parser and verify that streaming works.

We will use `StringOutputParser` to parse the output from the model. This is a simple parser that extracts the content field from an `AIMessageChunk`, giving us the `token` returned by the model.

:::{.callout-tip}
LCEL is a declarative way to specify a “program” by chainining together different LangChain primitives. Chains created using LCEL benefit from an automatic implementation of stream, allowing streaming of the final output. In fact, chains created with LCEL implement the entire standard Runnable interface.
:::
"""

import { StringOutputParser } from "@langchain/core/output_parsers";
import { ChatPromptTemplate } from "@langchain/core/prompts";

const prompt = ChatPromptTemplate.fromTemplate("Tell me a joke about {topic}");

const parser = new StringOutputParser();

const chain = prompt.pipe(model).pipe(parser);

const stream = await chain.stream({
  topic: "parrot",
});

for await (const chunk of stream) {
  console.log(`${chunk}|`)
}
# Output:
#   |

#   Sure|

#   ,|

#    here's|

#    a|

#    joke|

#    for|

#    you|

#   :

#   

#   |

#   Why|

#    did|

#    the|

#    par|

#   rot|

#    sit|

#    on|

#    the|

#    stick|

#   ?

#   

#   |

#   Because|

#    it|

#    wanted|

#    to|

#    be|

#    a|

#    "|

#   pol|

#   ly|

#   -stick|

#   -al|

#   "|

#    observer|

#   !|

#   |

#   |


"""
:::{.callout-note}
You do not have to use the `LangChain Expression Language` to use LangChain and can instead rely on a standard **imperative** programming approach by
caling `invoke`, `batch` or `stream` on each component individually, assigning the results to variables and then using them downstream as you see fit.

If that works for your needs, then that's fine by us 👌!
:::

### Working with Input Streams

What if you wanted to stream JSON from the output as it was being generated?

If you were to rely on `JSON.parse` to parse the partial json, the parsing would fail as the partial json wouldn't be valid json.

You'd likely be at a complete loss of what to do and claim that it wasn't possible to stream JSON.

Well, turns out there is a way to do it - the parser needs to operate on the **input stream**, and attempt to "auto-complete" the partial json into a valid state.

Let's see such a parser in action to understand what this means.
"""

import { JsonOutputParser } from "@langchain/core/output_parsers"

const chain = model.pipe(new JsonOutputParser());
const stream = await chain.stream(
  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`
);

for await (const chunk of stream) {
  console.log(chunk);
}
# Output:
#   {

#     countries: [

#       { name: 'France', population: 67390000 },

#       { name: 'Spain', population: 47350000 },

#       { name: 'Japan', population: 125800000 }

#     ]

#   }


"""
Now, let's **break** streaming. We'll use the previous example and append an extraction function at the end that extracts the country names from the finalized JSON. Since this new last step is just a function call with no defined streaming behavior, the streaming output from previous steps is aggregated, then passed as a single input to the function.

:::{.callout-warning}
Any steps in the chain that operate on **finalized inputs** rather than on **input streams** can break streaming functionality via `stream`.
:::

:::{.callout-tip}
Later, we will discuss the `streamEvents` API which streams results from intermediate steps. This API will stream results from intermediate steps even if the chain contains steps that only operate on **finalized inputs**.
:::
"""

// A function that operates on finalized inputs
// rather than on an input_stream

// A function that does not operates on input streams and breaks streaming.
const extractCountryNames = (inputs: Record<string, any>) => {
  if (!Array.isArray(inputs.countries)) {
    return "";
  }
  return JSON.stringify(inputs.countries.map((country) => country.name));
}

const chain = model.pipe(new JsonOutputParser()).pipe(extractCountryNames);

const stream = await chain.stream(
  `output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`
);

for await (const chunk of stream) {
  console.log(chunk);
}
# Output:
#   ["France","Spain","Japan"]


"""
### Non-streaming components

Like the above example, some built-in components like Retrievers do not offer any streaming. What happens if we try to `stream` them?
"""

import { OpenAIEmbeddings } from "@langchain/openai";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { ChatPromptTemplate } from "@langchain/core/prompts";

const template = `Answer the question based only on the following context:
{context}

Question: {question}
`;
const prompt = ChatPromptTemplate.fromTemplate(template);

const vectorstore = await MemoryVectorStore.fromTexts(
  ["mitochondria is the powerhouse of the cell", "buildings are made of brick"],
  [{}, {}],
  new OpenAIEmbeddings(),
);

const retriever = vectorstore.asRetriever();

const chunks = [];

for await (const chunk of await retriever.stream("What is the powerhouse of the cell?")) {
  chunks.push(chunk);
}

console.log(chunks);

# Output:
#   [

#     [

#       Document {

#         pageContent: 'mitochondria is the powerhouse of the cell',

#         metadata: {},

#         id: undefined

#       },

#       Document {

#         pageContent: 'buildings are made of brick',

#         metadata: {},

#         id: undefined

#       }

#     ]

#   ]


"""
Stream just yielded the final result from that component. 

This is OK! Not all components have to implement streaming -- in some cases streaming is either unnecessary, difficult or just doesn't make sense.

:::{.callout-tip}
An LCEL chain constructed using some non-streaming components will still be able to stream in a lot of cases, with streaming of partial output starting after the last non-streaming step in the chain.
:::

Here's an example of this:
"""

import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";
import type { Document } from "@langchain/core/documents";
import { StringOutputParser } from "@langchain/core/output_parsers";

const formatDocs = (docs: Document[]) => {
  return docs.map((doc) => doc.pageContent).join("\n-----\n")
}

const retrievalChain = RunnableSequence.from([
  {
    context: retriever.pipe(formatDocs),
    question: new RunnablePassthrough()
  },
  prompt,
  model,
  new StringOutputParser(),
]);

const stream = await retrievalChain.stream("What is the powerhouse of the cell?");

for await (const chunk of stream) {
  console.log(`${chunk}|`);
}
# Output:
#   |

#   M|

#   ito|

#   ch|

#   ond|

#   ria|

#    is|

#    the|

#    powerhouse|

#    of|

#    the|

#    cell|

#   .|

#   |

#   |


"""
Now that we've seen how the `stream` method works, let's venture into the world of streaming events!

## Using Stream Events

Event Streaming is a **beta** API. This API may change a bit based on feedback.

:::{.callout-note}
Introduced in @langchain/core **0.1.27**.
:::

For the `streamEvents` method to work properly:

* Any custom functions / runnables must propragate callbacks 
* Set proper parameters on models to force the LLM to stream tokens.
* Let us know if anything doesn't work as expected!

### Event Reference

Below is a reference table that shows some events that might be emitted by the various Runnable objects.

:::{.callout-note}
When streaming is implemented properly, the inputs to a runnable will not be known until after the input stream has been entirely consumed. This means that `inputs` will often be included only for `end` events and rather than for `start` events.
:::

| event                | name             | chunk                           | input                                         | output                                          |
|----------------------|------------------|---------------------------------|-----------------------------------------------|-------------------------------------------------|
| on_llm_start         | [model name]     |                                 | {'input': 'hello'}                            |                                                 |
| on_llm_stream        | [model name]     | 'Hello' `or` AIMessageChunk(content="hello")  |                                               |                                   |
| on_llm_end           | [model name]     |                                 | 'Hello human!'                                | {"generations": [...], "llmOutput": None, ...}  |
| on_chain_start       | format_docs      |                                 |                                               |                                                 |
| on_chain_stream      | format_docs      | "hello world!, goodbye world!"  |                                               |                                                 |
| on_chain_end         | format_docs      |                                 | [Document(...)]                               | "hello world!, goodbye world!"                  |
| on_tool_start        | some_tool        |                                 | {"x": 1, "y": "2"}                            |                                                 |
| on_tool_stream       | some_tool        | {"x": 1, "y": "2"}              |                                               |                                                 |
| on_tool_end          | some_tool        |                                 |                                               | {"x": 1, "y": "2"}                              |
| on_retriever_start   | [retriever name] |                                 | {"query": "hello"}                            |                                                 |
| on_retriever_chunk   | [retriever name] | {documents: [...]}              |                                               |                                                 |
| on_retriever_end     | [retriever name] |                                 | {"query": "hello"}                            | {documents: [...]}                              |
| on_prompt_start      | [template_name]  |                                 | {"question": "hello"}                         |                                                 |
| on_prompt_end        | [template_name]  |                                 | {"question": "hello"}                         | ChatPromptValue(messages: [SystemMessage, ...]) |

`streamEvents` will also emit dispatched custom events in `v2`. Please see [this guide](/docs/how_to/callbacks_custom_events/) for more.

### Chat Model

Let's start off by looking at the events produced by a chat model.
"""

const events = [];

const eventStream = await model.streamEvents("hello", { version: "v2" });

for await (const event of eventStream) {
  events.push(event);
}

console.log(events.length)
# Output:
#   25


"""
:::{.callout-note}

Hey what's that funny version="v2" parameter in the API?! 😾

This is a **beta API**, and we're almost certainly going to make some changes to it.

This version parameter will allow us to minimize such breaking changes to your code. 

In short, we are annoying you now, so we don't have to annoy you later.
:::
"""

"""
Let's take a look at the few of the start event and a few of the end events.
"""

events.slice(0, 3);
# Output:
#   [

#     {

#       event: 'on_chat_model_start',

#       data: { input: 'hello' },

#       name: 'ChatOpenAI',

#       tags: [],

#       run_id: 'c983e634-9f1d-4916-97d8-63c3a86102c2',

#       metadata: {

#         ls_provider: 'openai',

#         ls_model_name: 'gpt-4o',

#         ls_model_type: 'chat',

#         ls_temperature: 1,

#         ls_max_tokens: undefined,

#         ls_stop: undefined

#       }

#     },

#     {

#       event: 'on_chat_model_stream',

#       data: { chunk: [AIMessageChunk] },

#       run_id: 'c983e634-9f1d-4916-97d8-63c3a86102c2',

#       name: 'ChatOpenAI',

#       tags: [],

#       metadata: {

#         ls_provider: 'openai',

#         ls_model_name: 'gpt-4o',

#         ls_model_type: 'chat',

#         ls_temperature: 1,

#         ls_max_tokens: undefined,

#         ls_stop: undefined

#       }

#     },

#     {

#       event: 'on_chat_model_stream',

#       run_id: 'c983e634-9f1d-4916-97d8-63c3a86102c2',

#       name: 'ChatOpenAI',

#       tags: [],

#       metadata: {

#         ls_provider: 'openai',

#         ls_model_name: 'gpt-4o',

#         ls_model_type: 'chat',

#         ls_temperature: 1,

#         ls_max_tokens: undefined,

#         ls_stop: undefined

#       },

#       data: { chunk: [AIMessageChunk] }

#     }

#   ]


events.slice(-2);
# Output:
#   [

#     {

#       event: 'on_chat_model_stream',

#       run_id: 'c983e634-9f1d-4916-97d8-63c3a86102c2',

#       name: 'ChatOpenAI',

#       tags: [],

#       metadata: {

#         ls_provider: 'openai',

#         ls_model_name: 'gpt-4o',

#         ls_model_type: 'chat',

#         ls_temperature: 1,

#         ls_max_tokens: undefined,

#         ls_stop: undefined

#       },

#       data: { chunk: [AIMessageChunk] }

#     },

#     {

#       event: 'on_chat_model_end',

#       data: { output: [AIMessageChunk] },

#       run_id: 'c983e634-9f1d-4916-97d8-63c3a86102c2',

#       name: 'ChatOpenAI',

#       tags: [],

#       metadata: {

#         ls_provider: 'openai',

#         ls_model_name: 'gpt-4o',

#         ls_model_type: 'chat',

#         ls_temperature: 1,

#         ls_max_tokens: undefined,

#         ls_stop: undefined

#       }

#     }

#   ]


"""
### Chain

Let's revisit the example chain that parsed streaming JSON to explore the streaming events API.
"""

const chain = model.pipe(new JsonOutputParser());
const eventStream = await chain.streamEvents(
  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`,
  { version: "v2" },
);


const events = [];
for await (const event of eventStream) {
  events.push(event);
}

console.log(events.length)
# Output:
#   83


"""
If you examine at the first few events, you'll notice that there are **3** different start events rather than **2** start events.

The three start events correspond to:

1. The chain (model + parser)
2. The model
3. The parser
"""

events.slice(0, 3);
# Output:
#   [

#     {

#       event: 'on_chain_start',

#       data: {

#         input: 'Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"'

#       },

#       name: 'RunnableSequence',

#       tags: [],

#       run_id: '5dd960b8-4341-4401-8993-7d04d49fcc08',

#       metadata: {}

#     },

#     {

#       event: 'on_chat_model_start',

#       data: { input: [Object] },

#       name: 'ChatOpenAI',

#       tags: [ 'seq:step:1' ],

#       run_id: '5d2917b1-886a-47a1-807d-8a0ba4cb4f65',

#       metadata: {

#         ls_provider: 'openai',

#         ls_model_name: 'gpt-4o',

#         ls_model_type: 'chat',

#         ls_temperature: 1,

#         ls_max_tokens: undefined,

#         ls_stop: undefined

#       }

#     },

#     {

#       event: 'on_parser_start',

#       data: {},

#       name: 'JsonOutputParser',

#       tags: [ 'seq:step:2' ],

#       run_id: '756c57d6-d455-484f-a556-79a82c4e1d40',

#       metadata: {}

#     }

#   ]


"""
What do you think you'd see if you looked at the last 3 events? what about the middle?

Let's use this API to take output the stream events from the model and the parser. We're ignoring start events, end events and events from the chain.
"""

let eventCount = 0;

const eventStream = await chain.streamEvents(
  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`,
  { version: "v1" },
);

for await (const event of eventStream) {
  // Truncate the output
  if (eventCount > 30) {
    continue;
  }
  const eventType = event.event;
  if (eventType === "on_llm_stream") {
    console.log(`Chat model chunk: ${event.data.chunk.message.content}`);
  } else if (eventType === "on_parser_stream") {
    console.log(`Parser chunk: ${JSON.stringify(event.data.chunk)}`);
  }
  eventCount += 1;
}
# Output:
#   Chat model chunk: 

#   Chat model chunk: ```

#   Chat model chunk: json

#   Chat model chunk: 

#   

#   Chat model chunk: {

#   

#   Chat model chunk:    

#   Chat model chunk:  "

#   Chat model chunk: countries

#   Chat model chunk: ":

#   Chat model chunk:  [

#   

#   Chat model chunk:        

#   Chat model chunk:  {

#   

#   Chat model chunk:            

#   Chat model chunk:  "

#   Chat model chunk: name

#   Chat model chunk: ":

#   Chat model chunk:  "

#   Chat model chunk: France

#   Chat model chunk: ",

#   

#   Chat model chunk:            

#   Chat model chunk:  "

#   Chat model chunk: population

#   Chat model chunk: ":

#   Chat model chunk:  

#   Chat model chunk: 652

#   Chat model chunk: 735

#   Chat model chunk: 11

#   Chat model chunk: 

#   


"""
Because both the model and the parser support streaming, we see streaming events from both components in real time! Neat! 🦜

### Filtering Events

Because this API produces so many events, it is useful to be able to filter on events.

You can filter by either component `name`, component `tags` or component `type`.

#### By Name


"""

const chain = model.withConfig({ runName: "model" })
  .pipe(
    new JsonOutputParser().withConfig({ runName: "my_parser" })
  );


const eventStream = await chain.streamEvents(
  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`,
  { version: "v2" },
  { includeNames: ["my_parser"] },
);

let eventCount = 0;

for await (const event of eventStream) {
  // Truncate the output
  if (eventCount > 10) {
    continue;
  }
  console.log(event);
  eventCount += 1;
}
# Output:
#   {

#     event: 'on_parser_start',

#     data: {

#       input: 'Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"'

#     },

#     name: 'my_parser',

#     tags: [ 'seq:step:2' ],

#     run_id: '0a605976-a8f8-4259-8ef6-b3d7e52b3d4e',

#     metadata: {}

#   }

#   {

#     event: 'on_parser_stream',

#     run_id: '0a605976-a8f8-4259-8ef6-b3d7e52b3d4e',

#     name: 'my_parser',

#     tags: [ 'seq:step:2' ],

#     metadata: {},

#     data: { chunk: { countries: [Array] } }

#   }

#   {

#     event: 'on_parser_end',

#     data: { output: { countries: [Array] } },

#     run_id: '0a605976-a8f8-4259-8ef6-b3d7e52b3d4e',

#     name: 'my_parser',

#     tags: [ 'seq:step:2' ],

#     metadata: {}

#   }


"""
#### By type
"""

const chain = model.withConfig({ runName: "model" })
  .pipe(
    new JsonOutputParser().withConfig({ runName: "my_parser" })
  );


const eventStream = await chain.streamEvents(
  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`,
  { version: "v2" },
  { includeTypes: ["chat_model"] },
);

let eventCount = 0;

for await (const event of eventStream) {
  // Truncate the output
  if (eventCount > 10) {
    continue;
  }
  console.log(event);
  eventCount += 1;
}
# Output:
#   {

#     event: 'on_chat_model_start',

#     data: {

#       input: 'Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"'

#     },

#     name: 'model',

#     tags: [ 'seq:step:1' ],

#     run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',

#     metadata: {

#       ls_provider: 'openai',

#       ls_model_name: 'gpt-4o',

#       ls_model_type: 'chat',

#       ls_temperature: 1,

#       ls_max_tokens: undefined,

#       ls_stop: undefined

#     }

#   }

#   {

#     event: 'on_chat_model_stream',

#     data: {

#       chunk: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: '',

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: [Object],

#         id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: undefined

#       }

#     },

#     run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',

#     name: 'model',

#     tags: [ 'seq:step:1' ],

#     metadata: {

#       ls_provider: 'openai',

#       ls_model_name: 'gpt-4o',

#       ls_model_type: 'chat',

#       ls_temperature: 1,

#       ls_max_tokens: undefined,

#       ls_stop: undefined

#     }

#   }

#   {

#     event: 'on_chat_model_stream',

#     data: {

#       chunk: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: '```',

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: [Object],

#         id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: undefined

#       }

#     },

#     run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',

#     name: 'model',

#     tags: [ 'seq:step:1' ],

#     metadata: {

#       ls_provider: 'openai',

#       ls_model_name: 'gpt-4o',

#       ls_model_type: 'chat',

#       ls_temperature: 1,

#       ls_max_tokens: undefined,

#       ls_stop: undefined

#     }

#   }

#   {

#     event: 'on_chat_model_stream',

#     data: {

#       chunk: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: 'json',

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: [Object],

#         id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: undefined

#       }

#     },

#     run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',

#     name: 'model',

#     tags: [ 'seq:step:1' ],

#     metadata: {

#       ls_provider: 'openai',

#       ls_model_name: 'gpt-4o',

#       ls_model_type: 'chat',

#       ls_temperature: 1,

#       ls_max_tokens: undefined,

#       ls_stop: undefined

#     }

#   }

#   {

#     event: 'on_chat_model_stream',

#     data: {

#       chunk: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: '\n',

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: [Object],

#         id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: undefined

#       }

#     },

#     run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',

#     name: 'model',

#     tags: [ 'seq:step:1' ],

#     metadata: {

#       ls_provider: 'openai',

#       ls_model_name: 'gpt-4o',

#       ls_model_type: 'chat',

#       ls_temperature: 1,

#       ls_max_tokens: undefined,

#       ls_stop: undefined

#     }

#   }

#   {

#     event: 'on_chat_model_stream',

#     data: {

#       chunk: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: '{\n',

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: [Object],

#         id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: undefined

#       }

#     },

#     run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',

#     name: 'model',

#     tags: [ 'seq:step:1' ],

#     metadata: {

#       ls_provider: 'openai',

#       ls_model_name: 'gpt-4o',

#       ls_model_type: 'chat',

#       ls_temperature: 1,

#       ls_max_tokens: undefined,

#       ls_stop: undefined

#     }

#   }

#   {

#     event: 'on_chat_model_stream',

#     data: {

#       chunk: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: ' ',

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: [Object],

#         id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: undefined

#       }

#     },

#     run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',

#     name: 'model',

#     tags: [ 'seq:step:1' ],

#     metadata: {

#       ls_provider: 'openai',

#       ls_model_name: 'gpt-4o',

#       ls_model_type: 'chat',

#       ls_temperature: 1,

#       ls_max_tokens: undefined,

#       ls_stop: undefined

#     }

#   }

#   {

#     event: 'on_chat_model_stream',

#     data: {

#       chunk: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: ' "',

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: [Object],

#         id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: undefined

#       }

#     },

#     run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',

#     name: 'model',

#     tags: [ 'seq:step:1' ],

#     metadata: {

#       ls_provider: 'openai',

#       ls_model_name: 'gpt-4o',

#       ls_model_type: 'chat',

#       ls_temperature: 1,

#       ls_max_tokens: undefined,

#       ls_stop: undefined

#     }

#   }

#   {

#     event: 'on_chat_model_stream',

#     data: {

#       chunk: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: 'countries',

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: [Object],

#         id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: undefined

#       }

#     },

#     run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',

#     name: 'model',

#     tags: [ 'seq:step:1' ],

#     metadata: {

#       ls_provider: 'openai',

#       ls_model_name: 'gpt-4o',

#       ls_model_type: 'chat',

#       ls_temperature: 1,

#       ls_max_tokens: undefined,

#       ls_stop: undefined

#     }

#   }

#   {

#     event: 'on_chat_model_stream',

#     data: {

#       chunk: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: '":',

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: [Object],

#         id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: undefined

#       }

#     },

#     run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',

#     name: 'model',

#     tags: [ 'seq:step:1' ],

#     metadata: {

#       ls_provider: 'openai',

#       ls_model_name: 'gpt-4o',

#       ls_model_type: 'chat',

#       ls_temperature: 1,

#       ls_max_tokens: undefined,

#       ls_stop: undefined

#     }

#   }

#   {

#     event: 'on_chat_model_stream',

#     data: {

#       chunk: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: ' [\n',

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: [Object],

#         id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: undefined

#       }

#     },

#     run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',

#     name: 'model',

#     tags: [ 'seq:step:1' ],

#     metadata: {

#       ls_provider: 'openai',

#       ls_model_name: 'gpt-4o',

#       ls_model_type: 'chat',

#       ls_temperature: 1,

#       ls_max_tokens: undefined,

#       ls_stop: undefined

#     }

#   }


"""
#### By Tags

:::{.callout-caution}

Tags are inherited by child components of a given runnable. 

If you're using tags to filter, make sure that this is what you want.
:::
"""

const chain = model
  .pipe(new JsonOutputParser().withConfig({ runName: "my_parser" }))
  .withConfig({ tags: ["my_chain"] });


const eventStream = await chain.streamEvents(
  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`,
  { version: "v2" },
  { includeTags: ["my_chain"] },
);

let eventCount = 0;

for await (const event of eventStream) {
  // Truncate the output
  if (eventCount > 10) {
    continue;
  }
  console.log(event);
  eventCount += 1;
}
# Output:
#   {

#     event: 'on_chain_start',

#     data: {

#       input: 'Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"'

#     },

#     name: 'RunnableSequence',

#     tags: [ 'my_chain' ],

#     run_id: '1fed60d6-e0b7-4d5e-8ec7-cd7d3ee5c69f',

#     metadata: {}

#   }

#   {

#     event: 'on_chat_model_start',

#     data: { input: { messages: [Array] } },

#     name: 'ChatOpenAI',

#     tags: [ 'seq:step:1', 'my_chain' ],

#     run_id: 'ecb99d6e-ce03-445f-aadf-73e6cbbc52fe',

#     metadata: {

#       ls_provider: 'openai',

#       ls_model_name: 'gpt-4o',

#       ls_model_type: 'chat',

#       ls_temperature: 1,

#       ls_max_tokens: undefined,

#       ls_stop: undefined

#     }

#   }

#   {

#     event: 'on_parser_start',

#     data: {},

#     name: 'my_parser',

#     tags: [ 'seq:step:2', 'my_chain' ],

#     run_id: 'caf24a1e-255c-4937-9f38-6e46275d854a',

#     metadata: {}

#   }

#   {

#     event: 'on_chat_model_stream',

#     data: {

#       chunk: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: '',

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: [Object],

#         id: 'chatcmpl-9lO99nzUvCsZWCiq6vNtS1Soa1qNp',

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: undefined

#       }

#     },

#     run_id: 'ecb99d6e-ce03-445f-aadf-73e6cbbc52fe',

#     name: 'ChatOpenAI',

#     tags: [ 'seq:step:1', 'my_chain' ],

#     metadata: {

#       ls_provider: 'openai',

#       ls_model_name: 'gpt-4o',

#       ls_model_type: 'chat',

#       ls_temperature: 1,

#       ls_max_tokens: undefined,

#       ls_stop: undefined

#     }

#   }

#   {

#     event: 'on_chat_model_stream',

#     data: {

#       chunk: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: 'Certainly',

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: [Object],

#         id: 'chatcmpl-9lO99nzUvCsZWCiq6vNtS1Soa1qNp',

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: undefined

#       }

#     },

#     run_id: 'ecb99d6e-ce03-445f-aadf-73e6cbbc52fe',

#     name: 'ChatOpenAI',

#     tags: [ 'seq:step:1', 'my_chain' ],

#     metadata: {

#       ls_provider: 'openai',

#       ls_model_name: 'gpt-4o',

#       ls_model_type: 'chat',

#       ls_temperature: 1,

#       ls_max_tokens: undefined,

#       ls_stop: undefined

#     }

#   }

#   {

#     event: 'on_chat_model_stream',

#     data: {

#       chunk: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: '!',

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: [Object],

#         id: 'chatcmpl-9lO99nzUvCsZWCiq6vNtS1Soa1qNp',

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: undefined

#       }

#     },

#     run_id: 'ecb99d6e-ce03-445f-aadf-73e6cbbc52fe',

#     name: 'ChatOpenAI',

#     tags: [ 'seq:step:1', 'my_chain' ],

#     metadata: {

#       ls_provider: 'openai',

#       ls_model_name: 'gpt-4o',

#       ls_model_type: 'chat',

#       ls_temperature: 1,

#       ls_max_tokens: undefined,

#       ls_stop: undefined

#     }

#   }

#   {

#     event: 'on_chat_model_stream',

#     data: {

#       chunk: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: " Here's",

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: [Object],

#         id: 'chatcmpl-9lO99nzUvCsZWCiq6vNtS1Soa1qNp',

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: undefined

#       }

#     },

#     run_id: 'ecb99d6e-ce03-445f-aadf-73e6cbbc52fe',

#     name: 'ChatOpenAI',

#     tags: [ 'seq:step:1', 'my_chain' ],

#     metadata: {

#       ls_provider: 'openai',

#       ls_model_name: 'gpt-4o',

#       ls_model_type: 'chat',

#       ls_temperature: 1,

#       ls_max_tokens: undefined,

#       ls_stop: undefined

#     }

#   }

#   {

#     event: 'on_chat_model_stream',

#     data: {

#       chunk: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: ' the',

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: [Object],

#         id: 'chatcmpl-9lO99nzUvCsZWCiq6vNtS1Soa1qNp',

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: undefined

#       }

#     },

#     run_id: 'ecb99d6e-ce03-445f-aadf-73e6cbbc52fe',

#     name: 'ChatOpenAI',

#     tags: [ 'seq:step:1', 'my_chain' ],

#     metadata: {

#       ls_provider: 'openai',

#       ls_model_name: 'gpt-4o',

#       ls_model_type: 'chat',

#       ls_temperature: 1,

#       ls_max_tokens: undefined,

#       ls_stop: undefined

#     }

#   }

#   {

#     event: 'on_chat_model_stream',

#     data: {

#       chunk: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: ' JSON',

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: [Object],

#         id: 'chatcmpl-9lO99nzUvCsZWCiq6vNtS1Soa1qNp',

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: undefined

#       }

#     },

#     run_id: 'ecb99d6e-ce03-445f-aadf-73e6cbbc52fe',

#     name: 'ChatOpenAI',

#     tags: [ 'seq:step:1', 'my_chain' ],

#     metadata: {

#       ls_provider: 'openai',

#       ls_model_name: 'gpt-4o',

#       ls_model_type: 'chat',

#       ls_temperature: 1,

#       ls_max_tokens: undefined,

#       ls_stop: undefined

#     }

#   }

#   {

#     event: 'on_chat_model_stream',

#     data: {

#       chunk: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: ' format',

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: [Object],

#         id: 'chatcmpl-9lO99nzUvCsZWCiq6vNtS1Soa1qNp',

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: undefined

#       }

#     },

#     run_id: 'ecb99d6e-ce03-445f-aadf-73e6cbbc52fe',

#     name: 'ChatOpenAI',

#     tags: [ 'seq:step:1', 'my_chain' ],

#     metadata: {

#       ls_provider: 'openai',

#       ls_model_name: 'gpt-4o',

#       ls_model_type: 'chat',

#       ls_temperature: 1,

#       ls_max_tokens: undefined,

#       ls_stop: undefined

#     }

#   }

#   {

#     event: 'on_chat_model_stream',

#     data: {

#       chunk: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: ' output',

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: [Object],

#         id: 'chatcmpl-9lO99nzUvCsZWCiq6vNtS1Soa1qNp',

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: undefined

#       }

#     },

#     run_id: 'ecb99d6e-ce03-445f-aadf-73e6cbbc52fe',

#     name: 'ChatOpenAI',

#     tags: [ 'seq:step:1', 'my_chain' ],

#     metadata: {

#       ls_provider: 'openai',

#       ls_model_name: 'gpt-4o',

#       ls_model_type: 'chat',

#       ls_temperature: 1,

#       ls_max_tokens: undefined,

#       ls_stop: undefined

#     }

#   }


"""
### Streaming events over HTTP

For convenience, `streamEvents` supports encoding streamed intermediate events as HTTP [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events), encoded as bytes. Here's what that looks like (using a [`TextDecoder`](https://developer.mozilla.org/en-US/docs/Web/API/TextDecoder) to reconvert the binary data back into a human readable string):
"""

const chain = model
  .pipe(new JsonOutputParser().withConfig({ runName: "my_parser" }))
  .withConfig({ tags: ["my_chain"] });


const eventStream = await chain.streamEvents(
  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`,
  {
    version: "v2",
    encoding: "text/event-stream",
  },
);

let eventCount = 0;

const textDecoder = new TextDecoder();

for await (const event of eventStream) {
  // Truncate the output
  if (eventCount > 3) {
    continue;
  }
  console.log(textDecoder.decode(event));
  eventCount += 1;
}
# Output:
#   event: data

#   data: {"event":"on_chain_start","data":{"input":"Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key \"name\" and \"population\""},"name":"RunnableSequence","tags":["my_chain"],"run_id":"41cd92f8-9b8c-4365-8aa0-fda3abdae03d","metadata":{}}

#   

#   

#   event: data

#   data: {"event":"on_chat_model_start","data":{"input":{"messages":[[{"lc":1,"type":"constructor","id":["langchain_core","messages","HumanMessage"],"kwargs":{"content":"Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key \"name\" and \"population\"","additional_kwargs":{},"response_metadata":{}}}]]}},"name":"ChatOpenAI","tags":["seq:step:1","my_chain"],"run_id":"a6c2bc61-c868-4570-a143-164e64529ee0","metadata":{"ls_provider":"openai","ls_model_name":"gpt-4o","ls_model_type":"chat","ls_temperature":1}}

#   

#   

#   event: data

#   data: {"event":"on_parser_start","data":{},"name":"my_parser","tags":["seq:step:2","my_chain"],"run_id":"402533c5-0e4e-425d-a556-c30a350972d0","metadata":{}}

#   

#   

#   event: data

#   data: {"event":"on_chat_model_stream","data":{"chunk":{"lc":1,"type":"constructor","id":["langchain_core","messages","AIMessageChunk"],"kwargs":{"content":"","tool_call_chunks":[],"additional_kwargs":{},"id":"chatcmpl-9lO9BAQwbKDy2Ou2RNFUVi0VunAsL","tool_calls":[],"invalid_tool_calls":[],"response_metadata":{"prompt":0,"completion":0,"finish_reason":null}}}},"run_id":"a6c2bc61-c868-4570-a143-164e64529ee0","name":"ChatOpenAI","tags":["seq:step:1","my_chain"],"metadata":{"ls_provider":"openai","ls_model_name":"gpt-4o","ls_model_type":"chat","ls_temperature":1}}

#   

#   


"""
A nice feature of this format is that you can pass the resulting stream directly into a native [HTTP response object](https://developer.mozilla.org/en-US/docs/Web/API/Response) with the correct headers (commonly used by frameworks like [Hono](https://hono.dev/) and [Next.js](https://nextjs.org/)), then parse that stream on the frontend. Your server-side handler would look something like this:
"""

const handler = async () => {
  const eventStream = await chain.streamEvents(
    `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`,
    {
      version: "v2",
      encoding: "text/event-stream",
    },
  );
  return new Response(eventStream, {
    headers: {
      "content-type": "text/event-stream",
    }
  });
};

"""
And your frontend could look like this (using the [`@microsoft/fetch-event-source`](https://www.npmjs.com/package/@microsoft/fetch-event-source) pacakge to fetch and parse the event source):
"""

import { fetchEventSource } from "@microsoft/fetch-event-source";

const makeChainRequest = async () => {
  await fetchEventSource("https://your_url_here", {
    method: "POST",
    body: JSON.stringify({
      foo: 'bar'
    }),
    onmessage: (message) => {
      if (message.event === "data") {
        console.log(message.data);
      }
    },
    onerror: (err) => {
      console.log(err);
    }
  });
};

"""
### Non-streaming components

Remember how some components don't stream well because they don't operate on **input streams**?

While such components can break streaming of the final output when using `stream`, `streamEvents` will still yield streaming events from intermediate steps that support streaming!
"""

// A function that operates on finalized inputs
// rather than on an input_stream
import { JsonOutputParser } from "@langchain/core/output_parsers"
import { RunnablePassthrough } from "@langchain/core/runnables";

// A function that does not operates on input streams and breaks streaming.
const extractCountryNames = (inputs: Record<string, any>) => {
  if (!Array.isArray(inputs.countries)) {
    return "";
  }
  return JSON.stringify(inputs.countries.map((country) => country.name));
}

const chain = model.pipe(new JsonOutputParser()).pipe(extractCountryNames);

const stream = await chain.stream(
  `output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`
);

for await (const chunk of stream) {
  console.log(chunk);
}
# Output:
#   ["France","Spain","Japan"]


"""
As expected, the `stream` API doesn't work correctly because `extractCountryNames` doesn't operate on streams.

Now, let's confirm that with `streamEvents` we're still seeing streaming output from the model and the parser.
"""

const eventStream = await chain.streamEvents(
  `output a list of the countries france, spain and japan and their populations in JSON format.
Use a dict with an outer key of "countries" which contains a list of countries.
Each country should have the key "name" and "population"
Your output should ONLY contain valid JSON data. Do not include any other text or content in your output.`,
  { version: "v2" },
);

let eventCount = 0;

for await (const event of eventStream) {
  // Truncate the output
  if (eventCount > 30) {
    continue;
  }
  const eventType = event.event;
  if (eventType === "on_chat_model_stream") {
    console.log(`Chat model chunk: ${event.data.chunk.message.content}`);
  } else if (eventType === "on_parser_stream") {
    console.log(`Parser chunk: ${JSON.stringify(event.data.chunk)}`);
  } else {
    console.log(eventType)
  }
  eventCount += 1;
}

"""
Chat model chunk:
Chat model chunk: Here's
Chat model chunk:  how
Chat model chunk:  you
Chat model chunk:  can
Chat model chunk:  represent
Chat model chunk:  the
Chat model chunk:  countries
Chat model chunk:  France
Chat model chunk: ,
Chat model chunk:  Spain
Chat model chunk: ,
Chat model chunk:  and
Chat model chunk:  Japan
Chat model chunk: ,
Chat model chunk:  along
Chat model chunk:  with
Chat model chunk:  their
Chat model chunk:  populations
Chat model chunk: ,
Chat model chunk:  in
Chat model chunk:  JSON
Chat model chunk:  format
Chat model chunk: :


Chat model chunk: ```
Chat model chunk: json
Chat model chunk:

Chat model chunk: {
"""

"""
## Related

- [Dispatching custom events](/docs/how_to/callbacks_custom_events)
"""



================================================
FILE: docs/core_docs/docs/how_to/streaming_llm.mdx
================================================
---
sidebar_position: 1
---

# How to stream responses from an LLM

All [`LLM`s](https://api.js.langchain.com/classes/langchain_core.language_models_llms.BaseLLM.html) implement the [Runnable interface](https://api.js.langchain.com/classes/langchain_core.runnables.Runnable.html), which comes with **default** implementations of standard runnable methods (i.e. `ainvoke`, `batch`, `abatch`, `stream`, `astream`, `astream_events`).

The **default** streaming implementations provide an `AsyncGenerator` that yields a single value: the final output from the underlying chat model provider.

The ability to stream the output token-by-token depends on whether the provider has implemented proper streaming support.

See which [integrations support token-by-token streaming here](/docs/integrations/llms/).

:::{.callout-note}

The **default** implementation does **not** provide support for token-by-token streaming, but it ensures that the model can be swapped in for any other model as it supports the same standard interface.

:::

## Using `.stream()`

import CodeBlock from "@theme/CodeBlock";

The easiest way to stream is to use the `.stream()` method. This returns an readable stream that you can also iterate over:

import StreamMethodExample from "@examples/models/llm/llm_streaming_stream_method.ts";

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core
```

<CodeBlock language="typescript">{StreamMethodExample}</CodeBlock>

For models that do not support streaming, the entire response will be returned as a single chunk.

## Using a callback handler

You can also use a [`CallbackHandler`](https://api.js.langchain.com/classes/langchain_core.callbacks_base.BaseCallbackHandler.html) like so:

import StreamingExample from "@examples/models/llm/llm_streaming.ts";

<CodeBlock language="typescript">{StreamingExample}</CodeBlock>

We still have access to the end `LLMResult` if using `generate`. However, `tokenUsage` may not be currently supported for all model providers when streaming.



================================================
FILE: docs/core_docs/docs/how_to/structured_output.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_position: 3
---
"""

"""
# How to return structured data from a model
```{=mdx}
<span data-heading-keywords="with_structured_output"></span>
```

It is often useful to have a model return output that matches some specific schema. One common use-case is extracting data from arbitrary text to insert into a traditional database or use with some other downstrem system. This guide will show you a few different strategies you can use to do this.

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat models](/docs/concepts/chat_models)

:::

## The `.withStructuredOutput()` method

There are several strategies that models can use under the hood. For some of the most popular model providers, including [Anthropic](/docs/integrations/platforms/anthropic/), [Google VertexAI](/docs/integrations/platforms/google/), [Mistral](/docs/integrations/chat/mistral/), and [OpenAI](/docs/integrations/platforms/openai/) LangChain implements a common interface that abstracts away these strategies called `.withStructuredOutput`.

By invoking this method (and passing in [JSON schema](https://json-schema.org/) or a [Zod schema](https://zod.dev/)) the model will add whatever model parameters + output parsers are necessary to get back structured output matching the requested schema. If the model supports more than one way to do this (e.g., function calling vs JSON mode) - you can configure which method to use by passing into that method.

Let's look at some examples of this in action! We'll use Zod to create a simple response schema.

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs onlyWso={true} />
```
"""

import { z } from "zod";

const joke = z.object({
  setup: z.string().describe("The setup of the joke"),
  punchline: z.string().describe("The punchline to the joke"),
  rating: z.number().optional().describe("How funny the joke is, from 1 to 10"),
});

const structuredLlm = model.withStructuredOutput(joke);

await structuredLlm.invoke("Tell me a joke about cats")
# Output:
#   {

#     setup: [32m"Why don't cats play poker in the wild?"[39m,

#     punchline: [32m"Too many cheetahs."[39m,

#     rating: [33m7[39m

#   }

"""
One key point is that though we set our Zod schema as a variable named `joke`, Zod is not able to access that variable name, and therefore cannot pass it to the model. Though it is not required, we can pass a name for our schema in order to give the model additional context as to what our schema represents, improving performance:
"""

const structuredLlm = model.withStructuredOutput(joke, { name: "joke" });

await structuredLlm.invoke("Tell me a joke about cats")
# Output:
#   {

#     setup: [32m"Why don't cats play poker in the wild?"[39m,

#     punchline: [32m"Too many cheetahs!"[39m,

#     rating: [33m7[39m

#   }

"""
The result is a JSON object.

We can also pass in an OpenAI-style JSON schema dict if you prefer not to use Zod. This object should contain three properties:

- `name`: The name of the schema to output.
- `description`: A high level description of the schema to output.
- `parameters`: The nested details of the schema you want to extract, formatted as a [JSON schema](https://json-schema.org/) dict.

In this case, the response is also a dict:
"""

const structuredLlm = model.withStructuredOutput(
  {
    "name": "joke",
    "description": "Joke to tell user.",
    "parameters": {
      "title": "Joke",
      "type": "object",
      "properties": {
        "setup": {"type": "string", "description": "The setup for the joke"},
        "punchline": {"type": "string", "description": "The joke's punchline"},
      },
      "required": ["setup", "punchline"],
    },
  }
)

await structuredLlm.invoke("Tell me a joke about cats", { name: "joke" })
# Output:
#   {

#     setup: [32m"Why was the cat sitting on the computer?"[39m,

#     punchline: [32m"Because it wanted to keep an eye on the mouse!"[39m

#   }

"""
If you are using JSON Schema, you can take advantage of other more complex schema descriptions to create a similar effect.

You can also use tool calling directly to allow the model to choose between options, if your chosen model supports it. This involves a bit more parsing and setup. See [this how-to guide](/docs/how_to/tool_calling/) for more details.
"""

"""
### Specifying the output method (Advanced)

For models that support more than one means of outputting data, you can specify the preferred one like this:
"""

const structuredLlm = model.withStructuredOutput(joke, {
  method: "json_mode",
  name: "joke",
})

await structuredLlm.invoke(
  "Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys"
)
# Output:
#   {

#     setup: [32m"Why don't cats play poker in the jungle?"[39m,

#     punchline: [32m"Too many cheetahs!"[39m

#   }

"""
In the above example, we use OpenAI's alternate JSON mode capability along with a more specific prompt.

For specifics about the model you choose, peruse its entry in the [API reference pages](https://api.js.langchain.com/).

### (Advanced) Raw outputs

LLMs aren't perfect at generating structured output, especially as schemas become complex. You can avoid raising exceptions and handle the raw output yourself by passing `includeRaw: true`. This changes the output format to contain the raw message output and the `parsed` value (if successful):
"""

const joke = z.object({
  setup: z.string().describe("The setup of the joke"),
  punchline: z.string().describe("The punchline to the joke"),
  rating: z.number().optional().describe("How funny the joke is, from 1 to 10"),
});

const structuredLlm = model.withStructuredOutput(joke, { includeRaw: true, name: "joke" });

await structuredLlm.invoke("Tell me a joke about cats");
# Output:
#   {

#     raw: AIMessage {

#       lc_serializable: [33mtrue[39m,

#       lc_kwargs: {

#         content: [32m""[39m,

#         tool_calls: [

#           {

#             name: [32m"joke"[39m,

#             args: [36m[Object][39m,

#             id: [32m"call_0pEdltlfSXjq20RaBFKSQOeF"[39m

#           }

#         ],

#         invalid_tool_calls: [],

#         additional_kwargs: { function_call: [90mundefined[39m, tool_calls: [ [36m[Object][39m ] },

#         response_metadata: {}

#       },

#       lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#       content: [32m""[39m,

#       name: [90mundefined[39m,

#       additional_kwargs: {

#         function_call: [90mundefined[39m,

#         tool_calls: [

#           {

#             id: [32m"call_0pEdltlfSXjq20RaBFKSQOeF"[39m,

#             type: [32m"function"[39m,

#             function: [36m[Object][39m

#           }

#         ]

#       },

#       response_metadata: {

#         tokenUsage: { completionTokens: [33m33[39m, promptTokens: [33m88[39m, totalTokens: [33m121[39m },

#         finish_reason: [32m"stop"[39m

#       },

#       tool_calls: [

#         {

#           name: [32m"joke"[39m,

#           args: {

#             setup: [32m"Why was the cat sitting on the computer?"[39m,

#             punchline: [32m"Because it wanted to keep an eye on the mouse!"[39m,

#             rating: [33m7[39m

#           },

#           id: [32m"call_0pEdltlfSXjq20RaBFKSQOeF"[39m

#         }

#       ],

#       invalid_tool_calls: [],

#       usage_metadata: { input_tokens: [33m88[39m, output_tokens: [33m33[39m, total_tokens: [33m121[39m }

#     },

#     parsed: {

#       setup: [32m"Why was the cat sitting on the computer?"[39m,

#       punchline: [32m"Because it wanted to keep an eye on the mouse!"[39m,

#       rating: [33m7[39m

#     }

#   }

"""
## Prompting techniques

You can also prompt models to outputting information in a given format. This approach relies on designing good prompts and then parsing the output of the models. This is the only option for models that don't support `.with_structured_output()` or other built-in approaches.

### Using `JsonOutputParser`

The following example uses the built-in [`JsonOutputParser`](https://api.js.langchain.com/classes/langchain_core.output_parsers.JsonOutputParser.html) to parse the output of a chat model prompted to match a the given JSON schema. Note that we are adding `format_instructions` directly to the prompt from a method on the parser:
"""

import { JsonOutputParser } from "@langchain/core/output_parsers";
import { ChatPromptTemplate } from "@langchain/core/prompts";

type Person = {
    name: string;
    height_in_meters: number;
};

type People = {
    people: Person[];
};

const formatInstructions = `Respond only in valid JSON. The JSON object you return should match the following schema:
{{ people: [{{ name: "string", height_in_meters: "number" }}] }}

Where people is an array of objects, each with a name and height_in_meters field.
`

// Set up a parser
const parser = new JsonOutputParser<People>();

// Prompt
const prompt = await ChatPromptTemplate.fromMessages(
    [
        [
            "system",
            "Answer the user query. Wrap the output in `json` tags\n{format_instructions}",
        ],
        [
            "human",
            "{query}",
        ]
    ]
).partial({
    format_instructions: formatInstructions,
})

"""
Let’s take a look at what information is sent to the model:
"""

const query = "Anna is 23 years old and she is 6 feet tall"

console.log((await prompt.format({ query })).toString())
# Output:
#   System: Answer the user query. Wrap the output in `json` tags

#   Respond only in valid JSON. The JSON object you return should match the following schema:

#   {{ people: [{{ name: "string", height_in_meters: "number" }}] }}

#   

#   Where people is an array of objects, each with a name and height_in_meters field.

#   

#   Human: Anna is 23 years old and she is 6 feet tall


"""
And now let's invoke it:
"""

const chain = prompt.pipe(model).pipe(parser);

await chain.invoke({ query })
# Output:
#   { people: [ { name: [32m"Anna"[39m, height_in_meters: [33m1.83[39m } ] }

"""
For a deeper dive into using output parsers with prompting techniques for structured output, see [this guide](/docs/how_to/output_parser_structured).

### Custom Parsing

You can also create a custom prompt and parser with [LangChain Expression Language (LCEL)](/docs/concepts/lcel), using a plain function to parse the output from the model:
"""

import { AIMessage } from "@langchain/core/messages";
import { ChatPromptTemplate } from "@langchain/core/prompts";

type Person = {
    name: string;
    height_in_meters: number;
};

type People = {
    people: Person[];
};

const schema = `{{ people: [{{ name: "string", height_in_meters: "number" }}] }}`

// Prompt
const prompt = await ChatPromptTemplate.fromMessages(
    [
        [
            "system",
            `Answer the user query. Output your answer as JSON that
matches the given schema: \`\`\`json\n{schema}\n\`\`\`.
Make sure to wrap the answer in \`\`\`json and \`\`\` tags`
        ],
        [
            "human",
            "{query}",
        ]
    ]
).partial({
    schema
});

/**
 * Custom extractor
 * 
 * Extracts JSON content from a string where
 * JSON is embedded between ```json and ``` tags.
 */
const extractJson = (output: AIMessage): Array<People> => {
    const text = output.content as string;
    // Define the regular expression pattern to match JSON blocks
    const pattern = /```json(.*?)```/gs;

    // Find all non-overlapping matches of the pattern in the string
    const matches = text.match(pattern);

    // Process each match, attempting to parse it as JSON
    try {
        return matches?.map(match => {
            // Remove the markdown code block syntax to isolate the JSON string
            const jsonStr = match.replace(/```json|```/g, '').trim();
            return JSON.parse(jsonStr);
        }) ?? [];
    } catch (error) {
        throw new Error(`Failed to parse: ${output}`);
    }
}

"""
Here is the prompt sent to the model:
"""

const query = "Anna is 23 years old and she is 6 feet tall"

console.log((await prompt.format({ query })).toString())
# Output:
#   System: Answer the user query. Output your answer as JSON that

#   matches the given schema: ```json

#   {{ people: [{{ name: "string", height_in_meters: "number" }}] }}

#   ```.

#   Make sure to wrap the answer in ```json and ``` tags

#   Human: Anna is 23 years old and she is 6 feet tall


"""
And here's what it looks like when we invoke it:
"""

import { RunnableLambda } from "@langchain/core/runnables";

const chain = prompt.pipe(model).pipe(new RunnableLambda({ func: extractJson }));

await chain.invoke({ query })
# Output:
#   [

#     { people: [ { name: [32m"Anna"[39m, height_in_meters: [33m1.83[39m } ] }

#   ]

"""
## Next steps

Now you've learned a few methods to make a model output structured data.

To learn more, check out the other how-to guides in this section, or the conceptual guide on tool calling.
"""



================================================
FILE: docs/core_docs/docs/how_to/time_weighted_vectorstore.mdx
================================================
# How to create a time-weighted retriever

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Retrievers](/docs/concepts/retrievers)
- [Vector stores](/docs/concepts/#vectorstores)
- [Retrieval-augmented generation (RAG)](/docs/tutorials/rag)

:::

This guide covers the [`TimeWeightedVectorStoreRetriever`](https://api.js.langchain.com/classes/langchain.retrievers_time_weighted.TimeWeightedVectorStoreRetriever.html),
which uses a combination of semantic similarity and a time decay.

The algorithm for scoring them is:

```
semantic_similarity + (1.0 - decay_rate) ^ hours_passed
```

Notably, `hours_passed` refers to the hours passed since the object in the retriever **was last accessed**, not since it was created. This means that frequently accessed objects remain "fresh."

```typescript
let score = (1.0 - this.decayRate) ** hoursPassed + vectorRelevance;
```

`this.decayRate` is a configurable decimal number between 0 and 1. A lower number means that documents will be "remembered" for longer, while a higher number strongly weights more recently accessed documents.

Note that setting a decay rate of exactly 0 or 1 makes `hoursPassed` irrelevant and makes this retriever equivalent to a standard vector lookup.

It is important to note that due to required metadata, all documents must be added to the backing vector store using the `addDocuments` method on the **retriever**, not the vector store itself.

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/retrievers/time-weighted-retriever.ts";

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core
```

<CodeBlock language="typescript">{Example}</CodeBlock>

## Next steps

You've now learned how to use time as a factor when performing retrieval.

Next, check out the [broader tutorial on RAG](/docs/tutorials/rag), or this section to learn how to
[create your own custom retriever over any data source](/docs/how_to/custom_retriever/).



================================================
FILE: docs/core_docs/docs/how_to/tool_artifacts.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to return artifacts from a tool

```{=mdx}
:::info Prerequisites
This guide assumes familiarity with the following concepts:

- [ToolMessage](/docs/concepts/messages/#toolmessage)
- [Tools](/docs/concepts/tools)
- [Tool calling](/docs/concepts/tool_calling)

:::
```

Tools are utilities that can be called by a model, and whose outputs are designed to be fed back to a model. Sometimes, however, there are artifacts of a tool's execution that we want to make accessible to downstream components in our chain or agent, but that we don't want to expose to the model itself.

For example if a tool returns something like a custom object or an image, we may want to pass some metadata about this output to the model without passing the actual output to the model. At the same time, we may want to be able to access this full output elsewhere, for example in downstream tools.

The Tool and [ToolMessage](https://api.js.langchain.com/classes/langchain_core.messages_tool.ToolMessage.html) interfaces make it possible to distinguish between the parts of the tool output meant for the model (this is the `ToolMessage.content`) and those parts which are meant for use outside the model (`ToolMessage.artifact`).

```{=mdx}
:::caution Compatibility

This functionality requires `@langchain/core>=0.2.16`. Please see here for a [guide on upgrading](/docs/how_to/installation/#installing-integration-packages).

:::
```

## Defining the tool

If we want our tool to distinguish between message content and other artifacts, we need to specify `response_format: "content_and_artifact"` when defining our tool and make sure that we return a tuple of [`content`, `artifact`]:
"""

import { z } from "zod";
import { tool } from "@langchain/core/tools";

const randomIntToolSchema = z.object({
  min: z.number(),
  max: z.number(),
  size: z.number(),
});

const generateRandomInts = tool(async ({ min, max, size }) => {
  const array: number[] = [];
  for (let i = 0; i < size; i++) {
    array.push(Math.floor(Math.random() * (max - min + 1)) + min);
  }
  return [
    `Successfully generated array of ${size} random ints in [${min}, ${max}].`,
    array,
  ];
}, {
  name: "generateRandomInts",
  description: "Generate size random ints in the range [min, max].",
  schema: randomIntToolSchema,
  responseFormat: "content_and_artifact",
});

"""
## Invoking the tool with ToolCall

If we directly invoke our tool with just the tool arguments, you'll notice that we only get back the content part of the `Tool` output:
"""

await generateRandomInts.invoke({min: 0, max: 9, size: 10});
# Output:
#   Successfully generated array of 10 random ints in [0, 9].


"""
In order to get back both the content and the artifact, we need to invoke our model with a `ToolCall` (which is just a dictionary with `"name"`, `"args"`, `"id"` and `"type"` keys), which has additional info needed to generate a ToolMessage like the tool call ID:
"""

await generateRandomInts.invoke(
  {
    name: "generate_random_ints",
    args: {min: 0, max: 9, size: 10},
    id: "123", // Required
    type: "tool_call", // Required
  }
);
# Output:
#   ToolMessage {

#     lc_serializable: true,

#     lc_kwargs: {

#       content: 'Successfully generated array of 10 random ints in [0, 9].',

#       artifact: [

#         0, 6, 5, 5, 7,

#         0, 6, 3, 7, 5

#       ],

#       tool_call_id: '123',

#       name: 'generateRandomInts',

#       additional_kwargs: {},

#       response_metadata: {}

#     },

#     lc_namespace: [ 'langchain_core', 'messages' ],

#     content: 'Successfully generated array of 10 random ints in [0, 9].',

#     name: 'generateRandomInts',

#     additional_kwargs: {},

#     response_metadata: {},

#     id: undefined,

#     tool_call_id: '123',

#     artifact: [

#       0, 6, 5, 5, 7,

#       0, 6, 3, 7, 5

#     ]

#   }


"""
## Using with a model

With a [tool-calling model](/docs/how_to/tool_calling/), we can easily use a model to call our Tool and generate ToolMessages:

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs
  customVarName="llm"
/>
```
"""

const llmWithTools = llm.bindTools([generateRandomInts])

const aiMessage = await llmWithTools.invoke("generate 6 positive ints less than 25")
aiMessage.tool_calls
# Output:
#   [

#     {

#       name: 'generateRandomInts',

#       args: { min: 1, max: 24, size: 6 },

#       id: 'toolu_019ygj3YuoU6qFzR66juXALp',

#       type: 'tool_call'

#     }

#   ]


await generateRandomInts.invoke(aiMessage.tool_calls[0])
# Output:
#   ToolMessage {

#     lc_serializable: true,

#     lc_kwargs: {

#       content: 'Successfully generated array of 6 random ints in [1, 24].',

#       artifact: [ 18, 20, 16, 15, 17, 19 ],

#       tool_call_id: 'toolu_019ygj3YuoU6qFzR66juXALp',

#       name: 'generateRandomInts',

#       additional_kwargs: {},

#       response_metadata: {}

#     },

#     lc_namespace: [ 'langchain_core', 'messages' ],

#     content: 'Successfully generated array of 6 random ints in [1, 24].',

#     name: 'generateRandomInts',

#     additional_kwargs: {},

#     response_metadata: {},

#     id: undefined,

#     tool_call_id: 'toolu_019ygj3YuoU6qFzR66juXALp',

#     artifact: [ 18, 20, 16, 15, 17, 19 ]

#   }


"""
If we just pass in the tool call args, we'll only get back the content:
"""

await generateRandomInts.invoke(aiMessage.tool_calls[0]["args"])
# Output:
#   Successfully generated array of 6 random ints in [1, 24].


"""
If we wanted to declaratively create a chain, we could do this:
"""

const extractToolCalls = (aiMessage) => aiMessage.tool_calls;

const chain = llmWithTools.pipe(extractToolCalls).pipe(generateRandomInts.map());

await chain.invoke("give me a random number between 1 and 5");
# Output:
#   [

#     ToolMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: 'Successfully generated array of 1 random ints in [1, 5].',

#         artifact: [Array],

#         tool_call_id: 'toolu_01CskofJCQW8chkUzmVR1APU',

#         name: 'generateRandomInts',

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: 'Successfully generated array of 1 random ints in [1, 5].',

#       name: 'generateRandomInts',

#       additional_kwargs: {},

#       response_metadata: {},

#       id: undefined,

#       tool_call_id: 'toolu_01CskofJCQW8chkUzmVR1APU',

#       artifact: [ 1 ]

#     }

#   ]


"""
## Related

You've now seen how to return additional artifacts from a tool call.

These guides may interest you next:

- [Creating custom tools](/docs/how_to/custom_tools)
- [Building agents with LangGraph](https://langchain-ai.github.io/langgraphjs/)
"""



================================================
FILE: docs/core_docs/docs/how_to/tool_calling.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
keywords: [function, function calling, tool, tool call, tool calling]
---
"""

"""
# How to use chat models to call tools

```{=mdx}
:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat models](/docs/concepts/chat_models)
- [LangChain Tools](/docs/concepts/tools)
- [Tool calling](/docs/concepts/tool_calling)

:::
```

[Tool calling](/docs/concepts/tool_calling) allows a chat model to respond to a given prompt by "calling a tool".

Remember, while the name "tool calling" implies that the model is directly performing some action, this is actually not the case! The model only generates the arguments to a tool, and actually running the tool (or not) is up to the user.

Tool calling is a general technique that generates structured output from a model, and you can use it even when you don't intend to invoke any tools. An example use-case of that is [extraction from unstructured text](/docs/tutorials/extraction/).

![](../../static/img/tool_call.png)

If you want to see how to use the model-generated tool call to actually run a tool function [check out this guide](/docs/how_to/tool_results_pass_to_model/).

```{=mdx}
:::note Supported models

Tool calling is not universal, but is supported by many popular LLM providers, including [Anthropic](/docs/integrations/chat/anthropic/), 
[Cohere](/docs/integrations/chat/cohere/), [Google](/docs/integrations/chat/google_vertex_ai/), 
[Mistral](/docs/integrations/chat/mistral/), [OpenAI](/docs/integrations/chat/openai/), and even for locally-running models via [Ollama](/docs/integrations/chat/ollama/).

You can find a [list of all models that support tool calling here](/docs/integrations/chat/).

:::
```

LangChain implements standard interfaces for defining tools, passing them to LLMs, and representing tool calls.
This guide will cover how to bind tools to an LLM, then invoke the LLM to generate these arguments.

LangChain implements standard interfaces for defining tools, passing them to LLMs, 
and representing tool calls. This guide will show you how to use them.
"""

"""
## Passing tools to chat models

Chat models that support tool calling features implement a [`.bindTools()`](https://api.js.langchain.com/classes/langchain_core.language_models_chat_models.BaseChatModel.html#bindTools) method, which 
receives a list of LangChain [tool objects](https://api.js.langchain.com/classes/langchain_core.tools.StructuredTool.html)
and binds them to the chat model in its expected format. Subsequent invocations of the 
chat model will include tool schemas in its calls to the LLM.

```{=mdx}
:::note
As of `@langchain/core` version `0.2.9`, all chat models with tool calling capabilities now support [OpenAI-formatted tools](https://api.js.langchain.com/interfaces/langchain_core.language_models_base.ToolDefinition.html).
:::
```

Let's walk through an example:
"""

"""
```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" providers={["anthropic", "openai", "mistral", "fireworks"]} additionalDependencies="@langchain/core" />
```
"""

"""
We can use the `.bindTools()` method to handle the conversion from LangChain tool to our model provider's specific format and bind it to the model (i.e., passing it in each time the model is invoked). A number of models implement helper methods that will take care of formatting and binding different function-like objects to the model.
Let's create a new tool implementing a Zod schema, then bind it to the model:

```{=mdx}
:::note
The `tool` function is available in `@langchain/core` version 0.2.7 and above.

If you are on an older version of core, you should use instantiate and use [`DynamicStructuredTool`](https://api.js.langchain.com/classes/langchain_core.tools.DynamicStructuredTool.html) instead.
:::
```
"""

import { tool } from "@langchain/core/tools";
import { z } from "zod";

/**
 * Note that the descriptions here are crucial, as they will be passed along
 * to the model along with the class name.
 */
const calculatorSchema = z.object({
  operation: z
    .enum(["add", "subtract", "multiply", "divide"])
    .describe("The type of operation to execute."),
  number1: z.number().describe("The first number to operate on."),
  number2: z.number().describe("The second number to operate on."),
});

const calculatorTool = tool(async ({ operation, number1, number2 }) => {
  // Functions must return strings
  if (operation === "add") {
    return `${number1 + number2}`;
  } else if (operation === "subtract") {
    return `${number1 - number2}`;
  } else if (operation === "multiply") {
    return `${number1 * number2}`;
  } else if (operation === "divide") {
    return `${number1 / number2}`;
  } else {
    throw new Error("Invalid operation.");
  }
}, {
  name: "calculator",
  description: "Can perform mathematical operations.",
  schema: calculatorSchema,
});

const llmWithTools = llm.bindTools([calculatorTool]);

"""
Now, let's invoke it! We expect the model to use the calculator to answer the question:
"""

const res = await llmWithTools.invoke("What is 3 * 12");

console.log(res);
# Output:
#   AIMessage {

#     "id": "chatcmpl-9p1Ib4xfxV4yahv2ZWm1IRb1fRVD7",

#     "content": "",

#     "additional_kwargs": {

#       "tool_calls": [

#         {

#           "id": "call_CrZkMP0AvUrz7w9kim0splbl",

#           "type": "function",

#           "function": "[Object]"

#         }

#       ]

#     },

#     "response_metadata": {

#       "tokenUsage": {

#         "completionTokens": 24,

#         "promptTokens": 93,

#         "totalTokens": 117

#       },

#       "finish_reason": "tool_calls",

#       "system_fingerprint": "fp_400f27fa1f"

#     },

#     "tool_calls": [

#       {

#         "name": "calculator",

#         "args": {

#           "operation": "multiply",

#           "number1": 3,

#           "number2": 12

#         },

#         "type": "tool_call",

#         "id": "call_CrZkMP0AvUrz7w9kim0splbl"

#       }

#     ],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 93,

#       "output_tokens": 24,

#       "total_tokens": 117

#     }

#   }


"""
As we can see our LLM generated arguments to a tool!

**Note:** If you are finding that the model does not call a desired tool for a given prompt, you can see [this guide on how to force the LLM to call a tool](/docs/how_to/tool_choice/) rather than letting it decide.

```{=mdx}
:::tip
See a LangSmith trace for the above [here](https://smith.langchain.com/public/b2222205-7da9-4a5a-8efe-6bc62347705d/r).
:::
```
"""

"""
## Tool calls

If tool calls are included in a LLM response, they are attached to the corresponding 
[message](https://api.js.langchain.com/classes/langchain_core.messages.AIMessage.html) 
or [message chunk](https://api.js.langchain.com/classes/langchain_core.messages.AIMessageChunk.html) 
as a list of [tool call](https://api.js.langchain.com/types/langchain_core.messages_tool.ToolCall.html) 
objects in the `.tool_calls` attribute.

A `ToolCall` is a typed dict that includes a 
tool name, dict of argument values, and (optionally) an identifier. Messages with no 
tool calls default to an empty list for this attribute.

Chat models can call multiple tools at once. Here's an example:
"""

const res = await llmWithTools.invoke("What is 3 * 12? Also, what is 11 + 49?");

res.tool_calls;
# Output:
#   [

#     {

#       name: 'calculator',

#       args: { operation: 'multiply', number1: 3, number2: 12 },

#       type: 'tool_call',

#       id: 'call_01lvdk2COLV2hTjRUNAX8XWH'

#     },

#     {

#       name: 'calculator',

#       args: { operation: 'add', number1: 11, number2: 49 },

#       type: 'tool_call',

#       id: 'call_fB0vo8VC2HRojZcj120xIBxM'

#     }

#   ]


"""
The `.tool_calls` attribute should contain valid tool calls. Note that on occasion, 
model providers may output malformed tool calls (e.g., arguments that are not 
valid JSON). When parsing fails in these cases, instances 
of [`InvalidToolCall`](https://api.js.langchain.com/types/langchain_core.messages_tool.InvalidToolCall.html) 
are populated in the `.invalid_tool_calls` attribute. An `InvalidToolCall` can have 
a name, string arguments, identifier, and error message.
"""

"""
## Binding model-specific formats (advanced)

Providers adopt different conventions for formatting tool schemas. For instance, OpenAI uses a format like this:

- `type`: The type of the tool. At the time of writing, this is always "function".
- `function`: An object containing tool parameters.
- `function.name`: The name of the schema to output.
- `function.description`: A high level description of the schema to output.
- `function.parameters`: The nested details of the schema you want to extract, formatted as a [JSON schema](https://json-schema.org/) object.

We can bind this model-specific format directly to the model if needed. Here's an example:
"""

import { ChatOpenAI } from "@langchain/openai";

const model = new ChatOpenAI({ model: "gpt-4o" });

const modelWithTools = model.bind({
  tools: [{
    "type": "function",
    "function": {
      "name": "calculator",
      "description": "Can perform mathematical operations.",
      "parameters": {
        "type": "object",
        "properties": {
          "operation": {
            "type": "string",
            "description": "The type of operation to execute.",
            "enum": ["add", "subtract", "multiply", "divide"]
          },
          "number1": {"type": "number", "description": "First integer"},
          "number2": {"type": "number", "description": "Second integer"},
        },
        "required": ["number1", "number2"],
      },
    },
  }],
});

await modelWithTools.invoke(`Whats 119 times 8?`);
# Output:
#   AIMessage {

#     "id": "chatcmpl-9p1IeP7mIp3jPn1wgsP92zxEfNo7k",

#     "content": "",

#     "additional_kwargs": {

#       "tool_calls": [

#         {

#           "id": "call_P5Xgyi0Y7IfisaUmyapZYT7d",

#           "type": "function",

#           "function": "[Object]"

#         }

#       ]

#     },

#     "response_metadata": {

#       "tokenUsage": {

#         "completionTokens": 24,

#         "promptTokens": 85,

#         "totalTokens": 109

#       },

#       "finish_reason": "tool_calls",

#       "system_fingerprint": "fp_400f27fa1f"

#     },

#     "tool_calls": [

#       {

#         "name": "calculator",

#         "args": {

#           "operation": "multiply",

#           "number1": 119,

#           "number2": 8

#         },

#         "type": "tool_call",

#         "id": "call_P5Xgyi0Y7IfisaUmyapZYT7d"

#       }

#     ],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 85,

#       "output_tokens": 24,

#       "total_tokens": 109

#     }

#   }


"""
This is functionally equivalent to the `bind_tools()` calls above.
"""

"""
## Next steps

Now you've learned how to bind tool schemas to a chat model and have the model call the tool.

Next, check out this guide on actually using the tool by invoking the function and passing the results back to the model:

- Pass [tool results back to model](/docs/how_to/tool_results_pass_to_model)

You can also check out some more specific uses of tool calling:

- Few shot prompting [with tools](/docs/how_to/tools_few_shot/)
- Stream [tool calls](/docs/how_to/tool_streaming/)
- Pass [runtime values to tools](/docs/how_to/tool_runtime)
- Getting [structured outputs](/docs/how_to/structured_output/) from models
"""



================================================
FILE: docs/core_docs/docs/how_to/tool_calling_parallel.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to disable parallel tool calling

```{=mdx}
:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [LangChain Tools](/docs/concepts/tools)
- [Tool calling](/docs/concepts/tool_calling)
- [Custom tools](/docs/how_to/custom_tools)

:::
```

:::info OpenAI-specific

This API is currently only supported by OpenAI.

:::

OpenAI models perform tool calling in parallel by default. That means that if we ask a question like `"What is the weather in Tokyo, New York, and Chicago?"` and we have a tool for getting the weather, it will call the tool 3 times in parallel. We can force it to call only a single tool once by using the `parallel_tool_call` call option.
"""

"""
First let's set up our tools and model:
"""

import { ChatOpenAI } from "@langchain/openai";
import { z } from "zod";
import { tool } from "@langchain/core/tools";

const adderTool = tool(async ({ a, b }) => {
  return a + b;
}, {
  name: "add",
  description: "Adds a and b",
  schema: z.object({
    a: z.number(),
    b: z.number(),
  })
});

const multiplyTool = tool(async ({ a, b }) => {
  return a * b;
}, {
  name: "multiply",
  description: "Multiplies a and b",
  schema: z.object({
    a: z.number(),
    b: z.number(),
  })
});

const tools = [adderTool, multiplyTool];

const llm = new ChatOpenAI({
  model: "gpt-4o-mini",
  temperature: 0,
});

"""
Now let's show a quick example of how disabling parallel tool calls work:
"""

const llmWithTools = llm.bindTools(tools, { parallel_tool_calls: false });

const result = await llmWithTools.invoke("Please call the first tool two times");

result.tool_calls;
# Output:
#   [

#     {

#       name: 'add',

#       args: { a: 5, b: 3 },

#       type: 'tool_call',

#       id: 'call_5bKOYerdQU6J5ERJJYnzYsGn'

#     }

#   ]


"""
As we can see, even though we explicitly told the model to call a tool twice, by disabling parallel tool calls the model was constrained to only calling one.

Compare this to calling the model without passing `parallel_tool_calls` as false:
"""

const llmWithNoBoundParam = llm.bindTools(tools);

const result2 = await llmWithNoBoundParam.invoke("Please call the first tool two times");

result2.tool_calls;
# Output:
#   [

#     {

#       name: 'add',

#       args: { a: 1, b: 2 },

#       type: 'tool_call',

#       id: 'call_Ni0tF0nNtY66BBwB5vEP6oI4'

#     },

#     {

#       name: 'add',

#       args: { a: 3, b: 4 },

#       type: 'tool_call',

#       id: 'call_XucnTCfFqP1JBs3LtbOq5w3d'

#     }

#   ]


"""
You can see that you get two tool calls.

You can also pass the parameter in at runtime like this:
"""

const result3 = await llmWithNoBoundParam.invoke("Please call the first tool two times", {
  parallel_tool_calls: false,
});

result3.tool_calls;
# Output:
#   [

#     {

#       name: 'add',

#       args: { a: 1, b: 2 },

#       type: 'tool_call',

#       id: 'call_TWo6auul71NUg1p0suzBKARt'

#     }

#   ]


"""
## Related

- [How to: create custom tools](/docs/how_to/custom_tools)
- [How to: pass run time values to tools](/docs/how_to/tool_runtime)
"""



================================================
FILE: docs/core_docs/docs/how_to/tool_calls_multimodal.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to call tools with multimodal data

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat models](/docs/concepts/chat_models)
- [LangChain Tools](/docs/concepts/tools)

:::

Here we demonstrate how to call tools with multimodal data, such as images.

Some multimodal models, such as those that can reason over images or audio, support [tool calling](/docs/concepts/#tool-calling) features as well.

To call tools using such models, simply bind tools to them in the [usual way](/docs/how_to/tool_calling), and invoke the model using content blocks of the desired type (e.g., containing image data).

Below, we demonstrate examples using [OpenAI](/docs/integrations/platforms/openai) and [Anthropic](/docs/integrations/platforms/anthropic). We will use the same image and tool in all cases. Let's first select an image, and build a placeholder tool that expects as input the string "sunny", "cloudy", or "rainy". We will ask the models to describe the weather in the image.

:::note
The `tool` function is available in `@langchain/core` version 0.2.7 and above.

If you are on an older version of core, you should use instantiate and use [`DynamicStructuredTool`](https://api.js.langchain.com/classes/langchain_core.tools.DynamicStructuredTool.html) instead.
:::
"""

import { tool } from "@langchain/core/tools";
import { z } from "zod";

const imageUrl = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg";

const weatherTool = tool(async ({ weather }) => {
  console.log(weather);
  return weather;
}, {
  name: "multiply",
  description: "Describe the weather",
  schema: z.object({
    weather: z.enum(["sunny", "cloudy", "rainy"])
  }),
});

"""
## OpenAI

For OpenAI, we can feed the image URL directly in a content block of type "image_url":
"""

import { HumanMessage } from "@langchain/core/messages";
import { ChatOpenAI } from "@langchain/openai";

const model = new ChatOpenAI({
  model: "gpt-4o",
}).bindTools([weatherTool]);

const message = new HumanMessage({
  content: [
    {
      type: "text",
      text: "describe the weather in this image"
    },
    {
      type: "image_url",
      image_url: {
        url: imageUrl
      }
    }
  ],
});

const response = await model.invoke([message]);

console.log(response.tool_calls);
# Output:
#   [

#     {

#       name: "multiply",

#       args: { weather: "sunny" },

#       id: "call_ZaBYUggmrTSuDjcuZpMVKpMR"

#     }

#   ]


"""
Note that we recover tool calls with parsed arguments in LangChain's [standard format](/docs/how_to/tool_calling) in the model response.
"""

"""
## Anthropic

For Anthropic, we can format a base64-encoded image into a content block of type "image", as below:
"""

import * as fs from "node:fs/promises";

import { ChatAnthropic } from "@langchain/anthropic";
import { HumanMessage } from "@langchain/core/messages";

const imageData = await fs.readFile("../../data/sunny_day.jpeg");

const model = new ChatAnthropic({
  model: "claude-3-sonnet-20240229",
}).bindTools([weatherTool]);

const message = new HumanMessage({
  content: [
    {
      type: "text",
      text: "describe the weather in this image",
    },
    {
      type: "image_url",
      image_url: {
        url: `data:image/jpeg;base64,${imageData.toString("base64")}`,
      },
    },
  ],
});

const response = await model.invoke([message]);

console.log(response.tool_calls);
# Output:
#   [

#     {

#       name: "multiply",

#       args: { weather: "sunny" },

#       id: "toolu_01HLY1KmXZkKMn7Ar4ZtFuAM"

#     }

#   ]


"""
## Google Generative AI

For Google GenAI, we can format a base64-encoded image into a content block of type "image", as below:
"""

import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import axios from "axios";
import { ChatPromptTemplate, MessagesPlaceholder } from "@langchain/core/prompts";
import { HumanMessage } from "@langchain/core/messages";

const axiosRes = await axios.get(imageUrl, { responseType: "arraybuffer" });
const base64 = btoa(
  new Uint8Array(axiosRes.data).reduce(
    (data, byte) => data + String.fromCharCode(byte),
    ''
  )
);

const model = new ChatGoogleGenerativeAI({ model: "gemini-1.5-pro-latest" }).bindTools([weatherTool]);

const prompt = ChatPromptTemplate.fromMessages([
  ["system", "describe the weather in this image"],
  new MessagesPlaceholder("message")
]);

const response = await prompt.pipe(model).invoke({
  message: new HumanMessage({
    content: [{
      type: "media",
      mimeType: "image/jpeg",
      data: base64,
    }]
  })
});
console.log(response.tool_calls);
# Output:
#   [ { name: 'multiply', args: { weather: 'sunny' } } ]


"""
### Audio input

Google's Gemini also supports audio inputs. In this next example we'll see how we can pass an audio file to the model, and get back a summary in structured format.
"""

import { SystemMessage } from "@langchain/core/messages";
import { tool } from "@langchain/core/tools";

const summaryTool = tool((input) => {
  return input.summary;
}, {
  name: "summary_tool",
  description: "Log the summary of the content",
  schema: z.object({
    summary: z.string().describe("The summary of the content to log")
  }),
});

const audioUrl = "https://www.pacdv.com/sounds/people_sound_effects/applause-1.wav";

const axiosRes = await axios.get(audioUrl, { responseType: "arraybuffer" });
const base64 = btoa(
  new Uint8Array(axiosRes.data).reduce(
    (data, byte) => data + String.fromCharCode(byte),
    ''
  )
);

const model = new ChatGoogleGenerativeAI({ model: "gemini-1.5-pro-latest" }).bindTools([summaryTool]);

const response = await model.invoke([
  new SystemMessage("Summarize this content. always use the summary_tool in your response"),
  new HumanMessage({
  content: [{
    type: "media",
    mimeType: "audio/wav",
    data: base64,
  }]
})]);

console.log(response.tool_calls);
# Output:
#   [

#     {

#       name: 'summary_tool',

#       args: { summary: 'The video shows a person clapping their hands.' }

#     }

#   ]




================================================
FILE: docs/core_docs/docs/how_to/tool_choice.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to force tool calling behavior

```{=mdx}

:::info Prerequisites

This guide assumes familiarity with the following concepts:
- [Chat models](/docs/concepts/chat_models)
- [LangChain Tools](/docs/concepts/tools)
- [How to use a model to call tools](/docs/how_to/tool_calling)

:::

```

In order to force our LLM to select a specific tool, we can use the `tool_choice` parameter to ensure certain behavior. First, let's define our model and tools:
"""

import { tool } from '@langchain/core/tools';
import { z } from 'zod';

const add = tool((input) => {
    return `${input.a + input.b}`
}, {
    name: "add",
    description: "Adds a and b.",
    schema: z.object({
        a: z.number(),
        b: z.number(),
    })
})

const multiply = tool((input) => {
    return `${input.a * input.b}`
}, {
    name: "Multiply",
    description: "Multiplies a and b.",
    schema: z.object({
        a: z.number(),
        b: z.number(),
    })
})

const tools = [add, multiply]

import { ChatOpenAI } from '@langchain/openai';

const llm = new ChatOpenAI({
  model: "gpt-3.5-turbo",
})

"""
For example, we can force our tool to call the multiply tool by using the following code:
"""

const llmForcedToMultiply = llm.bindTools(tools, {
  tool_choice: "Multiply",
})
const multiplyResult = await llmForcedToMultiply.invoke("what is 2 + 4");
console.log(JSON.stringify(multiplyResult.tool_calls, null, 2));
# Output:
#   [

#     {

#       "name": "Multiply",

#       "args": {

#         "a": 2,

#         "b": 4

#       },

#       "type": "tool_call",

#       "id": "call_d5isFbUkn17Wjr6yEtNz7dDF"

#     }

#   ]


"""
Even if we pass it something that doesn't require multiplcation - it will still call the tool!
"""

"""
We can also just force our tool to select at least one of our tools by passing `"any"` (or for OpenAI models, the equivalent, `"required"`) to the `tool_choice` parameter.
"""

const llmForcedToUseTool = llm.bindTools(tools, {
  tool_choice: "any",
})
const anyToolResult = await llmForcedToUseTool.invoke("What day is today?");
console.log(JSON.stringify(anyToolResult.tool_calls, null, 2));
# Output:
#   [

#     {

#       "name": "add",

#       "args": {

#         "a": 2,

#         "b": 3

#       },

#       "type": "tool_call",

#       "id": "call_La72g7Aj0XHG0pfPX6Dwg2vT"

#     }

#   ]




================================================
FILE: docs/core_docs/docs/how_to/tool_configure.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to access the RunnableConfig from a tool

```{=mdx}
:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [LangChain Tools](/docs/concepts/tools)
- [Custom tools](/docs/how_to/custom_tools)
- [LangChain Expression Language (LCEL)](/docs/concepts/lcel)

:::
```

Tools are runnables, and you can treat them the same way as any other runnable at the interface level - you can call `invoke()`, `batch()`, and `stream()` on them as normal. However, when writing custom tools, you may want to invoke other runnables like chat models or retrievers. In order to properly trace and configure those sub-invocations, you'll need to manually access and pass in the tool's current [`RunnableConfig`](https://api.js.langchain.com/interfaces/langchain_core.runnables.RunnableConfig.html) object.

This guide covers how to do this for custom tools created in different ways.

## From the `tool` method

Accessing the `RunnableConfig` object for a custom tool created with the [`tool`](https://api.js.langchain.com/functions/langchain_core.tools.tool-1.html) helper method is simple - it's always the second parameter passed into your custom function. Here's an example:
"""

import { z } from "zod";
import { tool } from "@langchain/core/tools";
import type { RunnableConfig } from "@langchain/core/runnables";

const reverseTool = tool(
  async (input: { text: string }, config?: RunnableConfig) => {
    const originalString = input.text + (config?.configurable?.additional_field ?? "");
    return originalString.split("").reverse().join("");
  }, {
    name: "reverse",
    description: "A test tool that combines input text with a configurable parameter.",
    schema: z.object({
      text: z.string()
    }),
  }
);

"""
Then, if we invoke the tool with a `config` containing a `configurable` field, we can see that `additional_field` is passed through correctly:
"""

await reverseTool.invoke(
  {text: "abc"}, {configurable: {additional_field: "123"}}
)
# Output:
#   321cba


"""
## Next steps

You've now seen how to configure and stream events from within a tool. Next, check out the following guides for more on using tools:

- Pass [tool results back to a model](/docs/how_to/tool_results_pass_to_model)
- Building [tool-using chains and agents](/docs/how_to#tools)
- Getting [structured outputs](/docs/how_to/structured_output/) from models
"""



================================================
FILE: docs/core_docs/docs/how_to/tool_results_pass_to_model.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to pass tool outputs to chat models

```{=mdx}
:::info Prerequisites
This guide assumes familiarity with the following concepts:

- [LangChain Tools](/docs/concepts/tools)
- [Tool calling](/docs/concepts/tool_calling)
- [Using chat models to call tools](/docs/how_to/tool_calling)
- [Defining custom tools](/docs/how_to/custom_tools/)

:::
```

Some models are capable of [**tool calling**](/docs/concepts/tool_calling) - generating arguments that conform to a specific user-provided schema. This guide will demonstrate how to use those tool calls to actually call a function and properly pass the results back to the model.

![](../../static/img/tool_invocation.png)

![](../../static/img/tool_results.png)

First, let's define our tools and our model:
"""

import { z } from "zod";
import { tool } from "@langchain/core/tools";

const addTool = tool(async ({ a, b }) => {
  return a + b;
}, {
  name: "add",
  schema: z.object({
    a: z.number(),
    b: z.number(),
  }),
  description: "Adds a and b.",
});

const multiplyTool = tool(async ({ a, b }) => {
  return a * b;
}, {
  name: "multiply",
  schema: z.object({
    a: z.number(),
    b: z.number(),
  }),
  description: "Multiplies a and b.",
});

const tools = [addTool, multiplyTool];

"""
```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

"""
Now, let's get the model to call a tool. We'll add it to a list of messages that we'll treat as conversation history:
"""

import { HumanMessage } from "@langchain/core/messages";

const llmWithTools = llm.bindTools(tools);

const messages = [
  new HumanMessage("What is 3 * 12? Also, what is 11 + 49?"),
];

const aiMessage = await llmWithTools.invoke(messages);

console.log(aiMessage);

messages.push(aiMessage);
# Output:
#   AIMessage {

#     "id": "chatcmpl-9p1NbC7sfZP0FE0bNfFiVYbPuWivg",

#     "content": "",

#     "additional_kwargs": {

#       "tool_calls": [

#         {

#           "id": "call_RbUuLMYf3vgcdSQ8bhy1D5Ty",

#           "type": "function",

#           "function": "[Object]"

#         },

#         {

#           "id": "call_Bzz1qgQjTlQIHMcEaDAdoH8X",

#           "type": "function",

#           "function": "[Object]"

#         }

#       ]

#     },

#     "response_metadata": {

#       "tokenUsage": {

#         "completionTokens": 50,

#         "promptTokens": 87,

#         "totalTokens": 137

#       },

#       "finish_reason": "tool_calls",

#       "system_fingerprint": "fp_400f27fa1f"

#     },

#     "tool_calls": [

#       {

#         "name": "multiply",

#         "args": {

#           "a": 3,

#           "b": 12

#         },

#         "type": "tool_call",

#         "id": "call_RbUuLMYf3vgcdSQ8bhy1D5Ty"

#       },

#       {

#         "name": "add",

#         "args": {

#           "a": 11,

#           "b": 49

#         },

#         "type": "tool_call",

#         "id": "call_Bzz1qgQjTlQIHMcEaDAdoH8X"

#       }

#     ],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 87,

#       "output_tokens": 50,

#       "total_tokens": 137

#     }

#   }

#   2


"""
Next let's invoke the tool functions using the args the model populated!

Conveniently, if we invoke a LangChain `Tool` with a `ToolCall`, we'll automatically get back a `ToolMessage` that can be fed back to the model:

```{=mdx}
:::caution Compatibility

This functionality requires `@langchain/core>=0.2.16`. Please see here for a [guide on upgrading](/docs/how_to/installation/#installing-integration-packages).

If you are on earlier versions of `@langchain/core`, you will need to access construct a `ToolMessage` manually using fields from the tool call.

:::
```
"""

const toolsByName = {
  add: addTool,
  multiply: multiplyTool,
}

for (const toolCall of aiMessage.tool_calls) {
  const selectedTool = toolsByName[toolCall.name];
  const toolMessage = await selectedTool.invoke(toolCall);
  messages.push(toolMessage);
}

console.log(messages);
# Output:
#   [

#     HumanMessage {

#       "content": "What is 3 * 12? Also, what is 11 + 49?",

#       "additional_kwargs": {},

#       "response_metadata": {}

#     },

#     AIMessage {

#       "id": "chatcmpl-9p1NbC7sfZP0FE0bNfFiVYbPuWivg",

#       "content": "",

#       "additional_kwargs": {

#         "tool_calls": [

#           {

#             "id": "call_RbUuLMYf3vgcdSQ8bhy1D5Ty",

#             "type": "function",

#             "function": "[Object]"

#           },

#           {

#             "id": "call_Bzz1qgQjTlQIHMcEaDAdoH8X",

#             "type": "function",

#             "function": "[Object]"

#           }

#         ]

#       },

#       "response_metadata": {

#         "tokenUsage": {

#           "completionTokens": 50,

#           "promptTokens": 87,

#           "totalTokens": 137

#         },

#         "finish_reason": "tool_calls",

#         "system_fingerprint": "fp_400f27fa1f"

#       },

#       "tool_calls": [

#         {

#           "name": "multiply",

#           "args": {

#             "a": 3,

#             "b": 12

#           },

#           "type": "tool_call",

#           "id": "call_RbUuLMYf3vgcdSQ8bhy1D5Ty"

#         },

#         {

#           "name": "add",

#           "args": {

#             "a": 11,

#             "b": 49

#           },

#           "type": "tool_call",

#           "id": "call_Bzz1qgQjTlQIHMcEaDAdoH8X"

#         }

#       ],

#       "invalid_tool_calls": [],

#       "usage_metadata": {

#         "input_tokens": 87,

#         "output_tokens": 50,

#         "total_tokens": 137

#       }

#     },

#     ToolMessage {

#       "content": "36",

#       "name": "multiply",

#       "additional_kwargs": {},

#       "response_metadata": {},

#       "tool_call_id": "call_RbUuLMYf3vgcdSQ8bhy1D5Ty"

#     },

#     ToolMessage {

#       "content": "60",

#       "name": "add",

#       "additional_kwargs": {},

#       "response_metadata": {},

#       "tool_call_id": "call_Bzz1qgQjTlQIHMcEaDAdoH8X"

#     }

#   ]


"""
And finally, we'll invoke the model with the tool results. The model will use this information to generate a final answer to our original query:
"""

await llmWithTools.invoke(messages);
# Output:
#   AIMessage {

#     "id": "chatcmpl-9p1NttGpWjx1cQoVIDlMhumYq12Pe",

#     "content": "3 * 12 is 36, and 11 + 49 is 60.",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "completionTokens": 19,

#         "promptTokens": 153,

#         "totalTokens": 172

#       },

#       "finish_reason": "stop",

#       "system_fingerprint": "fp_18cc0f1fa0"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 153,

#       "output_tokens": 19,

#       "total_tokens": 172

#     }

#   }


"""
Note that each `ToolMessage` must include a `tool_call_id` that matches an `id` in the original tool calls that the model generates. This helps the model match tool responses with tool calls.

Tool calling agents, like those in [LangGraph](https://langchain-ai.github.io/langgraphjs/tutorials/introduction/), use this basic flow to answer queries and solve tasks.

## Related

You've now seen how to pass tool calls back to a model.

These guides may interest you next:

- [LangGraph quickstart](https://langchain-ai.github.io/langgraphjs/tutorials/introduction/)
- Few shot prompting [with tools](/docs/how_to/tools_few_shot/)
- Stream [tool calls](/docs/how_to/tool_streaming/)
- Pass [runtime values to tools](/docs/how_to/tool_runtime)
- Getting [structured outputs](/docs/how_to/structured_output/) from models
"""



================================================
FILE: docs/core_docs/docs/how_to/tool_runtime.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to pass run time values to tools

```{=mdx}
:::info Prerequisites

This guide assumes familiarity with the following concepts:
- [Chat models](/docs/concepts/chat_models)
- [LangChain Tools](/docs/concepts/tools)
- [How to create tools](/docs/how_to/custom_tools)
- [How to use a model to call tools](/docs/how_to/tool_calling/)
:::

:::info Supported models

This how-to guide uses models with native tool calling capability.
You can find a [list of all models that support tool calling](/docs/integrations/chat/).

:::
```

You may need to bind values to a tool that are only known at runtime. For example, the tool logic may require using the ID of the user who made the request.

Most of the time, such values should not be controlled by the LLM. In fact, allowing the LLM to control the user ID may lead to a security risk.

Instead, the LLM should only control the parameters of the tool that are meant to be controlled by the LLM, while other parameters (such as user ID) should be fixed by the application logic.
"""

"""
```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs
  customVarName="llm"
/>
```
"""

// @lc-docs-hide-cell

import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({ model: "gpt-4o-mini" })

"""
## Using context variables

```{=mdx}
:::caution Compatibility
This functionality was added in `@langchain/core>=0.3.10`. If you are using the LangSmith SDK separately in your project, we also recommend upgrading to `langsmith>=0.1.65`. Please make sure your packages are up to date.

It also requires [`async_hooks`](https://nodejs.org/api/async_hooks.html) support, which is not supported in all environments.
:::
```

One way to solve this problem is by using **context variables**. Context variables are a powerful feature that allows you to set values at a higher level of your application, then access them within child runnable (such as tools) called from that level.

They work outside of traditional scoping rules, so you don't need to have a direct reference to the declared variable to access its value.

Below, we declare a tool that updates a central `userToPets` state based on a context variable called `userId`. Note that this `userId` is not part of the tool schema or input:
"""

import { z } from "zod";
import { tool } from "@langchain/core/tools";
import { getContextVariable } from "@langchain/core/context";

let userToPets: Record<string, string[]> = {};

const updateFavoritePets = tool(async (input) => {
  const userId = getContextVariable("userId");
  if (userId === undefined) {
    throw new Error(`No "userId" found in current context. Remember to call "setContextVariable('userId', value)";`);
  }
  userToPets[userId] = input.pets;
  return "update_favorite_pets called."
}, {
  name: "update_favorite_pets",
  description: "add to the list of favorite pets.",
  schema: z.object({
    pets: z.array(z.string())
  }),
});

"""
If you were to invoke the above tool before setting a context variable at a higher level, `userId` would be `undefined`:
"""

await updateFavoritePets.invoke({ pets: ["cat", "dog" ]})
# Output:
#   Error: No "userId" found in current context. Remember to call "setContextVariable('userId', value)";

#       at updateFavoritePets.name (evalmachine.<anonymous>:14:15)

#       at /Users/jacoblee/langchain/langchainjs/langchain-core/dist/tools/index.cjs:329:33

#       at AsyncLocalStorage.run (node:async_hooks:346:14)

#       at AsyncLocalStorageProvider.runWithConfig (/Users/jacoblee/langchain/langchainjs/langchain-core/dist/singletons/index.cjs:58:24)

#       at /Users/jacoblee/langchain/langchainjs/langchain-core/dist/tools/index.cjs:325:68

#       at new Promise (<anonymous>)

#       at DynamicStructuredTool.func (/Users/jacoblee/langchain/langchainjs/langchain-core/dist/tools/index.cjs:321:20)

#       at DynamicStructuredTool._call (/Users/jacoblee/langchain/langchainjs/langchain-core/dist/tools/index.cjs:283:21)

#       at DynamicStructuredTool.call (/Users/jacoblee/langchain/langchainjs/langchain-core/dist/tools/index.cjs:111:33)

#       at async evalmachine.<anonymous>:3:22


"""
Instead, set a context variable with a parent of where the tools are invoked:
"""

import { setContextVariable } from "@langchain/core/context";
import { BaseChatModel } from "@langchain/core/language_models/chat_models";
import { RunnableLambda } from "@langchain/core/runnables";

const handleRunTimeRequestRunnable = RunnableLambda.from(async (params: {
  userId: string;
  query: string;
  llm: BaseChatModel;
}) => {
  const { userId, query, llm } = params;
  if (!llm.bindTools) {
    throw new Error("Language model does not support tools.");
  }
  // Set a context variable accessible to any child runnables called within this one.
  // You can also set context variables at top level that act as globals.
  setContextVariable("userId", userId);
  const tools = [updateFavoritePets];
  const llmWithTools = llm.bindTools(tools);
  const modelResponse = await llmWithTools.invoke(query);
  // For simplicity, skip checking the tool call's name field and assume
  // that the model is calling the "updateFavoritePets" tool
  if (modelResponse.tool_calls.length > 0) {
    return updateFavoritePets.invoke(modelResponse.tool_calls[0]);
  } else {
    return "No tool invoked.";
  }
});

"""
And when our method invokes the tools, you will see that the tool properly access the previously set `userId` context variable and runs successfully:
"""

await handleRunTimeRequestRunnable.invoke({
  userId: "brace",
  query: "my favorite animals are cats and parrots.",
  llm: llm
});
# Output:
#   ToolMessage {

#     "content": "update_favorite_pets called.",

#     "name": "update_favorite_pets",

#     "additional_kwargs": {},

#     "response_metadata": {},

#     "tool_call_id": "call_vsD2DbSpDquOtmFlOtbUME6h"

#   }


"""
And have additionally updated the `userToPets` object with a key matching the `userId` we passed, `"brace"`:
"""

console.log(userToPets);
# Output:
#   { brace: [ 'cats', 'parrots' ] }


"""
## Without context variables

If you are on an earlier version of core or an environment that does not support `async_hooks`, you can use the following design pattern that creates the tool dynamically at run time and binds to them appropriate values.

The idea is to create the tool dynamically at request time, and bind to it the appropriate information. For example,
this information may be the user ID as resolved from the request itself.
"""

import { z } from "zod";
import { tool } from "@langchain/core/tools";

userToPets = {};

function generateToolsForUser(userId: string) {
  const updateFavoritePets = tool(async (input) => {
    userToPets[userId] = input.pets;
    return "update_favorite_pets called."
  }, {
    name: "update_favorite_pets",
    description: "add to the list of favorite pets.",
    schema: z.object({
      pets: z.array(z.string())
    }),
  });
  // You can declare and return additional tools as well:
  return [updateFavoritePets];
}

"""
Verify that the tool works correctly
"""

const [updatePets] = generateToolsForUser("cobb");

await updatePets.invoke({ pets: ["tiger", "wolf"] });

console.log(userToPets);
# Output:
#   { cobb: [ 'tiger', 'wolf' ] }


import { BaseChatModel } from "@langchain/core/language_models/chat_models";

async function handleRunTimeRequest(userId: string, query: string, llm: BaseChatModel): Promise<any> {
  if (!llm.bindTools) {
    throw new Error("Language model does not support tools.");
  }
  const tools = generateToolsForUser(userId);
  const llmWithTools = llm.bindTools(tools);
  return llmWithTools.invoke(query);
}

"""
This code will allow the LLM to invoke the tools, but the LLM is **unaware** of the fact that a **user ID** even exists. You can see that `user_id` is not among the params the LLM generates:
"""

const aiMessage = await handleRunTimeRequest(
  "cobb", "my favorite pets are tigers and wolves.", llm,
);
console.log(aiMessage.tool_calls[0]);
# Output:
#   {

#     name: 'update_favorite_pets',

#     args: { pets: [ 'tigers', 'wolves' ] },

#     type: 'tool_call',

#     id: 'call_FBF4D51SkVK2clsLOQHX6wTv'

#   }


"""
```{=mdx}
:::tip
Click [here](https://smith.langchain.com/public/3d766ecc-8f28-400b-8636-632e6f1598c7/r) to see the LangSmith trace for the above run.
:::

:::tip
Chat models only output requests to invoke tools. They don't actually invoke the underlying tools.

To see how to invoke the tools, please refer to [how to use a model to call tools](/docs/how_to/tool_calling/).
:::
```
"""



================================================
FILE: docs/core_docs/docs/how_to/tool_stream_events.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to stream events from a tool

```{=mdx}
:::info Prerequisites

This guide assumes familiarity with the following concepts:
- [LangChain Tools](/docs/concepts/tools)
- [Custom tools](/docs/how_to/custom_tools)
- [Using stream events](/docs/how_to/streaming/#using-stream-events)
- [Accessing RunnableConfig within a custom tool](/docs/how_to/tool_configure/)

:::
```

If you have tools that call chat models, retrievers, or other runnables, you may want to access internal events from those runnables or configure them with additional properties. This guide shows you how to manually pass parameters properly so that you can do this using the [`.streamEvents()`](/docs/how_to/streaming/#using-stream-events) method.

```{=mdx}
:::caution Compatibility

In order to support a wider variety of JavaScript environments, the base LangChain package does not automatically propagate configuration to child runnables by default. This includes callbacks necessary for `.streamEvents()`. This is a common reason why you may fail to see events being emitted from custom runnables or tools.

You will need to manually propagate the `RunnableConfig` object to the child runnable. For an example of how to manually propagate the config, see the implementation of the `bar` RunnableLambda below.

This guide also requires `@langchain/core>=0.2.16`.
:::
```

Say you have a custom tool that calls a chain that condenses its input by prompting a chat model to return only 10 words, then reversing the output. First, define it in a naive way:

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="model" />
```
"""

import { ChatAnthropic } from "@langchain/anthropic";
const model = new ChatAnthropic({
  model: "claude-3-5-sonnet-20240620",
  temperature: 0,
});

import { z } from "zod";
import { tool } from "@langchain/core/tools";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { StringOutputParser } from "@langchain/core/output_parsers";

const specialSummarizationTool = tool(async (input) => {
  const prompt = ChatPromptTemplate.fromTemplate(
    "You are an expert writer. Summarize the following text in 10 words or less:\n\n{long_text}"
  );
  const reverse = (x: string) => {
    return x.split("").reverse().join("");
  };
  const chain = prompt
    .pipe(model)
    .pipe(new StringOutputParser())
    .pipe(reverse);
  const summary = await chain.invoke({ long_text: input.long_text });
  return summary;
}, {
  name: "special_summarization_tool",
  description: "A tool that summarizes input text using advanced techniques.",
  schema: z.object({
    long_text: z.string(),
  }),
});

"""
Invoking the tool directly works just fine:
"""

const LONG_TEXT = `
NARRATOR:
(Black screen with text; The sound of buzzing bees can be heard)
According to all known laws of aviation, there is no way a bee should be able to fly. Its wings are too small to get its fat little body off the ground. The bee, of course, flies anyway because bees don't care what humans think is impossible.
BARRY BENSON:
(Barry is picking out a shirt)
Yellow, black. Yellow, black. Yellow, black. Yellow, black. Ooh, black and yellow! Let's shake it up a little.
JANET BENSON:
Barry! Breakfast is ready!
BARRY:
Coming! Hang on a second.`;

await specialSummarizationTool.invoke({ long_text: LONG_TEXT });
# Output:
#   .yad noitaudarg rof tiftuo sesoohc yrraB ;scisyhp seifed eeB


"""
But if you wanted to access the raw output from the chat model rather than the full tool, you might try to use the [`.streamEvents()`](/docs/how_to/streaming/#using-stream-events) method and look for an `on_chat_model_end` event. Here's what happens:
"""

const stream = await specialSummarizationTool.streamEvents(
  { long_text: LONG_TEXT },
  { version: "v2" },
);

for await (const event of stream) {
  if (event.event === "on_chat_model_end") {
    // Never triggers!
    console.log(event);
  }
}

"""
You'll notice that there are no chat model events emitted from the child run!

This is because the example above does not pass the tool's config object into the internal chain. To fix this, redefine your tool to take a special parameter typed as `RunnableConfig` (see [this guide](/docs/how_to/tool_configure) for more details). You'll also need to pass that parameter through into the internal chain when executing it:
"""

const specialSummarizationToolWithConfig = tool(async (input, config) => {
  const prompt = ChatPromptTemplate.fromTemplate(
    "You are an expert writer. Summarize the following text in 10 words or less:\n\n{long_text}"
  );
  const reverse = (x: string) => {
    return x.split("").reverse().join("");
  };
  const chain = prompt
    .pipe(model)
    .pipe(new StringOutputParser())
    .pipe(reverse);
  // Pass the "config" object as an argument to any executed runnables
  const summary = await chain.invoke({ long_text: input.long_text }, config);
  return summary;
}, {
  name: "special_summarization_tool",
  description: "A tool that summarizes input text using advanced techniques.",
  schema: z.object({
    long_text: z.string(),
  }),
});

"""
And now try the same `.streamEvents()` call as before with your new tool:
"""

const stream = await specialSummarizationToolWithConfig.streamEvents(
  { long_text: LONG_TEXT },
  { version: "v2" },
);

for await (const event of stream) {
  if (event.event === "on_chat_model_end") {
    // Never triggers!
    console.log(event);
  }
}
# Output:
#   {

#     event: 'on_chat_model_end',

#     data: {

#       output: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: 'Bee defies physics; Barry chooses outfit for graduation day.',

#         name: undefined,

#         additional_kwargs: [Object],

#         response_metadata: {},

#         id: undefined,

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: [Object]

#       },

#       input: { messages: [Array] }

#     },

#     run_id: '27ac7b2e-591c-4adc-89ec-64d96e233ec8',

#     name: 'ChatAnthropic',

#     tags: [ 'seq:step:2' ],

#     metadata: {

#       ls_provider: 'anthropic',

#       ls_model_name: 'claude-3-5-sonnet-20240620',

#       ls_model_type: 'chat',

#       ls_temperature: 0,

#       ls_max_tokens: 2048,

#       ls_stop: undefined

#     }

#   }


"""
Awesome! This time there's an event emitted.

For streaming, `.streamEvents()` automatically calls internal runnables in a chain with streaming enabled if possible, so if you wanted to a stream of tokens as they are generated from the chat model, you could simply filter to look for `on_chat_model_stream` events with no other changes:
"""

const stream = await specialSummarizationToolWithConfig.streamEvents(
  { long_text: LONG_TEXT },
  { version: "v2" },
);

for await (const event of stream) {
  if (event.event === "on_chat_model_stream") {
    // Never triggers!
    console.log(event);
  }
}
# Output:
#   {

#     event: 'on_chat_model_stream',

#     data: {

#       chunk: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: 'Bee',

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: {},

#         id: undefined,

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: undefined

#       }

#     },

#     run_id: '938c0469-83c6-4dbd-862e-cd73381165de',

#     name: 'ChatAnthropic',

#     tags: [ 'seq:step:2' ],

#     metadata: {

#       ls_provider: 'anthropic',

#       ls_model_name: 'claude-3-5-sonnet-20240620',

#       ls_model_type: 'chat',

#       ls_temperature: 0,

#       ls_max_tokens: 2048,

#       ls_stop: undefined

#     }

#   }

#   {

#     event: 'on_chat_model_stream',

#     data: {

#       chunk: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: ' def',

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: {},

#         id: undefined,

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: undefined

#       }

#     },

#     run_id: '938c0469-83c6-4dbd-862e-cd73381165de',

#     name: 'ChatAnthropic',

#     tags: [ 'seq:step:2' ],

#     metadata: {

#       ls_provider: 'anthropic',

#       ls_model_name: 'claude-3-5-sonnet-20240620',

#       ls_model_type: 'chat',

#       ls_temperature: 0,

#       ls_max_tokens: 2048,

#       ls_stop: undefined

#     }

#   }

#   {

#     event: 'on_chat_model_stream',

#     data: {

#       chunk: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: 'ies physics',

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: {},

#         id: undefined,

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: undefined

#       }

#     },

#     run_id: '938c0469-83c6-4dbd-862e-cd73381165de',

#     name: 'ChatAnthropic',

#     tags: [ 'seq:step:2' ],

#     metadata: {

#       ls_provider: 'anthropic',

#       ls_model_name: 'claude-3-5-sonnet-20240620',

#       ls_model_type: 'chat',

#       ls_temperature: 0,

#       ls_max_tokens: 2048,

#       ls_stop: undefined

#     }

#   }

#   {

#     event: 'on_chat_model_stream',

#     data: {

#       chunk: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: ';',

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: {},

#         id: undefined,

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: undefined

#       }

#     },

#     run_id: '938c0469-83c6-4dbd-862e-cd73381165de',

#     name: 'ChatAnthropic',

#     tags: [ 'seq:step:2' ],

#     metadata: {

#       ls_provider: 'anthropic',

#       ls_model_name: 'claude-3-5-sonnet-20240620',

#       ls_model_type: 'chat',

#       ls_temperature: 0,

#       ls_max_tokens: 2048,

#       ls_stop: undefined

#     }

#   }

#   {

#     event: 'on_chat_model_stream',

#     data: {

#       chunk: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: ' Barry',

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: {},

#         id: undefined,

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: undefined

#       }

#     },

#     run_id: '938c0469-83c6-4dbd-862e-cd73381165de',

#     name: 'ChatAnthropic',

#     tags: [ 'seq:step:2' ],

#     metadata: {

#       ls_provider: 'anthropic',

#       ls_model_name: 'claude-3-5-sonnet-20240620',

#       ls_model_type: 'chat',

#       ls_temperature: 0,

#       ls_max_tokens: 2048,

#       ls_stop: undefined

#     }

#   }

#   {

#     event: 'on_chat_model_stream',

#     data: {

#       chunk: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: ' cho',

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: {},

#         id: undefined,

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: undefined

#       }

#     },

#     run_id: '938c0469-83c6-4dbd-862e-cd73381165de',

#     name: 'ChatAnthropic',

#     tags: [ 'seq:step:2' ],

#     metadata: {

#       ls_provider: 'anthropic',

#       ls_model_name: 'claude-3-5-sonnet-20240620',

#       ls_model_type: 'chat',

#       ls_temperature: 0,

#       ls_max_tokens: 2048,

#       ls_stop: undefined

#     }

#   }

#   {

#     event: 'on_chat_model_stream',

#     data: {

#       chunk: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: 'oses outfit',

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: {},

#         id: undefined,

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: undefined

#       }

#     },

#     run_id: '938c0469-83c6-4dbd-862e-cd73381165de',

#     name: 'ChatAnthropic',

#     tags: [ 'seq:step:2' ],

#     metadata: {

#       ls_provider: 'anthropic',

#       ls_model_name: 'claude-3-5-sonnet-20240620',

#       ls_model_type: 'chat',

#       ls_temperature: 0,

#       ls_max_tokens: 2048,

#       ls_stop: undefined

#     }

#   }

#   {

#     event: 'on_chat_model_stream',

#     data: {

#       chunk: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: ' for',

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: {},

#         id: undefined,

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: undefined

#       }

#     },

#     run_id: '938c0469-83c6-4dbd-862e-cd73381165de',

#     name: 'ChatAnthropic',

#     tags: [ 'seq:step:2' ],

#     metadata: {

#       ls_provider: 'anthropic',

#       ls_model_name: 'claude-3-5-sonnet-20240620',

#       ls_model_type: 'chat',

#       ls_temperature: 0,

#       ls_max_tokens: 2048,

#       ls_stop: undefined

#     }

#   }

#   {

#     event: 'on_chat_model_stream',

#     data: {

#       chunk: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: ' graduation',

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: {},

#         id: undefined,

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: undefined

#       }

#     },

#     run_id: '938c0469-83c6-4dbd-862e-cd73381165de',

#     name: 'ChatAnthropic',

#     tags: [ 'seq:step:2' ],

#     metadata: {

#       ls_provider: 'anthropic',

#       ls_model_name: 'claude-3-5-sonnet-20240620',

#       ls_model_type: 'chat',

#       ls_temperature: 0,

#       ls_max_tokens: 2048,

#       ls_stop: undefined

#     }

#   }

#   {

#     event: 'on_chat_model_stream',

#     data: {

#       chunk: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: ' day',

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: {},

#         id: undefined,

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: undefined

#       }

#     },

#     run_id: '938c0469-83c6-4dbd-862e-cd73381165de',

#     name: 'ChatAnthropic',

#     tags: [ 'seq:step:2' ],

#     metadata: {

#       ls_provider: 'anthropic',

#       ls_model_name: 'claude-3-5-sonnet-20240620',

#       ls_model_type: 'chat',

#       ls_temperature: 0,

#       ls_max_tokens: 2048,

#       ls_stop: undefined

#     }

#   }

#   {

#     event: 'on_chat_model_stream',

#     data: {

#       chunk: AIMessageChunk {

#         lc_serializable: true,

#         lc_kwargs: [Object],

#         lc_namespace: [Array],

#         content: '.',

#         name: undefined,

#         additional_kwargs: {},

#         response_metadata: {},

#         id: undefined,

#         tool_calls: [],

#         invalid_tool_calls: [],

#         tool_call_chunks: [],

#         usage_metadata: undefined

#       }

#     },

#     run_id: '938c0469-83c6-4dbd-862e-cd73381165de',

#     name: 'ChatAnthropic',

#     tags: [ 'seq:step:2' ],

#     metadata: {

#       ls_provider: 'anthropic',

#       ls_model_name: 'claude-3-5-sonnet-20240620',

#       ls_model_type: 'chat',

#       ls_temperature: 0,

#       ls_max_tokens: 2048,

#       ls_stop: undefined

#     }

#   }


"""
## Automatically passing config (Advanced)

If you've used [LangGraph](https://langchain-ai.github.io/langgraphjs/), you may have noticed that you don't need to pass config in nested calls. This is because LangGraph takes advantage of an API called [`async_hooks`](https://nodejs.org/api/async_hooks.html), which is not supported in many, but not all environments.

If you wish, you can enable automatic configuration passing by running the following code to import and enable `AsyncLocalStorage` globally:
"""

import { AsyncLocalStorageProviderSingleton } from "@langchain/core/singletons";
import { AsyncLocalStorage } from "async_hooks";

AsyncLocalStorageProviderSingleton.initializeGlobalInstance(
  new AsyncLocalStorage()
);

"""
## Next steps

You've now seen how to stream events from within a tool. Next, check out the following guides for more on using tools:

- Pass [runtime values to tools](/docs/how_to/tool_runtime)
- Pass [tool results back to a model](/docs/how_to/tool_results_pass_to_model)
- [Dispatch custom callback events](/docs/how_to/callbacks_custom_events)

You can also check out some more specific uses of tool calling:

- Building [tool-using chains and agents](/docs/how_to#tools)
- Getting [structured outputs](/docs/how_to/structured_output/) from models
"""



================================================
FILE: docs/core_docs/docs/how_to/tool_streaming.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to stream tool calls

When tools are called in a streaming context, 
[message chunks](https://api.js.langchain.com/classes/langchain_core_messages.AIMessageChunk.html) 
will be populated with [tool call chunk](https://api.js.langchain.com/types/langchain_core_messages_tool.ToolCallChunk.html) 
objects in a list via the `.tool_call_chunks` attribute. A `ToolCallChunk` includes 
optional string fields for the tool `name`, `args`, and `id`, and includes an optional 
integer field `index` that can be used to join chunks together. Fields are optional 
because portions of a tool call may be streamed across different chunks (e.g., a chunk 
that includes a substring of the arguments may have null values for the tool name and id).

Because message chunks inherit from their parent message class, an 
[`AIMessageChunk`](https://api.js.langchain.com/classes/langchain_core_messages.AIMessageChunk.html) 
with tool call chunks will also include `.tool_calls` and `.invalid_tool_calls` fields. 
These fields are parsed best-effort from the message's tool call chunks.

Note that not all providers currently support streaming for tool calls. Before we start let's define our tools and our model.
"""

import { z } from "zod";
import { tool } from "@langchain/core/tools";
import { ChatOpenAI } from "@langchain/openai";

const addTool = tool(async (input) => {
  return input.a + input.b;
}, {
  name: "add",
  description: "Adds a and b.",
  schema: z.object({
    a: z.number(),
    b: z.number(),
  }),
});

const multiplyTool = tool(async (input) => {
  return input.a * input.b;
}, {
  name: "multiply",
  description: "Multiplies a and b.",
  schema: z.object({
    a: z.number(),
    b: z.number(),
  }),
});

const tools = [addTool, multiplyTool];

const model = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0,
});

const modelWithTools = model.bindTools(tools);

"""
Now let's define our query and stream our output:
"""

const query = "What is 3 * 12? Also, what is 11 + 49?";

const stream = await modelWithTools.stream(query);

for await (const chunk of stream) {
  console.log(chunk.tool_call_chunks);
}
# Output:
#   []

#   [

#     {

#       name: 'multiply',

#       args: '',

#       id: 'call_MdIlJL5CAYD7iz9gTm5lwWtJ',

#       index: 0,

#       type: 'tool_call_chunk'

#     }

#   ]

#   [

#     {

#       name: undefined,

#       args: '{"a"',

#       id: undefined,

#       index: 0,

#       type: 'tool_call_chunk'

#     }

#   ]

#   [

#     {

#       name: undefined,

#       args: ': 3, ',

#       id: undefined,

#       index: 0,

#       type: 'tool_call_chunk'

#     }

#   ]

#   [

#     {

#       name: undefined,

#       args: '"b": 1',

#       id: undefined,

#       index: 0,

#       type: 'tool_call_chunk'

#     }

#   ]

#   [

#     {

#       name: undefined,

#       args: '2}',

#       id: undefined,

#       index: 0,

#       type: 'tool_call_chunk'

#     }

#   ]

#   [

#     {

#       name: 'add',

#       args: '',

#       id: 'call_ihL9W6ylSRlYigrohe9SClmW',

#       index: 1,

#       type: 'tool_call_chunk'

#     }

#   ]

#   [

#     {

#       name: undefined,

#       args: '{"a"',

#       id: undefined,

#       index: 1,

#       type: 'tool_call_chunk'

#     }

#   ]

#   [

#     {

#       name: undefined,

#       args: ': 11,',

#       id: undefined,

#       index: 1,

#       type: 'tool_call_chunk'

#     }

#   ]

#   [

#     {

#       name: undefined,

#       args: ' "b": ',

#       id: undefined,

#       index: 1,

#       type: 'tool_call_chunk'

#     }

#   ]

#   [

#     {

#       name: undefined,

#       args: '49}',

#       id: undefined,

#       index: 1,

#       type: 'tool_call_chunk'

#     }

#   ]

#   []

#   []


"""
Note that adding message chunks will merge their corresponding tool call chunks. This is the principle by which LangChain's various [tool output parsers](/docs/how_to/output_parser_structured) support streaming.

For example, below we accumulate tool call chunks:
"""

import { concat } from "@langchain/core/utils/stream";

const stream = await modelWithTools.stream(query);

let gathered = undefined;

for await (const chunk of stream) {
  gathered = gathered !== undefined ? concat(gathered, chunk) : chunk;
  console.log(gathered.tool_call_chunks);
}
# Output:
#   []

#   [

#     {

#       name: 'multiply',

#       args: '',

#       id: 'call_0zGpgVz81Ew0HA4oKblG0s0a',

#       index: 0,

#       type: 'tool_call_chunk'

#     }

#   ]

#   [

#     {

#       name: 'multiply',

#       args: '{"a"',

#       id: 'call_0zGpgVz81Ew0HA4oKblG0s0a',

#       index: 0,

#       type: 'tool_call_chunk'

#     }

#   ]

#   [

#     {

#       name: 'multiply',

#       args: '{"a": 3, ',

#       id: 'call_0zGpgVz81Ew0HA4oKblG0s0a',

#       index: 0,

#       type: 'tool_call_chunk'

#     }

#   ]

#   [

#     {

#       name: 'multiply',

#       args: '{"a": 3, "b": 1',

#       id: 'call_0zGpgVz81Ew0HA4oKblG0s0a',

#       index: 0,

#       type: 'tool_call_chunk'

#     }

#   ]

#   [

#     {

#       name: 'multiply',

#       args: '{"a": 3, "b": 12}',

#       id: 'call_0zGpgVz81Ew0HA4oKblG0s0a',

#       index: 0,

#       type: 'tool_call_chunk'

#     }

#   ]

#   [

#     {

#       name: 'multiply',

#       args: '{"a": 3, "b": 12}',

#       id: 'call_0zGpgVz81Ew0HA4oKblG0s0a',

#       index: 0,

#       type: 'tool_call_chunk'

#     },

#     {

#       name: 'add',

#       args: '',

#       id: 'call_ufY7lDSeCQwWbdq1XQQ2PBHR',

#       index: 1,

#       type: 'tool_call_chunk'

#     }

#   ]

#   [

#     {

#       name: 'multiply',

#       args: '{"a": 3, "b": 12}',

#       id: 'call_0zGpgVz81Ew0HA4oKblG0s0a',

#       index: 0,

#       type: 'tool_call_chunk'

#     },

#     {

#       name: 'add',

#       args: '{"a"',

#       id: 'call_ufY7lDSeCQwWbdq1XQQ2PBHR',

#       index: 1,

#       type: 'tool_call_chunk'

#     }

#   ]

#   [

#     {

#       name: 'multiply',

#       args: '{"a": 3, "b": 12}',

#       id: 'call_0zGpgVz81Ew0HA4oKblG0s0a',

#       index: 0,

#       type: 'tool_call_chunk'

#     },

#     {

#       name: 'add',

#       args: '{"a": 11,',

#       id: 'call_ufY7lDSeCQwWbdq1XQQ2PBHR',

#       index: 1,

#       type: 'tool_call_chunk'

#     }

#   ]

#   [

#     {

#       name: 'multiply',

#       args: '{"a": 3, "b": 12}',

#       id: 'call_0zGpgVz81Ew0HA4oKblG0s0a',

#       index: 0,

#       type: 'tool_call_chunk'

#     },

#     {

#       name: 'add',

#       args: '{"a": 11, "b": ',

#       id: 'call_ufY7lDSeCQwWbdq1XQQ2PBHR',

#       index: 1,

#       type: 'tool_call_chunk'

#     }

#   ]

#   [

#     {

#       name: 'multiply',

#       args: '{"a": 3, "b": 12}',

#       id: 'call_0zGpgVz81Ew0HA4oKblG0s0a',

#       index: 0,

#       type: 'tool_call_chunk'

#     },

#     {

#       name: 'add',

#       args: '{"a": 11, "b": 49}',

#       id: 'call_ufY7lDSeCQwWbdq1XQQ2PBHR',

#       index: 1,

#       type: 'tool_call_chunk'

#     }

#   ]

#   [

#     {

#       name: 'multiply',

#       args: '{"a": 3, "b": 12}',

#       id: 'call_0zGpgVz81Ew0HA4oKblG0s0a',

#       index: 0,

#       type: 'tool_call_chunk'

#     },

#     {

#       name: 'add',

#       args: '{"a": 11, "b": 49}',

#       id: 'call_ufY7lDSeCQwWbdq1XQQ2PBHR',

#       index: 1,

#       type: 'tool_call_chunk'

#     }

#   ]

#   [

#     {

#       name: 'multiply',

#       args: '{"a": 3, "b": 12}',

#       id: 'call_0zGpgVz81Ew0HA4oKblG0s0a',

#       index: 0,

#       type: 'tool_call_chunk'

#     },

#     {

#       name: 'add',

#       args: '{"a": 11, "b": 49}',

#       id: 'call_ufY7lDSeCQwWbdq1XQQ2PBHR',

#       index: 1,

#       type: 'tool_call_chunk'

#     }

#   ]


"""
At the end, we can see the final aggregated tool call chunks include the fully gathered raw string value:
"""

console.log(typeof gathered.tool_call_chunks[0].args);
# Output:
#   string


"""
And we can also see the fully parsed tool call as an object at the end:
"""

console.log(typeof gathered.tool_calls[0].args);
# Output:
#   object




================================================
FILE: docs/core_docs/docs/how_to/tools_builtin.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to use LangChain tools

Tools are interfaces that an agent, chain, or LLM can use to interact with the world.
They combine a few things:

1. The name of the tool
2. A description of what the tool is
3. JSON schema of what the inputs to the tool are
4. The function to call 
5. Whether the result of a tool should be returned directly to the user

It is useful to have all this information because this information can be used to build action-taking systems! The name, description, and schema can be used to prompt the LLM so it knows how to specify what action to take, and then the function to call is equivalent to taking that action.

The simpler the input to a tool is, the easier it is for an LLM to be able to use it.
Many agents will only work with tools that have a single string input.

Importantly, the name, description, and schema (if used) are all used in the prompt. Therefore, it is vitally important that they are clear and describe exactly how the tool should be used.

## Default Tools

Let's take a look at how to work with tools. To do this, we'll work with a built in tool.
"""

import { WikipediaQueryRun } from "@langchain/community/tools/wikipedia_query_run";

const tool = new WikipediaQueryRun({
  topKResults: 1,
  maxDocContentLength: 100,
});

"""
This is the default name:
"""

tool.name;
# Output:
#   [32m"wikipedia-api"[39m

"""
This is the default description:
"""

tool.description;
# Output:
#   [32m"A tool for interacting with and fetching data from the Wikipedia API."[39m

"""
This is the default schema of the inputs. This is a [Zod](https://zod.dev) schema on the tool class. We convert it to JSON schema for display purposes:
"""

import { zodToJsonSchema } from "zod-to-json-schema";

zodToJsonSchema(tool.schema);
# Output:
#   {

#     type: [32m"object"[39m,

#     properties: { input: { type: [32m"string"[39m } },

#     additionalProperties: [33mfalse[39m,

#     [32m"$schema"[39m: [32m"http://json-schema.org/draft-07/schema#"[39m

#   }

"""
We can see if the tool should return directly to the user
"""

tool.returnDirect;
# Output:
#   [33mfalse[39m

"""
We can invoke this tool with an object input:
"""

await tool.invoke({ input: "langchain" })
# Output:
#   [32m"Page: LangChain\n"[39m +

#     [32m"Summary: LangChain is a framework designed to simplify the creation of applications "[39m

"""
We can also invoke this tool with a single string input. 
We can do this because this tool expects only a single input.
If it required multiple inputs, we would not be able to do that.
"""

await tool.invoke("langchain")
# Output:
#   [32m"Page: LangChain\n"[39m +

#     [32m"Summary: LangChain is a framework designed to simplify the creation of applications "[39m

"""
## How to use built-in toolkits

Toolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.

For a complete list of available ready-made toolkits, visit [Integrations](/docs/integrations/toolkits/).

All Toolkits expose a `getTools()` method which returns a list of tools.

You're usually meant to use them this way:

```ts
// Initialize a toolkit
const toolkit = new ExampleTookit(...);

// Get list of tools
const tools = toolkit.getTools();
```
"""

"""
## More Topics

This was a quick introduction to tools in LangChain, but there is a lot more to learn

**[Built-In Tools](/docs/integrations/tools/)**: For a list of all built-in tools, see [this page](/docs/integrations/tools/)
    
**[Custom Tools](/docs/how_to/custom_tools)**: Although built-in tools are useful, it's highly likely that you'll have to define your own tools. See [this guide](/docs/how_to/custom_tools) for instructions on how to do so.
"""



================================================
FILE: docs/core_docs/docs/how_to/tools_error.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to handle tool errors

```{=mdx}
:::info Prerequisites

This guide assumes familiarity with the following concepts:
- [Chat models](/docs/concepts/chat_models)
- [LangChain Tools](/docs/concepts/tools)
- [How to use a model to call tools](/docs/how_to/tool_calling)

:::
```

Calling tools with an LLM isn't perfect. The model may try to call a tool that doesn't exist or fail to return arguments that match the requested schema. Strategies like keeping schemas simple, reducing the number of tools you pass at once, and having good names and descriptions can help mitigate this risk, but aren't foolproof.

This guide covers some ways to build error handling into your chains to mitigate these failure modes.
"""

"""
## Chain

Suppose we have the following (dummy) tool and tool-calling chain. We'll make our tool intentionally convoluted to try and trip up the model.
"""

import { z } from "zod";
import { ChatOpenAI } from "@langchain/openai";
import { tool } from "@langchain/core/tools";

const llm = new ChatOpenAI({
  model: "gpt-3.5-turbo-0125",
  temperature: 0,
});

const complexTool = tool(async (params) => {
  return params.int_arg * params.float_arg;
}, {
  name: "complex_tool",
  description: "Do something complex with a complex tool.",
  schema: z.object({
    int_arg: z.number(),
    float_arg: z.number(),
    number_arg: z.object({}),
  })
});

const llmWithTools = llm.bindTools([complexTool]);

const chain = llmWithTools
  .pipe((message) => message.tool_calls?.[0].args)
  .pipe(complexTool);

"""
We can see that when we try to invoke this chain the model fails to correctly call the tool:
"""

await chain.invoke(
  "use complex tool. the args are 5, 2.1, potato"
);
# Output:
#   Error: Error: Received tool input did not match expected schema

"""
## Try/except tool call

The simplest way to more gracefully handle errors is to try/except the tool-calling step and return a helpful message on errors:
"""

const tryExceptToolWrapper = async (input, config) => {
  try {
    const result = await complexTool.invoke(input);
    return result;
  } catch (e) {
    return `Calling tool with arguments:\n\n${JSON.stringify(input)}\n\nraised the following error:\n\n${e}`
  }
}

const chainWithTools = llmWithTools
  .pipe((message) => message.tool_calls?.[0].args)
  .pipe(tryExceptToolWrapper);

const res = await chainWithTools.invoke("use complex tool. the args are 5, 2.1, potato");

console.log(res);
# Output:
#   Calling tool with arguments:

#   

#   {"int_arg":5,"float_arg":2.1,"number_arg":"potato"}

#   

#   raised the following error:

#   

#   Error: Received tool input did not match expected schema


"""
## Fallbacks

We can also try to fallback to a better model in the event of a tool invocation error. In this case we'll fall back to an identical chain that uses `gpt-4-1106-preview` instead of `gpt-3.5-turbo`.
"""

const badChain = llmWithTools
  .pipe((message) => message.tool_calls?.[0].args)
  .pipe(complexTool);

const betterModel = new ChatOpenAI({
  model: "gpt-4-1106-preview",
  temperature: 0,
}).bindTools([complexTool]);

const betterChain = betterModel
  .pipe((message) => message.tool_calls?.[0].args)
  .pipe(complexTool);

const chainWithFallback = badChain.withFallbacks([betterChain]);

await chainWithFallback.invoke("use complex tool. the args are 5, 2.1, potato");
# Output:
#   [33m10.5[39m

"""
Looking at the [LangSmith trace](https://smith.langchain.com/public/ea31e7ca-4abc-48e3-9943-700100c86622/r) for this chain run, we can see that the first chain call fails as expected and it's the fallback that succeeds.
"""

"""
## Next steps

Now you've seen some strategies how to handle tool calling errors. Next, you can learn more about how to use tools:

- Few shot prompting [with tools](/docs/how_to/tool_calling#few-shotting-with-tools)
- Stream [tool calls](/docs/how_to/tool_streaming/)
- Pass [runtime values to tools](/docs/how_to/tool_runtime)

You can also check out some more specific uses of tool calling:

- Getting [structured outputs](/docs/how_to/structured_output/) from models
"""



================================================
FILE: docs/core_docs/docs/how_to/tools_few_shot.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
## How to use few-shot prompting with tool calling

```{=mdx}
:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat models](/docs/concepts/chat_models)
- [LangChain Tools](/docs/concepts/tools)
- [Tool calling](/docs/concepts/tool_calling)
- [Passing tool outputs to chat models](/docs/how_to/tool_results_pass_to_model/)

:::
```

For more complex tool use it's very useful to add few-shot examples to the prompt. We can do this by adding `AIMessages` with `ToolCalls` and corresponding `ToolMessages` to our prompt.

First define a model and a calculator tool:
"""

import { tool } from "@langchain/core/tools";
import { z } from "zod";
import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({ model: "gpt-4o", temperature: 0, })

/**
 * Note that the descriptions here are crucial, as they will be passed along
 * to the model along with the class name.
 */
const calculatorSchema = z.object({
  operation: z
    .enum(["add", "subtract", "multiply", "divide"])
    .describe("The type of operation to execute."),
  number1: z.number().describe("The first number to operate on."),
  number2: z.number().describe("The second number to operate on."),
});

const calculatorTool = tool(async ({ operation, number1, number2 }) => {
  // Functions must return strings
  if (operation === "add") {
    return `${number1 + number2}`;
  } else if (operation === "subtract") {
    return `${number1 - number2}`;
  } else if (operation === "multiply") {
    return `${number1 * number2}`;
  } else if (operation === "divide") {
    return `${number1 / number2}`;
  } else {
    throw new Error("Invalid operation.");
  }
}, {
  name: "calculator",
  description: "Can perform mathematical operations.",
  schema: calculatorSchema,
});

const llmWithTools = llm.bindTools([calculatorTool]);

"""
Our calculator can handle common addition, subtraction, multiplication, and division. But what happens if we ask about a new mathematical operator, `🦜`?

Let's see what happens when we use it naively:
"""

const res = await llmWithTools.invoke("What is 3 🦜 12");

console.log(res.content);
console.log(res.tool_calls);
# Output:
#   

#   [

#     {

#       name: 'calculator',

#       args: { operation: 'multiply', number1: 3, number2: 12 },

#       type: 'tool_call',

#       id: 'call_I0oQGmdESpIgcf91ej30p9aR'

#     }

#   ]


"""
It doesn't quite know how to interpret `🦜` as an operation, and it defaults to `multiply`. Now, let's try giving it some examples in the form of a manufactured messages to steer it towards `divide`:
"""

import { HumanMessage, AIMessage, ToolMessage } from "@langchain/core/messages";

const res = await llmWithTools.invoke([
  new HumanMessage("What is 333382 🦜 1932?"),
  new AIMessage({
    content: "The 🦜 operator is shorthand for division, so we call the divide tool.",
    tool_calls: [{
      id: "12345",
      name: "calculator",
      args: {
        number1: 333382,
        number2: 1932,
        operation: "divide",
      }
    }]
  }),
  new ToolMessage({
    tool_call_id: "12345",
    content: "The answer is 172.558."
  }),
  new AIMessage("The answer is 172.558."),
  new HumanMessage("What is 6 🦜 2?"),
  new AIMessage({
    content: "The 🦜 operator is shorthand for division, so we call the divide tool.",
    tool_calls: [{
      id: "54321",
      name: "calculator",
      args: {
        number1: 6,
        number2: 2,
        operation: "divide",
      }
    }]
  }),
  new ToolMessage({
    tool_call_id: "54321",
    content: "The answer is 3."
  }),
  new AIMessage("The answer is 3."),
  new HumanMessage("What is 3 🦜 12?")
]);

console.log(res.tool_calls);
# Output:
#   [

#     {

#       name: 'calculator',

#       args: { number1: 3, number2: 12, operation: 'divide' },

#       type: 'tool_call',

#       id: 'call_O6M4yDaA6s8oDqs2Zfl7TZAp'

#     }

#   ]


"""
And we can see that it now equates `🦜` with the `divide` operation in the correct way!

## Related

- Stream [tool calls](/docs/how_to/tool_streaming/)
- Pass [runtime values to tools](/docs/how_to/tool_runtime)
- Getting [structured outputs](/docs/how_to/structured_output/) from models
"""



================================================
FILE: docs/core_docs/docs/how_to/tools_prompting.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_position: 3
---
"""

"""
# How to add ad-hoc tool calling capability to LLMs and Chat Models

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [LangChain Expression Language (LCEL)](/docs/concepts/lcel)
- [Chaining runnables](/docs/how_to/sequence/)
- [Tool calling](/docs/how_to/tool_calling/)

:::

In this guide we'll build a Chain that does not rely on any special model APIs (like tool calling, which we showed in the [Quickstart](/docs/how_to/tool_calling)) and instead just prompts the model directly to invoke tools.
"""

"""
## Setup

We'll need to install the following packages:

```{=mdx}
import Npm2Yarn from '@theme/Npm2Yarn';

<Npm2Yarn>
  @langchain/core zod
</Npm2Yarn>
```

#### Set environment variables

```
# Optional, use LangSmith for best-in-class observability
LANGSMITH_API_KEY=your-api-key
LANGSMITH_TRACING=true

# Reduce tracing latency if you are not in a serverless environment
# LANGCHAIN_CALLBACKS_BACKGROUND=true
```
"""

"""
## Create a tool

First, we need to create a tool to call. For this example, we will create a custom tool from a function. For more information on all details related to creating custom tools, please see [this guide](/docs/how_to/custom_tools).
"""

import { tool } from "@langchain/core/tools";
import { z } from "zod";

const multiplyTool = tool((input) => {
    return (input.first_int * input.second_int).toString()
}, {
    name: "multiply",
    description: "Multiply two integers together.",
    schema: z.object({
        first_int: z.number(),
        second_int: z.number(),
    })
})


console.log(multiplyTool.name)
console.log(multiplyTool.description)
# Output:
#   multiply

#   Multiply two integers together.


await multiplyTool.invoke({ first_int: 4, second_int: 5 })
# Output:
#   20


"""
## Creating our prompt

We'll want to write a prompt that specifies the tools the model has access to, the arguments to those tools, and the desired output format of the model. In this case we'll instruct it to output a JSON blob of the form `{"name": "...", "arguments": {...}}`.

```{=mdx}
:::tip
As of `langchain` version `0.2.8`, the `renderTextDescription` function now supports [OpenAI-formatted tools](https://api.js.langchain.com/interfaces/langchain_core.language_models_base.ToolDefinition.html).
:::
```
"""

import { renderTextDescription } from "langchain/tools/render";

const renderedTools = renderTextDescription([multiplyTool])

import { ChatPromptTemplate } from "@langchain/core/prompts";

const systemPrompt = `You are an assistant that has access to the following set of tools. Here are the names and descriptions for each tool:

{rendered_tools}

Given the user input, return the name and input of the tool to use. Return your response as a JSON blob with 'name' and 'arguments' keys.`;

const prompt = ChatPromptTemplate.fromMessages(
    [["system", systemPrompt], ["user", "{input}"]]
)

"""
## Adding an output parser

We'll use the `JsonOutputParser` for parsing our models output to JSON.

```{=mdx}
import ChatModelTabs from '@theme/ChatModelTabs';

<ChatModelTabs />
```
"""

import { JsonOutputParser } from "@langchain/core/output_parsers";
const chain = prompt.pipe(model).pipe(new JsonOutputParser())
await chain.invoke({ input: "what's thirteen times 4", rendered_tools: renderedTools })
# Output:
#   { name: 'multiply', arguments: [ 13, 4 ] }


"""
## Invoking the tool

We can invoke the tool as part of the chain by passing along the model-generated "arguments" to it:
"""

import { RunnableLambda, RunnablePick } from "@langchain/core/runnables"

const chain = prompt.pipe(model).pipe(new JsonOutputParser()).pipe(new RunnablePick("arguments")).pipe(new RunnableLambda({ func: (input) => multiplyTool.invoke({
  first_int: input[0],
  second_int: input[1]
}) }))
await chain.invoke({ input: "what's thirteen times 4", rendered_tools: renderedTools })
# Output:
#   52


"""
## Choosing from multiple tools

Suppose we have multiple tools we want the chain to be able to choose from:
"""

const addTool = tool((input) => {
    return (input.first_int + input.second_int).toString()
}, {
    name: "add",
    description: "Add two integers together.",
    schema: z.object({
        first_int: z.number(),
        second_int: z.number(),
    }),
});

const exponentiateTool = tool((input) => {
    return Math.pow(input.first_int, input.second_int).toString()
}, {
    name: "exponentiate",
    description: "Exponentiate the base to the exponent power.",
    schema: z.object({
        first_int: z.number(),
        second_int: z.number(),
    }),
});



"""
With function calling, we can do this like so:
"""

"""
If we want to run the model selected tool, we can do so using a function that returns the tool based on the model output. Specifically, our function will action return it's own subchain that gets the "arguments" part of the model output and passes it to the chosen tool:
"""

import { StructuredToolInterface } from "@langchain/core/tools"

const tools = [addTool, exponentiateTool, multiplyTool]

const toolChain = (modelOutput) => {
    const toolMap: Record<string, StructuredToolInterface> = Object.fromEntries(tools.map(tool => [tool.name, tool]))
    const chosenTool = toolMap[modelOutput.name]
    return new RunnablePick("arguments").pipe(new RunnableLambda({ func: (input) => chosenTool.invoke({
        first_int: input[0],
        second_int: input[1]
      }) }))
}
const toolChainRunnable = new RunnableLambda({
  func: toolChain
})

const renderedTools = renderTextDescription(tools)
const systemPrompt = `You are an assistant that has access to the following set of tools. Here are the names and descriptions for each tool:

{rendered_tools}

Given the user input, return the name and input of the tool to use. Return your response as a JSON blob with 'name' and 'arguments' keys.`

const prompt = ChatPromptTemplate.fromMessages(
    [["system", systemPrompt], ["user", "{input}"]]
)
const chain = prompt.pipe(model).pipe(new JsonOutputParser()).pipe(toolChainRunnable)
await chain.invoke({ input: "what's 3 plus 1132", rendered_tools: renderedTools })
# Output:
#   1135


"""
## Returning tool inputs

It can be helpful to return not only tool outputs but also tool inputs. We can easily do this with LCEL by `RunnablePassthrough.assign`-ing the tool output. This will take whatever the input is to the RunnablePassrthrough components (assumed to be a dictionary) and add a key to it while still passing through everything that's currently in the input:
"""

import { RunnablePassthrough } from "@langchain/core/runnables"

const chain = prompt.pipe(model).pipe(new JsonOutputParser()).pipe(RunnablePassthrough.assign({ output: toolChainRunnable }))
await chain.invoke({ input: "what's 3 plus 1132", rendered_tools: renderedTools })

# Output:
#   { name: 'add', arguments: [ 3, 1132 ], output: '1135' }


"""
## What's next?

This how-to guide shows the "happy path" when the model correctly outputs all the required tool information.

In reality, if you're using more complex tools, you will start encountering errors from the model, especially for models that have not been fine tuned for tool calling and for less capable models.

You will need to be prepared to add strategies to improve the output from the model; e.g.,

- Provide few shot examples.
- Add error handling (e.g., catch the exception and feed it back to the LLM to ask it to correct its previous output).
"""



================================================
FILE: docs/core_docs/docs/how_to/trim_messages.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to trim messages

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Messages](/docs/concepts/messages)
- [Chat models](/docs/concepts/chat_models)
- [Chaining](/docs/how_to/sequence/)
- [Chat history](/docs/concepts/chat_history)

The methods in this guide also require `@langchain/core>=0.2.8`.
Please see here for a [guide on upgrading](/docs/how_to/installation/#installing-integration-packages).

:::

All models have finite context windows, meaning there's a limit to how many tokens they can take as input. If you have very long messages or a chain/agent that accumulates a long message is history, you'll need to manage the length of the messages you're passing in to the model.

The `trimMessages` util provides some basic strategies for trimming a list of messages to be of a certain token length.

## Getting the last `maxTokens` tokens

To get the last `maxTokens` in the list of Messages we can set `strategy: "last"`. Notice that for our `tokenCounter` we can pass in a function (more on that below) or a language model (since language models have a message token counting method). It makes sense to pass in a model when you're trimming your messages to fit into the context window of that specific model:
"""

import { AIMessage, HumanMessage, SystemMessage, trimMessages } from "@langchain/core/messages";
import { ChatOpenAI } from "@langchain/openai";

const messages = [
    new SystemMessage("you're a good assistant, you always respond with a joke."),
    new HumanMessage("i wonder why it's called langchain"),
    new AIMessage(
        'Well, I guess they thought "WordRope" and "SentenceString" just didn\'t have the same ring to it!'
    ),
    new HumanMessage("and who is harrison chasing anyways"),
    new AIMessage(
        "Hmmm let me think.\n\nWhy, he's probably chasing after the last cup of coffee in the office!"
    ),
    new HumanMessage("what do you call a speechless parrot"),
];

const trimmed = await trimMessages(
    messages,
    {
        maxTokens: 45,
        strategy: "last",
        tokenCounter: new ChatOpenAI({ modelName: "gpt-4" }),
    }
);

console.log(trimmed.map((x) => JSON.stringify({
    role: x._getType(),
    content: x.content,
}, null, 2)).join("\n\n"));
# Output:
#   {

#     "role": "human",

#     "content": "and who is harrison chasing anyways"

#   }

#   

#   {

#     "role": "ai",

#     "content": "Hmmm let me think.\n\nWhy, he's probably chasing after the last cup of coffee in the office!"

#   }

#   

#   {

#     "role": "human",

#     "content": "what do you call a speechless parrot"

#   }


"""
If we want to always keep the initial system message we can specify `includeSystem: true`:
"""

await trimMessages(
    messages,
    {
        maxTokens: 45,
        strategy: "last",
        tokenCounter: new ChatOpenAI({ modelName: "gpt-4" }),
        includeSystem: true
    }
);
# Output:
#   [

#     SystemMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: "you're a good assistant, you always respond with a joke.",

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: "you're a good assistant, you always respond with a joke.",

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {},

#       id: undefined

#     },

#     AIMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: 'Hmmm let me think.\n' +

#           '\n' +

#           "Why, he's probably chasing after the last cup of coffee in the office!",

#         tool_calls: [],

#         invalid_tool_calls: [],

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: 'Hmmm let me think.\n' +

#         '\n' +

#         "Why, he's probably chasing after the last cup of coffee in the office!",

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {},

#       id: undefined,

#       tool_calls: [],

#       invalid_tool_calls: [],

#       usage_metadata: undefined

#     },

#     HumanMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: 'what do you call a speechless parrot',

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: 'what do you call a speechless parrot',

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {},

#       id: undefined

#     }

#   ]


"""
If we want to allow splitting up the contents of a message we can specify `allowPartial: true`:
"""

await trimMessages(
    messages,
    {
        maxTokens: 50,
        strategy: "last",
        tokenCounter: new ChatOpenAI({ modelName: "gpt-4" }),
        includeSystem: true,
        allowPartial: true
    }
);
# Output:
#   [

#     SystemMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: "you're a good assistant, you always respond with a joke.",

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: "you're a good assistant, you always respond with a joke.",

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {},

#       id: undefined

#     },

#     AIMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: 'Hmmm let me think.\n' +

#           '\n' +

#           "Why, he's probably chasing after the last cup of coffee in the office!",

#         tool_calls: [],

#         invalid_tool_calls: [],

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: 'Hmmm let me think.\n' +

#         '\n' +

#         "Why, he's probably chasing after the last cup of coffee in the office!",

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {},

#       id: undefined,

#       tool_calls: [],

#       invalid_tool_calls: [],

#       usage_metadata: undefined

#     },

#     HumanMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: 'what do you call a speechless parrot',

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: 'what do you call a speechless parrot',

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {},

#       id: undefined

#     }

#   ]


"""
If we need to make sure that our first message (excluding the system message) is always of a specific type, we can specify `startOn`:
"""

await trimMessages(
    messages,
    {
        maxTokens: 60,
        strategy: "last",
        tokenCounter: new ChatOpenAI({ modelName: "gpt-4" }),
        includeSystem: true,
        startOn: "human"
    }
);
# Output:
#   [

#     SystemMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: "you're a good assistant, you always respond with a joke.",

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: "you're a good assistant, you always respond with a joke.",

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {},

#       id: undefined

#     },

#     HumanMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: 'and who is harrison chasing anyways',

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: 'and who is harrison chasing anyways',

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {},

#       id: undefined

#     },

#     AIMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: 'Hmmm let me think.\n' +

#           '\n' +

#           "Why, he's probably chasing after the last cup of coffee in the office!",

#         tool_calls: [],

#         invalid_tool_calls: [],

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: 'Hmmm let me think.\n' +

#         '\n' +

#         "Why, he's probably chasing after the last cup of coffee in the office!",

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {},

#       id: undefined,

#       tool_calls: [],

#       invalid_tool_calls: [],

#       usage_metadata: undefined

#     },

#     HumanMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: 'what do you call a speechless parrot',

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: 'what do you call a speechless parrot',

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {},

#       id: undefined

#     }

#   ]


"""
## Getting the first `maxTokens` tokens

We can perform the flipped operation of getting the *first* `maxTokens` by specifying `strategy: "first"`:
"""

await trimMessages(
    messages,
    {
        maxTokens: 45,
        strategy: "first",
        tokenCounter: new ChatOpenAI({ modelName: "gpt-4" }),
    }
);
# Output:
#   [

#     SystemMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: "you're a good assistant, you always respond with a joke.",

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: "you're a good assistant, you always respond with a joke.",

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {},

#       id: undefined

#     },

#     HumanMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: "i wonder why it's called langchain",

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: "i wonder why it's called langchain",

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {},

#       id: undefined

#     }

#   ]


"""
## Writing a custom token counter

We can write a custom token counter function that takes in a list of messages and returns an int.
"""

import { encodingForModel } from '@langchain/core/utils/tiktoken';
import { BaseMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage, MessageContent, MessageContentText } from '@langchain/core/messages';

async function strTokenCounter(messageContent: MessageContent): Promise<number> {
    if (typeof messageContent === 'string') {
        return (
            await encodingForModel("gpt-4")
          ).encode(messageContent).length;
    } else {
        if (messageContent.every((x) => x.type === "text" && x.text)) {
            return (
                await encodingForModel("gpt-4")
              ).encode((messageContent as MessageContentText[]).map(({ text }) => text).join("")).length;
        }
        throw new Error(`Unsupported message content ${JSON.stringify(messageContent)}`);
    }
}

async function tiktokenCounter(messages: BaseMessage[]): Promise<number> {
  let numTokens = 3; // every reply is primed with <|start|>assistant<|message|>
  const tokensPerMessage = 3;
  const tokensPerName = 1;

  for (const msg of messages) {
    let role: string;
    if (msg instanceof HumanMessage) {
      role = 'user';
    } else if (msg instanceof AIMessage) {
      role = 'assistant';
    } else if (msg instanceof ToolMessage) {
      role = 'tool';
    } else if (msg instanceof SystemMessage) {
      role = 'system';
    } else {
      throw new Error(`Unsupported message type ${msg.constructor.name}`);
    }

    numTokens += tokensPerMessage + (await strTokenCounter(role)) + (await strTokenCounter(msg.content));

    if (msg.name) {
      numTokens += tokensPerName + (await strTokenCounter(msg.name));
    }
  }

  return numTokens;
}

await trimMessages(messages, {
  maxTokens: 45,
  strategy: 'last',
  tokenCounter: tiktokenCounter,
});
# Output:
#   [

#     AIMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: 'Hmmm let me think.\n' +

#           '\n' +

#           "Why, he's probably chasing after the last cup of coffee in the office!",

#         tool_calls: [],

#         invalid_tool_calls: [],

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: 'Hmmm let me think.\n' +

#         '\n' +

#         "Why, he's probably chasing after the last cup of coffee in the office!",

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {},

#       id: undefined,

#       tool_calls: [],

#       invalid_tool_calls: [],

#       usage_metadata: undefined

#     },

#     HumanMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: 'what do you call a speechless parrot',

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: 'what do you call a speechless parrot',

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {},

#       id: undefined

#     }

#   ]


"""
## Chaining

`trimMessages` can be used in an imperatively (like above) or declaratively, making it easy to compose with other components in a chain
"""

import { ChatOpenAI } from "@langchain/openai";
import { trimMessages } from "@langchain/core/messages";

const llm = new ChatOpenAI({ model: "gpt-4o" })

// Notice we don't pass in messages. This creates
// a RunnableLambda that takes messages as input
const trimmer = trimMessages({
    maxTokens: 45,
    strategy: "last",
    tokenCounter: llm,
    includeSystem: true,
})

const chain = trimmer.pipe(llm);
await chain.invoke(messages)
# Output:
#   AIMessage {

#     lc_serializable: true,

#     lc_kwargs: {

#       content: 'Thanks! I do try to keep things light. But for a more serious answer, "LangChain" is likely named to reflect its focus on language processing and the way it connects different components or models together—essentially forming a "chain" of linguistic operations. The "Lang" part emphasizes its focus on language, while "Chain" highlights the interconnected workflows it aims to facilitate.',

#       tool_calls: [],

#       invalid_tool_calls: [],

#       additional_kwargs: { function_call: undefined, tool_calls: undefined },

#       response_metadata: {}

#     },

#     lc_namespace: [ 'langchain_core', 'messages' ],

#     content: 'Thanks! I do try to keep things light. But for a more serious answer, "LangChain" is likely named to reflect its focus on language processing and the way it connects different components or models together—essentially forming a "chain" of linguistic operations. The "Lang" part emphasizes its focus on language, while "Chain" highlights the interconnected workflows it aims to facilitate.',

#     name: undefined,

#     additional_kwargs: { function_call: undefined, tool_calls: undefined },

#     response_metadata: {

#       tokenUsage: { completionTokens: 77, promptTokens: 59, totalTokens: 136 },

#       finish_reason: 'stop'

#     },

#     id: undefined,

#     tool_calls: [],

#     invalid_tool_calls: [],

#     usage_metadata: { input_tokens: 59, output_tokens: 77, total_tokens: 136 }

#   }


"""
Looking at [the LangSmith trace](https://smith.langchain.com/public/3793312c-a74b-4e77-92b4-f91b3d74ac5f/r) we can see that before the messages are passed to the model they are first trimmed.

Looking at just the trimmer, we can see that it's a Runnable object that can be invoked like all Runnables:
"""

await trimmer.invoke(messages)
# Output:
#   [

#     SystemMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: "you're a good assistant, you always respond with a joke.",

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: "you're a good assistant, you always respond with a joke.",

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {},

#       id: undefined

#     },

#     AIMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: 'Hmmm let me think.\n' +

#           '\n' +

#           "Why, he's probably chasing after the last cup of coffee in the office!",

#         tool_calls: [],

#         invalid_tool_calls: [],

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: 'Hmmm let me think.\n' +

#         '\n' +

#         "Why, he's probably chasing after the last cup of coffee in the office!",

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {},

#       id: undefined,

#       tool_calls: [],

#       invalid_tool_calls: [],

#       usage_metadata: undefined

#     },

#     HumanMessage {

#       lc_serializable: true,

#       lc_kwargs: {

#         content: 'what do you call a speechless parrot',

#         additional_kwargs: {},

#         response_metadata: {}

#       },

#       lc_namespace: [ 'langchain_core', 'messages' ],

#       content: 'what do you call a speechless parrot',

#       name: undefined,

#       additional_kwargs: {},

#       response_metadata: {},

#       id: undefined

#     }

#   ]


"""
## Using with ChatMessageHistory

Trimming messages is especially useful when [working with chat histories](/docs/how_to/message_history/), which can get arbitrarily long:
"""

import { InMemoryChatMessageHistory } from "@langchain/core/chat_history";
import { RunnableWithMessageHistory } from "@langchain/core/runnables";
import { HumanMessage, trimMessages } from "@langchain/core/messages";
import { ChatOpenAI } from "@langchain/openai";

const chatHistory = new InMemoryChatMessageHistory(messages.slice(0, -1))

const dummyGetSessionHistory = async (sessionId: string) => {
    if (sessionId !== "1") {
        throw new Error("Session not found");
      }
      return chatHistory;
  }

  const llm = new ChatOpenAI({ model: "gpt-4o" });

  const trimmer = trimMessages({
    maxTokens: 45,
    strategy: "last",
    tokenCounter: llm,
    includeSystem: true,
  });

const chain = trimmer.pipe(llm);
const chainWithHistory = new RunnableWithMessageHistory({
    runnable: chain,
    getMessageHistory: dummyGetSessionHistory,
})
await chainWithHistory.invoke(
    [new HumanMessage("what do you call a speechless parrot")],
    { configurable: { sessionId: "1"} },
)
# Output:
#   AIMessage {

#     lc_serializable: true,

#     lc_kwargs: {

#       content: 'A "polly-no-want-a-cracker"!',

#       tool_calls: [],

#       invalid_tool_calls: [],

#       additional_kwargs: { function_call: undefined, tool_calls: undefined },

#       response_metadata: {}

#     },

#     lc_namespace: [ 'langchain_core', 'messages' ],

#     content: 'A "polly-no-want-a-cracker"!',

#     name: undefined,

#     additional_kwargs: { function_call: undefined, tool_calls: undefined },

#     response_metadata: {

#       tokenUsage: { completionTokens: 11, promptTokens: 57, totalTokens: 68 },

#       finish_reason: 'stop'

#     },

#     id: undefined,

#     tool_calls: [],

#     invalid_tool_calls: [],

#     usage_metadata: { input_tokens: 57, output_tokens: 11, total_tokens: 68 }

#   }


"""
Looking at [the LangSmith trace](https://smith.langchain.com/public/cfc76880-5895-4852-b7d0-12916448bdb2/r) we can see that we retrieve all of our messages but before the messages are passed to the model they are trimmed to be just the system message and last human message.
"""

"""
## API reference

For a complete description of all arguments head to the [API reference](https://api.js.langchain.com/functions/langchain_core.messages.trimMessages.html).
"""



================================================
FILE: docs/core_docs/docs/how_to/vectorstore_retriever.mdx
================================================
# How use a vector store to retrieve data

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Vector stores](/docs/concepts/#vectorstores)
- [Retrievers](/docs/concepts/retrievers)
- [Text splitters](/docs/concepts/text_splitters)
- [Chaining runnables](/docs/how_to/sequence/)

:::

Vector stores can be converted into retrievers using the [`.asRetriever()`](https://api.js.langchain.com/classes/langchain_core.vectorstores.VectorStore.html#asRetriever) method, which allows you to more easily compose them in chains.

Below, we show a retrieval-augmented generation (RAG) chain that performs question answering over documents using the following steps:

1. Initialize an vector store
2. Create a retriever from that vector store
3. Compose a question answering chain
4. Ask questions!

Each of the steps has multiple sub steps and potential configurations, but we'll go through one common flow.
First, install the required dependency:

import CodeBlock from "@theme/CodeBlock";

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core
```

You can download the `state_of_the_union.txt` file [here](https://github.com/langchain-ai/langchain/blob/master/docs/docs/modules/state_of_the_union.txt).

import RetrievalQAExample from "@examples/chains/retrieval_qa.ts";

<CodeBlock language="typescript">{RetrievalQAExample}</CodeBlock>

Let's walk through what's happening here.

1. We first load a long text and split it into smaller documents using a text splitter.
   We then load those documents (which also embeds the documents using the passed `OpenAIEmbeddings` instance) into HNSWLib, our vector store, creating our index.

2. Though we can query the vector store directly, we convert the vector store into a retriever to return retrieved documents in the right format for the question answering chain.

3. We initialize a retrieval chain, which we'll call later in step 4.

4. We ask questions!

## Next steps

You've now learned how to convert a vector store as a retriever.

See the individual sections for deeper dives on specific retrievers, the [broader tutorial on RAG](/docs/tutorials/rag), or this section to learn how to
[create your own custom retriever over any data source](/docs/how_to/custom_retriever/).



================================================
FILE: docs/core_docs/docs/how_to/vectorstores.mdx
================================================
---
keywords: [similaritySearchWithScore]
---

# How to create and query vector stores

:::info
Head to [Integrations](/docs/integrations/vectorstores) for documentation on built-in integrations with vectorstore providers.
:::

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Vector stores](/docs/concepts/#vectorstores)
- [Embeddings](/docs/concepts/embedding_models)
- [Document loaders](/docs/concepts/document_loaders)

:::

One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding
vectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are
'most similar' to the embedded query. A vector store takes care of storing embedded data and performing vector search
for you.

This walkthrough uses a basic, unoptimized implementation called [`MemoryVectorStore`](https://api.js.langchain.com/classes/langchain.vectorstores_memory.MemoryVectorStore.html) that stores embeddings in-memory and does an exact, linear search for the most similar embeddings.
LangChain contains many built-in integrations - see [this section](/docs/how_to/vectorstores/#which-one-to-pick) for more, or the [full list of integrations](/docs/integrations/vectorstores/).

## Creating a new index

Most of the time, you'll need to load and prepare the data you want to search over. Here's an example that loads a recent speech from a file:

import ExampleLoader from "@examples/indexes/vector_stores/memory_fromdocs.ts";

<CodeBlock language="typescript">{ExampleLoader}</CodeBlock>

Most of the time, you'll need to split the loaded text as a preparation step. See [this section](/docs/concepts/text_splitters) to learn more about text splitters.

## Creating a new index from texts

If you have already prepared the data you want to search over, you can initialize a vector store directly from text chunks:

import CodeBlock from "@theme/CodeBlock";
import ExampleTexts from "@examples/indexes/vector_stores/memory.ts";

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core
```

<CodeBlock language="typescript">{ExampleTexts}</CodeBlock>

## Which one to pick?

Here's a quick guide to help you pick the right vector store for your use case:

- If you're after something that can just run inside your Node.js application, in-memory, without any other servers to stand up, then go for [HNSWLib](/docs/integrations/vectorstores/hnswlib), [Faiss](/docs/integrations/vectorstores/faiss), [LanceDB](/docs/integrations/vectorstores/lancedb) or [CloseVector](/docs/integrations/vectorstores/closevector)
- If you're looking for something that can run in-memory in browser-like environments, then go for [MemoryVectorStore](/docs/integrations/vectorstores/memory) or [CloseVector](/docs/integrations/vectorstores/closevector)
- If you come from Python and you were looking for something similar to FAISS, try [HNSWLib](/docs/integrations/vectorstores/hnswlib) or [Faiss](/docs/integrations/vectorstores/faiss)
- If you're looking for an open-source full-featured vector database that you can run locally in a docker container, then go for [Chroma](/docs/integrations/vectorstores/chroma)
- If you're looking for an open-source vector database that offers low-latency, local embedding of documents and supports apps on the edge, then go for [Zep](/docs/integrations/vectorstores/zep)
- If you're looking for an open-source production-ready vector database that you can run locally (in a docker container) or hosted in the cloud, then go for [Weaviate](/docs/integrations/vectorstores/weaviate).
- If you're using Supabase already then look at the [Supabase](/docs/integrations/vectorstores/supabase) vector store to use the same Postgres database for your embeddings too
- If you're looking for a production-ready vector store you don't have to worry about hosting yourself, then go for [Pinecone](/docs/integrations/vectorstores/pinecone)
- If you are already utilizing SingleStore, or if you find yourself in need of a distributed, high-performance database, you might want to consider the [SingleStore](/docs/integrations/vectorstores/singlestore) vector store.
- If you are looking for an online MPP (Massively Parallel Processing) data warehousing service, you might want to consider the [AnalyticDB](/docs/integrations/vectorstores/analyticdb) vector store.
- If you're in search of a cost-effective vector database that allows run vector search with SQL, look no further than [MyScale](/docs/integrations/vectorstores/myscale).
- If you're in search of a vector database that you can load from both the browser and server side, check out [CloseVector](/docs/integrations/vectorstores/closevector). It's a vector database that aims to be cross-platform.
- If you're looking for a scalable, open-source columnar database with excellent performance for analytical queries, then consider [ClickHouse](/docs/integrations/vectorstores/clickhouse).

## Next steps

You've now learned how to load data into a vectorstore.

Next, check out the [full tutorial on retrieval-augmented generation](/docs/tutorials/rag).



================================================
FILE: docs/core_docs/docs/integrations/callbacks/datadog_tracer.mdx
================================================
---
sidebar_class_name: beta
---

import CodeBlock from "@theme/CodeBlock";

# Datadog LLM Observability

:::warning
LLM Observability is in public beta, and its API is subject to change.
:::

With [Datadog LLM Observability](https://docs.datadoghq.com/llm_observability/), you can monitor, troubleshoot, and evaluate your LLM-powered applications, such as chatbots. You can investigate the root cause of issues, monitor operational performance, and evaluate the quality, privacy, and safety of your LLM applications.

This is an experimental community implementation, and it is not officially supported by Datadog. It is based on the [Datadog LLM Observability API](https://docs.datadoghq.com/llm_observability/api).

## Setup

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core
```

## Usage

import UsageExample from "@examples/callbacks/datadog.ts";

<CodeBlock language="typescript">{UsageExample}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/callbacks/upstash_ratelimit_callback.mdx
================================================
# Upstash Ratelimit Callback

In this guide, we will go over how to add rate limiting based on number of requests or the number of tokens using `UpstashRatelimitHandler`. This handler uses [Upstash's ratelimit library](https://github.com/upstash/ratelimit-js/), which utilizes [Upstash Redis](https://upstash.com/docs/redis/overall/getstarted).

Upstash Ratelimit works by sending an HTTP request to Upstash Redis every time the `limit` method is called. Remaining tokens/requests of the user are checked and updated. Based on the remaining tokens, we can stop the execution of costly operations, like invoking an LLM or querying a vector store:

```tsx
const response = await ratelimit.limit();
if (response.success) {
  execute_costly_operation();
}
```

`UpstashRatelimitHandler` allows you to incorporate this ratelimit logic into your chain in a few minutes.

## Setup

First, you will need to go to [the Upstash Console](https://console.upstash.com/login) and create a redis database ([see our docs](https://upstash.com/docs/redis/overall/getstarted)). After creating a database, you will need to set the environment variables:

```
UPSTASH_REDIS_REST_URL="****"
UPSTASH_REDIS_REST_TOKEN="****"
```

Next, you will need to install Upstash Ratelimit and `@langchain/community`:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @upstash/ratelimit @langchain/community @langchain/core
```

You are now ready to add rate limiting to your chain!

## Ratelimiting Per Request

Let's imagine that we want to allow our users to invoke our chain 10 times per minute. Achieving this is as simple as:

```tsx
const UPSTASH_REDIS_REST_URL = "****";
const UPSTASH_REDIS_REST_TOKEN = "****";

import {
  UpstashRatelimitHandler,
  UpstashRatelimitError,
} from "@langchain/community/callbacks/handlers/upstash_ratelimit";
import { RunnableLambda } from "@langchain/core/runnables";
import { Ratelimit } from "@upstash/ratelimit";
import { Redis } from "@upstash/redis";

// create ratelimit
const ratelimit = new Ratelimit({
  redis: new Redis({
    url: UPSTASH_REDIS_REST_URL,
    token: UPSTASH_REDIS_REST_TOKEN,
  }),
  // 10 requests per window, where window size is 60 seconds:
  limiter: Ratelimit.fixedWindow(10, "60 s"),
});

// create handler
const user_id = "user_id"; // should be a method which gets the user id
const handler = new UpstashRatelimitHandler(user_id, {
  requestRatelimit: ratelimit,
});

// create mock chain
const chain = new RunnableLambda({ func: (str: string): string => str });

try {
  const response = await chain.invoke("hello world", {
    callbacks: [handler],
  });
  console.log(response);
} catch (err) {
  if (err instanceof UpstashRatelimitError) {
    console.log("Handling ratelimit.");
  }
}
```

Note that we pass the handler to the `invoke` method instead of passing the handler when defining the chain.

For rate limiting algorithms other than `FixedWindow`, see [upstash-ratelimit docs](https://upstash.com/docs/oss/sdks/ts/ratelimit/algorithms).

Before executing any steps in our pipeline, ratelimit will check whether the user has passed the request limit. If so, `UpstashRatelimitError` is raised.

## Ratelimiting Per Token

Another option is to rate limit chain invokations based on:

1. number of tokens in prompt
2. number of tokens in prompt and LLM completion

This only works if you have an LLM in your chain. Another requirement is that the LLM you are using should return the token usage in it's `LLMOutput`. The format of the token usage dictionary returned depends on the LLM. To learn about how you should configure the handler depending on your LLM, see the end of the Configuration section below.

### How it works

The handler will get the remaining tokens before calling the LLM. If the remaining tokens is more than 0, LLM will be called. Otherwise `UpstashRatelimitError` will be raised.

After LLM is called, token usage information will be used to subtracted from the remaining tokens of the user. No error is raised at this stage of the chain.

### Configuration

For the first configuration, simply initialize the handler like this:

```tsx
const user_id = "user_id"; // should be a method which gets the user id
const handler = new UpstashRatelimitHandler(user_id, {
  requestRatelimit: ratelimit,
});
```

For the second configuration, here is how to initialize the handler:

```tsx
const user_id = "user_id"; // should be a method which gets the user id
const handler = new UpstashRatelimitHandler(user_id, {
  tokenRatelimit: ratelimit,
});
```

You can also employ ratelimiting based on requests and tokens at the same time, simply by passing both `request_ratelimit` and `token_ratelimit` parameters.

For token usage to work correctly, the LLM step in LangChain.js should return a token usage field in the following format:

```json
{
  "tokenUsage": {
    "totalTokens": 123,
    "promptTokens": 456,
    "otherFields: "..."
  },
  "otherFields: "..."
}
```

Not all LLMs in LangChain.js comply with this format however. If your LLM returns the same values with different keys, you can use the parameters `llmOutputTokenUsageField`, `llmOutputTotalTokenField` and `llmOutputPromptTokenField` by passing them to the handler:

```tsx
const handler = new UpstashRatelimitHandler(
  user_id,
  {
    requestRatelimit: ratelimit
    llmOutputTokenUsageField: "usage",
    llmOutputTotalTokenField: "total",
    llmOutputPromptTokenField: "prompt"
  }
)
```

Here is an example with a chain utilizing an LLM:

```tsx
const UPSTASH_REDIS_REST_URL = "****";
const UPSTASH_REDIS_REST_TOKEN = "****";
const OPENAI_API_KEY = "****";

import {
  UpstashRatelimitHandler,
  UpstashRatelimitError,
} from "@langchain/community/callbacks/handlers/upstash_ratelimit";
import { RunnableLambda, RunnableSequence } from "@langchain/core/runnables";
import { OpenAI } from "@langchain/openai";
import { Ratelimit } from "@upstash/ratelimit";
import { Redis } from "@upstash/redis";

// create ratelimit
const ratelimit = new Ratelimit({
  redis: new Redis({
    url: UPSTASH_REDIS_REST_URL,
    token: UPSTASH_REDIS_REST_TOKEN,
  }),
  // 500 tokens per window, where window size is 60 seconds:
  limiter: Ratelimit.fixedWindow(500, "60 s"),
});

// create handler
const user_id = "user_id"; // should be a method which gets the user id
const handler = new UpstashRatelimitHandler(user_id, {
  tokenRatelimit: ratelimit,
});

// create mock chain
const asStr = new RunnableLambda({ func: (str: string): string => str });
const model = new OpenAI({
  apiKey: OPENAI_API_KEY,
});
const chain = RunnableSequence.from([asStr, model]);

// invoke chain with handler:
try {
  const response = await chain.invoke("hello world", {
    callbacks: [handler],
  });
  console.log(response);
} catch (err) {
  if (err instanceof UpstashRatelimitError) {
    console.log("Handling ratelimit.");
  }
}
```



================================================
FILE: docs/core_docs/docs/integrations/chat/alibaba_tongyi.mdx
================================================
---
sidebar_label: Alibaba Tongyi
---

import CodeBlock from "@theme/CodeBlock";

# ChatAlibabaTongyi

LangChain.js supports the Alibaba qwen family of models.

## Setup

You'll need to sign up for an Alibaba API key and set it as an environment variable named `ALIBABA_API_KEY`.

Then, you'll need to install the [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) package:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core
```

## Usage

Here's an example:

import Tongyi from "@examples/models/chat/integration_alitongyi.ts";

<CodeBlock language="typescript">{Tongyi}</CodeBlock>

## Related

- Chat model [conceptual guide](/docs/concepts/chat_models)
- Chat model [how-to guides](/docs/how_to/#chat-models)



================================================
FILE: docs/core_docs/docs/integrations/chat/anthropic.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Anthropic
---
"""

"""
# ChatAnthropic

[Anthropic](https://www.anthropic.com/) is an AI safety and research company. They are the creator of Claude.

This will help you getting started with Anthropic [chat models](/docs/concepts/chat_models). For detailed documentation of all `ChatAnthropic` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_anthropic.ChatAnthropic.html).

## Overview
### Integration details

| Class | Package | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/anthropic/) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [ChatAnthropic](https://api.js.langchain.com/classes/langchain_anthropic.ChatAnthropic.html) | [`@langchain/anthropic`](https://www.npmjs.com/package/@langchain/anthropic) | ❌ | ✅ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/anthropic?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/anthropic?style=flat-square&label=%20&) |

### Model features

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | [Image input](/docs/how_to/multimodal_inputs/) | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |
| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: |
| ✅ | ✅ | ❌ | ✅ | ❌ | ❌ | ✅ | ✅ | ❌ | 

## Setup

You'll need to sign up and obtain an [Anthropic API key](https://www.anthropic.com/), and install the `@langchain/anthropic` integration package.

### Credentials

Head to [Anthropic's website](https://www.anthropic.com/) to sign up to Anthropic and generate an API key. Once you've done this set the `ANTHROPIC_API_KEY` environment variable:

```bash
export ANTHROPIC_API_KEY="your-api-key"
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain `ChatAnthropic` integration lives in the `@langchain/anthropic` package:

```{=mdx}

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/anthropic @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

import { ChatAnthropic } from "@langchain/anthropic" 

const llm = new ChatAnthropic({
    model: "claude-3-haiku-20240307",
    temperature: 0,
    maxTokens: undefined,
    maxRetries: 2,
    // other params...
});

"""
## Invocation
"""

const aiMsg = await llm.invoke([
    [
        "system",
        "You are a helpful assistant that translates English to French. Translate the user sentence.",
    ],
    ["human", "I love programming."],
])
aiMsg
# Output:
#   AIMessage {

#     "id": "msg_013WBXXiggy6gMbAUY6NpsuU",

#     "content": "Voici la traduction en français :\n\nJ'adore la programmation.",

#     "additional_kwargs": {

#       "id": "msg_013WBXXiggy6gMbAUY6NpsuU",

#       "type": "message",

#       "role": "assistant",

#       "model": "claude-3-haiku-20240307",

#       "stop_reason": "end_turn",

#       "stop_sequence": null,

#       "usage": {

#         "input_tokens": 29,

#         "output_tokens": 20

#       }

#     },

#     "response_metadata": {

#       "id": "msg_013WBXXiggy6gMbAUY6NpsuU",

#       "model": "claude-3-haiku-20240307",

#       "stop_reason": "end_turn",

#       "stop_sequence": null,

#       "usage": {

#         "input_tokens": 29,

#         "output_tokens": 20

#       },

#       "type": "message",

#       "role": "assistant"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 29,

#       "output_tokens": 20,

#       "total_tokens": 49

#     }

#   }


console.log(aiMsg.content)
# Output:
#   Voici la traduction en français :

#   

#   J'adore la programmation.


"""
## Chaining

We can [chain](/docs/how_to/sequence/) our model with a prompt template like so:
"""

import { ChatPromptTemplate } from "@langchain/core/prompts"

const prompt = ChatPromptTemplate.fromMessages(
    [
        [
            "system",
            "You are a helpful assistant that translates {input_language} to {output_language}.",
        ],
        ["human", "{input}"],
    ]
)

const chain = prompt.pipe(llm);
await chain.invoke(
    {
        input_language: "English",
        output_language: "German",
        input: "I love programming.",
    }
)
# Output:
#   AIMessage {

#     "id": "msg_01Ca52fpd1mcGRhH4spzAWr4",

#     "content": "Ich liebe das Programmieren.",

#     "additional_kwargs": {

#       "id": "msg_01Ca52fpd1mcGRhH4spzAWr4",

#       "type": "message",

#       "role": "assistant",

#       "model": "claude-3-haiku-20240307",

#       "stop_reason": "end_turn",

#       "stop_sequence": null,

#       "usage": {

#         "input_tokens": 23,

#         "output_tokens": 11

#       }

#     },

#     "response_metadata": {

#       "id": "msg_01Ca52fpd1mcGRhH4spzAWr4",

#       "model": "claude-3-haiku-20240307",

#       "stop_reason": "end_turn",

#       "stop_sequence": null,

#       "usage": {

#         "input_tokens": 23,

#         "output_tokens": 11

#       },

#       "type": "message",

#       "role": "assistant"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 23,

#       "output_tokens": 11,

#       "total_tokens": 34

#     }

#   }


"""
## Content blocks

One key difference to note between Anthropic models and most others is that the contents of a single Anthropic AI message can either be a single string or a **list of content blocks**. For example when an Anthropic model [calls a tool](/docs/how_to/tool_calling), the tool invocation is part of the message content (as well as being exposed in the standardized `AIMessage.tool_calls` field):
"""

import { ChatAnthropic } from "@langchain/anthropic";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { z } from "zod";
import { zodToJsonSchema } from "zod-to-json-schema";

const calculatorSchema = z.object({
  operation: z
    .enum(["add", "subtract", "multiply", "divide"])
    .describe("The type of operation to execute."),
  number1: z.number().describe("The first number to operate on."),
  number2: z.number().describe("The second number to operate on."),
});

const calculatorTool = {
  name: "calculator",
  description: "A simple calculator tool",
  input_schema: zodToJsonSchema(calculatorSchema),
};

const toolCallingLlm = new ChatAnthropic({
  model: "claude-3-haiku-20240307",
}).bindTools([calculatorTool]);

const toolPrompt = ChatPromptTemplate.fromMessages([
  [
    "system",
    "You are a helpful assistant who always needs to use a calculator.",
  ],
  ["human", "{input}"],
]);

// Chain your prompt and model together
const toolCallChain = toolPrompt.pipe(toolCallingLlm);

await toolCallChain.invoke({
  input: "What is 2 + 2?",
});
# Output:
#   AIMessage {

#     "id": "msg_01DZGs9DyuashaYxJ4WWpWUP",

#     "content": [

#       {

#         "type": "text",

#         "text": "Here is the calculation for 2 + 2:"

#       },

#       {

#         "type": "tool_use",

#         "id": "toolu_01SQXBamkBr6K6NdHE7GWwF8",

#         "name": "calculator",

#         "input": {

#           "number1": 2,

#           "number2": 2,

#           "operation": "add"

#         }

#       }

#     ],

#     "additional_kwargs": {

#       "id": "msg_01DZGs9DyuashaYxJ4WWpWUP",

#       "type": "message",

#       "role": "assistant",

#       "model": "claude-3-haiku-20240307",

#       "stop_reason": "tool_use",

#       "stop_sequence": null,

#       "usage": {

#         "input_tokens": 449,

#         "output_tokens": 100

#       }

#     },

#     "response_metadata": {

#       "id": "msg_01DZGs9DyuashaYxJ4WWpWUP",

#       "model": "claude-3-haiku-20240307",

#       "stop_reason": "tool_use",

#       "stop_sequence": null,

#       "usage": {

#         "input_tokens": 449,

#         "output_tokens": 100

#       },

#       "type": "message",

#       "role": "assistant"

#     },

#     "tool_calls": [

#       {

#         "name": "calculator",

#         "args": {

#           "number1": 2,

#           "number2": 2,

#           "operation": "add"

#         },

#         "id": "toolu_01SQXBamkBr6K6NdHE7GWwF8",

#         "type": "tool_call"

#       }

#     ],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 449,

#       "output_tokens": 100,

#       "total_tokens": 549

#     }

#   }


"""
## Custom headers

You can pass custom headers in your requests like this:
"""

import { ChatAnthropic } from "@langchain/anthropic";

const llmWithCustomHeaders = new ChatAnthropic({
  model: "claude-3-sonnet-20240229",
  maxTokens: 1024,
  clientOptions: {
    defaultHeaders: {
      "X-Api-Key": process.env.ANTHROPIC_API_KEY,
    },
  },
});

await llmWithCustomHeaders.invoke("Why is the sky blue?");
# Output:
#   AIMessage {

#     "id": "msg_019z4nWpShzsrbSHTWXWQh6z",

#     "content": "The sky appears blue due to a phenomenon called Rayleigh scattering. Here's a brief explanation:\n\n1) Sunlight is made up of different wavelengths of visible light, including all the colors of the rainbow.\n\n2) As sunlight passes through the atmosphere, the gases (mostly nitrogen and oxygen) cause the shorter wavelengths of light, such as violet and blue, to be scattered more easily than the longer wavelengths like red and orange.\n\n3) This scattering of the shorter blue wavelengths occurs in all directions by the gas molecules in the atmosphere.\n\n4) Our eyes are more sensitive to the scattered blue light than the scattered violet light, so we perceive the sky as having a blue color.\n\n5) The scattering is more pronounced for light traveling over longer distances through the atmosphere. This is why the sky appears even darker blue when looking towards the horizon.\n\nSo in essence, the selective scattering of the shorter blue wavelengths of sunlight by the gases in the atmosphere is what causes the sky to appear blue to our eyes during the daytime.",

#     "additional_kwargs": {

#       "id": "msg_019z4nWpShzsrbSHTWXWQh6z",

#       "type": "message",

#       "role": "assistant",

#       "model": "claude-3-sonnet-20240229",

#       "stop_reason": "end_turn",

#       "stop_sequence": null,

#       "usage": {

#         "input_tokens": 13,

#         "output_tokens": 236

#       }

#     },

#     "response_metadata": {

#       "id": "msg_019z4nWpShzsrbSHTWXWQh6z",

#       "model": "claude-3-sonnet-20240229",

#       "stop_reason": "end_turn",

#       "stop_sequence": null,

#       "usage": {

#         "input_tokens": 13,

#         "output_tokens": 236

#       },

#       "type": "message",

#       "role": "assistant"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 13,

#       "output_tokens": 236,

#       "total_tokens": 249

#     }

#   }


"""
## Prompt caching

```{=mdx}

:::caution Compatibility
This feature is currently in beta.
:::

```

Anthropic supports [caching parts of your prompt](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching) in order to reduce costs for use-cases that require long context. You can cache tools and both entire messages and individual blocks.

The initial request containing one or more blocks or tool definitions with a `"cache_control": { "type": "ephemeral" }` field will automatically cache that part of the prompt. This initial caching step will cost extra, but subsequent requests will be billed at a reduced rate. The cache has a lifetime of 5 minutes, but this is refereshed each time the cache is hit.

There is also currently a minimum cacheable prompt length, which varies according to model. You can see this information [here](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching#structuring-your-prompt).

This currently requires you to initialize your model with a beta header. Here's an example of caching part of a system message that contains the LangChain [conceptual docs](/docs/concepts/):
"""

let CACHED_TEXT = "...";

// @lc-docs-hide-cell

CACHED_TEXT = `## Components

LangChain provides standard, extendable interfaces and external integrations for various components useful for building with LLMs.
Some components LangChain implements, some components we rely on third-party integrations for, and others are a mix.

### Chat models

<span data-heading-keywords="chat model,chat models"></span>

Language models that use a sequence of messages as inputs and return chat messages as outputs (as opposed to using plain text).
These are generally newer models (older models are generally \`LLMs\`, see below).
Chat models support the assignment of distinct roles to conversation messages, helping to distinguish messages from the AI, users, and instructions such as system messages.

Although the underlying models are messages in, message out, the LangChain wrappers also allow these models to take a string as input.
This gives them the same interface as LLMs (and simpler to use).
When a string is passed in as input, it will be converted to a \`HumanMessage\` under the hood before being passed to the underlying model.

LangChain does not host any Chat Models, rather we rely on third party integrations.

We have some standardized parameters when constructing ChatModels:

- \`model\`: the name of the model

Chat Models also accept other parameters that are specific to that integration.

:::important
Some chat models have been fine-tuned for **tool calling** and provide a dedicated API for it.
Generally, such models are better at tool calling than non-fine-tuned models, and are recommended for use cases that require tool calling.
Please see the [tool calling section](/docs/concepts/tool_calling) for more information.
:::

For specifics on how to use chat models, see the [relevant how-to guides here](/docs/how_to/#chat-models).

#### Multimodality

Some chat models are multimodal, accepting images, audio and even video as inputs.
These are still less common, meaning model providers haven't standardized on the "best" way to define the API.
Multimodal outputs are even less common. As such, we've kept our multimodal abstractions fairly light weight
and plan to further solidify the multimodal APIs and interaction patterns as the field matures.

In LangChain, most chat models that support multimodal inputs also accept those values in OpenAI's content blocks format.
So far this is restricted to image inputs. For models like Gemini which support video and other bytes input, the APIs also support the native, model-specific representations.

For specifics on how to use multimodal models, see the [relevant how-to guides here](/docs/how_to/#multimodal).

### LLMs

<span data-heading-keywords="llm,llms"></span>

:::caution
Pure text-in/text-out LLMs tend to be older or lower-level. Many popular models are best used as [chat completion models](/docs/concepts/chat_models),
even for non-chat use cases.

You are probably looking for [the section above instead](/docs/concepts/chat_models).
:::

Language models that takes a string as input and returns a string.
These are traditionally older models (newer models generally are [Chat Models](/docs/concepts/chat_models), see above).

Although the underlying models are string in, string out, the LangChain wrappers also allow these models to take messages as input.
This gives them the same interface as [Chat Models](/docs/concepts/chat_models).
When messages are passed in as input, they will be formatted into a string under the hood before being passed to the underlying model.

LangChain does not host any LLMs, rather we rely on third party integrations.

For specifics on how to use LLMs, see the [relevant how-to guides here](/docs/how_to/#llms).

### Message types

Some language models take an array of messages as input and return a message.
There are a few different types of messages.
All messages have a \`role\`, \`content\`, and \`response_metadata\` property.

The \`role\` describes WHO is saying the message.
LangChain has different message classes for different roles.

The \`content\` property describes the content of the message.
This can be a few different things:

- A string (most models deal this type of content)
- A List of objects (this is used for multi-modal input, where the object contains information about that input type and that input location)

#### HumanMessage

This represents a message from the user.

#### AIMessage

This represents a message from the model. In addition to the \`content\` property, these messages also have:

**\`response_metadata\`**

The \`response_metadata\` property contains additional metadata about the response. The data here is often specific to each model provider.
This is where information like log-probs and token usage may be stored.

**\`tool_calls\`**

These represent a decision from an language model to call a tool. They are included as part of an \`AIMessage\` output.
They can be accessed from there with the \`.tool_calls\` property.

This property returns a list of \`ToolCall\`s. A \`ToolCall\` is an object with the following arguments:

- \`name\`: The name of the tool that should be called.
- \`args\`: The arguments to that tool.
- \`id\`: The id of that tool call.

#### SystemMessage

This represents a system message, which tells the model how to behave. Not every model provider supports this.

#### ToolMessage

This represents the result of a tool call. In addition to \`role\` and \`content\`, this message has:

- a \`tool_call_id\` field which conveys the id of the call to the tool that was called to produce this result.
- an \`artifact\` field which can be used to pass along arbitrary artifacts of the tool execution which are useful to track but which should not be sent to the model.

#### (Legacy) FunctionMessage

This is a legacy message type, corresponding to OpenAI's legacy function-calling API. \`ToolMessage\` should be used instead to correspond to the updated tool-calling API.

This represents the result of a function call. In addition to \`role\` and \`content\`, this message has a \`name\` parameter which conveys the name of the function that was called to produce this result.

### Prompt templates

<span data-heading-keywords="prompt,prompttemplate,chatprompttemplate"></span>

Prompt templates help to translate user input and parameters into instructions for a language model.
This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.

Prompt Templates take as input an object, where each key represents a variable in the prompt template to fill in.

Prompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a string or an array of messages.
The reason this PromptValue exists is to make it easy to switch between strings and messages.

There are a few different types of prompt templates:

#### String PromptTemplates

These prompt templates are used to format a single string, and generally are used for simpler inputs.
For example, a common way to construct and use a PromptTemplate is as follows:

\`\`\`typescript
import { PromptTemplate } from "@langchain/core/prompts";

const promptTemplate = PromptTemplate.fromTemplate(
  "Tell me a joke about {topic}"
);

await promptTemplate.invoke({ topic: "cats" });
\`\`\`

#### ChatPromptTemplates

These prompt templates are used to format an array of messages. These "templates" consist of an array of templates themselves.
For example, a common way to construct and use a ChatPromptTemplate is as follows:

\`\`\`typescript
import { ChatPromptTemplate } from "@langchain/core/prompts";

const promptTemplate = ChatPromptTemplate.fromMessages([
  ["system", "You are a helpful assistant"],
  ["user", "Tell me a joke about {topic}"],
]);

await promptTemplate.invoke({ topic: "cats" });
\`\`\`

In the above example, this ChatPromptTemplate will construct two messages when called.
The first is a system message, that has no variables to format.
The second is a HumanMessage, and will be formatted by the \`topic\` variable the user passes in.

#### MessagesPlaceholder

<span data-heading-keywords="messagesplaceholder"></span>

This prompt template is responsible for adding an array of messages in a particular place.
In the above ChatPromptTemplate, we saw how we could format two messages, each one a string.
But what if we wanted the user to pass in an array of messages that we would slot into a particular spot?
This is how you use MessagesPlaceholder.

\`\`\`typescript
import {
  ChatPromptTemplate,
  MessagesPlaceholder,
} from "@langchain/core/prompts";
import { HumanMessage } from "@langchain/core/messages";

const promptTemplate = ChatPromptTemplate.fromMessages([
  ["system", "You are a helpful assistant"],
  new MessagesPlaceholder("msgs"),
]);

promptTemplate.invoke({ msgs: [new HumanMessage({ content: "hi!" })] });
\`\`\`

This will produce an array of two messages, the first one being a system message, and the second one being the HumanMessage we passed in.
If we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5 passed in).
This is useful for letting an array of messages be slotted into a particular spot.

An alternative way to accomplish the same thing without using the \`MessagesPlaceholder\` class explicitly is:

\`\`\`typescript
const promptTemplate = ChatPromptTemplate.fromMessages([
  ["system", "You are a helpful assistant"],
  ["placeholder", "{msgs}"], // <-- This is the changed part
]);
\`\`\`

For specifics on how to use prompt templates, see the [relevant how-to guides here](/docs/how_to/#prompt-templates).

### Example Selectors

One common prompting technique for achieving better performance is to include examples as part of the prompt.
This gives the language model concrete examples of how it should behave.
Sometimes these examples are hardcoded into the prompt, but for more advanced situations it may be nice to dynamically select them.
Example Selectors are classes responsible for selecting and then formatting examples into prompts.

For specifics on how to use example selectors, see the [relevant how-to guides here](/docs/how_to/#example-selectors).

### Output parsers

<span data-heading-keywords="output parser"></span>

:::note

The information here refers to parsers that take a text output from a model try to parse it into a more structured representation.
More and more models are supporting function (or tool) calling, which handles this automatically.
It is recommended to use function/tool calling rather than output parsing.
See documentation for that [here](/docs/concepts/tool_calling).

:::

Responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks.
Useful when you are using LLMs to generate structured data, or to normalize output from chat models and LLMs.

There are two main methods an output parser must implement:

- "Get format instructions": A method which returns a string containing instructions for how the output of a language model should be formatted.
- "Parse": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.

And then one optional one:

- "Parse with prompt": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.

Output parsers accept a string or \`BaseMessage\` as input and can return an arbitrary type.

LangChain has many different types of output parsers. This is a list of output parsers LangChain supports. The table below has various pieces of information:

**Name**: The name of the output parser

**Supports Streaming**: Whether the output parser supports streaming.

**Input Type**: Expected input type. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need a message with specific arguments.

**Output Type**: The output type of the object returned by the parser.

**Description**: Our commentary on this output parser and when to use it.

The current date is ${new Date().toISOString()}`;

// Noop statement to hide output
void 0;

import { ChatAnthropic } from "@langchain/anthropic";

const modelWithCaching = new ChatAnthropic({
  model: "claude-3-haiku-20240307",
  clientOptions: {
    defaultHeaders: {
      "anthropic-beta": "prompt-caching-2024-07-31",
    },
  },
});

const LONG_TEXT = `You are a pirate. Always respond in pirate dialect.

Use the following as context when answering questions:

${CACHED_TEXT}`;

const messages = [
  {
    role: "system",
    content: [
      {
        type: "text",
        text: LONG_TEXT,
        // Tell Anthropic to cache this block
        cache_control: { type: "ephemeral" },
      },
    ],
  },
  {
    role: "user",
    content: "What types of messages are supported in LangChain?",
  },
];

const res = await modelWithCaching.invoke(messages);

console.log("USAGE:", res.response_metadata.usage);
# Output:
#   USAGE: {

#     input_tokens: 19,

#     cache_creation_input_tokens: 2921,

#     cache_read_input_tokens: 0,

#     output_tokens: 355

#   }


"""
We can see that there's a new field called `cache_creation_input_tokens` in the raw usage field returned from Anthropic.

If we use the same messages again, we can see that the long text's input tokens are read from the cache:
"""

const res2 = await modelWithCaching.invoke(messages);

console.log("USAGE:", res2.response_metadata.usage);
# Output:
#   USAGE: {

#     input_tokens: 19,

#     cache_creation_input_tokens: 0,

#     cache_read_input_tokens: 2921,

#     output_tokens: 357

#   }


"""
### Tool caching

You can also cache tools by setting the same `"cache_control": { "type": "ephemeral" }` within a tool definition. This currently requires you to bind a tool in [Anthropic's raw tool format](https://docs.anthropic.com/en/docs/build-with-claude/tool-use) Here's an example:
"""

const SOME_LONG_DESCRIPTION = "...";

// Tool in Anthropic format
const anthropicTools = [{
  name: "get_weather",
  description: SOME_LONG_DESCRIPTION,
  input_schema: {
    type: "object",
    properties: {
      location: {
        type: "string",
        description: "Location to get the weather for",
      },
      unit: {
        type: "string",
        description: "Temperature unit to return",
      },
    },
    required: ["location"],
  },
  // Tell Anthropic to cache this tool
  cache_control: { type: "ephemeral" },
}]

const modelWithCachedTools = modelWithCaching.bindTools(anthropicTools);

await modelWithCachedTools.invoke("what is the weather in SF?");

"""


For more on how prompt caching works, see [Anthropic's docs](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching#how-prompt-caching-works).
"""

"""
## Custom clients

Anthropic models [may be hosted on cloud services such as Google Vertex](https://docs.anthropic.com/en/api/claude-on-vertex-ai) that rely on a different underlying client with the same interface as the primary Anthropic client. You can access these services by providing a `createClient` method that returns an initialized instance of an Anthropic client. Here's an example:
"""

import { AnthropicVertex } from "@anthropic-ai/vertex-sdk";

const customClient = new AnthropicVertex();

const modelWithCustomClient = new ChatAnthropic({
  modelName: "claude-3-sonnet@20240229",
  maxRetries: 0,
  createClient: () => customClient,
});

await modelWithCustomClient.invoke([{ role: "user", content: "Hello!" }]);

"""
## Citations

Anthropic supports a [citations](https://docs.anthropic.com/en/docs/build-with-claude/citations) feature that lets Claude attach context to its answers based on source documents supplied by the user. When [document content blocks](https://docs.anthropic.com/en/docs/build-with-claude/citations#document-types) with `"citations": {"enabled": True}` are included in a query, Claude may generate citations in its response.

### Simple example

In this example we pass a [plain text document](https://docs.anthropic.com/en/docs/build-with-claude/citations#plain-text-documents). In the background, Claude [automatically chunks](https://docs.anthropic.com/en/docs/build-with-claude/citations#plain-text-documents) the input text into sentences, which are used when generating citations.
"""

import { ChatAnthropic } from "@langchain/anthropic";

const citationsModel = new ChatAnthropic({
  model: "claude-3-5-haiku-latest",
});

const messagesWithCitations = [
  {
    role: "user",
    content: [
      {
        type: "document",
        source: {
          type: "text",
          media_type: "text/plain",
          data: "The grass is green. The sky is blue.",
        },
        title: "My Document",
        context: "This is a trustworthy document.",
        citations: {
          enabled: true,
        },
      },
      {
        type: "text",
        text: "What color is the grass and sky?",
      },
    ],
  }
];

const responseWithCitations = await citationsModel.invoke(messagesWithCitations);

console.log(JSON.stringify(responseWithCitations.content, null, 2));
# Output:
#   [

#     {

#       "type": "text",

#       "text": "Based on the document, I can tell you that:\n\n- "

#     },

#     {

#       "type": "text",

#       "text": "The grass is green",

#       "citations": [

#         {

#           "type": "char_location",

#           "cited_text": "The grass is green. ",

#           "document_index": 0,

#           "document_title": "My Document",

#           "start_char_index": 0,

#           "end_char_index": 20

#         }

#       ]

#     },

#     {

#       "type": "text",

#       "text": "\n- "

#     },

#     {

#       "type": "text",

#       "text": "The sky is blue",

#       "citations": [

#         {

#           "type": "char_location",

#           "cited_text": "The sky is blue.",

#           "document_index": 0,

#           "document_title": "My Document",

#           "start_char_index": 20,

#           "end_char_index": 36

#         }

#       ]

#     }

#   ]


"""
### Using with text splitters

Anthropic also lets you specify your own splits using [custom document](https://docs.anthropic.com/en/docs/build-with-claude/citations#custom-content-documents) types. LangChain [text splitters](/docs/concepts/text_splitters/) can be used to generate meaningful splits for this purpose. See the below example, where we split the LangChain.js README (a markdown document) and pass it to Claude as context:
"""

import { ChatAnthropic } from "@langchain/anthropic";
import { MarkdownTextSplitter } from "langchain/text_splitter";

function formatToAnthropicDocuments(documents: string[]) {
  return {
    type: "document",
    source: {
      type: "content",
      content: documents.map((document) => ({ type: "text", text: document })),
    },
    citations: { enabled: true },
  };
}

// Pull readme
const readmeResponse = await fetch(
  "https://raw.githubusercontent.com/langchain-ai/langchainjs/master/README.md"
);

const readme = await readmeResponse.text();

// Split into chunks
const splitter = new MarkdownTextSplitter({
  chunkOverlap: 0,
  chunkSize: 50,
});
const documents = await splitter.splitText(readme);

// Construct message
const messageWithSplitDocuments = {
  role: "user",
  content: [
    formatToAnthropicDocuments(documents),
    { type: "text", text: "Give me a link to LangChain's tutorials. Cite your sources" },
  ],
};

// Query LLM
const citationsModelWithSplits = new ChatAnthropic({
  model: "claude-3-5-sonnet-latest",
});
const resWithSplits = await citationsModelWithSplits.invoke([messageWithSplitDocuments]);

console.log(JSON.stringify(resWithSplits.content, null, 2));
# Output:
#   [

#     {

#       "type": "text",

#       "text": "Based on the documentation, I can provide you with a link to LangChain's tutorials:\n\n"

#     },

#     {

#       "type": "text",

#       "text": "The tutorials can be found at: https://js.langchain.com/docs/tutorials/",

#       "citations": [

#         {

#           "type": "content_block_location",

#           "cited_text": "[Tutorial](https://js.langchain.com/docs/tutorials/)walkthroughs",

#           "document_index": 0,

#           "document_title": null,

#           "start_block_index": 191,

#           "end_block_index": 194

#         }

#       ]

#     }

#   ]


"""
## API reference

For detailed documentation of all ChatAnthropic features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_anthropic.ChatAnthropic.html
"""



================================================
FILE: docs/core_docs/docs/integrations/chat/arcjet.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Arcjet Redact
---
"""

"""
# Arcjet Redact

The [Arcjet](https://arcjet.com) redact integration allows you to redact sensitive user information from your prompts before sending it to a Chat Model.

Arcjet Redact runs entirely on your own machine and never sends data anywhere else, ensuring best in class privacy and performance.

The Arcjet Redact object is not a chat model itself, instead it wraps an LLM. It redacts the text that is inputted to it and then unredacts the output of the wrapped chat model before returning it. 



## Overview
### Integration details

| Class | Package | Local | Serializable | PY Support | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| Arcjet | @langchain/community | ❌ | ✅ | ❌ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/community?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |

### Installation

Install the Arcjet Redaction Library:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @arcjet/redact
</Npm2Yarn>

```

And install LangChain Community:


```{=mdx}
<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core
</Npm2Yarn>

And now you're ready to start protecting your chat model calls with Arcjet Redaction!

```
"""

"""
## Usage
"""

import {
  ArcjetRedact,
  ArcjetSensitiveInfoType,
} from "@langchain/community/chat_models/arcjet";
import { ChatOpenAI } from "@langchain/openai";

// Create an instance of another chat model for Arcjet to wrap
const openai = new ChatOpenAI({
  temperature: 0.8,
  model: "gpt-3.5-turbo-0125",
});

const arcjetRedactOptions = {
  // Specify a LLM that Arcjet Redact will call once it has redacted the input.
  chatModel: openai,

  // Specify the list of entities that should be redacted.
  // If this isn't specified then all entities will be redacted.
  entities: ["email", "phone-number", "ip-address", "custom-entity"] as ArcjetSensitiveInfoType[],

  // You can provide a custom detect function to detect entities that we don't support yet.
  // It takes a list of tokens and you return a list of identified types or undefined.
  // The undefined types that you return should be added to the entities list if used.
  detect: (tokens: string[]) => {
    return tokens.map((t) => t === "some-sensitive-info" ? "custom-entity" : undefined)
  },

  // The number of tokens to provide to the custom detect function. This defaults to 1.
  // It can be used to provide additional context when detecting custom entity types.
  contextWindowSize: 1,

  // This allows you to provide custom replacements when redacting. Please ensure
  // that the replacements are unique so that unredaction works as expected.
  replace: (identifiedType: string) => {
    return identifiedType === "email" ? "redacted@example.com" : undefined;
  },
};

const arcjetRedact = new ArcjetRedact(arcjetRedactOptions);

const response = await arcjetRedact.invoke(
  "My email address is test@example.com, here is some-sensitive-info"
);



================================================
FILE: docs/core_docs/docs/integrations/chat/azure.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Azure OpenAI
---
"""

"""
# AzureChatOpenAI

Azure OpenAI is a Microsoft Azure service that provides powerful language models from OpenAI.

This will help you getting started with AzureChatOpenAI [chat models](/docs/concepts/chat_models). For detailed documentation of all AzureChatOpenAI features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_openai.AzureChatOpenAI.html).

## Overview
### Integration details

| Class | Package | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/azure_chat_openai) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [AzureChatOpenAI](https://api.js.langchain.com/classes/langchain_openai.AzureChatOpenAI.html) | [`@langchain/openai`](https://www.npmjs.com/package/@langchain/openai) | ❌ | ✅ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/openai?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/openai?style=flat-square&label=%20&) |

### Model features

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | [Image input](/docs/how_to/multimodal_inputs/) | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |
| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: |
| ✅ | ✅ | ✅ | ✅ | ❌ | ❌ | ✅ | ✅ | ✅ | 

## Setup

[Azure OpenAI](https://azure.microsoft.com/products/ai-services/openai-service/) is a cloud service to help you quickly develop generative AI experiences with a diverse set of prebuilt and curated models from OpenAI, Meta and beyond.

LangChain.js supports integration with [Azure OpenAI](https://azure.microsoft.com/products/ai-services/openai-service/) using the new Azure integration in the [OpenAI SDK](https://github.com/openai/openai-node).

You can learn more about Azure OpenAI and its difference with the OpenAI API on [this page](https://learn.microsoft.com/azure/ai-services/openai/overview).

### Credentials

If you don't have an Azure account, you can [create a free account](https://azure.microsoft.com/free/) to get started.

You'll also need to have an Azure OpenAI instance deployed. You can deploy a version on Azure Portal following [this guide](https://learn.microsoft.com/azure/ai-services/openai/how-to/create-resource?pivots=web-portal).

Once you have your instance running, make sure you have the name of your instance and key. You can find the key in the Azure Portal, under the "Keys and Endpoint" section of your instance. Then, if using Node.js, you can set your credentials as environment variables:

```bash
AZURE_OPENAI_API_INSTANCE_NAME=<YOUR_INSTANCE_NAME>
AZURE_OPENAI_API_DEPLOYMENT_NAME=<YOUR_DEPLOYMENT_NAME>
AZURE_OPENAI_API_KEY=<YOUR_KEY>
AZURE_OPENAI_API_VERSION="2024-02-01"
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain AzureChatOpenAI integration lives in the `@langchain/openai` package:

```{=mdx}

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/openai @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

import { AzureChatOpenAI } from "@langchain/openai" 

const llm = new AzureChatOpenAI({
    model: "gpt-4o",
    temperature: 0,
    maxTokens: undefined,
    maxRetries: 2,
    azureOpenAIApiKey: process.env.AZURE_OPENAI_API_KEY, // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY
    azureOpenAIApiInstanceName: process.env.AZURE_OPENAI_API_INSTANCE_NAME, // In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME
    azureOpenAIApiDeploymentName: process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME, // In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME
    azureOpenAIApiVersion: process.env.AZURE_OPENAI_API_VERSION, // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION
})

"""
## Invocation
"""

const aiMsg = await llm.invoke([
    [
        "system",
        "You are a helpful assistant that translates English to French. Translate the user sentence.",
    ],
    ["human", "I love programming."],
])
aiMsg
# Output:
#   AIMessage {

#     "id": "chatcmpl-9qrWKByvVrzWMxSn8joRZAklHoB32",

#     "content": "J'adore la programmation.",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "completionTokens": 8,

#         "promptTokens": 31,

#         "totalTokens": 39

#       },

#       "finish_reason": "stop"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 31,

#       "output_tokens": 8,

#       "total_tokens": 39

#     }

#   }


console.log(aiMsg.content)
# Output:
#   J'adore la programmation.


"""
## Chaining

We can [chain](/docs/how_to/sequence/) our model with a prompt template like so:
"""

import { ChatPromptTemplate } from "@langchain/core/prompts"

const prompt = ChatPromptTemplate.fromMessages(
    [
        [
            "system",
            "You are a helpful assistant that translates {input_language} to {output_language}.",
        ],
        ["human", "{input}"],
    ]
)

const chain = prompt.pipe(llm);
await chain.invoke(
    {
        input_language: "English",
        output_language: "German",
        input: "I love programming.",
    }
)
# Output:
#   AIMessage {

#     "id": "chatcmpl-9qrWR7WiNjZ3leSG4Wd77cnKEVivv",

#     "content": "Ich liebe das Programmieren.",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "completionTokens": 6,

#         "promptTokens": 26,

#         "totalTokens": 32

#       },

#       "finish_reason": "stop"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 26,

#       "output_tokens": 6,

#       "total_tokens": 32

#     }

#   }


"""
## Using Azure Managed Identity

If you're using Azure Managed Identity, you can configure the credentials like this:
"""

import {
  DefaultAzureCredential,
  getBearerTokenProvider,
} from "@azure/identity";
import { AzureChatOpenAI } from "@langchain/openai";

const credentials = new DefaultAzureCredential();
const azureADTokenProvider = getBearerTokenProvider(
  credentials,
  "https://cognitiveservices.azure.com/.default"
);

const llmWithManagedIdentity = new AzureChatOpenAI({
  azureADTokenProvider,
  azureOpenAIApiInstanceName: "<your_instance_name>",
  azureOpenAIApiDeploymentName: "<your_deployment_name>",
  azureOpenAIApiVersion: "<api_version>",
});

"""
## Using a different domain

If your instance is hosted under a domain other than the default `openai.azure.com`, you'll need to use the alternate `AZURE_OPENAI_BASE_PATH` environment variable.
For example, here's how you would connect to the domain `https://westeurope.api.microsoft.com/openai/deployments/{DEPLOYMENT_NAME}`:
"""

import { AzureChatOpenAI } from "@langchain/openai";

const llmWithDifferentDomain = new AzureChatOpenAI({
  temperature: 0.9,
  azureOpenAIApiKey: "<your_key>", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY
  azureOpenAIApiDeploymentName: "<your_deployment_name>", // In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME
  azureOpenAIApiVersion: "<api_version>", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION
  azureOpenAIBasePath:
    "https://westeurope.api.microsoft.com/openai/deployments", // In Node.js defaults to process.env.AZURE_OPENAI_BASE_PATH
});


"""
## Custom headers

You can specify custom headers by passing in a `configuration` field:
"""

import { AzureChatOpenAI } from "@langchain/openai";

const llmWithCustomHeaders = new AzureChatOpenAI({
  azureOpenAIApiKey: process.env.AZURE_OPENAI_API_KEY, // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY
  azureOpenAIApiInstanceName: process.env.AZURE_OPENAI_API_INSTANCE_NAME, // In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME
  azureOpenAIApiDeploymentName: process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME, // In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME
  azureOpenAIApiVersion: process.env.AZURE_OPENAI_API_VERSION, // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION
  configuration: {
    defaultHeaders: {
      "x-custom-header": `SOME_VALUE`,
    },
  },
});

await llmWithCustomHeaders.invoke("Hi there!");

"""
The `configuration` field also accepts other `ClientOptions` parameters accepted by the official SDK.

**Note:** The specific header `api-key` currently cannot be overridden in this manner and will pass through the value from `azureOpenAIApiKey`.
"""

"""
## Migration from Azure OpenAI SDK

If you are using the deprecated Azure OpenAI SDK with the `@langchain/azure-openai` package, you can update your code to use the new Azure integration following these steps:

1. Install the new `@langchain/openai` package and remove the previous `@langchain/azure-openai` package:

```{=mdx}

<Npm2Yarn>
  @langchain/openai
</Npm2Yarn>

```

```bash
npm uninstall @langchain/azure-openai
```

   
2. Update your imports to use the new `AzureChatOpenAI` class from the `@langchain/openai` package:
   ```typescript
   import { AzureChatOpenAI } from "@langchain/openai";
   ```
3. Update your code to use the new `AzureChatOpenAI` class and pass the required parameters:

   ```typescript
   const model = new AzureChatOpenAI({
     azureOpenAIApiKey: "<your_key>",
     azureOpenAIApiInstanceName: "<your_instance_name>",
     azureOpenAIApiDeploymentName: "<your_deployment_name>",
     azureOpenAIApiVersion: "<api_version>",
   });
   ```

   Notice that the constructor now requires the `azureOpenAIApiInstanceName` parameter instead of the `azureOpenAIEndpoint` parameter, and adds the `azureOpenAIApiVersion` parameter to specify the API version.

   - If you were using Azure Managed Identity, you now need to use the `azureADTokenProvider` parameter to the constructor instead of `credentials`, see the [Azure Managed Identity](#using-azure-managed-identity) section for more details.

   - If you were using environment variables, you now have to set the `AZURE_OPENAI_API_INSTANCE_NAME` environment variable instead of `AZURE_OPENAI_API_ENDPOINT`, and add the `AZURE_OPENAI_API_VERSION` environment variable to specify the API version.

"""

"""
## API reference

For detailed documentation of all AzureChatOpenAI features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_openai.AzureChatOpenAI.html
"""



================================================
FILE: docs/core_docs/docs/integrations/chat/baidu_qianfan.mdx
================================================
---
sidebar_label: Baidu Qianfan
---

import CodeBlock from "@theme/CodeBlock";

# ChatBaiduQianfan

## Setup

You'll first need to install the [`@langchain/baidu-qianfan`](https://www.npmjs.com/package/@langchain/baidu-qianfan) package:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/baidu-qianfan @langchain/core
```

Available models: `ERNIE-Bot`,`ERNIE-Lite-8K`,`ERNIE-Bot-4`,`ERNIE-Speed-8K`,`ERNIE-Speed-128K`,`ERNIE-4.0-8K`,
`ERNIE-4.0-8K-Preview`,`ERNIE-3.5-8K`,`ERNIE-3.5-8K-Preview`,`ERNIE-Lite-8K`,`ERNIE-Tiny-8K`,`ERNIE-Character-8K`,
`ERNIE Speed-AppBuilder`

Abandoned models: `ERNIE-Bot-turbo`

## Usage

import ChatBaiduQianfanExample from "@examples/models/chat/chat_baidu_qianfan.ts";

<CodeBlock language="typescript">{ChatBaiduQianfanExample}</CodeBlock>

## Streaming

Qianfan's API also supports streaming token responses. The example below demonstrates how to use this feature.

import ChatBaiduQianfanStreamExample from "@examples/models/chat/chat_stream_baidu_qianfan.ts";

<CodeBlock language="typescript">{ChatBaiduQianfanStreamExample}</CodeBlock>

## Related

- Chat model [conceptual guide](/docs/concepts/chat_models)
- Chat model [how-to guides](/docs/how_to/#chat-models)



================================================
FILE: docs/core_docs/docs/integrations/chat/baidu_wenxin.mdx
================================================
---
sidebar_label: Baidu Wenxin
sidebar_class_name: hidden
---

import CodeBlock from "@theme/CodeBlock";

# ChatBaiduWenxin

:::warning
This class has been deprecated.

Use the [`@langchain/baidu-qianfan`](/docs/integrations/chat/baidu_qianfan/) package instead.
:::

LangChain.js supports Baidu's ERNIE-bot family of models. Here's an example:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core
```

Available models: `ERNIE-Bot`,`ERNIE-Bot-turbo`,`ERNIE-Bot-4`,`ERNIE-Speed-8K`,`ERNIE-Speed-128K`,`ERNIE-4.0-8K`,
`ERNIE-4.0-8K-Preview`,`ERNIE-3.5-8K`,`ERNIE-3.5-8K-Preview`,`ERNIE-Lite-8K`,`ERNIE-Tiny-8K`,`ERNIE-Character-8K`,
`ERNIE Speed-AppBuilder`

Abandoned models: `ERNIE-Bot-turbo`

import Wenxin from "@examples/models/chat/integration_baiduwenxin.ts";

<CodeBlock language="typescript">{Wenxin}</CodeBlock>

## Related

- Chat model [conceptual guide](/docs/concepts/chat_models)
- Chat model [how-to guides](/docs/how_to/#chat-models)



================================================
FILE: docs/core_docs/docs/integrations/chat/bedrock.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Amazon Bedrock
---
"""

"""
# BedrockChat

[Amazon Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon via a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI. 

This will help you getting started with Amazon Bedrock [chat models](/docs/concepts/chat_models). For detailed documentation of all `BedrockChat` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_chat_models_bedrock.BedrockChat.html).

```{=mdx}
:::tip
The newer [`ChatBedrockConverse` chat model is now available via the dedicated `@langchain/aws`](/docs/integrations/chat/bedrock_converse) integration package. Use [tool calling](/docs/concepts/tool_calling) with more models with this package.
:::
```

## Overview
### Integration details

| Class | Package | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/bedrock/) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [`BedrockChat`](https://api.js.langchain.com/classes/langchain_community_chat_models_bedrock.BedrockChat.html) | [`@langchain/community`](https://npmjs.com/@langchain/community) | ❌ | ✅ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/community?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |

### Model features

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | [Image input](/docs/how_to/multimodal_inputs/) | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |
| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: |
| ✅ | ✅ | ❌ | ✅ | ❌ | ❌ | ✅ | ✅ | ❌ | 

## Setup

To access Bedrock models you'll need to create an AWS account, set up the Bedrock API service, get an access key ID and secret key, and install the `@langchain/community` integration package.

### Credentials

Head to the [AWS docs](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html) to sign up for AWS and setup your credentials. You'll also need to turn on model access for your account, which you can do by [following these instructions](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html).

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain `BedrockChat` integration lives in the `@langchain/community` package. You'll also need to install several official AWS packages as peer dependencies:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core @aws-crypto/sha256-js @aws-sdk/credential-provider-node @smithy/protocol-http @smithy/signature-v4 @smithy/eventstream-codec @smithy/util-utf8 @aws-sdk/types
</Npm2Yarn>
```

You can also use BedrockChat in web environments such as Edge functions or Cloudflare Workers by omitting the @aws-sdk/credential-provider-node dependency and using the web entrypoint:

```{=mdx}
<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core @aws-crypto/sha256-js @smithy/protocol-http @smithy/signature-v4 @smithy/eventstream-codec @smithy/util-utf8 @aws-sdk/types
</Npm2Yarn>

```
"""

"""
## Instantiation

Currently, only Anthropic, Cohere, and Mistral models are supported with the chat model integration. For foundation models from AI21 or Amazon, see the [text generation Bedrock variant](/docs/integrations/llms/bedrock/).

There are a few different ways to authenticate with AWS - the below examples rely on an access key, secret access key and region set in your environment variables:
"""

import { BedrockChat } from "@langchain/community/chat_models/bedrock";

const llm = new BedrockChat({
  model: "anthropic.claude-3-5-sonnet-20240620-v1:0",
  region: process.env.BEDROCK_AWS_REGION,
  credentials: {
    accessKeyId: process.env.BEDROCK_AWS_ACCESS_KEY_ID!,
    secretAccessKey: process.env.BEDROCK_AWS_SECRET_ACCESS_KEY!,
  },
  // endpointUrl: "custom.amazonaws.com",
  // modelKwargs: {
  //   anthropic_version: "bedrock-2023-05-31",
  // },
});

"""
## Invocation
"""

const aiMsg = await llm.invoke([
  [
    "system",
    "You are a helpful assistant that translates English to French. Translate the user sentence.",
  ],
  ["human", "I love programming."],
])
aiMsg
# Output:
#   AIMessage {

#     "content": "J'adore la programmation.",

#     "additional_kwargs": {

#       "id": "msg_bdrk_01RwhfuWkLLcp7ks1X3u8bwd"

#     },

#     "response_metadata": {

#       "type": "message",

#       "role": "assistant",

#       "model": "claude-3-5-sonnet-20240620",

#       "stop_reason": "end_turn",

#       "stop_sequence": null,

#       "usage": {

#         "input_tokens": 29,

#         "output_tokens": 11

#       }

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": []

#   }


console.log(aiMsg.content)
# Output:
#   J'adore la programmation.


"""
## Chaining

We can [chain](/docs/how_to/sequence/) our model with a prompt template like so:
"""

import { ChatPromptTemplate } from "@langchain/core/prompts"

const prompt = ChatPromptTemplate.fromMessages(
  [
    [
      "system",
      "You are a helpful assistant that translates {input_language} to {output_language}.",
    ],
    ["human", "{input}"],
  ]
)

const chain = prompt.pipe(llm);
await chain.invoke(
  {
    input_language: "English",
    output_language: "German",
    input: "I love programming.",
  }
)
# Output:
#   AIMessage {

#     "content": "Here's the German translation:\n\nIch liebe Programmieren.",

#     "additional_kwargs": {

#       "id": "msg_bdrk_01RtUH3qrYJPUdutYoxphFkv"

#     },

#     "response_metadata": {

#       "type": "message",

#       "role": "assistant",

#       "model": "claude-3-5-sonnet-20240620",

#       "stop_reason": "end_turn",

#       "stop_sequence": null,

#       "usage": {

#         "input_tokens": 23,

#         "output_tokens": 18

#       }

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": []

#   }


"""
## Tool calling

Tool calling with Bedrock models works in a similar way to [other models](/docs/how_to/tool_calling), but note that not all Bedrock models support tool calling. Please refer to the [AWS model documentation](https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html) for more information.
"""

"""
## API reference

For detailed documentation of all `BedrockChat` features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_community_chat_models_bedrock.BedrockChat.html
"""



================================================
FILE: docs/core_docs/docs/integrations/chat/bedrock_converse.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Amazon Bedrock Converse
---
"""

"""
# ChatBedrockConverse

[Amazon Bedrock Converse](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html) is a fully managed service that makes Foundation Models (FMs) from leading AI startups and Amazon available via an API. You can choose from a wide range of FMs to find the model that is best suited for your use case. It provides a unified conversational interface for Bedrock models, but does not yet have feature parity for all functionality within the older [Bedrock model service](/docs/integrations/chat/bedrock).

This will help you getting started with Amazon Bedrock Converse [chat models](/docs/concepts/chat_models). For detailed documentation of all `ChatBedrockConverse` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_aws.ChatBedrockConverse.html).

## Overview
### Integration details

| Class | Package | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/bedrock/#beta-bedrock-converse-api) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [`ChatBedrockConverse`](https://api.js.langchain.com/classes/langchain_aws.ChatBedrockConverse.html) | [`@langchain/aws`](https://npmjs.com/@langchain/aws) | ❌ | ✅ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/aws?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/aws?style=flat-square&label=%20&) |

### Model features

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | [Image input](/docs/how_to/multimodal_inputs/) | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |
| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: |
| ✅ | ✅ | ❌ | ✅ | ❌ | ❌ | ✅ | ✅ | ❌ | 

## Setup

To access Bedrock models you'll need to create an AWS account, set up the Bedrock API service, get an access key ID and secret key, and install the `@langchain/community` integration package.

### Credentials

Head to the [AWS docs](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html) to sign up for AWS and setup your credentials. You'll also need to turn on model access for your account, which you can do by [following these instructions](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html).

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain `ChatBedrockConverse` integration lives in the `@langchain/aws` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/aws @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions.

There are a few different ways to authenticate with AWS - the below examples rely on an access key, secret access key and region set in your environment variables:
"""

import { ChatBedrockConverse } from "@langchain/aws";

const llm = new ChatBedrockConverse({
  model: "anthropic.claude-3-5-sonnet-20240620-v1:0",
  region: process.env.BEDROCK_AWS_REGION,
  credentials: {
    accessKeyId: process.env.BEDROCK_AWS_ACCESS_KEY_ID!,
    secretAccessKey: process.env.BEDROCK_AWS_SECRET_ACCESS_KEY!,
  },
});

"""
## Invocation
"""

const aiMsg = await llm.invoke([
  [
    "system",
    "You are a helpful assistant that translates English to French. Translate the user sentence.",
  ],
  ["human", "I love programming."],
])
aiMsg
# Output:
#   AIMessage {

#     "id": "f5dc5791-224e-4fe5-ba2e-4cc51d9e7795",

#     "content": "J'adore la programmation.",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "$metadata": {

#         "httpStatusCode": 200,

#         "requestId": "f5dc5791-224e-4fe5-ba2e-4cc51d9e7795",

#         "attempts": 1,

#         "totalRetryDelay": 0

#       },

#       "metrics": {

#         "latencyMs": 584

#       },

#       "stopReason": "end_turn",

#       "usage": {

#         "inputTokens": 29,

#         "outputTokens": 11,

#         "totalTokens": 40

#       }

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 29,

#       "output_tokens": 11,

#       "total_tokens": 40

#     }

#   }


console.log(aiMsg.content)
# Output:
#   J'adore la programmation.


"""
## Chaining

We can [chain](/docs/how_to/sequence/) our model with a prompt template like so:
"""

import { ChatPromptTemplate } from "@langchain/core/prompts"

const prompt = ChatPromptTemplate.fromMessages(
  [
    [
      "system",
      "You are a helpful assistant that translates {input_language} to {output_language}.",
    ],
    ["human", "{input}"],
  ]
)

const chain = prompt.pipe(llm);
await chain.invoke(
  {
    input_language: "English",
    output_language: "German",
    input: "I love programming.",
  }
)
# Output:
#   AIMessage {

#     "id": "c6401e11-8f85-4a71-8e15-4856d55aef78",

#     "content": "Here's the German translation:\n\nIch liebe Programmieren.",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "$metadata": {

#         "httpStatusCode": 200,

#         "requestId": "c6401e11-8f85-4a71-8e15-4856d55aef78",

#         "attempts": 1,

#         "totalRetryDelay": 0

#       },

#       "metrics": {

#         "latencyMs": 760

#       },

#       "stopReason": "end_turn",

#       "usage": {

#         "inputTokens": 23,

#         "outputTokens": 18,

#         "totalTokens": 41

#       }

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 23,

#       "output_tokens": 18,

#       "total_tokens": 41

#     }

#   }


"""
## Tool calling

Tool calling with Bedrock models works in a similar way to [other models](/docs/how_to/tool_calling), but note that not all Bedrock models support tool calling. Please refer to the [AWS model documentation](https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html) for more information.
"""

"""
## API reference

For detailed documentation of all `ChatBedrockConverse` features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_aws.ChatBedrockConverse.html
"""



================================================
FILE: docs/core_docs/docs/integrations/chat/cerebras.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Cerebras
---
"""

"""
# ChatCerebras

[Cerebras](https://cerebras.ai/) is a model provider that serves open source models with an emphasis on speed. The Cerebras CS-3 system, powered by the the Wafer-Scale Engine-3 (WSE-3), represents a new class of AI supercomputer that sets the standard for generative AI training and inference with unparalleled performance and scalability.

With Cerebras as your inference provider, you can:

- Achieve unprecedented speed for AI inference workloads
- Build commercially with high throughput
- Effortlessly scale your AI workloads with our seamless clustering technology

Our CS-3 systems can be quickly and easily clustered to create the largest AI supercomputers in the world, making it simple to place and run the largest models. Leading corporations, research institutions, and governments are already using Cerebras solutions to develop proprietary models and train popular open-source models.

This will help you getting started with ChatCerebras [chat models](/docs/concepts/chat_models). For detailed documentation of all ChatCerebras features and configurations head to the [API reference](https://api.js.langchain.com/classes/_langchain_cerebras.ChatCerebras.html).

## Overview

### Integration details

| Class | Package | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/cerebras) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [ChatCerebras](https://api.js.langchain.com/classes/langchain_cerebras.ChatCerebras.html) | [`@langchain/cerebras`](https://www.npmjs.com/package/@langchain/cerebras) | ❌ | ❌ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/cerebras?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/cerebras?style=flat-square&label=%20&) |

### Model features

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | [Image input](/docs/how_to/multimodal_inputs/) | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |
| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: |
| ✅ | ✅ | ✅ | ❌ | ❌ | ❌ | ✅ | ✅ | ❌ | 

## Setup

To access ChatCerebras models you'll need to create a Cerebras account, get an API key, and install the `@langchain/cerebras` integration package.

### Credentials

Get an API Key from [cloud.cerebras.ai](https://cloud.cerebras.ai) and add it to your environment variables:

```bash
export CEREBRAS_API_KEY="your-api-key"
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain ChatCerebras integration lives in the `@langchain/cerebras` package:

```{=mdx}

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/cerebras @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

import { ChatCerebras } from "@langchain/cerebras" 

const llm = new ChatCerebras({
    model: "llama-3.3-70b",
    temperature: 0,
    maxTokens: undefined,
    maxRetries: 2,
    // other params...
})

"""
## Invocation
"""

const aiMsg = await llm.invoke([
    {
      role: "system",
      content: "You are a helpful assistant that translates English to French. Translate the user sentence.",
    },
    { role: "user", content: "I love programming." },
])
aiMsg
# Output:
#   AIMessage {

#     "id": "run-17c7d62d-67ac-4677-b33a-18298fc85e35",

#     "content": "J'adore la programmation.",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "id": "chatcmpl-2d1e2de5-4239-46fb-af2a-6200d89d7dde",

#       "created": 1735785598,

#       "model": "llama-3.3-70b",

#       "system_fingerprint": "fp_2e2a2a083c",

#       "object": "chat.completion",

#       "time_info": {

#         "queue_time": 0.00009063,

#         "prompt_time": 0.002163031,

#         "completion_time": 0.012339628,

#         "total_time": 0.01640915870666504,

#         "created": 1735785598

#       }

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 55,

#       "output_tokens": 9,

#       "total_tokens": 64

#     }

#   }


console.log(aiMsg.content)
# Output:
#   J'adore la programmation.


"""
## Json invocation
"""

const messages = [
  {
    role: "system",
    content: "You are a math tutor that handles math exercises and makes output in json in format { result: number }.",
  },
  { role: "user",  content: "2 + 2" },
];

const aiInvokeMsg = await llm.invoke(messages, { response_format: { type: "json_object" } });

// if you want not to pass response_format in every invoke, you can bind it to the instance
const llmWithResponseFormat = llm.bind({ response_format: { type: "json_object" } });
const aiBindMsg = await llmWithResponseFormat.invoke(messages);

// they are the same
console.log({ aiInvokeMsgContent: aiInvokeMsg.content, aiBindMsg: aiBindMsg.content });
# Output:
#   { aiInvokeMsgContent: '{"result":4}', aiBindMsg: '{"result":4}' }


"""
## Chaining

We can [chain](/docs/how_to/sequence/) our model with a prompt template like so:
"""

import { ChatPromptTemplate } from "@langchain/core/prompts"

const prompt = ChatPromptTemplate.fromMessages(
    [
        [
            "system",
            "You are a helpful assistant that translates {input_language} to {output_language}.",
        ],
        ["human", "{input}"],
    ]
)

const chain = prompt.pipe(llm);
await chain.invoke(
    {
        input_language: "English",
        output_language: "German",
        input: "I love programming.",
    }
)
# Output:
#   AIMessage {

#     "id": "run-5c8a9f25-0f57-499b-9c2b-87bd07135feb",

#     "content": "Ich liebe das Programmieren.",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "id": "chatcmpl-abd1e9eb-b873-492e-9e30-0d13dfc3a145",

#       "created": 1735785607,

#       "model": "llama-3.3-70b",

#       "system_fingerprint": "fp_2e2a2a083c",

#       "object": "chat.completion",

#       "time_info": {

#         "queue_time": 0.00009499,

#         "prompt_time": 0.002095266,

#         "completion_time": 0.008807576,

#         "total_time": 0.012718439102172852,

#         "created": 1735785607

#       }

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 50,

#       "output_tokens": 7,

#       "total_tokens": 57

#     }

#   }


"""
## API reference

For detailed documentation of all ChatCerebras features and configurations head to the API reference: https://api.js.langchain.com/classes/_langchain_cerebras.ChatCerebras.html
"""



================================================
FILE: docs/core_docs/docs/integrations/chat/cloudflare_workersai.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Cloudflare Workers AI
---
"""

"""
# ChatCloudflareWorkersAI

[Workers AI](https://developers.cloudflare.com/workers-ai/) allows you to run machine learning models, on the Cloudflare network, from your own code.

This will help you getting started with Cloudflare Workers AI [chat models](/docs/concepts/chat_models). For detailed documentation of all `ChatCloudflareWorkersAI` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_cloudflare.ChatCloudflareWorkersAI.html).

## Overview
### Integration details

| Class | Package | Local | Serializable | PY support | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [`ChatCloudflareWorkersAI`](https://api.js.langchain.com/classes/langchain_cloudflare.ChatCloudflareWorkersAI.html) | [`@langchain/cloudflare`](https://npmjs.com/@langchain/cloudflare) | ❌ | ✅ | ❌ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/cloudflare?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/cloudflare?style=flat-square&label=%20&) |

### Model features

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | [Image input](/docs/how_to/multimodal_inputs/) | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |
| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: |
| ❌ | ❌ | ❌ | ✅ | ❌ | ❌ | ✅ | ❌ | ❌ | 

## Setup

To access Cloudflare Workers AI models you'll need to create a Cloudflare account, get an API key, and install the `@langchain/cloudflare` integration package.

### Credentials

Head [to this page](https://developers.cloudflare.com/workers-ai/) to sign up to Cloudflare and generate an API key. Once you've done this, note your `CLOUDFLARE_ACCOUNT_ID` and `CLOUDFLARE_API_TOKEN`.

Passing a binding within a Cloudflare Worker is not yet supported.

### Installation

The LangChain ChatCloudflareWorkersAI integration lives in the `@langchain/cloudflare` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/cloudflare @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

// @lc-docs-hide-cell

// @ts-expect-error Deno is not recognized
const CLOUDFLARE_ACCOUNT_ID = Deno.env.get("CLOUDFLARE_ACCOUNT_ID");
// @ts-expect-error Deno is not recognized
const CLOUDFLARE_API_TOKEN = Deno.env.get("CLOUDFLARE_API_TOKEN");

import { ChatCloudflareWorkersAI } from "@langchain/cloudflare";

const llm = new ChatCloudflareWorkersAI({
  model: "@cf/meta/llama-2-7b-chat-int8", // Default value
  cloudflareAccountId: CLOUDFLARE_ACCOUNT_ID,
  cloudflareApiToken: CLOUDFLARE_API_TOKEN,
  // Pass a custom base URL to use Cloudflare AI Gateway
  // baseUrl: `https://gateway.ai.cloudflare.com/v1/{YOUR_ACCOUNT_ID}/{GATEWAY_NAME}/workers-ai/`,
});

"""
## Invocation
"""

const aiMsg = await llm.invoke([
  [
    "system",
    "You are a helpful assistant that translates English to French. Translate the user sentence.",
  ],
  ["human", "I love programming."],
])
aiMsg
# Output:
#   AIMessage {

#     lc_serializable: [33mtrue[39m,

#     lc_kwargs: {

#       content: [32m'I can help with that! The translation of "I love programming" in French is:\n'[39m +

#         [32m"\n"[39m +

#         [32m`"J'adore le programmati`[39m... 4 more characters,

#       tool_calls: [],

#       invalid_tool_calls: [],

#       additional_kwargs: {},

#       response_metadata: {}

#     },

#     lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#     content: [32m'I can help with that! The translation of "I love programming" in French is:\n'[39m +

#       [32m"\n"[39m +

#       [32m`"J'adore le programmati`[39m... 4 more characters,

#     name: [90mundefined[39m,

#     additional_kwargs: {},

#     response_metadata: {},

#     tool_calls: [],

#     invalid_tool_calls: []

#   }

console.log(aiMsg.content)
# Output:
#   I can help with that! The translation of "I love programming" in French is:

#   

#   "J'adore le programmation."


"""
## Chaining

We can [chain](/docs/how_to/sequence/) our model with a prompt template like so:
"""

import { ChatPromptTemplate } from "@langchain/core/prompts"

const prompt = ChatPromptTemplate.fromMessages(
  [
    [
      "system",
      "You are a helpful assistant that translates {input_language} to {output_language}.",
    ],
    ["human", "{input}"],
  ]
)

const chain = prompt.pipe(llm);
await chain.invoke(
  {
    input_language: "English",
    output_language: "German",
    input: "I love programming.",
  }
)
# Output:
#   AIMessage {

#     lc_serializable: [33mtrue[39m,

#     lc_kwargs: {

#       content: [32m"Das Programmieren ist für mich sehr Valent sein!"[39m,

#       tool_calls: [],

#       invalid_tool_calls: [],

#       additional_kwargs: {},

#       response_metadata: {}

#     },

#     lc_namespace: [ [32m"langchain_core"[39m, [32m"messages"[39m ],

#     content: [32m"Das Programmieren ist für mich sehr Valent sein!"[39m,

#     name: [90mundefined[39m,

#     additional_kwargs: {},

#     response_metadata: {},

#     tool_calls: [],

#     invalid_tool_calls: []

#   }

"""
## API reference

For detailed documentation of all `ChatCloudflareWorkersAI` features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_cloudflare.ChatCloudflareWorkersAI.html
"""



================================================
FILE: docs/core_docs/docs/integrations/chat/cohere.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Cohere
lc_docs_skip_validation: true
---
"""

"""
# ChatCohere

[Cohere](https://cohere.com/) is a Canadian startup that provides natural language processing models that help companies improve human-machine interactions.

This will help you getting started with Cohere [chat models](/docs/concepts/chat_models). For detailed documentation of all `ChatCohere` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_cohere.ChatCohere.html).

## Overview
### Integration details

| Class | Package | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/cohere) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [ChatCohere](https://api.js.langchain.com/classes/langchain_cohere.ChatCohere.html) | [`@langchain/cohere`](https://www.npmjs.com/package/@langchain/cohere) | ❌ | beta | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/cohere?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/cohere?style=flat-square&label=%20&) |

### Model features

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | [Image input](/docs/how_to/multimodal_inputs/) | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |
| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: |
| ✅ | ✅ | ❌ | ❌ | ❌ | ❌ | ✅ | ❌ | ❌ | 

## Setup

In order to use the LangChain.js Cohere integration you'll need an API key.
You can sign up for a Cohere account and create an API key [here](https://dashboard.cohere.com/welcome/register).

You'll first need to install the [`@langchain/cohere`](https://www.npmjs.com/package/@langchain/cohere) package.

### Credentials

Head to [Cohere's website](https://dashboard.cohere.com/welcome/register) to sign up to Cohere and generate an API key. Once you've done this set the `COHERE_API_KEY` environment variable:

```bash
export COHERE_API_KEY="your-api-key"
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain ChatCohere integration lives in the `@langchain/cohere` package:

```{=mdx}

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/cohere @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

import { ChatCohere } from "@langchain/cohere" 

const llm = new ChatCohere({
    model: "command-r-plus",
    temperature: 0,
    maxRetries: 2,
    // other params...
})

"""
### Custom client for Cohere on Azure, Cohere on AWS Bedrock, and Standalone Cohere Instance.

We can instantiate a custom `CohereClient` and pass it to the ChatCohere constructor.

**Note:** If a custom client is provided both `COHERE_API_KEY` environment variable and `apiKey` parameter in the constructor will be ignored.
"""

import { ChatCohere } from "@langchain/cohere";
import { CohereClient } from "cohere-ai";

const client = new CohereClient({
  token: "<your-api-key>",
  environment: "<your-cohere-deployment-url>", //optional
  // other params
});

const llmWithCustomClient = new ChatCohere({
  client,
  // other params...
});

"""
## Invocation
"""

const aiMsg = await llm.invoke([
    [
        "system",
        "You are a helpful assistant that translates English to French. Translate the user sentence.",
    ],
    ["human", "I love programming."],
])
aiMsg
# Output:
#   AIMessage {

#     "content": "J'adore programmer.",

#     "additional_kwargs": {

#       "response_id": "0056057a-6075-4436-b75a-b9455ac39f74",

#       "generationId": "3a0985db-92ff-41d8-b6b9-b7b77e300f3b",

#       "chatHistory": [

#         {

#           "role": "SYSTEM",

#           "message": "You are a helpful assistant that translates English to French. Translate the user sentence."

#         },

#         {

#           "role": "USER",

#           "message": "I love programming."

#         },

#         {

#           "role": "CHATBOT",

#           "message": "J'adore programmer."

#         }

#       ],

#       "finishReason": "COMPLETE",

#       "meta": {

#         "apiVersion": {

#           "version": "1"

#         },

#         "billedUnits": {

#           "inputTokens": 20,

#           "outputTokens": 5

#         },

#         "tokens": {

#           "inputTokens": 89,

#           "outputTokens": 5

#         }

#       }

#     },

#     "response_metadata": {

#       "estimatedTokenUsage": {

#         "completionTokens": 5,

#         "promptTokens": 89,

#         "totalTokens": 94

#       },

#       "response_id": "0056057a-6075-4436-b75a-b9455ac39f74",

#       "generationId": "3a0985db-92ff-41d8-b6b9-b7b77e300f3b",

#       "chatHistory": [

#         {

#           "role": "SYSTEM",

#           "message": "You are a helpful assistant that translates English to French. Translate the user sentence."

#         },

#         {

#           "role": "USER",

#           "message": "I love programming."

#         },

#         {

#           "role": "CHATBOT",

#           "message": "J'adore programmer."

#         }

#       ],

#       "finishReason": "COMPLETE",

#       "meta": {

#         "apiVersion": {

#           "version": "1"

#         },

#         "billedUnits": {

#           "inputTokens": 20,

#           "outputTokens": 5

#         },

#         "tokens": {

#           "inputTokens": 89,

#           "outputTokens": 5

#         }

#       }

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 89,

#       "output_tokens": 5,

#       "total_tokens": 94

#     }

#   }


console.log(aiMsg.content)
# Output:
#   J'adore programmer.


"""
## Chaining

We can [chain](/docs/how_to/sequence/) our model with a prompt template like so:
"""

import { ChatPromptTemplate } from "@langchain/core/prompts"

const prompt = ChatPromptTemplate.fromMessages(
    [
        [
            "system",
            "You are a helpful assistant that translates {input_language} to {output_language}.",
        ],
        ["human", "{input}"],
    ]
)

const chain = prompt.pipe(llm);
await chain.invoke(
    {
        input_language: "English",
        output_language: "German",
        input: "I love programming.",
    }
)
# Output:
#   AIMessage {

#     "content": "Ich liebe Programmieren.",

#     "additional_kwargs": {

#       "response_id": "271e1439-7220-40fa-953d-c9f2947e451a",

#       "generationId": "f99970a4-7b1c-4d76-a73a-4467a1db759c",

#       "chatHistory": [

#         {

#           "role": "SYSTEM",

#           "message": "You are a helpful assistant that translates English to German."

#         },

#         {

#           "role": "USER",

#           "message": "I love programming."

#         },

#         {

#           "role": "CHATBOT",

#           "message": "Ich liebe Programmieren."

#         }

#       ],

#       "finishReason": "COMPLETE",

#       "meta": {

#         "apiVersion": {

#           "version": "1"

#         },

#         "billedUnits": {

#           "inputTokens": 15,

#           "outputTokens": 6

#         },

#         "tokens": {

#           "inputTokens": 84,

#           "outputTokens": 6

#         }

#       }

#     },

#     "response_metadata": {

#       "estimatedTokenUsage": {

#         "completionTokens": 6,

#         "promptTokens": 84,

#         "totalTokens": 90

#       },

#       "response_id": "271e1439-7220-40fa-953d-c9f2947e451a",

#       "generationId": "f99970a4-7b1c-4d76-a73a-4467a1db759c",

#       "chatHistory": [

#         {

#           "role": "SYSTEM",

#           "message": "You are a helpful assistant that translates English to German."

#         },

#         {

#           "role": "USER",

#           "message": "I love programming."

#         },

#         {

#           "role": "CHATBOT",

#           "message": "Ich liebe Programmieren."

#         }

#       ],

#       "finishReason": "COMPLETE",

#       "meta": {

#         "apiVersion": {

#           "version": "1"

#         },

#         "billedUnits": {

#           "inputTokens": 15,

#           "outputTokens": 6

#         },

#         "tokens": {

#           "inputTokens": 84,

#           "outputTokens": 6

#         }

#       }

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 84,

#       "output_tokens": 6,

#       "total_tokens": 90

#     }

#   }


"""
## RAG

Cohere also comes out of the box with RAG support.
You can pass in documents as context to the API request and Cohere's models will use them when generating responses.
"""

import { ChatCohere } from "@langchain/cohere";
import { HumanMessage } from "@langchain/core/messages";

const llmForRag = new ChatCohere({
  apiKey: process.env.COHERE_API_KEY, // Default
});

const documents = [
  {
    title: "Harrison's work",
    snippet: "Harrison worked at Kensho as an engineer.",
  },
  {
    title: "Harrison's work duration",
    snippet: "Harrison worked at Kensho for 3 years.",
  },
  {
    title: "Polar berars in the Appalachian Mountains",
    snippet:
      "Polar bears have surprisingly adapted to the Appalachian Mountains, thriving in the diverse, forested terrain despite their traditional arctic habitat. This unique situation has sparked significant interest and study in climate adaptability and wildlife behavior.",
  },
];

const ragResponse = await llmForRag.invoke(
  [new HumanMessage("Where did Harrison work and for how long?")],
  {
    documents,
  }
);
console.log(ragResponse.content);
# Output:
#   Harrison worked at Kensho as an engineer for 3 years.


"""
## Connectors

The API also allows for other connections which are not static documents.
An example of this is their `web-search` connector which allows you to pass in a query and the API will search the web for relevant documents.
The example below demonstrates how to use this feature.
"""

import { ChatCohere } from "@langchain/cohere";
import { HumanMessage } from "@langchain/core/messages";

const llmWithConnectors = new ChatCohere({
  apiKey: process.env.COHERE_API_KEY, // Default
});

const connectorsRes = await llmWithConnectors.invoke(
  [new HumanMessage("How tall are the largest pengiuns?")],
  {
    connectors: [{ id: "web-search" }],
  }
);
console.dir(connectorsRes, { depth: null });
# Output:
#   AIMessage {

#     lc_serializable: true,

#     lc_kwargs: {

#       content: 'The largest penguin ever discovered is the prehistoric Palaeeudyptes klekowskii, or "colossus penguin", which stood at 6 feet 6 inches tall. The tallest penguin alive today is the emperor penguin, which stands at just over 4 feet tall.',

#       additional_kwargs: {

#         response_id: '8d5ae032-4c8e-492e-8686-289f198b5eb5',

#         generationId: '2224736b-430c-46cf-9ca0-a7f5737466aa',

#         chatHistory: [

#           { role: 'USER', message: 'How tall are the largest pengiuns?' },

#           {

#             role: 'CHATBOT',

#             message: 'The largest penguin ever discovered is the prehistoric Palaeeudyptes klekowskii, or "colossus penguin", which stood at 6 feet 6 inches tall. The tallest penguin alive today is the emperor penguin, which stands at just over 4 feet tall.'

#           }

#         ],

#         finishReason: 'COMPLETE',

#         meta: {

#           apiVersion: { version: '1' },

#           billedUnits: { inputTokens: 10474, outputTokens: 62 },

#           tokens: { inputTokens: 11198, outputTokens: 286 }

#         },

#         citations: [

#           {

#             start: 43,

#             end: 54,

#             text: 'prehistoric',

#             documentIds: [ 'web-search_1', 'web-search_2' ]

#           },

#           {

#             start: 55,

#             end: 79,

#             text: 'Palaeeudyptes klekowskii',

#             documentIds: [ 'web-search_0', 'web-search_1', 'web-search_2' ]

#           },

#           {

#             start: 84,

#             end: 102,

#             text: '"colossus penguin"',

#             documentIds: [ 'web-search_0', 'web-search_1', 'web-search_2' ]

#           },

#           {

#             start: 119,

#             end: 125,

#             text: '6 feet',

#             documentIds: [ 'web-search_0', 'web-search_1' ]

#           },

#           {

#             start: 126,

#             end: 134,

#             text: '6 inches',

#             documentIds: [ 'web-search_1' ]

#           },

#           {

#             start: 161,

#             end: 172,

#             text: 'alive today',

#             documentIds: [ 'web-search_0', 'web-search_5' ]

#           },

#           {

#             start: 180,

#             end: 195,

#             text: 'emperor penguin',

#             documentIds: [

#               'web-search_0',

#               'web-search_1',

#               'web-search_2',

#               'web-search_4',

#               'web-search_5'

#             ]

#           },

#           {

#             start: 213,

#             end: 235,

#             text: 'just over 4 feet tall.',

#             documentIds: [ 'web-search_0', 'web-search_5' ]

#           }

#         ],

#         documents: [

#           {

#             id: 'web-search_1',

#             snippet: 'Largest species of penguin ever\n' +

#               '\n' +

#               'TencentContact an Account Manager\n' +

#               '\n' +

#               "The largest species of penguin ever recorded is a newly described prehistoric species, Kumimanu fordycei, known from fossil remains discovered inside boulders in North Otago, on New Zealand's South Island. By comparing the size and density of its bones with those of modern-day penguins, researchers estimate that it weighed 154 kilograms (340 pounds), which is three times that of today's largest species, the emperor penguin (Aptenodytes forsteri). The rocks containing the remains of this new giant fossil species date between 55.5 million years and 59.5 million years old, meaning that it existed during the Late Palaeocene. Details of the record-breaking prehistoric penguin were published in the Journal of Paleontology on 8 February 2023.\n" +

#               '\n' +

#               'The height of K. fordycei is debated, though a related extinct species, K. biceae, has been estimated to have stood up to 1.77 m (5 ft). A lack of complete skeletons of extinct giant penguins found to date makes it difficult for height to be determined with any degree of certainty.\n' +

#               '\n' +

#               "Prior to the recent discovery and description of K. fordycei, the largest species of penguin known to science was the colossus penguin (Palaeeudyptes klekowskii), which is estimated to have weighed as much as 115 kg (253 lb 8 oz), and stood up to 2 m (6 ft 6 in) tall. It lived in Antarctica's Seymour Island approximately 37 million years ago, during the Late Eocene, and is represented by the most complete fossil remains ever found for a penguin species in Antarctica.\n" +

#               '\n' +

#               "This species exceeds in height the previous record holder, Nordenskjoeld's giant penguin (Anthropornis nordenskjoeldi), which stood 1.7 m (5 ft 6 in) tall and also existed during the Eocene epoch, occurring in New Zealand and in Antarctica's Seymour Island.\n" +

#               '\n' +

#               'Records change on a daily basis and are not immediately published online. For a full list of record titles, please use our Record Application Search. (You will need to register / login for access)\n' +

#               '\n' +

#               'Comments below may relate to previous holders of this record.',

#             timestamp: '2024-07-28T02:56:04',

#             title: 'Largest species of penguin ever',

#             url: 'https://www.guinnessworldrecords.com/world-records/84903-largest-species-of-penguin'

#           },

#           {

#             id: 'web-search_2',

#             snippet: 'Mega penguins: These are the largest penguins to have ever lived\n' +

#               '\n' +

#               'No penguin alive today can compare with some of the extinct giants that once roamed the planet, including Kumimanu fordycei, Petradyptes stonehousei and Palaeeudyptes klekowskii\n' +

#               '\n' +

#               'An illustration of Kumimanu fordycei (the larger, single bird) and Petradyptes stonehousei penguins on an ancient New Zealand beach\n' +

#               '\n' +

#               'Artwork by Dr. Simone Giovanardi\n' +

#               '\n' +

#               'Penguins come in all shapes and sizes, from the fairy penguin (Eudyptula minor) which stands at just over 30 centimetres tall to the 1-metre-high emperor penguin (Aptenodytes forsteri). But even the biggest emperors alive today would be dwarfed by the mega-penguins that roamed Earth millions of years ago. Here are the most impressive of these ancient giants.\n' +

#               '\n' +

#               'The title of the largest penguin ever documented goes to the species Kumimanu fordycei, which was first described in February 2023.\n' +

#               '\n' +

#               'Daniel Ksepka at the Bruce Museum in Connecticut and his colleagues unearthed an unusually huge flipper bone of a penguin in southern New Zealand in 2018. “The big humerus was shocking to me,” he says. “I almost thought it was maybe some other animal.”\n' +

#               '\n' +

#               'The team quickly determined that this belonged to a new species of penguin that lived in what is now New Zealand over 55 million years ago. The sheer size of the bone suggested that the bird probably weighed between 148 and 160 kilograms and stood around 1.6 metres tall. “The emperor penguin just looks like a child next to it,” says Ksepka.\n' +

#               '\n' +

#               'The species was named after palaeontologist Ewan Fordyce, who made his own mega penguin discoveries in the 1970s (see below).\n' +

#               '\n' +

#               'Sign up to our Wild Wild Life newsletter\n' +

#               '\n' +

#               'A monthly celebration of the biodiversity of our planet’s animals, plants and other organisms.\n' +

#               '\n' +

#               'Sign up to newsletter\n' +

#               '\n' +

#               'Skeletons of Kumimanu, Petradyptes and a modern emperor penguin\n' +

#               '\n' +

#               'Artwork by Dr. Simone Giovanardi\n' +

#               '\n' +

#               'Petradyptes stonehousei\n' +

#               '\n' +

#               'Ksepka and his colleagues discovered another giant penguin alongside K. fordycei, called Petradyptes stonehousei. With an estimated mass of 50 kilograms, it was quite a bit smaller than its contemporary. Its name comes from the Greek “petra” for rock and “dyptes” for diver, while “stonehousei” was chosen to honour British polar scientist Bernard Stonehouse.\n' +

#               '\n' +

#               'Both K. fordycei and P. stonehousei retained features seen in much earlier penguin species, such as slimmer flipper bones and muscle attachment points that look like those seen in flying birds.\n' +

#               '\n' +

#               '“Both penguins really add to the case that penguins got their start in New Zealand,” says Ksepka.\n' +

#               '\n' +

#               'Illustration of the extinct Palaeeudyptes klekowskii with a human and emperor penguin for scale\n' +

#               '\n' +

#               'Nature Picture Library / Alamy\n' +

#               '\n' +

#               'Palaeeudyptes klekowskii\n' +

#               '\n' +

#               'While K. fordycei was the heaviest penguin, it wasn’t the tallest. That award goes to Palaeeudyptes klekowskii, dubbed the colossus penguin, which towered at 2 metres and weighed a hefty 115 kilograms.\n' +

#               '\n' +

#               'The species lived 37 to 40 million years ago along the Antarctic coast. Its fossil, which included the longest fused ankle-foot bone, is one of the most complete ever uncovered from the Antarctic.\n' +

#               '\n' +

#               'Owing to their larger body size, giant penguins could remain underwater longer than smaller ones. Experts reckon that a species such as P. klekowskii could have remained submerged for up to 40 minutes hunting for fish.\n' +

#               '\n' +

#               'Pachydyptes ponderosus\n' +

#               '\n' +

#               'Pachydyptes ponderosus is prehistoric giant that lived more recently than those already mentioned – around 37 to 34 million years ago. Based on the few bones from the species that have been recovered, in 2006 Ksepka and his colleagues put it around 1.5 metres tall with a weight of over 100 kilograms.\n' +

#               '\n' +

#               '“We really only have parts of the flipper and shoulder, but we think it would have been quite a thick, stocky animal,” says Ksepka. “Its humerus is just so wide.”\n' +

#               '\n' +

#               'Daniel Ksepka with a model of a Kairuku penguin\n' +

#               '\n' +

#               'The three species that belonged to the genus Kairuku (K. grebneffi, K. waitaki and K. waewaeroa), however, were the complete opposite.\n' +

#               '\n' +

#               '“If Pachydyptes is like a big, heavy football lineman, then you can think of Kairuku as a really tall, skinny basketball player,” says Ksepka. “They’re both really big, but in different ways.”\n' +

#               '\n' +

#               'The first Kairuku bones were discovered by Ewan Fordyce in the 1970s, in New Zealand. All three species lived roughly 34 to 27 million years ago. The tallest, K. waewaeroa, stood at a height of around 1.4 metres and weighed around 80 kilograms.\n' +

#               '\n' +

#               '“They were graceful penguins, with slender trunks,” says Ksepka.\n' +

#               '\n' +

#               'Sign up to our weekly newsletter\n' +

#               '\n' +

#               "Receive a weekly dose of discovery in your inbox! We'll also keep you up to date with New Scientist events and special offers. Sign up\n" +

#               '\n' +

#               'More from New Scientist\n' +

#               '\n' +

#               'Explore the latest news, articles and features\n' +

#               '\n' +

#               'Extremely rare black penguin spotted in Antarctica\n' +

#               '\n' +

#               'How you can help with penguin research by browsing images at home\n' +

#               '\n' +

#               'Adélie penguins show signs of self-awareness on the mirror test\n' +

#               '\n' +

#               'Penguins adapt their accents to sound more like their friends\n' +

#               '\n' +

#               'Trending New Scientist articles\n' +

#               '\n' +

#               "SpaceX prepares for Starship flight with first 'chopstick' landing\n" +

#               '\n' +

#               'Evidence mounts that shingles vaccines protect against dementia\n' +

#               '\n' +

#               'When is the best time to exercise to get the most from your workout?\n' +

#               '\n' +

#               'Why slow running could be even more beneficial than running fast\n' +

#               '\n' +

#               'Wafer-thin light sail could help us reach another star sooner\n' +

#               '\n' +

#               'The remarkable science-backed ways to get fit as fast as possible\n' +

#               '\n' +

#               "One of Earth's major carbon sinks collapsed in 2023\n" +

#               '\n' +

#               'How to use psychology to hack your mind and fall in love with exercise\n' +

#               '\n' +

#               'Gene therapy enables five children who were born deaf to hear\n' +

#               '\n' +

#               'Why midlife is the perfect time to take control of your future health',

#             timestamp: '2024-07-28T02:56:04',

#             title: 'Mega penguins: The tallest, largest, most amazing penguin species to have ever lived | New Scientist',

#             url: 'https://www.newscientist.com/article/2397894-mega-penguins-these-are-the-largest-penguins-to-have-ever-lived/'

#           },

#           {

#             id: 'web-search_0',

#             snippet: 'Sustainability for All.\n' +

#               '\n' +

#               'Giant 6-Foot-8 Penguin Discovered in Antarctica\n' +

#               '\n' +

#               'University of Houston\n' +

#               '\n' +

#               'Bryan Nelson is a science writer and award-winning documentary filmmaker with over a decade of experience covering technology, astronomy, medicine, animals, and more.\n' +

#               '\n' +

#               'Learn about our editorial process\n' +

#               '\n' +

#               'Updated May 9, 2020 10:30AM EDT\n' +

#               '\n' +

#               "Modern emperor penguins are certainly statuesque, but not quite as impressive as the 'colossus penguin' would have been. . Christopher Michel/flickr\n" +

#               '\n' +

#               'The largest penguin species ever discovered has been unearthed in Antarctica, and its size is almost incomprehensible. Standing at 6 foot 8 inches from toe to beak tip, the mountainous bird would have dwarfed most adult humans, reports the Guardian.\n' +

#               '\n' +

#               'In fact, if it were alive today the penguin could have looked basketball superstar LeBron James square in the eyes.\n' +

#               '\n' +

#               "Fossils Provide Clues to the Bird's Size\n" +

#               '\n' +

#               `The bird's 37-million-year-old fossilized remains, which include the longest recorded fused ankle-foot bone as well as parts of the animal's wing bone, represent the most complete fossil ever uncovered in the Antarctic. Appropriately dubbed the "colossus penguin," Palaeeudyptes klekowskii was truly the Godzilla of aquatic birds.\n` +

#               '\n' +

#               `Scientists calculated the penguin's dimensions by scaling the sizes of its bones against those of modern penguin species. They estimate that the bird probably would have weighed about 250 pounds — again, roughly comparable to LeBron James. By comparison, the largest species of penguin alive today, the emperor penguin, is "only" about 4 feet tall and can weigh as much as 100 pounds.\n` +

#               '\n' +

#               'Interestingly, because larger bodied penguins can hold their breath for longer, the colossus penguin probably could have stayed underwater for 40 minutes or more. It boggles the mind to imagine the kinds of huge, deep sea fish this mammoth bird might have been capable of hunting.\n' +

#               '\n' +

#               "The fossil was found at the La Meseta formation on Seymour Island, an island in a chain of 16 major islands around the tip of the Graham Land on the Antarctic Peninsula. (It's the region that is the closest part of Antarctica to South America.) The area is known for its abundance of penguin bones, though in prehistoric times it would have been much warmer than it is today.\n" +

#               '\n' +

#               "P. klekowskii towers over the next largest penguin ever discovered, a 5-foot-tall bird that lived about 36 million years ago in Peru. Since these two species were near contemporaries, it's fun to imagine a time between 35 and 40 million years ago when giant penguins walked the Earth, and perhaps swam alongside the ancestors of whales.\n" +

#               '\n' +

#               '10 of the Largest Living Sea Creatures\n' +

#               '\n' +

#               '11 Facts About Blue Whales, the Largest Animals Ever on Earth\n' +

#               '\n' +

#               '16 Ocean Creatures That Live in Total Darkness\n' +

#               '\n' +

#               'National Monuments Designated By President Obama\n' +

#               '\n' +

#               '20 Pygmy Animal Species From Around the World\n' +

#               '\n' +

#               'School Kids Discover New Penguin Species in New Zealand\n' +

#               '\n' +

#               '16 of the Most Surreal Landscapes on Earth\n' +

#               '\n' +

#               '12 Peculiar Penguin Facts\n' +

#               '\n' +

#               "10 Amazing Hoodoos Around the World and How They're Formed\n" +

#               '\n' +

#               '8 Titanic Facts About Patagotitans\n' +

#               '\n' +

#               '9 Extinct Megafauna That Are Out of This World\n' +

#               '\n' +

#               '10 Places Where Penguins Live in the Wild\n' +

#               '\n' +

#               '16 Animals That Are Living Fossils\n' +

#               '\n' +

#               'A Timeline of the Distant Future for Life on Earth\n' +

#               '\n' +

#               '12 Animals That May Have Inspired Mythical Creatures\n' +

#               '\n' +

#               '12 Dinosaur Theme Parks\n' +

#               '\n' +

#               'By clicking “Accept All Cookies”, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts.\n' +

#               '\n' +

#               'Cookies Settings Accept All Cookies',

#             timestamp: '2024-07-27T06:29:15',

#             title: 'Giant 6-Foot-8 Penguin Discovered in Antarctica',

#             url: 'https://www.treehugger.com/giant-foot-penguin-discovered-in-antarctica-4864169'

#           },

#           {

#             id: 'web-search_5',

#             snippet: 'Skip to main content\n' +

#               '\n' +

#               'Smithsonian Institution\n' +

#               '\n' +

#               'Search Smithsonian Ocean\n' +

#               '\n' +

#               'Follow us on Facebook Follow us on Twitter Follow us on Flickr Follow us on Tumbr\n' +

#               '\n' +

#               'How Big Do Penguins Get?\n' +

#               '\n' +

#               '(Smithsonian Institution)\n' +

#               '\n' +

#               'The largest of the penguins, the emperor, stands at just over four feet while the smallest, the little penguin, has a maximum height of a foot. \n' +

#               '\n' +

#               'Coasts & Shallow Water\n' +

#               '\n' +

#               'Census of Marine Life\n' +

#               '\n' +

#               'Waves, Storms & Tsunamis\n' +

#               '\n' +

#               'Temperature & Chemistry\n' +

#               '\n' +

#               'Solutions & Success Stories\n' +

#               '\n' +

#               'Books, Film & The Arts\n' +

#               '\n' +

#               'Search Smithsonian Ocean',

#             timestamp: '2024-07-30T03:47:03',

#             title: 'How Big Do Penguins Get? | Smithsonian Ocean',

#             url: 'https://ocean.si.edu/ocean-life/seabirds/how-big-do-penguins-get'

#           },

#           {

#             id: 'web-search_4',

#             snippet: 'The emperor penguin (Aptenodytes forsteri) is the tallest and heaviest of all living penguin species and is endemic to Antarctica. The male and female are similar in plumage and size, reaching 100 cm (39 in) in length and weighing from 22 to 45 kg (49 to 99 lb). Feathers of the head and back are black and sharply delineated from the white belly, pale-yellow breast and bright-yellow ear patches.\n' +

#               '\n' +

#               'Like all penguins, it is flightless, with a streamlined body, and wings stiffened and flattened into flippers for a marine habitat. Its diet consists primarily of fish, but also includes crustaceans, such as krill, and cephalopods, such as squid. While hunting, the species can remain submerged around 20 minutes, diving to a depth of 535 m (1,755 ft). It has several adaptations to facilitate this, including an unusually structured haemoglobin to allow it to function at low oxygen levels, solid bones to reduce barotrauma, and the ability to reduce its metabolism and shut down non-essential organ functions.\n' +

#               '\n' +

#               'The only penguin species that breeds during the Antarctic winter, emperor penguins trek 50–120 km (31–75 mi) over the ice to breeding colonies which can contain up to several thousand individuals. The female lays a single egg, which is incubated for just over two months by the male while the female returns to the sea to feed; parents subsequently take turns foraging at sea and caring for their chick in the colony. The lifespan is typically 20 years in the wild, although observations suggest that some individuals may live to 50 years of age.\n' +

#               '\n' +

#               'Emperor penguins were described in 1844 by English zoologist George Robert Gray, who created the generic name from Ancient Greek word elements, ἀ-πτηνο-δύτης [a-ptēno-dytēs], "without-wings-diver". Its specific name is in honour of the German naturalist Johann Reinhold Forster, who accompanied Captain James Cook on his second voyage and officially named five other penguin species. Forster may have been the first person to see the penguins in 1773–74, when he recorded a sighting of what he believed was the similar king penguin (A. patagonicus) but given the location, may very well have been A. forsteri.\n' +

#               '\n' +

#               "Together with the king penguin, the emperor penguin is one of two extant species in the genus Aptenodytes. Fossil evidence of a third species—Ridgen's penguin (A. ridgeni)—has been found in fossil records from the late Pliocene, about three million years ago, in New Zealand. Studies of penguin behaviour and genetics have proposed that the genus Aptenodytes is basal; in other words, that it split off from a branch which led to all other living penguin species. Mitochondrial and nuclear DNA evidence suggests this split occurred around 40 million years ago.\n" +

#               '\n' +

#               'Adult emperor penguins are 110–120 cm (43–47 in) in length, averaging 115 centimetres (45 in) according to Stonehouse (1975). Due to method of bird measurement that measures length between bill to tail, sometimes body length and standing height are confused, and some reported height even reaching 1.5 metres (4.9 ft) tall. There are still more than a few papers mentioning that they reach a standing height of 1.2 metres (3.9 ft) instead of body length. Although standing height of emperor penguin is rarely provided at scientific reports, Prévost (1961) recorded 86 wild individuals and measured maximum height of 1.08 metres (3.5 ft). Friedman (1945) recorded measurements from 22 wild individuals and resulted height ranging 83–97 cm (33–38 in). Ksepka et al. (2012) measured standing height of 81–94 cm (32–37 in) according to 11 complete skins collected in American Museum of Natural History. The weight ranges from 22.7 to 45.4 kg (50 to 100 lb) and varies by sex, with males weighing more than females. It is the fifth heaviest living bird species, after only the larger varieties of ratite. The weight also varies by season, as both male and female penguins lose substantial mass while raising hatchlings and incubating their egg. A male emperor penguin must withstand the extreme Antarctic winter cold for more than two months while protecting his egg. He eats nothing during this time. Most male emperors will lose around 12 kg (26 lb) while they wait for their eggs to hatch. The mean weight of males at the start of the breeding season is 38 kg (84 lb) and that of females is 29.5 kg (65 lb). After the breeding season this drops to 23 kg (51 lb) for both sexes.\n' +

#               '\n' +

#               'Like all penguin species, emperor penguins have streamlined bodies to minimize drag while swimming, and wings that are more like stiff, flat flippers. The tongue is equipped with rear-facing barbs to prevent prey from escaping when caught. Males and females are similar in size and colouration. The adult has deep black dorsal feathers, covering the head, chin, throat, back, dorsal part of the flippers, and tail. The black plumage is sharply delineated from the light-coloured plumage elsewhere. The underparts of the wings and belly are white, becoming pale yellow in the upper breast, while the ear patches are bright yellow. The upper mandible of the 8 cm (3 in) long bill is black, and the lower mandible can be pink, orange or lilac. In juveniles, the auricular patches, chin and throat are white, while its bill is black. Emperor penguin chicks are typically covered with silver-grey down and have black heads and white masks. A chick with all-white plumage was seen in 2001, but was not considered to be an albino as it did not have pink eyes. Chicks weigh around 315 g (11 oz) after hatching, and fledge when they reach about 50% of adult weight.\n' +

#               '\n' +

#               "The emperor penguin's dark plumage fades to brown from November until February (the Antarctic summer), before the yearly moult in January and February. Moulting is rapid in this species compared with other birds, taking only around 34 days. Emperor penguin feathers emerge from the skin after they have grown to a third of their total length, and before old feathers are lost, to help reduce heat loss. New feathers then push out the old ones before finishing their growth.\n" +

#               '\n' +

#               'The average yearly survival rate of an adult emperor penguin has been measured at 95.1%, with an average life expectancy of 19.9 years. The same researchers estimated that 1% of emperor penguins hatched could feasibly reach an age of 50 years. In contrast, only 19% of chicks survive their first year of life. Therefore, 80% of the emperor penguin population comprises adults five years and older.\n' +

#               '\n' +

#               'As the species has no fixed nest sites that individuals can use to locate their own partner or chick, emperor penguins must rely on vocal calls alone for identification. They use a complex set of calls that are critical to individual recognition between parents, offspring and mates, displaying the widest variation in individual calls of all penguins. Vocalizing emperor penguins use two frequency bands simultaneously. Chicks use a frequency-modulated whistle to beg for food and to contact parents.\n' +

#               '\n' +

#               "The emperor penguin breeds in the coldest environment of any bird species; air temperatures may reach −40 °C (−40 °F), and wind speeds may reach 144 km/h (89 mph). Water temperature is a frigid −1.8 °C (28.8 °F), which is much lower than the emperor penguin's average body temperature of 39 °C (102 °F). The species has adapted in several ways to counteract heat loss. Dense feathers provide 80–90% of its insulation and it has a layer of sub-dermal fat which may be up to 3 cm (1.2 in) thick before breeding. While the density of contour feathers is approximately 9 per square centimetre (58 per square inch), a combination of dense afterfeathers and down feathers (plumules) likely play a critical role for insulation. Muscles allow the feathers to be held erect on land, reducing heat loss by trapping a layer of air next to the skin. Conversely, the plumage is flattened in water, thus waterproofing the skin and the downy underlayer. Preening is vital in facilitating insulation and in keeping the plumage oily and water-repellent.\n" +

#               '\n' +

#               'The emperor penguin is able to thermoregulate (maintain its core body temperature) without altering its metabolism, over a wide range of temperatures. Known as the thermoneutral range, this extends from −10 to 20 °C (14 to 68 °F). Below this temperature range, its metabolic rate increases significantly, although an individual can maintain its core temperature from 38.0 °C (100.4 °F) down to −47 °C (−53 °F). Movement by swimming, walking, and shivering are three mechanisms for increasing metabolism; a fourth process involves an increase in the breakdown of fats by enzymes, which is induced by the hormone glucagon. At temperatures above 20 °C (68 °F), an emperor penguin may become agitated as its body temperature and metabolic rate rise to increase heat loss. Raising its wings and exposing the undersides increases the exposure of its body surface to the air by 16%, facilitating further heat loss.\n' +

#               '\n' +

#               'Adaptations to pressure and low oxygen\n' +

#               '\n' +

#               'In addition to the cold, the emperor penguin encounters another stressful condition on deep dives—markedly increased pressure of up to 40 times that of the surface, which in most other terrestrial organisms would cause barotrauma. The bones of the penguin are solid rather than air-filled, which eliminates the risk of mechanical barotrauma.\n' +

#               '\n' +

#               "While diving, the emperor penguin's oxygen use is markedly reduced, as its heart rate is reduced to as low as 15–20 beats per minute and non-essential organs are shut down, thus facilitating longer dives. Its haemoglobin and myoglobin are able to bind and transport oxygen at low blood concentrations; this allows the bird to function with very low oxygen levels that would otherwise result in loss of consciousness.\n" +

#               '\n' +

#               'Distribution and habitat\n' +

#               '\n' +

#               'The emperor penguin has a circumpolar distribution in the Antarctic almost exclusively between the 66° and 77° south latitudes. It almost always breeds on stable pack ice near the coast and up to 18 km (11 mi) offshore. Breeding colonies are usually in areas where ice cliffs and i'... 22063 more characters,

#             timestamp: '2024-07-31T07:59:36',

#             title: 'Emperor penguin - Wikipedia',

#             url: 'https://en.wikipedia.org/wiki/Emperor_penguin'

#           }

#         ],

#         searchResults: [

#           {

#             searchQuery: {

#               text: 'How tall are the largest penguins?',

#               generationId: '8d5ae032-4c8e-492e-8686-289f198b5eb5'

#             },

#             documentIds: [

#               'web-search_0',

#               'web-search_1',

#               'web-search_2',

#               'web-search_3',

#               'web-search_4',

#               'web-search_5'

#             ],

#             connector: { id: 'web-search' }

#           }

#         ],

#         searchQueries: [

#           {

#             text: 'How tall are the largest penguins?',

#             generationId: '8d5ae032-4c8e-492e-8686-289f198b5eb5'

#           }

#         ]

#       },

#       tool_calls: [],

#       usage_metadata: { input_tokens: 11198, output_tokens: 286, total_tokens: 11484 },

#       invalid_tool_calls: [],

#       response_metadata: {}

#     },

#     lc_namespace: [ 'langchain_core', 'messages' ],

#     content: 'The largest penguin ever discovered is the prehistoric Palaeeudyptes klekowskii, or "colossus penguin", which stood at 6 feet 6 inches tall. The tallest penguin alive today is the emperor penguin, which stands at just over 4 feet tall.',

#     name: undefined,

#     additional_kwargs: {

#       response_id: '8d5ae032-4c8e-492e-8686-289f198b5eb5',

#       generationId: '2224736b-430c-46cf-9ca0-a7f5737466aa',

#       chatHistory: [

#         { role: 'USER', message: 'How tall are the largest pengiuns?' },

#         {

#           role: 'CHATBOT',

#           message: 'The largest penguin ever discovered is the prehistoric Palaeeudyptes klekowskii, or "colossus penguin", which stood at 6 feet 6 inches tall. The tallest penguin alive today is the emperor penguin, which stands at just over 4 feet tall.'

#         }

#       ],

#       finishReason: 'COMPLETE',

#       meta: {

#         apiVersion: { version: '1' },

#         billedUnits: { inputTokens: 10474, outputTokens: 62 },

#         tokens: { inputTokens: 11198, outputTokens: 286 }

#       },

#       citations: [

#         {

#           start: 43,

#           end: 54,

#           text: 'prehistoric',

#           documentIds: [ 'web-search_1', 'web-search_2' ]

#         },

#         {

#           start: 55,

#           end: 79,

#           text: 'Palaeeudyptes klekowskii',

#           documentIds: [ 'web-search_0', 'web-search_1', 'web-search_2' ]

#         },

#         {

#           start: 84,

#           end: 102,

#           text: '"colossus penguin"',

#           documentIds: [ 'web-search_0', 'web-search_1', 'web-search_2' ]

#         },

#         {

#           start: 119,

#           end: 125,

#           text: '6 feet',

#           documentIds: [ 'web-search_0', 'web-search_1' ]

#         },

#         {

#           start: 126,

#           end: 134,

#           text: '6 inches',

#           documentIds: [ 'web-search_1' ]

#         },

#         {

#           start: 161,

#           end: 172,

#           text: 'alive today',

#           documentIds: [ 'web-search_0', 'web-search_5' ]

#         },

#         {

#           start: 180,

#           end: 195,

#           text: 'emperor penguin',

#           documentIds: [

#             'web-search_0',

#             'web-search_1',

#             'web-search_2',

#             'web-search_4',

#             'web-search_5'

#           ]

#         },

#         {

#           start: 213,

#           end: 235,

#           text: 'just over 4 feet tall.',

#           documentIds: [ 'web-search_0', 'web-search_5' ]

#         }

#       ],

#       documents: [

#         {

#           id: 'web-search_1',

#           snippet: 'Largest species of penguin ever\n' +

#             '\n' +

#             'TencentContact an Account Manager\n' +

#             '\n' +

#             "The largest species of penguin ever recorded is a newly described prehistoric species, Kumimanu fordycei, known from fossil remains discovered inside boulders in North Otago, on New Zealand's South Island. By comparing the size and density of its bones with those of modern-day penguins, researchers estimate that it weighed 154 kilograms (340 pounds), which is three times that of today's largest species, the emperor penguin (Aptenodytes forsteri). The rocks containing the remains of this new giant fossil species date between 55.5 million years and 59.5 million years old, meaning that it existed during the Late Palaeocene. Details of the record-breaking prehistoric penguin were published in the Journal of Paleontology on 8 February 2023.\n" +

#             '\n' +

#             'The height of K. fordycei is debated, though a related extinct species, K. biceae, has been estimated to have stood up to 1.77 m (5 ft). A lack of complete skeletons of extinct giant penguins found to date makes it difficult for height to be determined with any degree of certainty.\n' +

#             '\n' +

#             "Prior to the recent discovery and description of K. fordycei, the largest species of penguin known to science was the colossus penguin (Palaeeudyptes klekowskii), which is estimated to have weighed as much as 115 kg (253 lb 8 oz), and stood up to 2 m (6 ft 6 in) tall. It lived in Antarctica's Seymour Island approximately 37 million years ago, during the Late Eocene, and is represented by the most complete fossil remains ever found for a penguin species in Antarctica.\n" +

#             '\n' +

#             "This species exceeds in height the previous record holder, Nordenskjoeld's giant penguin (Anthropornis nordenskjoeldi), which stood 1.7 m (5 ft 6 in) tall and also existed during the Eocene epoch, occurring in New Zealand and in Antarctica's Seymour Island.\n" +

#             '\n' +

#             'Records change on a daily basis and are not immediately published online. For a full list of record titles, please use our Record Application Search. (You will need to register / login for access)\n' +

#             '\n' +

#             'Comments below may relate to previous holders of this record.',

#           timestamp: '2024-07-28T02:56:04',

#           title: 'Largest species of penguin ever',

#           url: 'https://www.guinnessworldrecords.com/world-records/84903-largest-species-of-penguin'

#         },

#         {

#           id: 'web-search_2',

#           snippet: 'Mega penguins: These are the largest penguins to have ever lived\n' +

#             '\n' +

#             'No penguin alive today can compare with some of the extinct giants that once roamed the planet, including Kumimanu fordycei, Petradyptes stonehousei and Palaeeudyptes klekowskii\n' +

#             '\n' +

#             'An illustration of Kumimanu fordycei (the larger, single bird) and Petradyptes stonehousei penguins on an ancient New Zealand beach\n' +

#             '\n' +

#             'Artwork by Dr. Simone Giovanardi\n' +

#             '\n' +

#             'Penguins come in all shapes and sizes, from the fairy penguin (Eudyptula minor) which stands at just over 30 centimetres tall to the 1-metre-high emperor penguin (Aptenodytes forsteri). But even the biggest emperors alive today would be dwarfed by the mega-penguins that roamed Earth millions of years ago. Here are the most impressive of these ancient giants.\n' +

#             '\n' +

#             'The title of the largest penguin ever documented goes to the species Kumimanu fordycei, which was first described in February 2023.\n' +

#             '\n' +

#             'Daniel Ksepka at the Bruce Museum in Connecticut and his colleagues unearthed an unusually huge flipper bone of a penguin in southern New Zealand in 2018. “The big humerus was shocking to me,” he says. “I almost thought it was maybe some other animal.”\n' +

#             '\n' +

#             'The team quickly determined that this belonged to a new species of penguin that lived in what is now New Zealand over 55 million years ago. The sheer size of the bone suggested that the bird probably weighed between 148 and 160 kilograms and stood around 1.6 metres tall. “The emperor penguin just looks like a child next to it,” says Ksepka.\n' +

#             '\n' +

#             'The species was named after palaeontologist Ewan Fordyce, who made his own mega penguin discoveries in the 1970s (see below).\n' +

#             '\n' +

#             'Sign up to our Wild Wild Life newsletter\n' +

#             '\n' +

#             'A monthly celebration of the biodiversity of our planet’s animals, plants and other organisms.\n' +

#             '\n' +

#             'Sign up to newsletter\n' +

#             '\n' +

#             'Skeletons of Kumimanu, Petradyptes and a modern emperor penguin\n' +

#             '\n' +

#             'Artwork by Dr. Simone Giovanardi\n' +

#             '\n' +

#             'Petradyptes stonehousei\n' +

#             '\n' +

#             'Ksepka and his colleagues discovered another giant penguin alongside K. fordycei, called Petradyptes stonehousei. With an estimated mass of 50 kilograms, it was quite a bit smaller than its contemporary. Its name comes from the Greek “petra” for rock and “dyptes” for diver, while “stonehousei” was chosen to honour British polar scientist Bernard Stonehouse.\n' +

#             '\n' +

#             'Both K. fordycei and P. stonehousei retained features seen in much earlier penguin species, such as slimmer flipper bones and muscle attachment points that look like those seen in flying birds.\n' +

#             '\n' +

#             '“Both penguins really add to the case that penguins got their start in New Zealand,” says Ksepka.\n' +

#             '\n' +

#             'Illustration of the extinct Palaeeudyptes klekowskii with a human and emperor penguin for scale\n' +

#             '\n' +

#             'Nature Picture Library / Alamy\n' +

#             '\n' +

#             'Palaeeudyptes klekowskii\n' +

#             '\n' +

#             'While K. fordycei was the heaviest penguin, it wasn’t the tallest. That award goes to Palaeeudyptes klekowskii, dubbed the colossus penguin, which towered at 2 metres and weighed a hefty 115 kilograms.\n' +

#             '\n' +

#             'The species lived 37 to 40 million years ago along the Antarctic coast. Its fossil, which included the longest fused ankle-foot bone, is one of the most complete ever uncovered from the Antarctic.\n' +

#             '\n' +

#             'Owing to their larger body size, giant penguins could remain underwater longer than smaller ones. Experts reckon that a species such as P. klekowskii could have remained submerged for up to 40 minutes hunting for fish.\n' +

#             '\n' +

#             'Pachydyptes ponderosus\n' +

#             '\n' +

#             'Pachydyptes ponderosus is prehistoric giant that lived more recently than those already mentioned – around 37 to 34 million years ago. Based on the few bones from the species that have been recovered, in 2006 Ksepka and his colleagues put it around 1.5 metres tall with a weight of over 100 kilograms.\n' +

#             '\n' +

#             '“We really only have parts of the flipper and shoulder, but we think it would have been quite a thick, stocky animal,” says Ksepka. “Its humerus is just so wide.”\n' +

#             '\n' +

#             'Daniel Ksepka with a model of a Kairuku penguin\n' +

#             '\n' +

#             'The three species that belonged to the genus Kairuku (K. grebneffi, K. waitaki and K. waewaeroa), however, were the complete opposite.\n' +

#             '\n' +

#             '“If Pachydyptes is like a big, heavy football lineman, then you can think of Kairuku as a really tall, skinny basketball player,” says Ksepka. “They’re both really big, but in different ways.”\n' +

#             '\n' +

#             'The first Kairuku bones were discovered by Ewan Fordyce in the 1970s, in New Zealand. All three species lived roughly 34 to 27 million years ago. The tallest, K. waewaeroa, stood at a height of around 1.4 metres and weighed around 80 kilograms.\n' +

#             '\n' +

#             '“They were graceful penguins, with slender trunks,” says Ksepka.\n' +

#             '\n' +

#             'Sign up to our weekly newsletter\n' +

#             '\n' +

#             "Receive a weekly dose of discovery in your inbox! We'll also keep you up to date with New Scientist events and special offers. Sign up\n" +

#             '\n' +

#             'More from New Scientist\n' +

#             '\n' +

#             'Explore the latest news, articles and features\n' +

#             '\n' +

#             'Extremely rare black penguin spotted in Antarctica\n' +

#             '\n' +

#             'How you can help with penguin research by browsing images at home\n' +

#             '\n' +

#             'Adélie penguins show signs of self-awareness on the mirror test\n' +

#             '\n' +

#             'Penguins adapt their accents to sound more like their friends\n' +

#             '\n' +

#             'Trending New Scientist articles\n' +

#             '\n' +

#             "SpaceX prepares for Starship flight with first 'chopstick' landing\n" +

#             '\n' +

#             'Evidence mounts that shingles vaccines protect against dementia\n' +

#             '\n' +

#             'When is the best time to exercise to get the most from your workout?\n' +

#             '\n' +

#             'Why slow running could be even more beneficial than running fast\n' +

#             '\n' +

#             'Wafer-thin light sail could help us reach another star sooner\n' +

#             '\n' +

#             'The remarkable science-backed ways to get fit as fast as possible\n' +

#             '\n' +

#             "One of Earth's major carbon sinks collapsed in 2023\n" +

#             '\n' +

#             'How to use psychology to hack your mind and fall in love with exercise\n' +

#             '\n' +

#             'Gene therapy enables five children who were born deaf to hear\n' +

#             '\n' +

#             'Why midlife is the perfect time to take control of your future health',

#           timestamp: '2024-07-28T02:56:04',

#           title: 'Mega penguins: The tallest, largest, most amazing penguin species to have ever lived | New Scientist',

#           url: 'https://www.newscientist.com/article/2397894-mega-penguins-these-are-the-largest-penguins-to-have-ever-lived/'

#         },

#         {

#           id: 'web-search_0',

#           snippet: 'Sustainability for All.\n' +

#             '\n' +

#             'Giant 6-Foot-8 Penguin Discovered in Antarctica\n' +

#             '\n' +

#             'University of Houston\n' +

#             '\n' +

#             'Bryan Nelson is a science writer and award-winning documentary filmmaker with over a decade of experience covering technology, astronomy, medicine, animals, and more.\n' +

#             '\n' +

#             'Learn about our editorial process\n' +

#             '\n' +

#             'Updated May 9, 2020 10:30AM EDT\n' +

#             '\n' +

#             "Modern emperor penguins are certainly statuesque, but not quite as impressive as the 'colossus penguin' would have been. . Christopher Michel/flickr\n" +

#             '\n' +

#             'The largest penguin species ever discovered has been unearthed in Antarctica, and its size is almost incomprehensible. Standing at 6 foot 8 inches from toe to beak tip, the mountainous bird would have dwarfed most adult humans, reports the Guardian.\n' +

#             '\n' +

#             'In fact, if it were alive today the penguin could have looked basketball superstar LeBron James square in the eyes.\n' +

#             '\n' +

#             "Fossils Provide Clues to the Bird's Size\n" +

#             '\n' +

#             `The bird's 37-million-year-old fossilized remains, which include the longest recorded fused ankle-foot bone as well as parts of the animal's wing bone, represent the most complete fossil ever uncovered in the Antarctic. Appropriately dubbed the "colossus penguin," Palaeeudyptes klekowskii was truly the Godzilla of aquatic birds.\n` +

#             '\n' +

#             `Scientists calculated the penguin's dimensions by scaling the sizes of its bones against those of modern penguin species. They estimate that the bird probably would have weighed about 250 pounds — again, roughly comparable to LeBron James. By comparison, the largest species of penguin alive today, the emperor penguin, is "only" about 4 feet tall and can weigh as much as 100 pounds.\n` +

#             '\n' +

#             'Interestingly, because larger bodied penguins can hold their breath for longer, the colossus penguin probably could have stayed underwater for 40 minutes or more. It boggles the mind to imagine the kinds of huge, deep sea fish this mammoth bird might have been capable of hunting.\n' +

#             '\n' +

#             "The fossil was found at the La Meseta formation on Seymour Island, an island in a chain of 16 major islands around the tip of the Graham Land on the Antarctic Peninsula. (It's the region that is the closest part of Antarctica to South America.) The area is known for its abundance of penguin bones, though in prehistoric times it would have been much warmer than it is today.\n" +

#             '\n' +

#             "P. klekowskii towers over the next largest penguin ever discovered, a 5-foot-tall bird that lived about 36 million years ago in Peru. Since these two species were near contemporaries, it's fun to imagine a time between 35 and 40 million years ago when giant penguins walked the Earth, and perhaps swam alongside the ancestors of whales.\n" +

#             '\n' +

#             '10 of the Largest Living Sea Creatures\n' +

#             '\n' +

#             '11 Facts About Blue Whales, the Largest Animals Ever on Earth\n' +

#             '\n' +

#             '16 Ocean Creatures That Live in Total Darkness\n' +

#             '\n' +

#             'National Monuments Designated By President Obama\n' +

#             '\n' +

#             '20 Pygmy Animal Species From Around the World\n' +

#             '\n' +

#             'School Kids Discover New Penguin Species in New Zealand\n' +

#             '\n' +

#             '16 of the Most Surreal Landscapes on Earth\n' +

#             '\n' +

#             '12 Peculiar Penguin Facts\n' +

#             '\n' +

#             "10 Amazing Hoodoos Around the World and How They're Formed\n" +

#             '\n' +

#             '8 Titanic Facts About Patagotitans\n' +

#             '\n' +

#             '9 Extinct Megafauna That Are Out of This World\n' +

#             '\n' +

#             '10 Places Where Penguins Live in the Wild\n' +

#             '\n' +

#             '16 Animals That Are Living Fossils\n' +

#             '\n' +

#             'A Timeline of the Distant Future for Life on Earth\n' +

#             '\n' +

#             '12 Animals That May Have Inspired Mythical Creatures\n' +

#             '\n' +

#             '12 Dinosaur Theme Parks\n' +

#             '\n' +

#             'By clicking “Accept All Cookies”, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts.\n' +

#             '\n' +

#             'Cookies Settings Accept All Cookies',

#           timestamp: '2024-07-27T06:29:15',

#           title: 'Giant 6-Foot-8 Penguin Discovered in Antarctica',

#           url: 'https://www.treehugger.com/giant-foot-penguin-discovered-in-antarctica-4864169'

#         },

#         {

#           id: 'web-search_5',

#           snippet: 'Skip to main content\n' +

#             '\n' +

#             'Smithsonian Institution\n' +

#             '\n' +

#             'Search Smithsonian Ocean\n' +

#             '\n' +

#             'Follow us on Facebook Follow us on Twitter Follow us on Flickr Follow us on Tumbr\n' +

#             '\n' +

#             'How Big Do Penguins Get?\n' +

#             '\n' +

#             '(Smithsonian Institution)\n' +

#             '\n' +

#             'The largest of the penguins, the emperor, stands at just over four feet while the smallest, the little penguin, has a maximum height of a foot. \n' +

#             '\n' +

#             'Coasts & Shallow Water\n' +

#             '\n' +

#             'Census of Marine Life\n' +

#             '\n' +

#             'Waves, Storms & Tsunamis\n' +

#             '\n' +

#             'Temperature & Chemistry\n' +

#             '\n' +

#             'Solutions & Success Stories\n' +

#             '\n' +

#             'Books, Film & The Arts\n' +

#             '\n' +

#             'Search Smithsonian Ocean',

#           timestamp: '2024-07-30T03:47:03',

#           title: 'How Big Do Penguins Get? | Smithsonian Ocean',

#           url: 'https://ocean.si.edu/ocean-life/seabirds/how-big-do-penguins-get'

#         },

#         {

#           id: 'web-search_4',

#           snippet: 'The emperor penguin (Aptenodytes forsteri) is the tallest and heaviest of all living penguin species and is endemic to Antarctica. The male and female are similar in plumage and size, reaching 100 cm (39 in) in length and weighing from 22 to 45 kg (49 to 99 lb). Feathers of the head and back are black and sharply delineated from the white belly, pale-yellow breast and bright-yellow ear patches.\n' +

#             '\n' +

#             'Like all penguins, it is flightless, with a streamlined body, and wings stiffened and flattened into flippers for a marine habitat. Its diet consists primarily of fish, but also includes crustaceans, such as krill, and cephalopods, such as squid. While hunting, the species can remain submerged around 20 minutes, diving to a depth of 535 m (1,755 ft). It has several adaptations to facilitate this, including an unusually structured haemoglobin to allow it to function at low oxygen levels, solid bones to reduce barotrauma, and the ability to reduce its metabolism and shut down non-essential organ functions.\n' +

#             '\n' +

#             'The only penguin species that breeds during the Antarctic winter, emperor penguins trek 50–120 km (31–75 mi) over the ice to breeding colonies which can contain up to several thousand individuals. The female lays a single egg, which is incubated for just over two months by the male while the female returns to the sea to feed; parents subsequently take turns foraging at sea and caring for their chick in the colony. The lifespan is typically 20 years in the wild, although observations suggest that some individuals may live to 50 years of age.\n' +

#             '\n' +

#             'Emperor penguins were described in 1844 by English zoologist George Robert Gray, who created the generic name from Ancient Greek word elements, ἀ-πτηνο-δύτης [a-ptēno-dytēs], "without-wings-diver". Its specific name is in honour of the German naturalist Johann Reinhold Forster, who accompanied Captain James Cook on his second voyage and officially named five other penguin species. Forster may have been the first person to see the penguins in 1773–74, when he recorded a sighting of what he believed was the similar king penguin (A. patagonicus) but given the location, may very well have been A. forsteri.\n' +

#             '\n' +

#             "Together with the king penguin, the emperor penguin is one of two extant species in the genus Aptenodytes. Fossil evidence of a third species—Ridgen's penguin (A. ridgeni)—has been found in fossil records from the late Pliocene, about three million years ago, in New Zealand. Studies of penguin behaviour and genetics have proposed that the genus Aptenodytes is basal; in other words, that it split off from a branch which led to all other living penguin species. Mitochondrial and nuclear DNA evidence suggests this split occurred around 40 million years ago.\n" +

#             '\n' +

#             'Adult emperor penguins are 110–120 cm (43–47 in) in length, averaging 115 centimetres (45 in) according to Stonehouse (1975). Due to method of bird measurement that measures length between bill to tail, sometimes body length and standing height are confused, and some reported height even reaching 1.5 metres (4.9 ft) tall. There are still more than a few papers mentioning that they reach a standing height of 1.2 metres (3.9 ft) instead of body length. Although standing height of emperor penguin is rarely provided at scientific reports, Prévost (1961) recorded 86 wild individuals and measured maximum height of 1.08 metres (3.5 ft). Friedman (1945) recorded measurements from 22 wild individuals and resulted height ranging 83–97 cm (33–38 in). Ksepka et al. (2012) measured standing height of 81–94 cm (32–37 in) according to 11 complete skins collected in American Museum of Natural History. The weight ranges from 22.7 to 45.4 kg (50 to 100 lb) and varies by sex, with males weighing more than females. It is the fifth heaviest living bird species, after only the larger varieties of ratite. The weight also varies by season, as both male and female penguins lose substantial mass while raising hatchlings and incubating their egg. A male emperor penguin must withstand the extreme Antarctic winter cold for more than two months while protecting his egg. He eats nothing during this time. Most male emperors will lose around 12 kg (26 lb) while they wait for their eggs to hatch. The mean weight of males at the start of the breeding season is 38 kg (84 lb) and that of females is 29.5 kg (65 lb). After the breeding season this drops to 23 kg (51 lb) for both sexes.\n' +

#             '\n' +

#             'Like all penguin species, emperor penguins have streamlined bodies to minimize drag while swimming, and wings that are more like stiff, flat flippers. The tongue is equipped with rear-facing barbs to prevent prey from escaping when caught. Males and females are similar in size and colouration. The adult has deep black dorsal feathers, covering the head, chin, throat, back, dorsal part of the flippers, and tail. The black plumage is sharply delineated from the light-coloured plumage elsewhere. The underparts of the wings and belly are white, becoming pale yellow in the upper breast, while the ear patches are bright yellow. The upper mandible of the 8 cm (3 in) long bill is black, and the lower mandible can be pink, orange or lilac. In juveniles, the auricular patches, chin and throat are white, while its bill is black. Emperor penguin chicks are typically covered with silver-grey down and have black heads and white masks. A chick with all-white plumage was seen in 2001, but was not considered to be an albino as it did not have pink eyes. Chicks weigh around 315 g (11 oz) after hatching, and fledge when they reach about 50% of adult weight.\n' +

#             '\n' +

#             "The emperor penguin's dark plumage fades to brown from November until February (the Antarctic summer), before the yearly moult in January and February. Moulting is rapid in this species compared with other birds, taking only around 34 days. Emperor penguin feathers emerge from the skin after they have grown to a third of their total length, and before old feathers are lost, to help reduce heat loss. New feathers then push out the old ones before finishing their growth.\n" +

#             '\n' +

#             'The average yearly survival rate of an adult emperor penguin has been measured at 95.1%, with an average life expectancy of 19.9 years. The same researchers estimated that 1% of emperor penguins hatched could feasibly reach an age of 50 years. In contrast, only 19% of chicks survive their first year of life. Therefore, 80% of the emperor penguin population comprises adults five years and older.\n' +

#             '\n' +

#             'As the species has no fixed nest sites that individuals can use to locate their own partner or chick, emperor penguins must rely on vocal calls alone for identification. They use a complex set of calls that are critical to individual recognition between parents, offspring and mates, displaying the widest variation in individual calls of all penguins. Vocalizing emperor penguins use two frequency bands simultaneously. Chicks use a frequency-modulated whistle to beg for food and to contact parents.\n' +

#             '\n' +

#             "The emperor penguin breeds in the coldest environment of any bird species; air temperatures may reach −40 °C (−40 °F), and wind speeds may reach 144 km/h (89 mph). Water temperature is a frigid −1.8 °C (28.8 °F), which is much lower than the emperor penguin's average body temperature of 39 °C (102 °F). The species has adapted in several ways to counteract heat loss. Dense feathers provide 80–90% of its insulation and it has a layer of sub-dermal fat which may be up to 3 cm (1.2 in) thick before breeding. While the density of contour feathers is approximately 9 per square centimetre (58 per square inch), a combination of dense afterfeathers and down feathers (plumules) likely play a critical role for insulation. Muscles allow the feathers to be held erect on land, reducing heat loss by trapping a layer of air next to the skin. Conversely, the plumage is flattened in water, thus waterproofing the skin and the downy underlayer. Preening is vital in facilitating insulation and in keeping the plumage oily and water-repellent.\n" +

#             '\n' +

#             'The emperor penguin is able to thermoregulate (maintain its core body temperature) without altering its metabolism, over a wide range of temperatures. Known as the thermoneutral range, this extends from −10 to 20 °C (14 to 68 °F). Below this temperature range, its metabolic rate increases significantly, although an individual can maintain its core temperature from 38.0 °C (100.4 °F) down to −47 °C (−53 °F). Movement by swimming, walking, and shivering are three mechanisms for increasing metabolism; a fourth process involves an increase in the breakdown of fats by enzymes, which is induced by the hormone glucagon. At temperatures above 20 °C (68 °F), an emperor penguin may become agitated as its body temperature and metabolic rate rise to increase heat loss. Raising its wings and exposing the undersides increases the exposure of its body surface to the air by 16%, facilitating further heat loss.\n' +

#             '\n' +

#             'Adaptations to pressure and low oxygen\n' +

#             '\n' +

#             'In addition to the cold, the emperor penguin encounters another stressful condition on deep dives—markedly increased pressure of up to 40 times that of the surface, which in most other terrestrial organisms would cause barotrauma. The bones of the penguin are solid rather than air-filled, which eliminates the risk of mechanical barotrauma.\n' +

#             '\n' +

#             "While diving, the emperor penguin's oxygen use is markedly reduced, as its heart rate is reduced to as low as 15–20 beats per minute and non-essential organs are shut down, thus facilitating longer dives. Its haemoglobin and myoglobin are able to bind and transport oxygen at low blood concentrations; this allows the bird to function with very low oxygen levels that would otherwise result in loss of consciousness.\n" +

#             '\n' +

#             'Distribution and habitat\n' +

#             '\n' +

#             'The emperor penguin has a circumpolar distribution in the Antarctic almost exclusively between the 66° and 77° south latitudes. It almost always breeds on stable pack ice near the coast and up to 18 km (11 mi) offshore. Breeding colonies are usually in areas where ice cliffs and i'... 22063 more characters,

#           timestamp: '2024-07-31T07:59:36',

#           title: 'Emperor penguin - Wikipedia',

#           url: 'https://en.wikipedia.org/wiki/Emperor_penguin'

#         }

#       ],

#       searchResults: [

#         {

#           searchQuery: {

#             text: 'How tall are the largest penguins?',

#             generationId: '8d5ae032-4c8e-492e-8686-289f198b5eb5'

#           },

#           documentIds: [

#             'web-search_0',

#             'web-search_1',

#             'web-search_2',

#             'web-search_3',

#             'web-search_4',

#             'web-search_5'

#           ],

#           connector: { id: 'web-search' }

#         }

#       ],

#       searchQueries: [

#         {

#           text: 'How tall are the largest penguins?',

#           generationId: '8d5ae032-4c8e-492e-8686-289f198b5eb5'

#         }

#       ]

#     },

#     response_metadata: {

#       estimatedTokenUsage: { completionTokens: 286, promptTokens: 11198, totalTokens: 11484 },

#       response_id: '8d5ae032-4c8e-492e-8686-289f198b5eb5',

#       generationId: '2224736b-430c-46cf-9ca0-a7f5737466aa',

#       chatHistory: [

#         { role: 'USER', message: 'How tall are the largest pengiuns?' },

#         {

#           role: 'CHATBOT',

#           message: 'The largest penguin ever discovered is the prehistoric Palaeeudyptes klekowskii, or "colossus penguin", which stood at 6 feet 6 inches tall. The tallest penguin alive today is the emperor penguin, which stands at just over 4 feet tall.'

#         }

#       ],

#       finishReason: 'COMPLETE',

#       meta: {

#         apiVersion: { version: '1' },

#         billedUnits: { inputTokens: 10474, outputTokens: 62 },

#         tokens: { inputTokens: 11198, outputTokens: 286 }

#       },

#       citations: [

#         {

#           start: 43,

#           end: 54,

#           text: 'prehistoric',

#           documentIds: [ 'web-search_1', 'web-search_2' ]

#         },

#         {

#           start: 55,

#           end: 79,

#           text: 'Palaeeudyptes klekowskii',

#           documentIds: [ 'web-search_0', 'web-search_1', 'web-search_2' ]

#         },

#         {

#           start: 84,

#           end: 102,

#           text: '"colossus penguin"',

#           documentIds: [ 'web-search_0', 'web-search_1', 'web-search_2' ]

#         },

#         {

#           start: 119,

#           end: 125,

#           text: '6 feet',

#           documentIds: [ 'web-search_0', 'web-search_1' ]

#         },

#         {

#           start: 126,

#           end: 134,

#           text: '6 inches',

#           documentIds: [ 'web-search_1' ]

#         },

#         {

#           start: 161,

#           end: 172,

#           text: 'alive today',

#           documentIds: [ 'web-search_0', 'web-search_5' ]

#         },

#         {

#           start: 180,

#           end: 195,

#           text: 'emperor penguin',

#           documentIds: [

#             'web-search_0',

#             'web-search_1',

#             'web-search_2',

#             'web-search_4',

#             'web-search_5'

#           ]

#         },

#         {

#           start: 213,

#           end: 235,

#           text: 'just over 4 feet tall.',

#           documentIds: [ 'web-search_0', 'web-search_5' ]

#         }

#       ],

#       documents: [

#         {

#           id: 'web-search_1',

#           snippet: 'Largest species of penguin ever\n' +

#             '\n' +

#             'TencentContact an Account Manager\n' +

#             '\n' +

#             "The largest species of penguin ever recorded is a newly described prehistoric species, Kumimanu fordycei, known from fossil remains discovered inside boulders in North Otago, on New Zealand's South Island. By comparing the size and density of its bones with those of modern-day penguins, researchers estimate that it weighed 154 kilograms (340 pounds), which is three times that of today's largest species, the emperor penguin (Aptenodytes forsteri). The rocks containing the remains of this new giant fossil species date between 55.5 million years and 59.5 million years old, meaning that it existed during the Late Palaeocene. Details of the record-breaking prehistoric penguin were published in the Journal of Paleontology on 8 February 2023.\n" +

#             '\n' +

#             'The height of K. fordycei is debated, though a related extinct species, K. biceae, has been estimated to have stood up to 1.77 m (5 ft). A lack of complete skeletons of extinct giant penguins found to date makes it difficult for height to be determined with any degree of certainty.\n' +

#             '\n' +

#             "Prior to the recent discovery and description of K. fordycei, the largest species of penguin known to science was the colossus penguin (Palaeeudyptes klekowskii), which is estimated to have weighed as much as 115 kg (253 lb 8 oz), and stood up to 2 m (6 ft 6 in) tall. It lived in Antarctica's Seymour Island approximately 37 million years ago, during the Late Eocene, and is represented by the most complete fossil remains ever found for a penguin species in Antarctica.\n" +

#             '\n' +

#             "This species exceeds in height the previous record holder, Nordenskjoeld's giant penguin (Anthropornis nordenskjoeldi), which stood 1.7 m (5 ft 6 in) tall and also existed during the Eocene epoch, occurring in New Zealand and in Antarctica's Seymour Island.\n" +

#             '\n' +

#             'Records change on a daily basis and are not immediately published online. For a full list of record titles, please use our Record Application Search. (You will need to register / login for access)\n' +

#             '\n' +

#             'Comments below may relate to previous holders of this record.',

#           timestamp: '2024-07-28T02:56:04',

#           title: 'Largest species of penguin ever',

#           url: 'https://www.guinnessworldrecords.com/world-records/84903-largest-species-of-penguin'

#         },

#         {

#           id: 'web-search_2',

#           snippet: 'Mega penguins: These are the largest penguins to have ever lived\n' +

#             '\n' +

#             'No penguin alive today can compare with some of the extinct giants that once roamed the planet, including Kumimanu fordycei, Petradyptes stonehousei and Palaeeudyptes klekowskii\n' +

#             '\n' +

#             'An illustration of Kumimanu fordycei (the larger, single bird) and Petradyptes stonehousei penguins on an ancient New Zealand beach\n' +

#             '\n' +

#             'Artwork by Dr. Simone Giovanardi\n' +

#             '\n' +

#             'Penguins come in all shapes and sizes, from the fairy penguin (Eudyptula minor) which stands at just over 30 centimetres tall to the 1-metre-high emperor penguin (Aptenodytes forsteri). But even the biggest emperors alive today would be dwarfed by the mega-penguins that roamed Earth millions of years ago. Here are the most impressive of these ancient giants.\n' +

#             '\n' +

#             'The title of the largest penguin ever documented goes to the species Kumimanu fordycei, which was first described in February 2023.\n' +

#             '\n' +

#             'Daniel Ksepka at the Bruce Museum in Connecticut and his colleagues unearthed an unusually huge flipper bone of a penguin in southern New Zealand in 2018. “The big humerus was shocking to me,” he says. “I almost thought it was maybe some other animal.”\n' +

#             '\n' +

#             'The team quickly determined that this belonged to a new species of penguin that lived in what is now New Zealand over 55 million years ago. The sheer size of the bone suggested that the bird probably weighed between 148 and 160 kilograms and stood around 1.6 metres tall. “The emperor penguin just looks like a child next to it,” says Ksepka.\n' +

#             '\n' +

#             'The species was named after palaeontologist Ewan Fordyce, who made his own mega penguin discoveries in the 1970s (see below).\n' +

#             '\n' +

#             'Sign up to our Wild Wild Life newsletter\n' +

#             '\n' +

#             'A monthly celebration of the biodiversity of our planet’s animals, plants and other organisms.\n' +

#             '\n' +

#             'Sign up to newsletter\n' +

#             '\n' +

#             'Skeletons of Kumimanu, Petradyptes and a modern emperor penguin\n' +

#             '\n' +

#             'Artwork by Dr. Simone Giovanardi\n' +

#             '\n' +

#             'Petradyptes stonehousei\n' +

#             '\n' +

#             'Ksepka and his colleagues discovered another giant penguin alongside K. fordycei, called Petradyptes stonehousei. With an estimated mass of 50 kilograms, it was quite a bit smaller than its contemporary. Its name comes from the Greek “petra” for rock and “dyptes” for diver, while “stonehousei” was chosen to honour British polar scientist Bernard Stonehouse.\n' +

#             '\n' +

#             'Both K. fordycei and P. stonehousei retained features seen in much earlier penguin species, such as slimmer flipper bones and muscle attachment points that look like those seen in flying birds.\n' +

#             '\n' +

#             '“Both penguins really add to the case that penguins got their start in New Zealand,” says Ksepka.\n' +

#             '\n' +

#             'Illustration of the extinct Palaeeudyptes klekowskii with a human and emperor penguin for scale\n' +

#             '\n' +

#             'Nature Picture Library / Alamy\n' +

#             '\n' +

#             'Palaeeudyptes klekowskii\n' +

#             '\n' +

#             'While K. fordycei was the heaviest penguin, it wasn’t the tallest. That award goes to Palaeeudyptes klekowskii, dubbed the colossus penguin, which towered at 2 metres and weighed a hefty 115 kilograms.\n' +

#             '\n' +

#             'The species lived 37 to 40 million years ago along the Antarctic coast. Its fossil, which included the longest fused ankle-foot bone, is one of the most complete ever uncovered from the Antarctic.\n' +

#             '\n' +

#             'Owing to their larger body size, giant penguins could remain underwater longer than smaller ones. Experts reckon that a species such as P. klekowskii could have remained submerged for up to 40 minutes hunting for fish.\n' +

#             '\n' +

#             'Pachydyptes ponderosus\n' +

#             '\n' +

#             'Pachydyptes ponderosus is prehistoric giant that lived more recently than those already mentioned – around 37 to 34 million years ago. Based on the few bones from the species that have been recovered, in 2006 Ksepka and his colleagues put it around 1.5 metres tall with a weight of over 100 kilograms.\n' +

#             '\n' +

#             '“We really only have parts of the flipper and shoulder, but we think it would have been quite a thick, stocky animal,” says Ksepka. “Its humerus is just so wide.”\n' +

#             '\n' +

#             'Daniel Ksepka with a model of a Kairuku penguin\n' +

#             '\n' +

#             'The three species that belonged to the genus Kairuku (K. grebneffi, K. waitaki and K. waewaeroa), however, were the complete opposite.\n' +

#             '\n' +

#             '“If Pachydyptes is like a big, heavy football lineman, then you can think of Kairuku as a really tall, skinny basketball player,” says Ksepka. “They’re both really big, but in different ways.”\n' +

#             '\n' +

#             'The first Kairuku bones were discovered by Ewan Fordyce in the 1970s, in New Zealand. All three species lived roughly 34 to 27 million years ago. The tallest, K. waewaeroa, stood at a height of around 1.4 metres and weighed around 80 kilograms.\n' +

#             '\n' +

#             '“They were graceful penguins, with slender trunks,” says Ksepka.\n' +

#             '\n' +

#             'Sign up to our weekly newsletter\n' +

#             '\n' +

#             "Receive a weekly dose of discovery in your inbox! We'll also keep you up to date with New Scientist events and special offers. Sign up\n" +

#             '\n' +

#             'More from New Scientist\n' +

#             '\n' +

#             'Explore the latest news, articles and features\n' +

#             '\n' +

#             'Extremely rare black penguin spotted in Antarctica\n' +

#             '\n' +

#             'How you can help with penguin research by browsing images at home\n' +

#             '\n' +

#             'Adélie penguins show signs of self-awareness on the mirror test\n' +

#             '\n' +

#             'Penguins adapt their accents to sound more like their friends\n' +

#             '\n' +

#             'Trending New Scientist articles\n' +

#             '\n' +

#             "SpaceX prepares for Starship flight with first 'chopstick' landing\n" +

#             '\n' +

#             'Evidence mounts that shingles vaccines protect against dementia\n' +

#             '\n' +

#             'When is the best time to exercise to get the most from your workout?\n' +

#             '\n' +

#             'Why slow running could be even more beneficial than running fast\n' +

#             '\n' +

#             'Wafer-thin light sail could help us reach another star sooner\n' +

#             '\n' +

#             'The remarkable science-backed ways to get fit as fast as possible\n' +

#             '\n' +

#             "One of Earth's major carbon sinks collapsed in 2023\n" +

#             '\n' +

#             'How to use psychology to hack your mind and fall in love with exercise\n' +

#             '\n' +

#             'Gene therapy enables five children who were born deaf to hear\n' +

#             '\n' +

#             'Why midlife is the perfect time to take control of your future health',

#           timestamp: '2024-07-28T02:56:04',

#           title: 'Mega penguins: The tallest, largest, most amazing penguin species to have ever lived | New Scientist',

#           url: 'https://www.newscientist.com/article/2397894-mega-penguins-these-are-the-largest-penguins-to-have-ever-lived/'

#         },

#         {

#           id: 'web-search_0',

#           snippet: 'Sustainability for All.\n' +

#             '\n' +

#             'Giant 6-Foot-8 Penguin Discovered in Antarctica\n' +

#             '\n' +

#             'University of Houston\n' +

#             '\n' +

#             'Bryan Nelson is a science writer and award-winning documentary filmmaker with over a decade of experience covering technology, astronomy, medicine, animals, and more.\n' +

#             '\n' +

#             'Learn about our editorial process\n' +

#             '\n' +

#             'Updated May 9, 2020 10:30AM EDT\n' +

#             '\n' +

#             "Modern emperor penguins are certainly statuesque, but not quite as impressive as the 'colossus penguin' would have been. . Christopher Michel/flickr\n" +

#             '\n' +

#             'The largest penguin species ever discovered has been unearthed in Antarctica, and its size is almost incomprehensible. Standing at 6 foot 8 inches from toe to beak tip, the mountainous bird would have dwarfed most adult humans, reports the Guardian.\n' +

#             '\n' +

#             'In fact, if it were alive today the penguin could have looked basketball superstar LeBron James square in the eyes.\n' +

#             '\n' +

#             "Fossils Provide Clues to the Bird's Size\n" +

#             '\n' +

#             `The bird's 37-million-year-old fossilized remains, which include the longest recorded fused ankle-foot bone as well as parts of the animal's wing bone, represent the most complete fossil ever uncovered in the Antarctic. Appropriately dubbed the "colossus penguin," Palaeeudyptes klekowskii was truly the Godzilla of aquatic birds.\n` +

#             '\n' +

#             `Scientists calculated the penguin's dimensions by scaling the sizes of its bones against those of modern penguin species. They estimate that the bird probably would have weighed about 250 pounds — again, roughly comparable to LeBron James. By comparison, the largest species of penguin alive today, the emperor penguin, is "only" about 4 feet tall and can weigh as much as 100 pounds.\n` +

#             '\n' +

#             'Interestingly, because larger bodied penguins can hold their breath for longer, the colossus penguin probably could have stayed underwater for 40 minutes or more. It boggles the mind to imagine the kinds of huge, deep sea fish this mammoth bird might have been capable of hunting.\n' +

#             '\n' +

#             "The fossil was found at the La Meseta formation on Seymour Island, an island in a chain of 16 major islands around the tip of the Graham Land on the Antarctic Peninsula. (It's the region that is the closest part of Antarctica to South America.) The area is known for its abundance of penguin bones, though in prehistoric times it would have been much warmer than it is today.\n" +

#             '\n' +

#             "P. klekowskii towers over the next largest penguin ever discovered, a 5-foot-tall bird that lived about 36 million years ago in Peru. Since these two species were near contemporaries, it's fun to imagine a time between 35 and 40 million years ago when giant penguins walked the Earth, and perhaps swam alongside the ancestors of whales.\n" +

#             '\n' +

#             '10 of the Largest Living Sea Creatures\n' +

#             '\n' +

#             '11 Facts About Blue Whales, the Largest Animals Ever on Earth\n' +

#             '\n' +

#             '16 Ocean Creatures That Live in Total Darkness\n' +

#             '\n' +

#             'National Monuments Designated By President Obama\n' +

#             '\n' +

#             '20 Pygmy Animal Species From Around the World\n' +

#             '\n' +

#             'School Kids Discover New Penguin Species in New Zealand\n' +

#             '\n' +

#             '16 of the Most Surreal Landscapes on Earth\n' +

#             '\n' +

#             '12 Peculiar Penguin Facts\n' +

#             '\n' +

#             "10 Amazing Hoodoos Around the World and How They're Formed\n" +

#             '\n' +

#             '8 Titanic Facts About Patagotitans\n' +

#             '\n' +

#             '9 Extinct Megafauna That Are Out of This World\n' +

#             '\n' +

#             '10 Places Where Penguins Live in the Wild\n' +

#             '\n' +

#             '16 Animals That Are Living Fossils\n' +

#             '\n' +

#             'A Timeline of the Distant Future for Life on Earth\n' +

#             '\n' +

#             '12 Animals That May Have Inspired Mythical Creatures\n' +

#             '\n' +

#             '12 Dinosaur Theme Parks\n' +

#             '\n' +

#             'By clicking “Accept All Cookies”, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts.\n' +

#             '\n' +

#             'Cookies Settings Accept All Cookies',

#           timestamp: '2024-07-27T06:29:15',

#           title: 'Giant 6-Foot-8 Penguin Discovered in Antarctica',

#           url: 'https://www.treehugger.com/giant-foot-penguin-discovered-in-antarctica-4864169'

#         },

#         {

#           id: 'web-search_5',

#           snippet: 'Skip to main content\n' +

#             '\n' +

#             'Smithsonian Institution\n' +

#             '\n' +

#             'Search Smithsonian Ocean\n' +

#             '\n' +

#             'Follow us on Facebook Follow us on Twitter Follow us on Flickr Follow us on Tumbr\n' +

#             '\n' +

#             'How Big Do Penguins Get?\n' +

#             '\n' +

#             '(Smithsonian Institution)\n' +

#             '\n' +

#             'The largest of the penguins, the emperor, stands at just over four feet while the smallest, the little penguin, has a maximum height of a foot. \n' +

#             '\n' +

#             'Coasts & Shallow Water\n' +

#             '\n' +

#             'Census of Marine Life\n' +

#             '\n' +

#             'Waves, Storms & Tsunamis\n' +

#             '\n' +

#             'Temperature & Chemistry\n' +

#             '\n' +

#             'Solutions & Success Stories\n' +

#             '\n' +

#             'Books, Film & The Arts\n' +

#             '\n' +

#             'Search Smithsonian Ocean',

#           timestamp: '2024-07-30T03:47:03',

#           title: 'How Big Do Penguins Get? | Smithsonian Ocean',

#           url: 'https://ocean.si.edu/ocean-life/seabirds/how-big-do-penguins-get'

#         },

#         {

#           id: 'web-search_4',

#           snippet: 'The emperor penguin (Aptenodytes forsteri) is the tallest and heaviest of all living penguin species and is endemic to Antarctica. The male and female are similar in plumage and size, reaching 100 cm (39 in) in length and weighing from 22 to 45 kg (49 to 99 lb). Feathers of the head and back are black and sharply delineated from the white belly, pale-yellow breast and bright-yellow ear patches.\n' +

#             '\n' +

#             'Like all penguins, it is flightless, with a streamlined body, and wings stiffened and flattened into flippers for a marine habitat. Its diet consists primarily of fish, but also includes crustaceans, such as krill, and cephalopods, such as squid. While hunting, the species can remain submerged around 20 minutes, diving to a depth of 535 m (1,755 ft). It has several adaptations to facilitate this, including an unusually structured haemoglobin to allow it to function at low oxygen levels, solid bones to reduce barotrauma, and the ability to reduce its metabolism and shut down non-essential organ functions.\n' +

#             '\n' +

#             'The only penguin species that breeds during the Antarctic winter, emperor penguins trek 50–120 km (31–75 mi) over the ice to breeding colonies which can contain up to several thousand individuals. The female lays a single egg, which is incubated for just over two months by the male while the female returns to the sea to feed; parents subsequently take turns foraging at sea and caring for their chick in the colony. The lifespan is typically 20 years in the wild, although observations suggest that some individuals may live to 50 years of age.\n' +

#             '\n' +

#             'Emperor penguins were described in 1844 by English zoologist George Robert Gray, who created the generic name from Ancient Greek word elements, ἀ-πτηνο-δύτης [a-ptēno-dytēs], "without-wings-diver". Its specific name is in honour of the German naturalist Johann Reinhold Forster, who accompanied Captain James Cook on his second voyage and officially named five other penguin species. Forster may have been the first person to see the penguins in 1773–74, when he recorded a sighting of what he believed was the similar king penguin (A. patagonicus) but given the location, may very well have been A. forsteri.\n' +

#             '\n' +

#             "Together with the king penguin, the emperor penguin is one of two extant species in the genus Aptenodytes. Fossil evidence of a third species—Ridgen's penguin (A. ridgeni)—has been found in fossil records from the late Pliocene, about three million years ago, in New Zealand. Studies of penguin behaviour and genetics have proposed that the genus Aptenodytes is basal; in other words, that it split off from a branch which led to all other living penguin species. Mitochondrial and nuclear DNA evidence suggests this split occurred around 40 million years ago.\n" +

#             '\n' +

#             'Adult emperor penguins are 110–120 cm (43–47 in) in length, averaging 115 centimetres (45 in) according to Stonehouse (1975). Due to method of bird measurement that measures length between bill to tail, sometimes body length and standing height are confused, and some reported height even reaching 1.5 metres (4.9 ft) tall. There are still more than a few papers mentioning that they reach a standing height of 1.2 metres (3.9 ft) instead of body length. Although standing height of emperor penguin is rarely provided at scientific reports, Prévost (1961) recorded 86 wild individuals and measured maximum height of 1.08 metres (3.5 ft). Friedman (1945) recorded measurements from 22 wild individuals and resulted height ranging 83–97 cm (33–38 in). Ksepka et al. (2012) measured standing height of 81–94 cm (32–37 in) according to 11 complete skins collected in American Museum of Natural History. The weight ranges from 22.7 to 45.4 kg (50 to 100 lb) and varies by sex, with males weighing more than females. It is the fifth heaviest living bird species, after only the larger varieties of ratite. The weight also varies by season, as both male and female penguins lose substantial mass while raising hatchlings and incubating their egg. A male emperor penguin must withstand the extreme Antarctic winter cold for more than two months while protecting his egg. He eats nothing during this time. Most male emperors will lose around 12 kg (26 lb) while they wait for their eggs to hatch. The mean weight of males at the start of the breeding season is 38 kg (84 lb) and that of females is 29.5 kg (65 lb). After the breeding season this drops to 23 kg (51 lb) for both sexes.\n' +

#             '\n' +

#             'Like all penguin species, emperor penguins have streamlined bodies to minimize drag while swimming, and wings that are more like stiff, flat flippers. The tongue is equipped with rear-facing barbs to prevent prey from escaping when caught. Males and females are similar in size and colouration. The adult has deep black dorsal feathers, covering the head, chin, throat, back, dorsal part of the flippers, and tail. The black plumage is sharply delineated from the light-coloured plumage elsewhere. The underparts of the wings and belly are white, becoming pale yellow in the upper breast, while the ear patches are bright yellow. The upper mandible of the 8 cm (3 in) long bill is black, and the lower mandible can be pink, orange or lilac. In juveniles, the auricular patches, chin and throat are white, while its bill is black. Emperor penguin chicks are typically covered with silver-grey down and have black heads and white masks. A chick with all-white plumage was seen in 2001, but was not considered to be an albino as it did not have pink eyes. Chicks weigh around 315 g (11 oz) after hatching, and fledge when they reach about 50% of adult weight.\n' +

#             '\n' +

#             "The emperor penguin's dark plumage fades to brown from November until February (the Antarctic summer), before the yearly moult in January and February. Moulting is rapid in this species compared with other birds, taking only around 34 days. Emperor penguin feathers emerge from the skin after they have grown to a third of their total length, and before old feathers are lost, to help reduce heat loss. New feathers then push out the old ones before finishing their growth.\n" +

#             '\n' +

#             'The average yearly survival rate of an adult emperor penguin has been measured at 95.1%, with an average life expectancy of 19.9 years. The same researchers estimated that 1% of emperor penguins hatched could feasibly reach an age of 50 years. In contrast, only 19% of chicks survive their first year of life. Therefore, 80% of the emperor penguin population comprises adults five years and older.\n' +

#             '\n' +

#             'As the species has no fixed nest sites that individuals can use to locate their own partner or chick, emperor penguins must rely on vocal calls alone for identification. They use a complex set of calls that are critical to individual recognition between parents, offspring and mates, displaying the widest variation in individual calls of all penguins. Vocalizing emperor penguins use two frequency bands simultaneously. Chicks use a frequency-modulated whistle to beg for food and to contact parents.\n' +

#             '\n' +

#             "The emperor penguin breeds in the coldest environment of any bird species; air temperatures may reach −40 °C (−40 °F), and wind speeds may reach 144 km/h (89 mph). Water temperature is a frigid −1.8 °C (28.8 °F), which is much lower than the emperor penguin's average body temperature of 39 °C (102 °F). The species has adapted in several ways to counteract heat loss. Dense feathers provide 80–90% of its insulation and it has a layer of sub-dermal fat which may be up to 3 cm (1.2 in) thick before breeding. While the density of contour feathers is approximately 9 per square centimetre (58 per square inch), a combination of dense afterfeathers and down feathers (plumules) likely play a critical role for insulation. Muscles allow the feathers to be held erect on land, reducing heat loss by trapping a layer of air next to the skin. Conversely, the plumage is flattened in water, thus waterproofing the skin and the downy underlayer. Preening is vital in facilitating insulation and in keeping the plumage oily and water-repellent.\n" +

#             '\n' +

#             'The emperor penguin is able to thermoregulate (maintain its core body temperature) without altering its metabolism, over a wide range of temperatures. Known as the thermoneutral range, this extends from −10 to 20 °C (14 to 68 °F). Below this temperature range, its metabolic rate increases significantly, although an individual can maintain its core temperature from 38.0 °C (100.4 °F) down to −47 °C (−53 °F). Movement by swimming, walking, and shivering are three mechanisms for increasing metabolism; a fourth process involves an increase in the breakdown of fats by enzymes, which is induced by the hormone glucagon. At temperatures above 20 °C (68 °F), an emperor penguin may become agitated as its body temperature and metabolic rate rise to increase heat loss. Raising its wings and exposing the undersides increases the exposure of its body surface to the air by 16%, facilitating further heat loss.\n' +

#             '\n' +

#             'Adaptations to pressure and low oxygen\n' +

#             '\n' +

#             'In addition to the cold, the emperor penguin encounters another stressful condition on deep dives—markedly increased pressure of up to 40 times that of the surface, which in most other terrestrial organisms would cause barotrauma. The bones of the penguin are solid rather than air-filled, which eliminates the risk of mechanical barotrauma.\n' +

#             '\n' +

#             "While diving, the emperor penguin's oxygen use is markedly reduced, as its heart rate is reduced to as low as 15–20 beats per minute and non-essential organs are shut down, thus facilitating longer dives. Its haemoglobin and myoglobin are able to bind and transport oxygen at low blood concentrations; this allows the bird to function with very low oxygen levels that would otherwise result in loss of consciousness.\n" +

#             '\n' +

#             'Distribution and habitat\n' +

#             '\n' +

#             'The emperor penguin has a circumpolar distribution in the Antarctic almost exclusively between the 66° and 77° south latitudes. It almost always breeds on stable pack ice near the coast and up to 18 km (11 mi) offshore. Breeding colonies are usually in areas where ice cliffs and i'... 22063 more characters,

#           timestamp: '2024-07-31T07:59:36',

#           title: 'Emperor penguin - Wikipedia',

#           url: 'https://en.wikipedia.org/wiki/Emperor_penguin'

#         }

#       ],

#       searchResults: [

#         {

#           searchQuery: {

#             text: 'How tall are the largest penguins?',

#             generationId: '8d5ae032-4c8e-492e-8686-289f198b5eb5'

#           },

#           documentIds: [

#             'web-search_0',

#             'web-search_1',

#             'web-search_2',

#             'web-search_3',

#             'web-search_4',

#             'web-search_5'

#           ],

#           connector: { id: 'web-search' }

#         }

#       ],

#       searchQueries: [

#         {

#           text: 'How tall are the largest penguins?',

#           generationId: '8d5ae032-4c8e-492e-8686-289f198b5eb5'

#         }

#       ]

#     },

#     id: undefined,

#     tool_calls: [],

#     invalid_tool_calls: [],

#     usage_metadata: { input_tokens: 11198, output_tokens: 286, total_tokens: 11484 }

#   }


"""
We can see in the `additional_kwargs` object that the API request did a few things:

- Performed a search query, storing the result data in the `searchQueries` and `searchResults` fields. In the `searchQueries` field we see they rephrased our query for better results.
- Generated three documents from the search query.
- Generated a list of citations
- Generated a final response based on the above actions & content.
"""

"""
## API reference

For detailed documentation of all ChatCohere features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_cohere.ChatCohere.html
"""



================================================
FILE: docs/core_docs/docs/integrations/chat/deep_infra.mdx
================================================
---
sidebar_label: Deep Infra
---

import CodeBlock from "@theme/CodeBlock";

# ChatDeepInfra

LangChain supports chat models hosted by [Deep Infra](https://deepinfra.com/) through the `ChatDeepInfra` wrapper.
First, you'll need to install the `@langchain/community` package:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core
```

You'll need to obtain an API key and set it as an environment variable named `DEEPINFRA_API_TOKEN`
(or pass it into the constructor), then call the model as shown below:

import Example from "@examples/models/chat/integration_deepinfra.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## Related

- Chat model [conceptual guide](/docs/concepts/chat_models)
- Chat model [how-to guides](/docs/how_to/#chat-models)



================================================
FILE: docs/core_docs/docs/integrations/chat/deepseek.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: DeepSeek
---
"""

"""
# ChatDeepSeek

This will help you getting started with DeepSeek [chat models](/docs/concepts/#chat-models). For detailed documentation of all `ChatDeepSeek` features and configurations head to the [API reference](https://api.js.langchain.com/classes/_langchain_deepseek.ChatDeepSeek.html).

## Overview
### Integration details

| Class | Package | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/deepseek) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [`ChatDeepSeek`](https://api.js.langchain.com/classes/_langchain_deepseek.ChatDeepSeek.html) | [`@langchain/deepseek`](https://npmjs.com/@langchain/deepseek) | ❌ (see [Ollama](/docs/integrations/chat/ollama)) | beta | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/deepseek?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/deepseek?style=flat-square&label=%20&) |

### Model features

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | [Image input](/docs/how_to/multimodal_inputs/) | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |
| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: |
| ✅ | ✅ | ✅ | ❌ | ❌ | ❌ | ✅ | ✅ | ✅ | 

Note that as of 1/27/25, tool calling and structured output are not currently supported for `deepseek-reasoner`.

## Setup

To access DeepSeek models you'll need to create a DeepSeek account, get an API key, and install the `@langchain/deepseek` integration package.

You can also access the DeepSeek API through providers like [Together AI](/docs/integrations/chat/togetherai) or [Ollama](/docs/integrations/chat/ollama).

### Credentials

Head to https://deepseek.com/ to sign up to DeepSeek and generate an API key. Once you've done this set the `DEEPSEEK_API_KEY` environment variable:

```bash
export DEEPSEEK_API_KEY="your-api-key"
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain ChatDeepSeek integration lives in the `@langchain/deepseek` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/deepseek @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

import { ChatDeepSeek } from "@langchain/deepseek";

const llm = new ChatDeepSeek({
  model: "deepseek-reasoner",
  temperature: 0,
  // other params...
})

"""
<!-- ## Invocation -->
"""

const aiMsg = await llm.invoke([
  [
    "system",
    "You are a helpful assistant that translates English to French. Translate the user sentence.",
  ],
  ["human", "I love programming."],
])
aiMsg
# Output:
#   AIMessage {

#     "id": "e2874482-68a7-4552-8154-b6a245bab429",

#     "content": "J'adore la programmation.",

#     "additional_kwargs": {,

#       "reasoning_content": "...",

#     },

#     "response_metadata": {

#       "tokenUsage": {

#         "promptTokens": 23,

#         "completionTokens": 7,

#         "totalTokens": 30

#       },

#       "finish_reason": "stop",

#       "model_name": "deepseek-reasoner",

#       "usage": {

#         "prompt_tokens": 23,

#         "completion_tokens": 7,

#         "total_tokens": 30,

#         "prompt_tokens_details": {

#           "cached_tokens": 0

#         },

#         "prompt_cache_hit_tokens": 0,

#         "prompt_cache_miss_tokens": 23

#       },

#       "system_fingerprint": "fp_3a5770e1b4"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "output_tokens": 7,

#       "input_tokens": 23,

#       "total_tokens": 30,

#       "input_token_details": {

#         "cache_read": 0

#       },

#       "output_token_details": {}

#     }

#   }


console.log(aiMsg.content)
# Output:
#   J'adore la programmation.


"""
## Chaining

We can [chain](/docs/how_to/sequence/) our model with a prompt template like so:
"""

import { ChatPromptTemplate } from "@langchain/core/prompts"

const prompt = ChatPromptTemplate.fromMessages(
  [
    [
      "system",
      "You are a helpful assistant that translates {input_language} to {output_language}.",
    ],
    ["human", "{input}"],
  ]
)

const chain = prompt.pipe(llm);
await chain.invoke(
  {
    input_language: "English",
    output_language: "German",
    input: "I love programming.",
  }
)
# Output:
#   AIMessage {

#     "id": "6e7f6f8c-8d7a-4dad-be07-425384038fd4",

#     "content": "Ich liebe es zu programmieren.",

#     "additional_kwargs": {,

#       "reasoning_content": "...",

#     },

#     "response_metadata": {

#       "tokenUsage": {

#         "promptTokens": 18,

#         "completionTokens": 9,

#         "totalTokens": 27

#       },

#       "finish_reason": "stop",

#       "model_name": "deepseek-reasoner",

#       "usage": {

#         "prompt_tokens": 18,

#         "completion_tokens": 9,

#         "total_tokens": 27,

#         "prompt_tokens_details": {

#           "cached_tokens": 0

#         },

#         "prompt_cache_hit_tokens": 0,

#         "prompt_cache_miss_tokens": 18

#       },

#       "system_fingerprint": "fp_3a5770e1b4"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "output_tokens": 9,

#       "input_tokens": 18,

#       "total_tokens": 27,

#       "input_token_details": {

#         "cache_read": 0

#       },

#       "output_token_details": {}

#     }

#   }


"""
## API reference

For detailed documentation of all ChatDeepSeek features and configurations head to the API reference: https://api.js.langchain.com/classes/_langchain_deepseek.ChatDeepSeek.html
"""



================================================
FILE: docs/core_docs/docs/integrations/chat/fake.mdx
================================================
# Fake LLM

LangChain provides a fake LLM chat model for testing purposes. This allows you to mock out calls to the LLM and and simulate what would happen if the LLM responded in a certain way.

## Usage

import CodeBlock from "@theme/CodeBlock";
import FakeListChatExample from "@examples/models/chat/integration_fake.ts";

<CodeBlock language="typescript">{FakeListChatExample}</CodeBlock>

## Related

- Chat model [conceptual guide](/docs/concepts/chat_models)
- Chat model [how-to guides](/docs/how_to/#chat-models)



================================================
FILE: docs/core_docs/docs/integrations/chat/fireworks.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Fireworks
---
"""

"""
# ChatFireworks

[Fireworks AI](https://fireworks.ai/) is an AI inference platform to run and customize models. For a list of all models served by Fireworks see the [Fireworks docs](https://fireworks.ai/models).

This guide will help you getting started with `ChatFireworks` [chat models](/docs/concepts/chat_models). For detailed documentation of all `ChatFireworks` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_chat_models_fireworks.ChatFireworks.html).

## Overview
### Integration details

| Class | Package | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/fireworks) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [ChatFireworks](https://api.js.langchain.com/classes/langchain_community_chat_models_fireworks.ChatFireworks.html) | [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) | ❌ | ✅ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/community?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |

### Model features

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | [Image input](/docs/how_to/multimodal_inputs/) | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |
| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: |
| ✅ | ✅ | ✅ | ❌ | ❌ | ❌ | ✅ | ✅ | ✅ | 

## Setup

To access `ChatFireworks` models you'll need to create a Fireworks account, get an API key, and install the `@langchain/community` integration package.

### Credentials

Head to [the Fireworks website](https://fireworks.ai/login) to sign up to Fireworks and generate an API key. Once you've done this set the `FIREWORKS_API_KEY` environment variable:

```bash
export FIREWORKS_API_KEY="your-api-key"
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain `ChatFireworks` integration lives in the `@langchain/community` package:

```{=mdx}

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

import { ChatFireworks } from "@langchain/community/chat_models/fireworks" 

const llm = new ChatFireworks({
    model: "accounts/fireworks/models/llama-v3p1-70b-instruct",
    temperature: 0,
    maxTokens: undefined,
    timeout: undefined,
    maxRetries: 2,
    // other params...
})

"""
## Invocation
"""

const aiMsg = await llm.invoke([
    [
        "system",
        "You are a helpful assistant that translates English to French. Translate the user sentence.",
    ],
    ["human", "I love programming."],
])
aiMsg
# Output:
#   AIMessage {

#     "id": "chatcmpl-9rBYHbb6QYRrKyr2tMhO9pH4AYXR4",

#     "content": "J'adore la programmation.",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "completionTokens": 8,

#         "promptTokens": 31,

#         "totalTokens": 39

#       },

#       "finish_reason": "stop"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 31,

#       "output_tokens": 8,

#       "total_tokens": 39

#     }

#   }


console.log(aiMsg.content)
# Output:
#   J'adore la programmation.


"""
## Chaining

We can [chain](/docs/how_to/sequence/) our model with a prompt template like so:
"""

import { ChatPromptTemplate } from "@langchain/core/prompts"

const prompt = ChatPromptTemplate.fromMessages(
    [
        [
            "system",
            "You are a helpful assistant that translates {input_language} to {output_language}.",
        ],
        ["human", "{input}"],
    ]
)

const chain = prompt.pipe(llm);
await chain.invoke(
    {
        input_language: "English",
        output_language: "German",
        input: "I love programming.",
    }
)
# Output:
#   AIMessage {

#     "id": "chatcmpl-9rBYM3KSIhHOuTXpBvA5oFyk8RSaN",

#     "content": "Ich liebe das Programmieren.",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "completionTokens": 6,

#         "promptTokens": 26,

#         "totalTokens": 32

#       },

#       "finish_reason": "stop"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 26,

#       "output_tokens": 6,

#       "total_tokens": 32

#     }

#   }


"""
Behind the scenes, Fireworks AI uses the OpenAI SDK and OpenAI compatible API, with some caveats:

- Certain properties are not supported by the Fireworks API, see [here](https://readme.fireworks.ai/docs/openai-compatibility#api-compatibility).
- Generation using multiple prompts is not supported.
"""

"""
## API reference

For detailed documentation of all ChatFireworks features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_community_chat_models_fireworks.ChatFireworks.html
"""



================================================
FILE: docs/core_docs/docs/integrations/chat/friendli.mdx
================================================
# Friendli

> [Friendli](https://friendli.ai/) enhances AI application performance and optimizes cost savings with scalable, efficient deployment options, tailored for high-demand AI workloads.

This tutorial guides you through integrating `ChatFriendli` for chat applications using LangChain. `ChatFriendli` offers a flexible approach to generating conversational AI responses, supporting both synchronous and asynchronous calls.

## Setup

Ensure the `@langchain/community` is installed.

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core
```

Sign in to [Friendli Suite](https://suite.friendli.ai/) to create a Personal Access Token, and set it as the `FRIENDLI_TOKEN` environment.
You can set team id as `FRIENDLI_TEAM` environment.

You can initialize a Friendli chat model with selecting the model you want to use. The default model is `meta-llama-3-8b-instruct`. You can check the available models at [docs.friendli.ai](https://docs.friendli.ai/guides/serverless_endpoints/pricing#text-generation-models).

## Usage

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/models/chat/friendli.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## Related

- Chat model [conceptual guide](/docs/concepts/chat_models)
- Chat model [how-to guides](/docs/how_to/#chat-models)



================================================
FILE: docs/core_docs/docs/integrations/chat/google_generativeai.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Google GenAI
keywords: [gemini, gemini-pro, ChatGoogleGenerativeAI]
---
"""

"""
# ChatGoogleGenerativeAI

[Google AI](https://ai.google.dev/) offers a number of different chat models, including the powerful Gemini series. For information on the latest models, their features, context windows, etc. head to the [Google AI docs](https://ai.google.dev/gemini-api/docs/models/gemini).

This will help you getting started with `ChatGoogleGenerativeAI` [chat models](/docs/concepts/chat_models). For detailed documentation of all `ChatGoogleGenerativeAI` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_google_genai.ChatGoogleGenerativeAI.html).

## Overview
### Integration details

| Class | Package | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/google_generative_ai) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [ChatGoogleGenerativeAI](https://api.js.langchain.com/classes/langchain_google_genai.ChatGoogleGenerativeAI.html) | [@langchain/google-genai](https://api.js.langchain.com/modules/langchain_google_genai.html) | ❌ | ✅ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/google-genai?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/google-genai?style=flat-square&label=%20&) |

### Model features

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | [Image input](/docs/how_to/multimodal_inputs/) | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |
| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: |
| ✅ | ✅ | ❌ | ✅ | ✅ | ✅ | ✅ | ✅ | ❌ | 

## Setup

You can access Google's `gemini` and `gemini-vision` models, as well as other
generative models in LangChain through `ChatGoogleGenerativeAI` class in the
`@langchain/google-genai` integration package.

```{=mdx}

:::tip
You can also access Google's `gemini` family of models via the LangChain VertexAI and VertexAI-web integrations.

Click [here](/docs/integrations/chat/google_vertex_ai) to read the docs.
:::

```

### Credentials

Get an API key here: [https://ai.google.dev/tutorials/setup](https://ai.google.dev/tutorials/setup)

Then set the `GOOGLE_API_KEY` environment variable:

```bash
export GOOGLE_API_KEY="your-api-key"
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain `ChatGoogleGenerativeAI` integration lives in the `@langchain/google-genai` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/google-genai @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

import { ChatGoogleGenerativeAI } from "@langchain/google-genai"

const llm = new ChatGoogleGenerativeAI({
    model: "gemini-1.5-pro",
    temperature: 0,
    maxRetries: 2,
    // other params...
})

"""
## Invocation
"""

const aiMsg = await llm.invoke([
    [
        "system",
        "You are a helpful assistant that translates English to French. Translate the user sentence.",
    ],
    ["human", "I love programming."],
])
aiMsg
# Output:
#   AIMessage {

#     "content": "J'adore programmer. \n",

#     "additional_kwargs": {

#       "finishReason": "STOP",

#       "index": 0,

#       "safetyRatings": [

#         {

#           "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",

#           "probability": "NEGLIGIBLE"

#         },

#         {

#           "category": "HARM_CATEGORY_HATE_SPEECH",

#           "probability": "NEGLIGIBLE"

#         },

#         {

#           "category": "HARM_CATEGORY_HARASSMENT",

#           "probability": "NEGLIGIBLE"

#         },

#         {

#           "category": "HARM_CATEGORY_DANGEROUS_CONTENT",

#           "probability": "NEGLIGIBLE"

#         }

#       ]

#     },

#     "response_metadata": {

#       "finishReason": "STOP",

#       "index": 0,

#       "safetyRatings": [

#         {

#           "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",

#           "probability": "NEGLIGIBLE"

#         },

#         {

#           "category": "HARM_CATEGORY_HATE_SPEECH",

#           "probability": "NEGLIGIBLE"

#         },

#         {

#           "category": "HARM_CATEGORY_HARASSMENT",

#           "probability": "NEGLIGIBLE"

#         },

#         {

#           "category": "HARM_CATEGORY_DANGEROUS_CONTENT",

#           "probability": "NEGLIGIBLE"

#         }

#       ]

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 21,

#       "output_tokens": 5,

#       "total_tokens": 26

#     }

#   }


console.log(aiMsg.content)
# Output:
#   J'adore programmer. 

#   


"""
## Chaining

We can [chain](/docs/how_to/sequence/) our model with a prompt template like so:
"""

import { ChatPromptTemplate } from "@langchain/core/prompts"

const prompt = ChatPromptTemplate.fromMessages(
    [
        [
            "system",
            "You are a helpful assistant that translates {input_language} to {output_language}.",
        ],
        ["human", "{input}"],
    ]
)

const chain = prompt.pipe(llm);
await chain.invoke(
    {
        input_language: "English",
        output_language: "German",
        input: "I love programming.",
    }
)
# Output:
#   AIMessage {

#     "content": "Ich liebe das Programmieren. \n",

#     "additional_kwargs": {

#       "finishReason": "STOP",

#       "index": 0,

#       "safetyRatings": [

#         {

#           "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",

#           "probability": "NEGLIGIBLE"

#         },

#         {

#           "category": "HARM_CATEGORY_HATE_SPEECH",

#           "probability": "NEGLIGIBLE"

#         },

#         {

#           "category": "HARM_CATEGORY_HARASSMENT",

#           "probability": "NEGLIGIBLE"

#         },

#         {

#           "category": "HARM_CATEGORY_DANGEROUS_CONTENT",

#           "probability": "NEGLIGIBLE"

#         }

#       ]

#     },

#     "response_metadata": {

#       "finishReason": "STOP",

#       "index": 0,

#       "safetyRatings": [

#         {

#           "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",

#           "probability": "NEGLIGIBLE"

#         },

#         {

#           "category": "HARM_CATEGORY_HATE_SPEECH",

#           "probability": "NEGLIGIBLE"

#         },

#         {

#           "category": "HARM_CATEGORY_HARASSMENT",

#           "probability": "NEGLIGIBLE"

#         },

#         {

#           "category": "HARM_CATEGORY_DANGEROUS_CONTENT",

#           "probability": "NEGLIGIBLE"

#         }

#       ]

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 16,

#       "output_tokens": 7,

#       "total_tokens": 23

#     }

#   }


"""
## Safety Settings

Gemini models have default safety settings that can be overridden. If you are receiving lots of "Safety Warnings" from your models, you can try tweaking the safety_settings attribute of the model. For example, to turn off safety blocking for dangerous content, you can import enums from the `@google/generative-ai` package, then construct your LLM as follows:
"""

import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { HarmBlockThreshold, HarmCategory } from "@google/generative-ai";

const llmWithSafetySettings = new ChatGoogleGenerativeAI({
  model: "gemini-1.5-pro",
  temperature: 0,
  safetySettings: [
    {
      category: HarmCategory.HARM_CATEGORY_HARASSMENT,
      threshold: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    },
  ],
  // other params...
});

"""
## Tool calling

Tool calling with Google AI is mostly the same [as tool calling with other models](/docs/how_to/tool_calling), but has a few restrictions on schema.

The Google AI API does not allow tool schemas to contain an object with unknown properties. For example, the following Zod schemas will throw an error:

`const invalidSchema = z.object({ properties: z.record(z.unknown()) });`

and

`const invalidSchema2 = z.record(z.unknown());`

Instead, you should explicitly define the properties of the object field. Here's an example:
"""

import { tool } from "@langchain/core/tools";
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { z } from "zod";

// Define your tool
const fakeBrowserTool = tool((_) => {
  return "The search result is xyz..."
}, {
  name: "browser_tool",
  description: "Useful for when you need to find something on the web or summarize a webpage.",
  schema: z.object({
    url: z.string().describe("The URL of the webpage to search."),
    query: z.string().optional().describe("An optional search query to use."),
  }),
})

const llmWithTool = new ChatGoogleGenerativeAI({
  model: "gemini-pro",
}).bindTools([fakeBrowserTool]) // Bind your tools to the model

const toolRes = await llmWithTool.invoke([
  [
    "human",
    "Search the web and tell me what the weather will be like tonight in new york. use a popular weather website",
  ],
]);

console.log(toolRes.tool_calls);
# Output:
#   [

#     {

#       name: 'browser_tool',

#       args: {

#         url: 'https://www.weather.com',

#         query: 'weather tonight in new york'

#       },

#       type: 'tool_call'

#     }

#   ]


"""
### Built in Google Search Retrieval

Google also offers a built in search tool which you can use to ground content generation in real-world information. Here's an example of how to use it:
"""

import { DynamicRetrievalMode, GoogleSearchRetrievalTool } from "@google/generative-ai";
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";

const searchRetrievalTool: GoogleSearchRetrievalTool = {
  googleSearchRetrieval: {
    dynamicRetrievalConfig: {
      mode: DynamicRetrievalMode.MODE_DYNAMIC,
      dynamicThreshold: 0.7, // default is 0.7
    }
  }
};
const searchRetrievalModel = new ChatGoogleGenerativeAI({
  model: "gemini-1.5-pro",
  temperature: 0,
  maxRetries: 0,
}).bindTools([searchRetrievalTool]);

const searchRetrievalResult = await searchRetrievalModel.invoke("Who won the 2024 MLB World Series?");

console.log(searchRetrievalResult.content);
# Output:
#   The Los Angeles Dodgers won the 2024 World Series, defeating the New York Yankees in Game 5 on October 30, 2024, by a score of 7-6. This victory marks the Dodgers' eighth World Series title and their first in a full season since 1988.  They achieved this win by overcoming a 5-0 deficit, making them the first team in World Series history to win a clinching game after being behind by such a margin.  The Dodgers also became the first team in MLB postseason history to overcome a five-run deficit, fall behind again, and still win.  Walker Buehler earned the save in the final game, securing the championship for the Dodgers.

#   


"""
The response also includes metadata about the search result:
"""

console.dir(searchRetrievalResult.response_metadata?.groundingMetadata, { depth: null });
# Output:
#   {

#     searchEntryPoint: {

#       renderedContent: '<style>\n' +

#         '.container {\n' +

#         '  align-items: center;\n' +

#         '  border-radius: 8px;\n' +

#         '  display: flex;\n' +

#         '  font-family: Google Sans, Roboto, sans-serif;\n' +

#         '  font-size: 14px;\n' +

#         '  line-height: 20px;\n' +

#         '  padding: 8px 12px;\n' +

#         '}\n' +

#         '.chip {\n' +

#         '  display: inline-block;\n' +

#         '  border: solid 1px;\n' +

#         '  border-radius: 16px;\n' +

#         '  min-width: 14px;\n' +

#         '  padding: 5px 16px;\n' +

#         '  text-align: center;\n' +

#         '  user-select: none;\n' +

#         '  margin: 0 8px;\n' +

#         '  -webkit-tap-highlight-color: transparent;\n' +

#         '}\n' +

#         '.carousel {\n' +

#         '  overflow: auto;\n' +

#         '  scrollbar-width: none;\n' +

#         '  white-space: nowrap;\n' +

#         '  margin-right: -12px;\n' +

#         '}\n' +

#         '.headline {\n' +

#         '  display: flex;\n' +

#         '  margin-right: 4px;\n' +

#         '}\n' +

#         '.gradient-container {\n' +

#         '  position: relative;\n' +

#         '}\n' +

#         '.gradient {\n' +

#         '  position: absolute;\n' +

#         '  transform: translate(3px, -9px);\n' +

#         '  height: 36px;\n' +

#         '  width: 9px;\n' +

#         '}\n' +

#         '@media (prefers-color-scheme: light) {\n' +

#         '  .container {\n' +

#         '    background-color: #fafafa;\n' +

#         '    box-shadow: 0 0 0 1px #0000000f;\n' +

#         '  }\n' +

#         '  .headline-label {\n' +

#         '    color: #1f1f1f;\n' +

#         '  }\n' +

#         '  .chip {\n' +

#         '    background-color: #ffffff;\n' +

#         '    border-color: #d2d2d2;\n' +

#         '    color: #5e5e5e;\n' +

#         '    text-decoration: none;\n' +

#         '  }\n' +

#         '  .chip:hover {\n' +

#         '    background-color: #f2f2f2;\n' +

#         '  }\n' +

#         '  .chip:focus {\n' +

#         '    background-color: #f2f2f2;\n' +

#         '  }\n' +

#         '  .chip:active {\n' +

#         '    background-color: #d8d8d8;\n' +

#         '    border-color: #b6b6b6;\n' +

#         '  }\n' +

#         '  .logo-dark {\n' +

#         '    display: none;\n' +

#         '  }\n' +

#         '  .gradient {\n' +

#         '    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n' +

#         '  }\n' +

#         '}\n' +

#         '@media (prefers-color-scheme: dark) {\n' +

#         '  .container {\n' +

#         '    background-color: #1f1f1f;\n' +

#         '    box-shadow: 0 0 0 1px #ffffff26;\n' +

#         '  }\n' +

#         '  .headline-label {\n' +

#         '    color: #fff;\n' +

#         '  }\n' +

#         '  .chip {\n' +

#         '    background-color: #2c2c2c;\n' +

#         '    border-color: #3c4043;\n' +

#         '    color: #fff;\n' +

#         '    text-decoration: none;\n' +

#         '  }\n' +

#         '  .chip:hover {\n' +

#         '    background-color: #353536;\n' +

#         '  }\n' +

#         '  .chip:focus {\n' +

#         '    background-color: #353536;\n' +

#         '  }\n' +

#         '  .chip:active {\n' +

#         '    background-color: #464849;\n' +

#         '    border-color: #53575b;\n' +

#         '  }\n' +

#         '  .logo-light {\n' +

#         '    display: none;\n' +

#         '  }\n' +

#         '  .gradient {\n' +

#         '    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n' +

#         '  }\n' +

#         '}\n' +

#         '</style>\n' +

#         '<div class="container">\n' +

#         '  <div class="headline">\n' +

#         '    <svg class="logo-light" width="18" height="18" viewBox="9 9 35 35" fill="none" xmlns="http://www.w3.org/2000/svg">\n' +

#         '      <path fill-rule="evenodd" clip-rule="evenodd" d="M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z" fill="#4285F4"/>\n' +

#         '      <path fill-rule="evenodd" clip-rule="evenodd" d="M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z" fill="#34A853"/>\n' +

#         '      <path fill-rule="evenodd" clip-rule="evenodd" d="M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z" fill="#FBBC05"/>\n' +

#         '      <path fill-rule="evenodd" clip-rule="evenodd" d="M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z" fill="#EA4335"/>\n' +

#         '    </svg>\n' +

#         '    <svg class="logo-dark" width="18" height="18" viewBox="0 0 48 48" xmlns="http://www.w3.org/2000/svg">\n' +

#         '      <circle cx="24" cy="23" fill="#FFF" r="22"/>\n' +

#         '      <path d="M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z" fill="#4285F4"/>\n' +

#         '      <path d="M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z" fill="#34A853"/>\n' +

#         '      <path d="M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z" fill="#FBBC05"/>\n' +

#         '      <path d="M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z" fill="#EA4335"/>\n' +

#         '    </svg>\n' +

#         '    <div class="gradient-container"><div class="gradient"></div></div>\n' +

#         '  </div>\n' +

#         '  <div class="carousel">\n' +

#         '    <a class="chip" href="https://vertexaisearch.cloud.google.com/grounding-api-redirect/AZnLMfyXqJN3K4FKueRIZDY2Owjs5Rw4dqgDOc6ZjYKsFo4GgENxLktR2sPHtNUuEBIUeqmUYc3jz9pLRq2cgSpc-4EoGBwQSTTpKk71CX7revnXUa54r9LxcxKgYxrUNBm5HpEm6JDNeJykc6NacPYv43M2wgkrhHCHCzHRyjEP2YR0Pxq4JQMUuOrLeTAYWB9oUb87FE5ksfuB6gimqO5-6uS3psR6">who won the 2024 mlb world series</a>\n' +

#         '  </div>\n' +

#         '</div>\n'

#     },

#     groundingChunks: [

#       {

#         web: {

#           uri: 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AZnLMfwvs0gpiM4BbIcNXZnnp4d4ED_rLnIYz2ZwM-lwFnoUxXNlKzy7ZSbbs_E27yhARG6Gx2AuW7DsoqkWPfDFMqPdXfvG3n0qFOQxQ4MBQ9Ox9mTk3KH5KPRJ79m8V118RQRyhi6oK5qg5-fLQunXUVn_a42K7eMk7Kjb8VpZ4onl8Glv1lQQsAK7YWyYkQ7WkTHDHVGB-vrL2U2yRQ==',

#           title: 'foxsports.com'

#         }

#       },

#       {

#         web: {

#           uri: 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AZnLMfwxwBq8VYgKAhf3UC8U6U5D-i0lK4TwP-2Jf8ClqB-sI0iptm9GxgeaH1iHFbSi-j_C3UqYj8Ok0YDTyvg87S7JamU48pndrd467ZQbI2sI0yWxsCCZ_dosXHwemBHFL5TW2hbAqasq93CfJ09cp1jU',

#           title: 'mlb.com'

#         }

#       }

#     ],

#     groundingSupports: [

#       {

#         segment: {

#           endIndex: 131,

#           text: 'The Los Angeles Dodgers won the 2024 World Series, defeating the New York Yankees in Game 5 on October 30, 2024, by a score of 7-6.'

#         },

#         groundingChunkIndices: [ 0, 1 ],

#         confidenceScores: [ 0.7652759, 0.7652759 ]

#       },

#       {

#         segment: {

#           startIndex: 401,

#           endIndex: 531,

#           text: 'The Dodgers also became the first team in MLB postseason history to overcome a five-run deficit, fall behind again, and still win.'

#         },

#         groundingChunkIndices: [ 1 ],

#         confidenceScores: [ 0.8487609 ]

#       }

#     ],

#     retrievalMetadata: { googleSearchDynamicRetrievalScore: 0.93359375 },

#     webSearchQueries: [ 'who won the 2024 mlb world series' ]

#   }


"""
### Code Execution

Google Generative AI also supports code execution. Using the built in `CodeExecutionTool`, you can make the model generate code, execute it, and use the results in a final completion:
"""

import { CodeExecutionTool } from "@google/generative-ai";
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";

const codeExecutionTool: CodeExecutionTool = {
  codeExecution: {}, // Simply pass an empty object to enable it.
};
const codeExecutionModel = new ChatGoogleGenerativeAI({
  model: "gemini-1.5-pro",
  temperature: 0,
  maxRetries: 0,
}).bindTools([codeExecutionTool]);

const codeExecutionResult = await codeExecutionModel.invoke("Use code execution to find the sum of the first and last 3 numbers in the following list: [1, 2, 3, 72638, 8, 727, 4, 5, 6]");

console.dir(codeExecutionResult.content, { depth: null });
# Output:
#   [

#     {

#       type: 'text',

#       text: "Here's how to find the sum of the first and last three numbers in the given list using Python:\n" +

#         '\n'

#     },

#     {

#       type: 'executableCode',

#       executableCode: {

#         language: 'PYTHON',

#         code: '\n' +

#           'my_list = [1, 2, 3, 72638, 8, 727, 4, 5, 6]\n' +

#           '\n' +

#           'first_three_sum = sum(my_list[:3])\n' +

#           'last_three_sum = sum(my_list[-3:])\n' +

#           'total_sum = first_three_sum + last_three_sum\n' +

#           '\n' +

#           'print(f"{first_three_sum=}")\n' +

#           'print(f"{last_three_sum=}")\n' +

#           'print(f"{total_sum=}")\n' +

#           '\n'

#       }

#     },

#     {

#       type: 'codeExecutionResult',

#       codeExecutionResult: {

#         outcome: 'OUTCOME_OK',

#         output: 'first_three_sum=6\nlast_three_sum=15\ntotal_sum=21\n'

#       }

#     },

#     {

#       type: 'text',

#       text: 'Therefore, the sum of the first three numbers (1, 2, 3) is 6, the sum of the last three numbers (4, 5, 6) is 15, and their total sum is 21.\n'

#     }

#   ]


"""
You can also pass this generation back to the model as chat history:
"""

const codeExecutionExplanation = await codeExecutionModel.invoke([
  codeExecutionResult,
  {
    role: "user",
    content: "Please explain the question I asked, the code you wrote, and the answer you got.",
  }
])

console.log(codeExecutionExplanation.content);
# Output:
#   You asked for the sum of the first three and the last three numbers in the list `[1, 2, 3, 72638, 8, 727, 4, 5, 6]`.

#   

#   Here's a breakdown of the code:

#   

#   1. **`my_list = [1, 2, 3, 72638, 8, 727, 4, 5, 6]`**: This line defines the list of numbers you provided.

#   

#   2. **`first_three_sum = sum(my_list[:3])`**: This calculates the sum of the first three numbers.  `my_list[:3]` is a slice of the list that takes elements from the beginning up to (but not including) the index 3.  So, it takes elements at indices 0, 1, and 2, which are 1, 2, and 3. The `sum()` function then adds these numbers together.

#   

#   3. **`last_three_sum = sum(my_list[-3:])`**: This calculates the sum of the last three numbers. `my_list[-3:]` is a slice that takes elements starting from the third element from the end and goes to the end of the list. So it takes elements at indices -3, -2, and -1 which correspond to 4, 5, and 6. The `sum()` function adds these numbers.

#   

#   4. **`total_sum = first_three_sum + last_three_sum`**: This adds the sum of the first three numbers and the sum of the last three numbers to get the final result.

#   

#   5. **`print(f"{first_three_sum=}")`**, **`print(f"{last_three_sum=}")`**, and **`print(f"{total_sum=}")`**: These lines print the calculated sums in a clear and readable format.

#   

#   

#   The output of the code was:

#   

#   * `first_three_sum=6`

#   * `last_three_sum=15`

#   * `total_sum=21`

#   

#   Therefore, the answer to your question is 21.

#   


"""
## Context Caching

Context caching allows you to pass some content to the model once, cache the input tokens, and then refer to the cached tokens for subsequent requests to reduce cost. You can create a `CachedContent` object using `GoogleAICacheManager` class and then pass the `CachedContent` object to your `ChatGoogleGenerativeAIModel` with `enableCachedContent()` method.
"""

import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import {
  GoogleAICacheManager,
  GoogleAIFileManager,
} from "@google/generative-ai/server";

const fileManager = new GoogleAIFileManager(process.env.GOOGLE_API_KEY);
const cacheManager = new GoogleAICacheManager(process.env.GOOGLE_API_KEY);

// uploads file for caching
const pathToVideoFile = "/path/to/video/file";
const displayName = "example-video";
const fileResult = await fileManager.uploadFile(pathToVideoFile, {
    displayName,
    mimeType: "video/mp4",
});

// creates cached content AFTER uploading is finished
const cachedContent = await cacheManager.create({
    model: "models/gemini-1.5-flash-001",
    displayName: displayName,
    systemInstruction: "You are an expert video analyzer, and your job is to answer " +
      "the user's query based on the video file you have access to.",
    contents: [
        {
            role: "user",
            parts: [
                {
                    fileData: {
                        mimeType: fileResult.file.mimeType,
                        fileUri: fileResult.file.uri,
                    },
                },
            ],
        },
    ],
    ttlSeconds: 300,
});

// passes cached video to model
const model = new ChatGoogleGenerativeAI({});
model.useCachedContent(cachedContent);

// invokes model with cached video
await model.invoke("Summarize the video");

"""
**Note**
- Context caching supports both Gemini 1.5 Pro and Gemini 1.5 Flash. Context caching is only available for stable models with fixed versions (for example, gemini-1.5-pro-001). You must include the version postfix (for example, the -001 in gemini-1.5-pro-001).
- The minimum input token count for context caching is 32,768, and the maximum is the same as the maximum for the given model.
"""

"""
## Gemini Prompting FAQs

As of the time this doc was written (2023/12/12), Gemini has some restrictions on the types and structure of prompts it accepts. Specifically:

1. When providing multimodal (image) inputs, you are restricted to at most 1 message of "human" (user) type. You cannot pass multiple messages (though the single human message may have multiple content entries)
2. System messages are not natively supported, and will be merged with the first human message if present.
3. For regular chat conversations, messages must follow the human/ai/human/ai alternating pattern. You may not provide 2 AI or human messages in sequence.
4. Message may be blocked if they violate the safety checks of the LLM. In this case, the model will return an empty response.

"""

"""
## API reference

For detailed documentation of all ChatGoogleGenerativeAI features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_google_genai.ChatGoogleGenerativeAI.html
"""



================================================
FILE: docs/core_docs/docs/integrations/chat/google_vertex_ai.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Google Vertex AI
---
"""

"""
# ChatVertexAI

[Google Vertex](https://cloud.google.com/vertex-ai) is a service that exposes all foundation models available in Google Cloud, like `gemini-1.5-pro`, `gemini-2.0-flash-exp`, etc.
It also provides some non-Google models such as [Anthropic's Claude](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude).


This will help you getting started with `ChatVertexAI` [chat models](/docs/concepts/chat_models). For detailed documentation of all `ChatVertexAI` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_google_vertexai.ChatVertexAI.html).

## Overview

### Integration details

| Class | Package | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/google_vertex_ai_palm) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [ChatVertexAI](https://api.js.langchain.com/classes/langchain_google_vertexai.ChatVertexAI.html) | [`@langchain/google-vertexai`](https://www.npmjs.com/package/@langchain/google-vertexai) | ❌ | ✅ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/google-vertexai?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/google-vertexai?style=flat-square&label=%20&) |

### Model features

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | [Image input](/docs/how_to/multimodal_inputs/) | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |
| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: |
| ✅ | ✅ | ❌ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | 

Note that while logprobs are supported, Gemini has fairly restricted usage of them.

## Setup

LangChain.js supports two different authentication methods based on whether
you're running in a Node.js environment or a web environment. It also supports
the authentication method used by Vertex AI Express Mode using either package.

To access `ChatVertexAI` models you'll need to setup Google VertexAI in your Google Cloud Platform (GCP) account, save the credentials file, and install the `@langchain/google-vertexai` integration package.

### Credentials

Head to your [GCP account](https://console.cloud.google.com/) and generate a credentials file. Once you've done this set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable:

```bash
export GOOGLE_APPLICATION_CREDENTIALS="path/to/your/credentials.json"
```

If running in a web environment, you should set the `GOOGLE_VERTEX_AI_WEB_CREDENTIALS` environment variable as a JSON stringified object, and install the `@langchain/google-vertexai-web` package:

```bash
GOOGLE_VERTEX_AI_WEB_CREDENTIALS={"type":"service_account","project_id":"YOUR_PROJECT-12345",...}
```

If you are using Vertex AI Express Mode, you can install either the `@langchain/google-vertexai` or `@langchain/google-vertexai-web` package.
You can then go to the [Express Mode](https://console.cloud.google.com/vertex-ai/studio) API Key page and set your API Key in the `GOOGLE_API_KEY` environment variable:

```bash
export GOOGLE_API_KEY="api_key_value"
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain `ChatVertexAI` integration lives in the `@langchain/google-vertexai` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/google-vertexai @langchain/core
</Npm2Yarn>

Or if using in a web environment like a [Vercel Edge function](https://vercel.com/blog/edge-functions-generally-available):

<Npm2Yarn>
  @langchain/google-vertexai-web @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

import { ChatVertexAI } from "@langchain/google-vertexai"
// Uncomment the following line if you're running in a web environment:
// import { ChatVertexAI } from "@langchain/google-vertexai-web"

const llm = new ChatVertexAI({
    model: "gemini-2.0-flash-exp",
    temperature: 0,
    maxRetries: 2,
    // For web, authOptions.credentials
    // authOptions: { ... }
    // other params...
})

"""
## Invocation
"""

const aiMsg = await llm.invoke([
    [
        "system",
        "You are a helpful assistant that translates English to French. Translate the user sentence.",
    ],
    ["human", "I love programming."],
])
aiMsg
# Output:
#   AIMessageChunk {

#     "content": "J'adore programmer. \n",

#     "additional_kwargs": {},

#     "response_metadata": {},

#     "tool_calls": [],

#     "tool_call_chunks": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 20,

#       "output_tokens": 7,

#       "total_tokens": 27

#     }

#   }


console.log(aiMsg.content)
# Output:
#   J'adore programmer. 

#   


"""
## Tool Calling with Google Search Retrieval

It is possible to call the model with a Google search tool which you can use to [ground](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/grounding) content generation with real-world information and reduce hallucinations.

Grounding is currently not supported by `gemini-2.0-flash-exp`.

You can choose to either ground using Google Search or by using a custom data store. Here are examples of both:  
"""

"""
### Google Search Retrieval

Grounding example that uses Google Search:

"""

import { ChatVertexAI } from "@langchain/google-vertexai"

const searchRetrievalTool = {
  googleSearchRetrieval: {
    dynamicRetrievalConfig: {
      mode: "MODE_DYNAMIC", // Use Dynamic Retrieval
      dynamicThreshold: 0.7, // Default for Dynamic Retrieval threshold
    },
  },
};

const searchRetrievalModel = new ChatVertexAI({
  model: "gemini-1.5-pro",
  temperature: 0,
  maxRetries: 0,
}).bindTools([searchRetrievalTool]);

const searchRetrievalResult = await searchRetrievalModel.invoke("Who won the 2024 NBA Finals?");

console.log(searchRetrievalResult.content);
# Output:
#   The Boston Celtics won the 2024 NBA Finals, defeating the Dallas Mavericks 4-1 in the series to claim their 18th NBA championship. This victory marked their first title since 2008 and established them as the team with the most NBA championships, surpassing the Los Angeles Lakers' 17 titles.

#   


"""
### Google Search Retrieval with Data Store

First, set up your data store (this is a schema of an example data store):

|    ID   |     Date     |    Team 1   |   Score  |   Team 2   |
|:-------:|:------------:|:-----------:|:--------:|:----------:|
|  3001   |  2023-09-07  |  Argentina  |  1 - 0   |  Ecuador   |
|  3002   |  2023-09-12  |  Venezuela  |  1 - 0   |  Paraguay  |
|  3003   |  2023-09-12  |  Chile      |  0 - 0   |  Colombia  |
|  3004   |  2023-09-12  |  Peru       |  0 - 1   |  Brazil    |
|  3005   |  2024-10-15  |  Argentina  |  6 - 0   |  Bolivia   |

Then, use this data store in the example provided below:

(Note that you have to use your own variables for `projectId` and `datastoreId`)

"""

import { ChatVertexAI } from "@langchain/google-vertexai";

const projectId = "YOUR_PROJECT_ID";
const datastoreId = "YOUR_DATASTORE_ID";

const searchRetrievalToolWithDataset = {
  retrieval: {
    vertexAiSearch: {
      datastore: `projects/${projectId}/locations/global/collections/default_collection/dataStores/${datastoreId}`,
    },
    disableAttribution: false,
  },
};

const searchRetrievalModelWithDataset = new ChatVertexAI({
  model: "gemini-1.5-pro",
  temperature: 0,
  maxRetries: 0,
}).bindTools([searchRetrievalToolWithDataset]);

const searchRetrievalModelResult = await searchRetrievalModelWithDataset.invoke(
  "What is the score of Argentina vs Bolivia football game?"
);

console.log(searchRetrievalModelResult.content);
# Output:
#   Argentina won against Bolivia with a score of 6-0 on October 15, 2024.

#   


"""
You should now get results that are grounded in the data from your provided data store.
"""

"""
## Context Caching

Vertex AI offers context caching functionality, which helps optimize costs by storing and reusing long blocks of message content across multiple API requests. This is particularly useful when you have lengthy conversation histories or message segments that appear frequently in your interactions.

To use this feature, first create a context cache by following [this official guide](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-create).

Once you've created a cache, you can pass its id in as a runtime param as follows:
"""

import { ChatVertexAI } from "@langchain/google-vertexai";

const modelWithCachedContent = new ChatVertexAI({
  model: "gemini-1.5-pro-002",
  location: "us-east5",
});

await modelWithCachedContent.invoke("What is in the content?", {
  cachedContent:
    "projects/PROJECT_NUMBER/locations/LOCATION/cachedContents/CACHE_ID",
});

"""
You can also bind this field directly onto the model instance:
"""

const modelWithBoundCachedContent = new ChatVertexAI({
  model: "gemini-1.5-pro-002",
  location: "us-east5",
}).bind({
  cachedContent:
    "projects/PROJECT_NUMBER/locations/LOCATION/cachedContents/CACHE_ID",
});


"""
Note that not all models currently support context caching.
"""

"""
## Chaining

We can [chain](/docs/how_to/sequence/) our model with a prompt template like so:
"""

import { ChatPromptTemplate } from "@langchain/core/prompts"

const prompt = ChatPromptTemplate.fromMessages(
    [
        [
            "system",
            "You are a helpful assistant that translates {input_language} to {output_language}.",
        ],
        ["human", "{input}"],
    ]
)

const chain = prompt.pipe(llm);
await chain.invoke(
  {
    input_language: "English",
    output_language: "German",
    input: "I love programming.",
  }
);
# Output:
#   AIMessageChunk {

#     "content": "Ich liebe das Programmieren. \n",

#     "additional_kwargs": {},

#     "response_metadata": {},

#     "tool_calls": [],

#     "tool_call_chunks": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 15,

#       "output_tokens": 9,

#       "total_tokens": 24

#     }

#   }


"""
## API reference

For detailed documentation of all ChatVertexAI features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_google_vertexai.ChatVertexAI.html
"""



================================================
FILE: docs/core_docs/docs/integrations/chat/groq.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Groq
---
"""

"""
# ChatGroq

[Groq](https://groq.com/) is a company that offers fast AI inference, powered by LPU™ AI inference technology which delivers fast, affordable, and energy efficient AI.

This will help you getting started with ChatGroq [chat models](/docs/concepts/chat_models). For detailed documentation of all ChatGroq features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_groq.ChatGroq.html).

## Overview
### Integration details

| Class | Package | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/groq) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [ChatGroq](https://api.js.langchain.com/classes/langchain_groq.ChatGroq.html) | [`@langchain/groq`](https://www.npmjs.com/package/@langchain/groq) | ❌ | ❌ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/groq?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/groq?style=flat-square&label=%20&) |

### Model features

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | [Image input](/docs/how_to/multimodal_inputs/) | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |
| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: |
| ✅ | ✅ | ✅ | ❌ | ❌ | ❌ | ✅ | ✅ | ✅ | 

## Setup

To access ChatGroq models you'll need to create a Groq account, get an API key, and install the `@langchain/groq` integration package.

### Credentials

In order to use the Groq API you'll need an API key. You can sign up for a Groq account and create an API key [here](https://wow.groq.com/).
Then, you can set the API key as an environment variable in your terminal:

```bash
export GROQ_API_KEY="your-api-key"
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain ChatGroq integration lives in the `@langchain/groq` package:

```{=mdx}

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/groq @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

import { ChatGroq } from "@langchain/groq" 

const llm = new ChatGroq({
    model: "llama-3.3-70b-versatile",
    temperature: 0,
    maxTokens: undefined,
    maxRetries: 2,
    // other params...
})

"""
## Invocation
"""

const aiMsg = await llm.invoke([
    {
      role: "system",
      content: "You are a helpful assistant that translates English to French. Translate the user sentence.",
    },
    { role: "user", content: "I love programming." },
])
aiMsg
# Output:
#   AIMessage {

#     "content": "I enjoy programming. (The French translation is: \"J'aime programmer.\")\n\nNote: I chose to translate \"I love programming\" as \"J'aime programmer\" instead of \"Je suis amoureux de programmer\" because the latter has a romantic connotation that is not present in the original English sentence.",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "completionTokens": 73,

#         "promptTokens": 31,

#         "totalTokens": 104

#       },

#       "finish_reason": "stop"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": []

#   }


console.log(aiMsg.content)
# Output:
#   I enjoy programming. (The French translation is: "J'aime programmer.")

#   

#   Note: I chose to translate "I love programming" as "J'aime programmer" instead of "Je suis amoureux de programmer" because the latter has a romantic connotation that is not present in the original English sentence.


"""
## Json invocation
"""

const messages = [
  {
    role: "system",
    content: "You are a math tutor that handles math exercises and makes output in json in format { result: number }.",
  },
  { role: "user",  content: "2 + 2 * 2" },
];

const aiInvokeMsg = await llm.invoke(messages, { response_format: { type: "json_object" } });

// if you want not to pass response_format in every invoke, you can bind it to the instance
const llmWithResponseFormat = llm.bind({ response_format: { type: "json_object" } });
const aiBindMsg = await llmWithResponseFormat.invoke(messages);

// they are the same
console.log({ aiInvokeMsgContent: aiInvokeMsg.content, aiBindMsg: aiBindMsg.content });
# Output:
#   {

#     aiInvokeMsgContent: '{\n"result": 6\n}',

#     aiBindMsg: '{\n"result": 6\n}'

#   }


"""
## Chaining

We can [chain](/docs/how_to/sequence/) our model with a prompt template like so:
"""

import { ChatPromptTemplate } from "@langchain/core/prompts"

const prompt = ChatPromptTemplate.fromMessages(
    [
        [
            "system",
            "You are a helpful assistant that translates {input_language} to {output_language}.",
        ],
        ["human", "{input}"],
    ]
)

const chain = prompt.pipe(llm);
await chain.invoke(
    {
        input_language: "English",
        output_language: "German",
        input: "I love programming.",
    }
)
# Output:
#   AIMessage {

#     "content": "That's great! I can help you translate English phrases related to programming into German.\n\n\"I love programming\" can be translated to German as \"Ich liebe Programmieren\".\n\nHere are some more programming-related phrases translated into German:\n\n* \"Programming language\" = \"Programmiersprache\"\n* \"Code\" = \"Code\"\n* \"Variable\" = \"Variable\"\n* \"Function\" = \"Funktion\"\n* \"Array\" = \"Array\"\n* \"Object-oriented programming\" = \"Objektorientierte Programmierung\"\n* \"Algorithm\" = \"Algorithmus\"\n* \"Data structure\" = \"Datenstruktur\"\n* \"Debugging\" = \"Debuggen\"\n* \"Compile\" = \"Kompilieren\"\n* \"Link\" = \"Verknüpfen\"\n* \"Run\" = \"Ausführen\"\n* \"Test\" = \"Testen\"\n* \"Deploy\" = \"Bereitstellen\"\n* \"Version control\" = \"Versionskontrolle\"\n* \"Open source\" = \"Open Source\"\n* \"Software development\" = \"Softwareentwicklung\"\n* \"Agile methodology\" = \"Agile Methodik\"\n* \"DevOps\" = \"DevOps\"\n* \"Cloud computing\" = \"Cloud Computing\"\n\nI hope this helps! Let me know if you have any other questions or if you need further translations.",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "completionTokens": 327,

#         "promptTokens": 25,

#         "totalTokens": 352

#       },

#       "finish_reason": "stop"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": []

#   }


"""
## API reference

For detailed documentation of all ChatGroq features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_groq.ChatGroq.html
"""



================================================
FILE: docs/core_docs/docs/integrations/chat/ibm.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: IBM watsonx.ai
---
"""

"""
# IBM watsonx.ai

This will help you getting started with IBM watsonx.ai [chat models](/docs/concepts/chat_models). For detailed documentation of all `IBM watsonx.ai` features and configurations head to the [IBM watsonx.ai](https://api.js.langchain.com/modules/_langchain_community.chat_models_ibm.html).

## Overview
### Integration details

| Class | Package | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/ibm_watsonx/) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [`ChatWatsonx`](https://api.js.langchain.com/classes/_langchain_community.chat_models_ibm.ChatWatsonx.html) | [@langchain/community](https://www.npmjs.com/package/@langchain/community) | ❌ | ✅ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/community?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |

### Model features


| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | [Image input](/docs/how_to/multimodal_inputs/) | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |
| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: |
| ✅ | ✅ | ✅  | ✅ | ❌ | ❌ | ✅ | ✅ | ❌ | 

## Setup

To access IBM watsonx.ai models you'll need to create a/an IBM watsonx.ai account, get an API key, and install the `@langchain/community` integration package.

### Credentials


Head to [IBM Cloud](https://cloud.ibm.com/login) to sign up to IBM watsonx.ai and generate an API key or provide any other authentication form as presented below.

#### IAM authentication

```bash
export WATSONX_AI_AUTH_TYPE=iam
export WATSONX_AI_APIKEY=<YOUR-APIKEY>
```

#### Bearer token authentication

```bash
export WATSONX_AI_AUTH_TYPE=bearertoken
export WATSONX_AI_BEARER_TOKEN=<YOUR-BEARER-TOKEN>
```

#### IBM watsonx.ai software authentication

```bash
export WATSONX_AI_AUTH_TYPE=cp4d
export WATSONX_AI_USERNAME=<YOUR_USERNAME>
export WATSONX_AI_PASSWORD=<YOUR_PASSWORD>
export WATSONX_AI_URL=<URL>
```

Once these are places in your enviromental variables and object is initialized authentication will proceed automatically.

Authentication can also be accomplished by passing these values as parameters to a new instance.

## IAM authentication

```typescript
import { WatsonxLLM } from "@langchain/community/llms/ibm";

const props = {
  version: "YYYY-MM-DD",
  serviceUrl: "<SERVICE_URL>",
  projectId: "<PROJECT_ID>",
  watsonxAIAuthType: "iam",
  watsonxAIApikey: "<YOUR-APIKEY>",
};
const instance = new WatsonxLLM(props);
```

## Bearer token authentication

```typescript
import { WatsonxLLM } from "@langchain/community/llms/ibm";

const props = {
  version: "YYYY-MM-DD",
  serviceUrl: "<SERVICE_URL>",
  projectId: "<PROJECT_ID>",
  watsonxAIAuthType: "bearertoken",
  watsonxAIBearerToken: "<YOUR-BEARERTOKEN>",
};
const instance = new WatsonxLLM(props);
```

### IBM watsonx.ai software authentication

```typescript
import { WatsonxLLM } from "@langchain/community/llms/ibm";

const props = {
  version: "YYYY-MM-DD",
  serviceUrl: "<SERVICE_URL>",
  projectId: "<PROJECT_ID>",
  watsonxAIAuthType: "cp4d",
  watsonxAIUsername: "<YOUR-USERNAME>",
  watsonxAIPassword: "<YOUR-PASSWORD>",
  watsonxAIUrl: "<url>",
};
const instance = new WatsonxLLM(props);
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain IBM watsonx.ai integration lives in the `@langchain/community` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:

"""

import { ChatWatsonx } from "@langchain/community/chat_models/ibm";
const props = {
  maxTokens: 200,
  temperature: 0.5
};

const instance = new ChatWatsonx({
  version: "YYYY-MM-DD",
  serviceUrl: process.env.API_URL,
  projectId: "<PROJECT_ID>",
  // spaceId: "<SPACE_ID>",
  // idOrName: "<DEPLOYMENT_ID>",
  model: "<MODEL_ID>",
  ...props
});

"""
Note:

- You must provide `spaceId`, `projectId` or `idOrName`(deployment id) unless you use lighweight engine which works without specifying either (refer to [watsonx.ai docs](https://www.ibm.com/docs/en/cloud-paks/cp-data/5.0.x?topic=install-choosing-installation-mode))
- Depending on the region of your provisioned service instance, use correct serviceUrl.
"""

"""
## Invocation
"""

const aiMsg = await instance.invoke([{
  role: "system",
  content: "You are a helpful assistant that translates English to French. Translate the user sentence.",
},
{
  role: "user",
  content: "I love programming."
}]);
console.log(aiMsg)
# Output:
#   AIMessage {

#     "id": "chat-c5341b2062dc42f091e5ae2558e905e3",

#     "content": " J'adore la programmation.",

#     "additional_kwargs": {

#       "tool_calls": []

#     },

#     "response_metadata": {

#       "tokenUsage": {

#         "completion_tokens": 10,

#         "prompt_tokens": 28,

#         "total_tokens": 38

#       },

#       "finish_reason": "stop"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 28,

#       "output_tokens": 10,

#       "total_tokens": 38

#     }

#   }


console.log(aiMsg.content)
# Output:
#    J'adore la programmation.


"""
## Chaining

We can [chain](/docs/how_to/sequence/) our model with a prompt template like so:
"""

import { ChatPromptTemplate } from "@langchain/core/prompts";

const prompt = ChatPromptTemplate.fromMessages(
  [
    [
      "system",
      "You are a helpful assistant that translates {input_language} to {output_language}.",
    ],
    ["human", "{input}"],
  ]
)
const chain = prompt.pipe(instance);
await chain.invoke(
    {
      input_language: "English",
      output_language: "German",
      input: "I love programming.",
    }
  )
# Output:
#   AIMessage {

#     "id": "chat-c5c2c08d3c984254acc48225c39c6a08",

#     "content": " Ich liebe Programmieren.",

#     "additional_kwargs": {

#       "tool_calls": []

#     },

#     "response_metadata": {

#       "tokenUsage": {

#         "completion_tokens": 8,

#         "prompt_tokens": 22,

#         "total_tokens": 30

#       },

#       "finish_reason": "stop"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 22,

#       "output_tokens": 8,

#       "total_tokens": 30

#     }

#   }


"""
## Streaming the Model output
"""

import { HumanMessage, SystemMessage } from "@langchain/core/messages";

const messages = [
    new SystemMessage('You are a helpful assistant which telling short-info about provided topic.'),
    new HumanMessage("moon")
]
const stream = await instance.stream(messages);
for await(const chunk of stream){
    console.log(chunk)
}
# Output:
#    The

#    Moon

#    is

#    Earth

#   '

#   s

#    only

#    natural

#    satellite

#    and


"""
## Tool calling
"""

import { tool } from "@langchain/core/tools";
import { z } from "zod";

const calculatorSchema = z.object({
    operation: z
      .enum(["add", "subtract", "multiply", "divide"])
      .describe("The type of operation to execute."),
    number1: z.number().describe("The first number to operate on."),
    number2: z.number().describe("The second number to operate on."),
  });
  
const calculatorTool = tool(
async ({ operation, number1, number2 }) => {
    if (operation === "add") {
    return `${number1 + number2}`;
    } else if (operation === "subtract") {
    return `${number1 - number2}`;
    } else if (operation === "multiply") {
    return `${number1 * number2}`;
    } else if (operation === "divide") {
    return `${number1 / number2}`;
    } else {
    throw new Error("Invalid operation.");
    }
},
{
    name: "calculator",
    description: "Can perform mathematical operations.",
    schema: calculatorSchema,
}
);

const instanceWithTools = instance.bindTools([calculatorTool]);

const res = await instanceWithTools.invoke("What is 3 * 12");
console.log(res)
# Output:
#   AIMessage {

#     "id": "chat-d2214d0bdb794483a213b3211cf0d819",

#     "content": "",

#     "additional_kwargs": {

#       "tool_calls": [

#         {

#           "id": "chatcmpl-tool-257f3d39532141b89178c2120f81f0cb",

#           "type": "function",

#           "function": "[Object]"

#         }

#       ]

#     },

#     "response_metadata": {

#       "tokenUsage": {

#         "completion_tokens": 38,

#         "prompt_tokens": 177,

#         "total_tokens": 215

#       },

#       "finish_reason": "tool_calls"

#     },

#     "tool_calls": [

#       {

#         "name": "calculator",

#         "args": {

#           "number1": 3,

#           "number2": 12,

#           "operation": "multiply"

#         },

#         "type": "tool_call",

#         "id": "chatcmpl-tool-257f3d39532141b89178c2120f81f0cb"

#       }

#     ],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 177,

#       "output_tokens": 38,

#       "total_tokens": 215

#     }

#   }


"""
## API reference

For detailed documentation of all `IBM watsonx.ai` features and configurations head to the API reference: [API docs](https://api.js.langchain.com/modules/_langchain_community.embeddings_ibm.html)
"""



================================================
FILE: docs/core_docs/docs/integrations/chat/index.mdx
================================================
---
sidebar_position: 1
sidebar_class_name: hidden
hide_table_of_contents: true
---

# Chat models

[Chat models](/docs/concepts/chat_models) are language models that use a sequence of [messages](/docs/concepts/messages) as inputs and return messages as outputs (as opposed to using plain text). These are generally newer models.

:::info
If you'd like to write your own chat model, see [this how-to](/docs/how_to/custom_chat). If you'd like to contribute an integration, see [Contributing integrations](/docs/contributing).
:::

import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs openaiParams={`{ model: "gpt-4o-mini" }`} />

```python
await model.invoke("Hello, world!")
```

## Featured providers

| Model                                                                    | Stream | JSON mode | [Tool Calling](/docs/how_to/tool_calling/) | [`withStructuredOutput()`](/docs/how_to/structured_output/#the-.withstructuredoutput-method) | [Multimodal](/docs/how_to/multimodal_inputs/) |
| :----------------------------------------------------------------------- | :----: | :-------: | :----------------------------------------: | :------------------------------------------------------------------------------------------: | :-------------------------------------------: |
| [BedrockChat](/docs/integrations/chat/bedrock/)                          |   ✅   |    ❌     |        🟡 (Bedrock Anthropic only)         |                                 🟡 (Bedrock Anthropic only)                                  |          🟡 (Bedrock Anthropic only)          |
| [ChatBedrockConverse](/docs/integrations/chat/bedrock_converse/)         |   ✅   |    ❌     |                     ✅                     |                                              ✅                                              |                      ✅                       |
| [ChatAnthropic](/docs/integrations/chat/anthropic/)                      |   ✅   |    ❌     |                     ✅                     |                                              ✅                                              |                      ✅                       |
| [ChatCloudflareWorkersAI](/docs/integrations/chat/cloudflare_workersai/) |   ✅   |    ❌     |                     ❌                     |                                              ❌                                              |                      ❌                       |
| [ChatCohere](/docs/integrations/chat/cohere/)                            |   ✅   |    ❌     |                     ✅                     |                                              ✅                                              |                      ✅                       |
| [ChatFireworks](/docs/integrations/chat/fireworks/)                      |   ✅   |    ✅     |                     ✅                     |                                              ✅                                              |                      ✅                       |
| [ChatGoogleGenerativeAI](/docs/integrations/chat/google_generativeai/)   |   ✅   |    ❌     |                     ✅                     |                                              ✅                                              |                      ✅                       |
| [ChatVertexAI](/docs/integrations/chat/google_vertex_ai/)                |   ✅   |    ❌     |                     ✅                     |                                              ✅                                              |                      ✅                       |
| [ChatGroq](/docs/integrations/chat/groq/)                                |   ✅   |    ✅     |                     ✅                     |                                              ✅                                              |                      ✅                       |
| [ChatMistralAI](/docs/integrations/chat/mistral/)                        |   ✅   |    ✅     |                     ✅                     |                                              ✅                                              |                      ✅                       |
| [ChatOllama](/docs/integrations/chat/ollama/)                            |   ✅   |    ✅     |                     ✅                     |                                              ✅                                              |                      ✅                       |
| [ChatOpenAI](/docs/integrations/chat/openai/)                            |   ✅   |    ✅     |                     ✅                     |                                              ✅                                              |                      ✅                       |
| [ChatTogetherAI](/docs/integrations/chat/togetherai/)                    |   ✅   |    ✅     |                     ✅                     |                                              ✅                                              |                      ✅                       |
| [ChatXAI](/docs/integrations/chat/xai/)                                  |   ✅   |    ✅     |                     ✅                     |                                              ✅                                              |                      ❌                       |

## All chat models

import { IndexTable } from "@theme/FeatureTables";

<IndexTable />



================================================
FILE: docs/core_docs/docs/integrations/chat/llama_cpp.mdx
================================================
---
sidebar_class_name: node-only
---

# Llama CPP

:::tip Compatibility
Only available on Node.js.
:::

This module is based on the [node-llama-cpp](https://github.com/withcatai/node-llama-cpp) Node.js bindings for [llama.cpp](https://github.com/ggerganov/llama.cpp), allowing you to work with a locally running LLM. This allows you to work with a much smaller quantized model capable of running on a laptop environment, ideal for testing and scratch padding ideas without running up a bill!

## Setup

You'll need to install major version `3` of the [node-llama-cpp](https://github.com/withcatai/node-llama-cpp) module to communicate with your local model.

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install -S node-llama-cpp@3 @langchain/community @langchain/core
```

You will also need a local Llama 3 model (or a model supported by [node-llama-cpp](https://github.com/withcatai/node-llama-cpp)). You will need to pass the path to this model to the LlamaCpp module as a part of the parameters (see example).

Out-of-the-box `node-llama-cpp` is tuned for running on a MacOS platform with support for the Metal GPU of Apple M-series of processors. If you need to turn this off or need support for the CUDA architecture then refer to the documentation at [node-llama-cpp](https://withcatai.github.io/node-llama-cpp/).

For advice on getting and preparing `llama3` see the documentation for the LLM version of this module.

A note to LangChain.js contributors: if you want to run the tests associated with this module you will need to put the path to your local model in the environment variable `LLAMA_PATH`.

## Usage

### Basic use

In this case we pass in a prompt wrapped as a message and expect a response.

import CodeBlock from "@theme/CodeBlock";
import BasicExample from "@examples/models/chat/integration_llama_cpp.ts";

<CodeBlock language="typescript">{BasicExample}</CodeBlock>

### System messages

We can also provide a system message, note that with the `llama_cpp` module a system message will cause the creation of a new session.

import SystemExample from "@examples/models/chat/integration_llama_cpp_system.ts";

<CodeBlock language="typescript">{SystemExample}</CodeBlock>

### Chains

This module can also be used with chains, note that using more complex chains will require suitably powerful version of `llama3` such as the 70B version.

import ChainExample from "@examples/models/chat/integration_llama_cpp_chain.ts";

<CodeBlock language="typescript">{ChainExample}</CodeBlock>

### Streaming

We can also stream with Llama CPP, this can be using a raw 'single prompt' string:

import StreamExample from "@examples/models/chat/integration_llama_cpp_stream.ts";

<CodeBlock language="typescript">{StreamExample}</CodeBlock>

Or you can provide multiple messages, note that this takes the input and then submits a Llama3 formatted prompt to the model.

import StreamMultiExample from "@examples/models/chat/integration_llama_cpp_stream_multi.ts";

<CodeBlock language="typescript">{StreamMultiExample}</CodeBlock>

Using the `invoke` method, we can also achieve stream generation, and use `signal` to abort the generation.

import StreamInvokeExample from "@examples/models/chat/integration_llama_cpp_stream_invoke.ts";

<CodeBlock language="typescript">{StreamInvokeExample}</CodeBlock>

## Related

- Chat model [conceptual guide](/docs/concepts/chat_models)
- Chat model [how-to guides](/docs/how_to/#chat-models)



================================================
FILE: docs/core_docs/docs/integrations/chat/minimax.mdx
================================================
---
sidebar_label: Minimax
---

import CodeBlock from "@theme/CodeBlock";

# Minimax

[Minimax](https://api.minimax.chat) is a Chinese startup that provides natural language processing models for companies and individuals.

This example demonstrates using LangChain.js to interact with Minimax.

## Setup

To use Minimax models, you'll need a [Minimax account](https://api.minimax.chat), an [API key](https://api.minimax.chat/user-center/basic-information/interface-key), and a [Group ID](https://api.minimax.chat/user-center/basic-information)

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core
```

import UnifiedModelParamsTooltip from "@mdx_components/unified_model_params_tooltip.mdx";

<UnifiedModelParamsTooltip></UnifiedModelParamsTooltip>

## Basic usage

import Minimax from "@examples/models/chat/integration_minimax.ts";

<CodeBlock language="typescript">{Minimax}</CodeBlock>

## Chain model calls

import MinimaxChain from "@examples/models/chat/minimax_chain.ts";

<CodeBlock language="typescript">{MinimaxChain}</CodeBlock>

## With function calls

import MinimaxFunctions from "@examples/models/chat/minimax_functions.ts";
import MinimaxFunctionsZod from "@examples/models/chat/minimax_functions_zod.ts";

<CodeBlock language="typescript">{MinimaxFunctions}</CodeBlock>

## Functions with Zod

<CodeBlock language="typescript">{MinimaxFunctionsZod}</CodeBlock>

## With glyph

This feature can help users force the model to return content in the requested format.

import MinimaxGlyph from "@examples/models/chat/minimax_glyph.ts";

<CodeBlock language="typescript">{MinimaxGlyph}</CodeBlock>

## With sample messages

This feature can help the model better understand the return information the user wants to get,
including but not limited to the content, format, and response mode of the information.

import MinimaxSampleMessages from "@examples/models/chat/minimax_sample_messages.ts";

<CodeBlock language="typescript">{MinimaxSampleMessages}</CodeBlock>

## With plugins

This feature supports calling tools like a search engine to get additional data that can assist the model.

import MinimaxPlugins from "@examples/models/chat/minimax_plugins.ts";

<CodeBlock language="typescript">{MinimaxPlugins}</CodeBlock>

## Related

- Chat model [conceptual guide](/docs/concepts/chat_models)
- Chat model [how-to guides](/docs/how_to/#chat-models)



================================================
FILE: docs/core_docs/docs/integrations/chat/mistral.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: MistralAI
---
"""

"""
# ChatMistralAI

[Mistral AI](https://mistral.ai/) is a platform that offers hosting for their powerful [open source models](https://docs.mistral.ai/getting-started/models/).

This will help you getting started with ChatMistralAI [chat models](/docs/concepts/chat_models). For detailed documentation of all ChatMistralAI features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_mistralai.ChatMistralAI.html).

## Overview
### Integration details

| Class | Package | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/mistralai) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [ChatMistralAI](https://api.js.langchain.com/classes/langchain_mistralai.ChatMistralAI.html) | [`@langchain/mistralai`](https://www.npmjs.com/package/@langchain/mistralai) | ❌ | ❌ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/mistralai?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/mistralai?style=flat-square&label=%20&) |

### Model features

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | [Image input](/docs/how_to/multimodal_inputs/) | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |
| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: |
| ✅ | ✅ | ✅ | ✅ | ❌ | ❌ | ✅ | ✅ | ❌ | 

## Setup

To access Mistral AI models you'll need to create a Mistral AI account, get an API key, and install the `@langchain/mistralai` integration package.

### Credentials

Head [here](https://console.mistral.ai/) to sign up to Mistral AI and generate an API key. Once you've done this set the `MISTRAL_API_KEY` environment variable:

```bash
export MISTRAL_API_KEY="your-api-key"
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain ChatMistralAI integration lives in the `@langchain/mistralai` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
@langchain/mistralai @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

import { ChatMistralAI } from "@langchain/mistralai" 

const llm = new ChatMistralAI({
    model: "mistral-large-latest",
    temperature: 0,
    maxRetries: 2,
    // other params...
})

"""
## Invocation

When sending chat messages to mistral, there are a few requirements to follow:

- The first message can _*not*_ be an assistant (ai) message.
- Messages _*must*_ alternate between user and assistant (ai) messages.
- Messages can _*not*_ end with an assistant (ai) or system message.
"""

const aiMsg = await llm.invoke([
    [
        "system",
        "You are a helpful assistant that translates English to French. Translate the user sentence.",
    ],
    ["human", "I love programming."],
])
aiMsg
# Output:
#   AIMessage {

#     "content": "J'adore la programmation.",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "completionTokens": 9,

#         "promptTokens": 27,

#         "totalTokens": 36

#       },

#       "finish_reason": "stop"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 27,

#       "output_tokens": 9,

#       "total_tokens": 36

#     }

#   }


console.log(aiMsg.content)
# Output:
#   J'adore la programmation.


"""
## Chaining

We can [chain](/docs/how_to/sequence/) our model with a prompt template like so:
"""

import { ChatPromptTemplate } from "@langchain/core/prompts"

const prompt = ChatPromptTemplate.fromMessages(
    [
        [
            "system",
            "You are a helpful assistant that translates {input_language} to {output_language}.",
        ],
        ["human", "{input}"],
    ]
)

const chain = prompt.pipe(llm);
await chain.invoke(
    {
        input_language: "English",
        output_language: "German",
        input: "I love programming.",
    }
)
# Output:
#   AIMessage {

#     "content": "Ich liebe Programmieren.",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "completionTokens": 7,

#         "promptTokens": 21,

#         "totalTokens": 28

#       },

#       "finish_reason": "stop"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 21,

#       "output_tokens": 7,

#       "total_tokens": 28

#     }

#   }


"""
## Tool calling

Mistral's API supports [tool calling](/docs/concepts/tool_calling) for a subset of their models. You can see which models support tool calling [on this page](https://docs.mistral.ai/capabilities/function_calling/).

The examples below demonstrates how to use it:
"""

import { ChatMistralAI } from "@langchain/mistralai";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { z } from "zod";
import { tool } from "@langchain/core/tools";

const calculatorSchema = z.object({
  operation: z
    .enum(["add", "subtract", "multiply", "divide"])
    .describe("The type of operation to execute."),
  number1: z.number().describe("The first number to operate on."),
  number2: z.number().describe("The second number to operate on."),
});

const calculatorTool = tool((input) => {
  return JSON.stringify(input);
}, {
  name: "calculator",
  description: "A simple calculator tool",
  schema: calculatorSchema,
});

// Bind the tool to the model
const modelWithTool = new ChatMistralAI({
  model: "mistral-large-latest",
}).bindTools([calculatorTool]);


const calcToolPrompt = ChatPromptTemplate.fromMessages([
  [
    "system",
    "You are a helpful assistant who always needs to use a calculator.",
  ],
  ["human", "{input}"],
]);

// Chain your prompt, model, and output parser together
const chainWithCalcTool = calcToolPrompt.pipe(modelWithTool);

const calcToolRes = await chainWithCalcTool.invoke({
  input: "What is 2 + 2?",
});
console.log(calcToolRes.tool_calls);
# Output:
#   [

#     {

#       name: 'calculator',

#       args: { operation: 'add', number1: 2, number2: 2 },

#       type: 'tool_call',

#       id: 'DD9diCL1W'

#     }

#   ]


"""
## Hooks

Mistral AI supports custom hooks for three events: beforeRequest, requestError, and reponse. Examples of the function signature for each hook type can be seen below:
"""

const beforeRequestHook = (req: Request): Request | void | Promise<Request | void> => {
    // Code to run before a request is processed by Mistral
};

const requestErrorHook = (err: unknown, req: Request): void | Promise<void> => {
    // Code to run when an error occurs as Mistral is processing a request
};

const responseHook = (res: Response, req: Request): void | Promise<void> => {
    // Code to run before Mistral sends a successful response
};

"""
To add these hooks to the chat model, either pass them as arguments and they are automatically added:
"""

import { ChatMistralAI } from "@langchain/mistralai" 

const modelWithHooks = new ChatMistralAI({
    model: "mistral-large-latest",
    temperature: 0,
    maxRetries: 2,
    beforeRequestHooks: [ beforeRequestHook ],
    requestErrorHooks: [ requestErrorHook ],
    responseHooks: [ responseHook ],
    // other params...
});

"""
Or assign and add them manually after instantiation:
"""

import { ChatMistralAI } from "@langchain/mistralai" 

const model = new ChatMistralAI({
    model: "mistral-large-latest",
    temperature: 0,
    maxRetries: 2,
    // other params...
});

model.beforeRequestHooks = [ ...model.beforeRequestHooks, beforeRequestHook ];
model.requestErrorHooks = [ ...model.requestErrorHooks, requestErrorHook ];
model.responseHooks = [ ...model.responseHooks, responseHook ];

model.addAllHooksToHttpClient();

"""
The method addAllHooksToHttpClient clears all currently added hooks before assigning the entire updated hook lists to avoid hook duplication.

Hooks can be removed one at a time, or all hooks can be cleared from the model at once.
"""

model.removeHookFromHttpClient(beforeRequestHook);

model.removeAllHooksFromHttpClient();

"""
## API reference

For detailed documentation of all ChatMistralAI features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_mistralai.ChatMistralAI.html
"""



================================================
FILE: docs/core_docs/docs/integrations/chat/moonshot.mdx
================================================
---
sidebar_label: Moonshot
---

import CodeBlock from "@theme/CodeBlock";

# ChatMoonshot

LangChain.js supports the Moonshot AI family of models.

https://platform.moonshot.cn/docs/intro

## Setup

You'll need to sign up for an Moonshot API key and set it as an environment variable named `MOONSHOT_API_KEY`

https://platform.moonshot.cn/console

You'll also need to install the following dependencies:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core
```

## Usage

Here's an example:

import Moonshot from "@examples/models/chat/integration_moonshot.ts";

<CodeBlock language="typescript">{Moonshot}</CodeBlock>

## Related

- Chat model [conceptual guide](/docs/concepts/chat_models)
- Chat model [how-to guides](/docs/how_to/#chat-models)



================================================
FILE: docs/core_docs/docs/integrations/chat/ni_bittensor.mdx
================================================
---
sidebar_label: NIBittensorChatModel
sidebar_class_name: hidden
---

# NIBittensorChatModel

:::warning
This module has been deprecated and is no longer supported. The documentation below will not work in versions 0.2.0 or later.
:::

LangChain.js offers experimental support for Neural Internet's Bittensor chat models.

Here's an example:

```typescript
import { NIBittensorChatModel } from "langchain/experimental/chat_models/bittensor";
import { HumanMessage } from "@langchain/core/messages";

const chat = new NIBittensorChatModel();
const message = new HumanMessage("What is bittensor?");
const res = await chat.invoke([message]);
console.log({ res });
/*
  {
    res: "\nBittensor is opensource protocol..."
  }
 */
```

## Related

- Chat model [conceptual guide](/docs/concepts/chat_models)
- Chat model [how-to guides](/docs/how_to/#chat-models)



================================================
FILE: docs/core_docs/docs/integrations/chat/novita.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Novita AI
---
"""

"""
# ChatNovita

Delivers an affordable, reliable, and simple inference platform for running top LLM models.

You can find all the models we support here: [Novita AI Featured Models](https://novita.ai/models/llm?utm_source=github_langchain&utm_medium=github_readme&utm_campaign=link) or request the [Models API](https://novita.ai/docs/guides/llm-models?utm_source=github_langchain&utm_medium=github_readme&utm_campaign=link) to get all available models.

Try the [Novita AI DeepSeek R1 API Demo](https://novita.ai/models/llm/deepseek-deepseek-r1?utm_source=github_langchain&utm_medium=github_readme&utm_campaign=link) today!
"""

"""
## Overview

### Model features
| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | [Image input](/docs/how_to/multimodal_inputs/) | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | Native async | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |
| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: | :---: |
| ❌ | ✅ | ✅ | ❌ | ❌ | ❌ | ❌ | ❌ | ✅ | ❌ |
"""

"""
## Setup

To access Novita AI models you'll need to create a Novita account and get an API key.

### Credentials

Head to [this page](https://novita.ai/settings#key-management?utm_source=github_langchain&utm_medium=github_readme&utm_campaign=link) to sign up to Novita AI and generate an API key. Once you've done this set the NOVITA_API_KEY environment variable:

```bash
export NOVITA_API_KEY="your-api-key"
```
"""

"""
### Installation

The LangChain Novita integration lives in the `@langchain-community` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core
</Npm2Yarn>
```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions. Try the [Novita AI DeepSeek R1 API Demo](https://novita.ai/models/llm/deepseek-deepseek-r1?utm_source=github_langchain&utm_medium=github_readme&utm_campaign=link) today!
"""

import { ChatNovitaAI } from "@langchain/community/chat_models/novita";

const llm = new ChatNovitaAI({
  model: "deepseek/deepseek-r1",
  temperature: 0,
  // other params...
})

"""
## Invocation
"""

const aiMsg = await llm.invoke([
  {
    role: "system",
    content: "You are a helpful assistant that translates English to French. Translate the user sentence.",
  },
  {
    role: "human",
    content: "I love programming."
  },
]);

console.log(aiMsg.content)

"""
## Chaining

We can [chain](/docs/how_to/sequence) our model with a prompt template like so:
"""

import { ChatPromptTemplate } from "@langchain/core/prompts"

const prompt = ChatPromptTemplate.fromMessages(
  [
    [
      "system",
      "You are a helpful assistant that translates {input_language} to {output_language}.",
    ],
    ["human", "{input}"],
  ]
)

const chain = prompt.pipe(llm);
await chain.invoke(
  {
    input_language: "English",
    output_language: "German",
    input: "I love programming.",
  }
)

"""
## API reference

For detailed documentation of Novita AI LLM APIs, head to [Novita AI LLM API reference](https://novita.ai/docs/guides/llm-api?utm_source=github_langchain&utm_medium=github_readme&utm_campaign=link)

"""



================================================
FILE: docs/core_docs/docs/integrations/chat/ollama.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Ollama
---
"""

"""
# ChatOllama

[Ollama](https://ollama.ai/) allows you to run open-source large language models, such as Llama 3.1, locally.

Ollama bundles model weights, configuration, and data into a single package, defined by a Modelfile. It optimizes setup and configuration details, including GPU usage.

This guide will help you getting started with `ChatOllama` [chat models](/docs/concepts/chat_models). For detailed documentation of all `ChatOllama` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_ollama.ChatOllama.html).

## Overview
### Integration details

Ollama allows you to use a wide range of models with different capabilities. Some of the fields in the details table below only apply to a subset of models that Ollama offers.

For a complete list of supported models and model variants, see the [Ollama model library](https://ollama.com/search) and search by tag.

| Class | Package | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/ollama) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [ChatOllama](https://api.js.langchain.com/classes/langchain_ollama.ChatOllama.html) | [`@langchain/ollama`](https://www.npmjs.com/package/@langchain/ollama) | ✅ | beta | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/ollama?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/ollama?style=flat-square&label=%20&) |

### Model features

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | [Image input](/docs/how_to/multimodal_inputs/) | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |
| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: |
| ✅ | ✅ | ✅ | ✅ | ❌ | ❌ | ✅ | ✅ | ❌ | 

## Setup

Follow [these instructions](https://github.com/ollama/ollama) to set up and run a local Ollama instance. Then, download the `@langchain/ollama` package.

### Credentials

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain ChatOllama integration lives in the `@langchain/ollama` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/ollama @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

import { ChatOllama } from "@langchain/ollama"

const llm = new ChatOllama({
    model: "llama3",
    temperature: 0,
    maxRetries: 2,
    // other params...
})

"""
## Invocation
"""

const aiMsg = await llm.invoke([
    [
        "system",
        "You are a helpful assistant that translates English to French. Translate the user sentence.",
    ],
    ["human", "I love programming."],
])
aiMsg
# Output:
#   AIMessage {

#     "content": "Je adore le programmation.\n\n(Note: \"programmation\" is the feminine form of the noun in French, but if you want to use the masculine form, it would be \"le programme\" instead.)",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "model": "llama3",

#       "created_at": "2024-08-01T16:59:17.359302Z",

#       "done_reason": "stop",

#       "done": true,

#       "total_duration": 6399311167,

#       "load_duration": 5575776417,

#       "prompt_eval_count": 35,

#       "prompt_eval_duration": 110053000,

#       "eval_count": 43,

#       "eval_duration": 711744000

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 35,

#       "output_tokens": 43,

#       "total_tokens": 78

#     }

#   }


console.log(aiMsg.content)
# Output:
#   Je adore le programmation.

#   

#   (Note: "programmation" is the feminine form of the noun in French, but if you want to use the masculine form, it would be "le programme" instead.)


"""
## Chaining

We can [chain](/docs/how_to/sequence/) our model with a prompt template like so:
"""

import { ChatPromptTemplate } from "@langchain/core/prompts"

const prompt = ChatPromptTemplate.fromMessages(
    [
        [
            "system",
            "You are a helpful assistant that translates {input_language} to {output_language}.",
        ],
        ["human", "{input}"],
    ]
)

const chain = prompt.pipe(llm);
await chain.invoke(
    {
        input_language: "English",
        output_language: "German",
        input: "I love programming.",
    }
)
# Output:
#   AIMessage {

#     "content": "Ich liebe Programmieren!\n\n(Note: \"Ich liebe\" means \"I love\", \"Programmieren\" is the verb for \"programming\")",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "model": "llama3",

#       "created_at": "2024-08-01T16:59:18.088423Z",

#       "done_reason": "stop",

#       "done": true,

#       "total_duration": 585146125,

#       "load_duration": 27557166,

#       "prompt_eval_count": 30,

#       "prompt_eval_duration": 74241000,

#       "eval_count": 29,

#       "eval_duration": 481195000

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 30,

#       "output_tokens": 29,

#       "total_tokens": 59

#     }

#   }


"""
## Tools

Ollama now offers support for native tool calling [for a subset of their available models](https://ollama.com/search?c=tools). The example below demonstrates how you can invoke a tool from an Ollama model.
"""

import { tool } from "@langchain/core/tools";
import { ChatOllama } from "@langchain/ollama";
import { z } from "zod";

const weatherTool = tool((_) => "Da weather is weatherin", {
  name: "get_current_weather",
  description: "Get the current weather in a given location",
  schema: z.object({
    location: z.string().describe("The city and state, e.g. San Francisco, CA"),
  }),
});

// Define the model
const llmForTool = new ChatOllama({
  model: "llama3-groq-tool-use",
});

// Bind the tool to the model
const llmWithTools = llmForTool.bindTools([weatherTool]);

const resultFromTool = await llmWithTools.invoke(
  "What's the weather like today in San Francisco? Ensure you use the 'get_current_weather' tool."
);

console.log(resultFromTool);
# Output:
#   AIMessage {

#     "content": "",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "model": "llama3-groq-tool-use",

#       "created_at": "2024-08-01T18:43:13.2181Z",

#       "done_reason": "stop",

#       "done": true,

#       "total_duration": 2311023875,

#       "load_duration": 1560670292,

#       "prompt_eval_count": 177,

#       "prompt_eval_duration": 263603000,

#       "eval_count": 30,

#       "eval_duration": 485582000

#     },

#     "tool_calls": [

#       {

#         "name": "get_current_weather",

#         "args": {

#           "location": "San Francisco, CA"

#         },

#         "id": "c7a9d590-99ad-42af-9996-41b90efcf827",

#         "type": "tool_call"

#       }

#     ],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 177,

#       "output_tokens": 30,

#       "total_tokens": 207

#     }

#   }


"""
### `.withStructuredOutput`

For [models that support tool calling](https://ollama.com/search?c=tools), you can also call `.withStructuredOutput()` to get a structured output from the tool.
"""

import { ChatOllama } from "@langchain/ollama";
import { z } from "zod";

// Define the model
const llmForWSO = new ChatOllama({
  model: "llama3-groq-tool-use",
});

// Define the tool schema you'd like the model to use.
const schemaForWSO = z.object({
  location: z.string().describe("The city and state, e.g. San Francisco, CA"),
});

// Pass the schema to the withStructuredOutput method to bind it to the model.
const llmWithStructuredOutput = llmForWSO.withStructuredOutput(schemaForWSO, {
  name: "get_current_weather",
});

const resultFromWSO = await llmWithStructuredOutput.invoke(
  "What's the weather like today in San Francisco? Ensure you use the 'get_current_weather' tool."
);
console.log(resultFromWSO);
# Output:
#   { location: 'San Francisco, CA' }


"""
### JSON mode

Ollama also supports a JSON mode for all chat models that coerces model outputs to only return JSON. Here's an example of how this can be useful for extraction:
"""

import { ChatOllama } from "@langchain/ollama";
import { ChatPromptTemplate } from "@langchain/core/prompts";

const promptForJsonMode = ChatPromptTemplate.fromMessages([
  [
    "system",
    `You are an expert translator. Format all responses as JSON objects with two keys: "original" and "translated".`,
  ],
  ["human", `Translate "{input}" into {language}.`],
]);

const llmJsonMode = new ChatOllama({
  baseUrl: "http://localhost:11434", // Default value
  model: "llama3",
  format: "json",
});

const chainForJsonMode = promptForJsonMode.pipe(llmJsonMode);

const resultFromJsonMode = await chainForJsonMode.invoke({
  input: "I love programming",
  language: "German",
});

console.log(resultFromJsonMode);
# Output:
#   AIMessage {

#     "content": "{\n\"original\": \"I love programming\",\n\"translated\": \"Ich liebe Programmierung\"\n}",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "model": "llama3",

#       "created_at": "2024-08-01T17:24:54.35568Z",

#       "done_reason": "stop",

#       "done": true,

#       "total_duration": 1754811583,

#       "load_duration": 1297200208,

#       "prompt_eval_count": 47,

#       "prompt_eval_duration": 128532000,

#       "eval_count": 20,

#       "eval_duration": 318519000

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 47,

#       "output_tokens": 20,

#       "total_tokens": 67

#     }

#   }


"""
## Multimodal models

Ollama supports open source multimodal models like [LLaVA](https://ollama.ai/library/llava) in versions 0.1.15 and up.
You can pass images as part of a message's `content` field to [multimodal-capable](/docs/how_to/multimodal_inputs/) models like this:
"""

import { ChatOllama } from "@langchain/ollama";
import { HumanMessage } from "@langchain/core/messages";
import * as fs from "node:fs/promises";

const imageData = await fs.readFile("../../../../../examples/hotdog.jpg");
const llmForMultiModal = new ChatOllama({
  model: "llava",
  baseUrl: "http://127.0.0.1:11434",
});
const multiModalRes = await llmForMultiModal.invoke([
  new HumanMessage({
    content: [
      {
        type: "text",
        text: "What is in this image?",
      },
      {
        type: "image_url",
        image_url: `data:image/jpeg;base64,${imageData.toString("base64")}`,
      },
    ],
  }),
]);
console.log(multiModalRes);
# Output:
#   AIMessage {

#     "content": " The image shows a hot dog in a bun, which appears to be a footlong. It has been cooked or grilled to the point where it's browned and possibly has some blackened edges, indicating it might be slightly overcooked. Accompanying the hot dog is a bun that looks toasted as well. There are visible char marks on both the hot dog and the bun, suggesting they have been cooked directly over a source of heat, such as a grill or broiler. The background is white, which puts the focus entirely on the hot dog and its bun. ",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "model": "llava",

#       "created_at": "2024-08-01T17:25:02.169957Z",

#       "done_reason": "stop",

#       "done": true,

#       "total_duration": 5700249458,

#       "load_duration": 2543040666,

#       "prompt_eval_count": 1,

#       "prompt_eval_duration": 1032591000,

#       "eval_count": 127,

#       "eval_duration": 2114201000

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 1,

#       "output_tokens": 127,

#       "total_tokens": 128

#     }

#   }


"""
## API reference

For detailed documentation of all ChatOllama features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_ollama.ChatOllama.html
"""



================================================
FILE: docs/core_docs/docs/integrations/chat/ollama_functions.mdx
================================================
---
sidebar_label: Ollama Functions
sidebar_class_name: hidden
---

# Ollama Functions

:::warning

The LangChain Ollama integration package has official support for tool calling. [Click here to view the documentation](/docs/integrations/chat/ollama#tools).

:::

LangChain offers an experimental wrapper around open source models run locally via [Ollama](https://github.com/jmorganca/ollama)
that gives it the same API as OpenAI Functions.

Note that more powerful and capable models will perform better with complex schema and/or multiple functions. The examples below
use [Mistral](https://ollama.ai/library/mistral).

:::warning

This is an experimental wrapper that attempts to bolt-on tool calling support to models that do not natively support it. Use with caution.

:::

## Setup

Follow [these instructions](https://github.com/jmorganca/ollama) to set up and run a local Ollama instance.

## Initialize model

You can initialize this wrapper the same way you'd initialize a standard `ChatOllama` instance:

```typescript
import { OllamaFunctions } from "@langchain/community/experimental/chat_models/ollama_functions";

const model = new OllamaFunctions({
  temperature: 0.1,
  model: "mistral",
});
```

## Passing in functions

You can now pass in functions the same way as OpenAI:

import CodeBlock from "@theme/CodeBlock";
import OllamaFunctionsCalling from "@examples/models/chat/ollama_functions/function_calling.ts";

<CodeBlock language="typescript">{OllamaFunctionsCalling}</CodeBlock>

## Using for extraction

import OllamaFunctionsExtraction from "@examples/models/chat/ollama_functions/extraction.ts";

<CodeBlock language="typescript">{OllamaFunctionsExtraction}</CodeBlock>

:::tip
You can see a simple LangSmith trace of this [here](https://smith.langchain.com/public/74692bfc-0224-4221-b187-ddbf20d7ecc0/r)
:::

## Customization

Behind the scenes, this uses Ollama's JSON mode to constrain output to JSON, then passes tools schemas as JSON schema into the prompt.

Because different models have different strengths, it may be helpful to pass in your own system prompt. Here's an example:

import OllamaFunctionsCustomPrompt from "@examples/models/chat/ollama_functions/custom_prompt.ts";

<CodeBlock language="typescript">{OllamaFunctionsCustomPrompt}</CodeBlock>

## Related

- Chat model [conceptual guide](/docs/concepts/chat_models)
- Chat model [how-to guides](/docs/how_to/#chat-models)



================================================
FILE: docs/core_docs/docs/integrations/chat/openai.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: OpenAI
---
"""

"""
# ChatOpenAI

[OpenAI](https://en.wikipedia.org/wiki/OpenAI) is an artificial intelligence (AI) research laboratory.

This guide will help you getting started with ChatOpenAI [chat models](/docs/concepts/chat_models). For detailed documentation of all ChatOpenAI features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_openai.ChatOpenAI.html).

## Overview
### Integration details

| Class | Package | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/openai) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [ChatOpenAI](https://api.js.langchain.com/classes/langchain_openai.ChatOpenAI.html) | [`@langchain/openai`](https://www.npmjs.com/package/@langchain/openai) | ❌ | ✅ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/openai?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/openai?style=flat-square&label=%20&) |

### Model features

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | [Image input](/docs/how_to/multimodal_inputs/) | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |
| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: |
| ✅ | ✅ | ✅ | ✅ | ❌ | ❌ | ✅ | ✅ | ✅ | 

## Setup

To access OpenAI chat models you'll need to create an OpenAI account, get an API key, and install the `@langchain/openai` integration package.

### Credentials

Head to [OpenAI's website](https://platform.openai.com/) to sign up for OpenAI and generate an API key. Once you've done this set the `OPENAI_API_KEY` environment variable:

```bash
export OPENAI_API_KEY="your-api-key"
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain `ChatOpenAI` integration lives in the `@langchain/openai` package:

```{=mdx}

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/openai @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

import { ChatOpenAI } from "@langchain/openai" 

const llm = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0,
  // other params...
})

"""
## Invocation
"""

const aiMsg = await llm.invoke([
  {
    role: "system",
    content: "You are a helpful assistant that translates English to French. Translate the user sentence.",
  },
  {
    role: "user",
    content: "I love programming."
  },
])
aiMsg
# Output:
#   AIMessage {

#     "id": "chatcmpl-ADItECqSPuuEuBHHPjeCkh9wIO1H5",

#     "content": "J'adore la programmation.",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "completionTokens": 5,

#         "promptTokens": 31,

#         "totalTokens": 36

#       },

#       "finish_reason": "stop",

#       "system_fingerprint": "fp_5796ac6771"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 31,

#       "output_tokens": 5,

#       "total_tokens": 36

#     }

#   }


console.log(aiMsg.content)
# Output:
#   J'adore la programmation.


"""
## Chaining

We can [chain](/docs/how_to/sequence/) our model with a prompt template like so:
"""

import { ChatPromptTemplate } from "@langchain/core/prompts"

const prompt = ChatPromptTemplate.fromMessages(
  [
    [
      "system",
      "You are a helpful assistant that translates {input_language} to {output_language}.",
    ],
    ["human", "{input}"],
  ]
)

const chain = prompt.pipe(llm);
await chain.invoke(
  {
    input_language: "English",
    output_language: "German",
    input: "I love programming.",
  }
)
# Output:
#   AIMessage {

#     "id": "chatcmpl-ADItFaWFNqkSjSmlxeGk6HxcBHzVN",

#     "content": "Ich liebe Programmieren.",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "completionTokens": 5,

#         "promptTokens": 26,

#         "totalTokens": 31

#       },

#       "finish_reason": "stop",

#       "system_fingerprint": "fp_5796ac6771"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 26,

#       "output_tokens": 5,

#       "total_tokens": 31

#     }

#   }


"""
## Custom URLs

You can customize the base URL the SDK sends requests to by passing a `configuration` parameter like this:
"""

import { ChatOpenAI } from "@langchain/openai";

const llmWithCustomURL = new ChatOpenAI({
  temperature: 0.9,
  configuration: {
    baseURL: "https://your_custom_url.com",
  },
});

await llmWithCustomURL.invoke("Hi there!");

"""
The `configuration` field also accepts other `ClientOptions` parameters accepted by the official SDK.

If you are hosting on Azure OpenAI, see the [dedicated page instead](/docs/integrations/chat/azure).

## Custom headers

You can specify custom headers in the same `configuration` field:
"""

import { ChatOpenAI } from "@langchain/openai";

const llmWithCustomHeaders = new ChatOpenAI({
  temperature: 0.9,
  configuration: {
    defaultHeaders: {
      "Authorization": `Bearer SOME_CUSTOM_VALUE`,
    },
  },
});

await llmWithCustomHeaders.invoke("Hi there!");

"""
## Disabling streaming usage metadata

Some proxies or third-party providers present largely the same API interface as OpenAI, but don't support the more recently added `stream_options` parameter to return streaming usage. You can use `ChatOpenAI` to access these providers by disabling streaming usage like this:
"""

import { ChatOpenAI } from "@langchain/openai";

const llmWithoutStreamUsage = new ChatOpenAI({
  temperature: 0.9,
  streamUsage: false,
  configuration: {
    baseURL: "https://proxy.com",
  },
});

await llmWithoutStreamUsage.invoke("Hi there!");

"""
## Calling fine-tuned models

You can call fine-tuned OpenAI models by passing in your corresponding `modelName` parameter.

This generally takes the form of `ft:{OPENAI_MODEL_NAME}:{ORG_NAME}::{MODEL_ID}`. For example:
"""

import { ChatOpenAI } from "@langchain/openai";

const fineTunedLlm = new ChatOpenAI({
  temperature: 0.9,
  model: "ft:gpt-3.5-turbo-0613:{ORG_NAME}::{MODEL_ID}",
});

await fineTunedLlm.invoke("Hi there!");

"""
## Generation metadata

If you need additional information like logprobs or token usage, these will be returned directly in the `.invoke` response within the `response_metadata` field on the message.

```{=mdx}

:::tip
Requires `@langchain/core` version >=0.1.48.
:::

```
"""

import { ChatOpenAI } from "@langchain/openai";

// See https://cookbook.openai.com/examples/using_logprobs for details
const llmWithLogprobs = new ChatOpenAI({
  logprobs: true,
  // topLogprobs: 5,
});

const responseMessageWithLogprobs = await llmWithLogprobs.invoke("Hi there!");
console.dir(responseMessageWithLogprobs.response_metadata.logprobs, { depth: null });
# Output:
#   {

#     content: [

#       {

#         token: 'Hello',

#         logprob: -0.0004740447,

#         bytes: [ 72, 101, 108, 108, 111 ],

#         top_logprobs: []

#       },

#       {

#         token: '!',

#         logprob: -0.00004334534,

#         bytes: [ 33 ],

#         top_logprobs: []

#       },

#       {

#         token: ' How',

#         logprob: -0.000030113732,

#         bytes: [ 32, 72, 111, 119 ],

#         top_logprobs: []

#       },

#       {

#         token: ' can',

#         logprob: -0.0004797665,

#         bytes: [ 32, 99, 97, 110 ],

#         top_logprobs: []

#       },

#       {

#         token: ' I',

#         logprob: -7.89631e-7,

#         bytes: [ 32, 73 ],

#         top_logprobs: []

#       },

#       {

#         token: ' assist',

#         logprob: -0.114006,

#         bytes: [

#            32,  97, 115,

#           115, 105, 115,

#           116

#         ],

#         top_logprobs: []

#       },

#       {

#         token: ' you',

#         logprob: -4.3202e-7,

#         bytes: [ 32, 121, 111, 117 ],

#         top_logprobs: []

#       },

#       {

#         token: ' today',

#         logprob: -0.00004501419,

#         bytes: [ 32, 116, 111, 100, 97, 121 ],

#         top_logprobs: []

#       },

#       {

#         token: '?',

#         logprob: -0.000010206721,

#         bytes: [ 63 ],

#         top_logprobs: []

#       }

#     ],

#     refusal: null

#   }


"""
## Tool calling

Tool calling with OpenAI models works in a similar to [other models](/docs/how_to/tool_calling). Additionally, the following guides have some information especially relevant to OpenAI:

- [How to: disable parallel tool calling](/docs/how_to/tool_calling_parallel/)
- [How to: force a tool call](/docs/how_to/tool_choice/)
- [How to: bind model-specific tool formats to a model](/docs/how_to/tool_calling#binding-model-specific-formats-advanced).
"""

"""
## ``strict: true``

As of Aug 6, 2024, OpenAI supports a `strict` argument when calling tools that will enforce that the tool argument schema is respected by the model. See more here: https://platform.openai.com/docs/guides/function-calling.

```{=mdx}

:::info Requires ``@langchain/openai >= 0.2.6``

**Note**: If ``strict: true`` the tool definition will also be validated, and a subset of JSON schema are accepted. Crucially, schema cannot have optional args (those with default values). Read the full docs on what types of schema are supported here: https://platform.openai.com/docs/guides/structured-outputs/supported-schemas. 
:::


```

Here's an example with tool calling. Passing an extra `strict: true` argument to `.bindTools` will pass the param through to all tool definitions:
"""

import { ChatOpenAI } from "@langchain/openai";
import { tool } from "@langchain/core/tools";
import { z } from "zod";

const weatherTool = tool((_) => "no-op", {
  name: "get_current_weather",
  description: "Get the current weather",
  schema: z.object({
    location: z.string(),
  }),
})

const llmWithStrictTrue = new ChatOpenAI({
  model: "gpt-4o",
}).bindTools([weatherTool], {
  strict: true,
  tool_choice: weatherTool.name,
});

// Although the question is not about the weather, it will call the tool with the correct arguments
// because we passed `tool_choice` and `strict: true`.
const strictTrueResult = await llmWithStrictTrue.invoke("What is 127862 times 12898 divided by 2?");

console.dir(strictTrueResult.tool_calls, { depth: null });
# Output:
#   [

#     {

#       name: 'get_current_weather',

#       args: { location: 'current' },

#       type: 'tool_call',

#       id: 'call_hVFyYNRwc6CoTgr9AQFQVjm9'

#     }

#   ]


"""
If you only want to apply this parameter to a select number of tools, you can also pass OpenAI formatted tool schemas directly:
"""

import { zodToJsonSchema } from "zod-to-json-schema";

const toolSchema = {
  type: "function",
  function: {
    name: "get_current_weather",
    description: "Get the current weather",
    strict: true,
    parameters: zodToJsonSchema(
      z.object({
        location: z.string(),
      })
    ),
  },
};

const llmWithStrictTrueTools = new ChatOpenAI({
  model: "gpt-4o",
}).bindTools([toolSchema], {
  strict: true,
});

const weatherToolResult = await llmWithStrictTrueTools.invoke([{
  role: "user",
  content: "What is the current weather in London?"
}])

weatherToolResult.tool_calls;
# Output:
#   [

#     {

#       name: 'get_current_weather',

#       args: { location: 'London' },

#       type: 'tool_call',

#       id: 'call_EOSejtax8aYtqpchY8n8O82l'

#     }

#   ]


"""
## Structured output

We can also pass `strict: true` to the [`.withStructuredOutput()`](https://js.langchain.com/docs/how_to/structured_output/#the-.withstructuredoutput-method). Here's an example:
"""

import { ChatOpenAI } from "@langchain/openai";

const traitSchema = z.object({
  traits: z.array(z.string()).describe("A list of traits contained in the input"),
});

const structuredLlm = new ChatOpenAI({
  model: "gpt-4o-mini",
}).withStructuredOutput(traitSchema, {
  name: "extract_traits",
  strict: true,
});

await structuredLlm.invoke([{
  role: "user",
  content: `I am 6'5" tall and love fruit.`
}]);
# Output:
#   { traits: [ `6'5" tall`, 'love fruit' ] }


"""
## Responses API

:::caution Compatibility

The below points apply to `@langchain/openai>=0.4.5-rc.0`. Please see here for a [guide on upgrading](/docs/how_to/installation/#installing-integration-packages).

:::

OpenAI supports a [Responses](https://platform.openai.com/docs/guides/responses-vs-chat-completions) API that is oriented toward building [agentic](/docs/concepts/agents/) applications. It includes a suite of [built-in tools](https://platform.openai.com/docs/guides/tools?api-mode=responses), including web and file search. It also supports management of [conversation state](https://platform.openai.com/docs/guides/conversation-state?api-mode=responses), allowing you to continue a conversational thread without explicitly passing in previous messages.

`ChatOpenAI` will route to the Responses API if one of these features is used. You can also specify `useResponsesAPI: true` when instantiating `ChatOpenAI`.

### Built-in tools

Equipping `ChatOpenAI` with built-in tools will ground its responses with outside information, such as via context in files or the web. The [AIMessage](/docs/concepts/messages/#aimessage) generated from the model will include information about the built-in tool invocation.

#### Web search

To trigger a web search, pass `{"type": "web_search_preview"}` to the model as you would another tool.

:::tip

You can also pass built-in tools as invocation params:

```ts
llm.invoke("...", { tools: [{ type: "web_search_preview" }] });
```

:::

"""

import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({ model: "gpt-4o-mini" }).bindTools([
  { type: "web_search_preview" },
]);

await llm.invoke("What was a positive news story from today?");


"""
Note that the response includes structured [content blocks](/docs/concepts/messages/#content-1) that include both the text of the response and OpenAI [annotations](https://platform.openai.com/docs/guides/tools-web-search?api-mode=responses#output-and-citations) citing its sources. The output message will also contain information from any tool invocations.

#### File search

To trigger a file search, pass a [file search tool](https://platform.openai.com/docs/guides/tools-file-search) to the model as you would another tool. You will need to populate an OpenAI-managed vector store and include the vector store ID in the tool definition. See [OpenAI documentation](https://platform.openai.com/docs/guides/tools-file-search) for more details.

"""

import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({ model: "gpt-4o-mini" }).bindTools([
  { type: "file_search", vector_store_ids: ["vs..."] },
]);

await llm.invoke("Is deep research by OpenAI?");


"""
As with [web search](#web-search), the response will include content blocks with citations. It will also include information from the built-in tool invocations.

#### Computer Use

ChatOpenAI supports the `computer-use-preview` model, which is a specialized model for the built-in computer use tool. To enable, pass a [computer use tool](https://platform.openai.com/docs/guides/tools-computer-use) as you would pass another tool. 

Currently tool outputs for computer use are present in `AIMessage.additional_kwargs.tool_outputs`. To reply to the computer use tool call, you need to set `additional_kwargs.type: "computer_call_output"` while creating a corresponding `ToolMessage`.

See [OpenAI documentation](https://platform.openai.com/docs/guides/tools-computer-use) for more details.
"""

import { AIMessage, ToolMessage } from "@langchain/core/messages";
import { ChatOpenAI } from "@langchain/openai";
import * as fs from "node:fs/promises";

const findComputerCall = (message: AIMessage) => {
  const toolOutputs = message.additional_kwargs.tool_outputs as
    | { type: "computer_call"; call_id: string; action: { type: string } }[]
    | undefined;

  return toolOutputs?.find((toolOutput) => toolOutput.type === "computer_call");
};

const llm = new ChatOpenAI({ model: "computer-use-preview" })
  .bindTools([
    {
      type: "computer-preview",
      display_width: 1024,
      display_height: 768,
      environment: "browser",
    },
  ])
  .bind({ truncation: "auto" });

let message = await llm.invoke("Check the latest OpenAI news on bing.com.");
const computerCall = findComputerCall(message);

if (computerCall) {
  // Act on a computer call action
  const screenshot = await fs.readFile("./screenshot.png", {
    encoding: "base64",
  });

  message = await llm.invoke(
    [
      new ToolMessage({
        additional_kwargs: { type: "computer_call_output" },
        tool_call_id: computerCall.call_id,
        content: [
          {
            type: "computer_screenshot",
            image_url: `data:image/png;base64,${screenshot}`,
          },
        ],
      }),
    ],
    { previous_response_id: message.response_metadata["id"] }
  );
}


"""
### Reasoning models

```{=mdx}
:::caution Compatibility

The below points apply to `@langchain/openai>=0.4.0`. Please see here for a [guide on upgrading](/docs/how_to/installation/#installing-integration-packages).

:::
```

When using reasoning models like `o1`, the default method for `withStructuredOutput` is OpenAI's built-in method for structured output (equivalent to passing `method: "jsonSchema"` as an option into `withStructuredOutput`). JSON schema mostly works the same as other models, but with one important caveat: when defining schema, `z.optional()` is not respected, and you should instead use `z.nullable()`.

Here's an example:
"""

import { z } from "zod";
import { ChatOpenAI } from "@langchain/openai";

// Will not work
const reasoningModelSchemaOptional = z.object({
  color: z.optional(z.string()).describe("A color mentioned in the input"),
});

const reasoningModelOptionalSchema = new ChatOpenAI({
  model: "o1",
}).withStructuredOutput(reasoningModelSchemaOptional, {
  name: "extract_color",
});

await reasoningModelOptionalSchema.invoke([{
  role: "user",
  content: `I am 6'5" tall and love fruit.`
}]);
# Output:
#   { color: 'No color mentioned' }


"""
And here's an example with `z.nullable()`:
"""

import { z } from "zod";
import { ChatOpenAI } from "@langchain/openai";

// Will not work
const reasoningModelSchemaNullable = z.object({
  color: z.nullable(z.string()).describe("A color mentioned in the input"),
});

const reasoningModelNullableSchema = new ChatOpenAI({
  model: "o1",
}).withStructuredOutput(reasoningModelSchemaNullable, {
  name: "extract_color",
});

await reasoningModelNullableSchema.invoke([{
  role: "user",
  content: `I am 6'5" tall and love fruit.`
}]);
# Output:
#   { color: null }


"""
## Prompt caching

Newer OpenAI models will automatically [cache parts of your prompt](https://openai.com/index/api-prompt-caching/) if your inputs are above a certain size (1024 tokens at the time of writing) in order to reduce costs for use-cases that require long context.

**Note:** The number of tokens cached for a given query is not yet standardized in `AIMessage.usage_metadata`, and is instead contained in the `AIMessage.response_metadata` field.

Here's an example
"""

// @lc-docs-hide-cell

const CACHED_TEXT = `## Components

LangChain provides standard, extendable interfaces and external integrations for various components useful for building with LLMs.
Some components LangChain implements, some components we rely on third-party integrations for, and others are a mix.

### Chat models

<span data-heading-keywords="chat model,chat models"></span>

Language models that use a sequence of messages as inputs and return chat messages as outputs (as opposed to using plain text).
These are generally newer models (older models are generally \`LLMs\`, see below).
Chat models support the assignment of distinct roles to conversation messages, helping to distinguish messages from the AI, users, and instructions such as system messages.

Although the underlying models are messages in, message out, the LangChain wrappers also allow these models to take a string as input.
This gives them the same interface as LLMs (and simpler to use).
When a string is passed in as input, it will be converted to a \`HumanMessage\` under the hood before being passed to the underlying model.

LangChain does not host any Chat Models, rather we rely on third party integrations.

We have some standardized parameters when constructing ChatModels:

- \`model\`: the name of the model

Chat Models also accept other parameters that are specific to that integration.

:::important
Some chat models have been fine-tuned for **tool calling** and provide a dedicated API for it.
Generally, such models are better at tool calling than non-fine-tuned models, and are recommended for use cases that require tool calling.
Please see the [tool calling section](/docs/concepts/tool_calling) for more information.
:::

For specifics on how to use chat models, see the [relevant how-to guides here](/docs/how_to/#chat-models).

#### Multimodality

Some chat models are multimodal, accepting images, audio and even video as inputs.
These are still less common, meaning model providers haven't standardized on the "best" way to define the API.
Multimodal outputs are even less common. As such, we've kept our multimodal abstractions fairly light weight
and plan to further solidify the multimodal APIs and interaction patterns as the field matures.

In LangChain, most chat models that support multimodal inputs also accept those values in OpenAI's content blocks format.
So far this is restricted to image inputs. For models like Gemini which support video and other bytes input, the APIs also support the native, model-specific representations.

For specifics on how to use multimodal models, see the [relevant how-to guides here](/docs/how_to/#multimodal).

### LLMs

<span data-heading-keywords="llm,llms"></span>

:::caution
Pure text-in/text-out LLMs tend to be older or lower-level. Many popular models are best used as [chat completion models](/docs/concepts/chat_models),
even for non-chat use cases.

You are probably looking for [the section above instead](/docs/concepts/chat_models).
:::

Language models that takes a string as input and returns a string.
These are traditionally older models (newer models generally are [Chat Models](/docs/concepts/chat_models), see above).

Although the underlying models are string in, string out, the LangChain wrappers also allow these models to take messages as input.
This gives them the same interface as [Chat Models](/docs/concepts/chat_models).
When messages are passed in as input, they will be formatted into a string under the hood before being passed to the underlying model.

LangChain does not host any LLMs, rather we rely on third party integrations.

For specifics on how to use LLMs, see the [relevant how-to guides here](/docs/how_to/#llms).

### Message types

Some language models take an array of messages as input and return a message.
There are a few different types of messages.
All messages have a \`role\`, \`content\`, and \`response_metadata\` property.

The \`role\` describes WHO is saying the message.
LangChain has different message classes for different roles.

The \`content\` property describes the content of the message.
This can be a few different things:

- A string (most models deal this type of content)
- A List of objects (this is used for multi-modal input, where the object contains information about that input type and that input location)

#### HumanMessage

This represents a message from the user.

#### AIMessage

This represents a message from the model. In addition to the \`content\` property, these messages also have:

**\`response_metadata\`**

The \`response_metadata\` property contains additional metadata about the response. The data here is often specific to each model provider.
This is where information like log-probs and token usage may be stored.

**\`tool_calls\`**

These represent a decision from an language model to call a tool. They are included as part of an \`AIMessage\` output.
They can be accessed from there with the \`.tool_calls\` property.

This property returns a list of \`ToolCall\`s. A \`ToolCall\` is an object with the following arguments:

- \`name\`: The name of the tool that should be called.
- \`args\`: The arguments to that tool.
- \`id\`: The id of that tool call.

#### SystemMessage

This represents a system message, which tells the model how to behave. Not every model provider supports this.

#### ToolMessage

This represents the result of a tool call. In addition to \`role\` and \`content\`, this message has:

- a \`tool_call_id\` field which conveys the id of the call to the tool that was called to produce this result.
- an \`artifact\` field which can be used to pass along arbitrary artifacts of the tool execution which are useful to track but which should not be sent to the model.

#### (Legacy) FunctionMessage

This is a legacy message type, corresponding to OpenAI's legacy function-calling API. \`ToolMessage\` should be used instead to correspond to the updated tool-calling API.

This represents the result of a function call. In addition to \`role\` and \`content\`, this message has a \`name\` parameter which conveys the name of the function that was called to produce this result.

### Prompt templates

<span data-heading-keywords="prompt,prompttemplate,chatprompttemplate"></span>

Prompt templates help to translate user input and parameters into instructions for a language model.
This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.

Prompt Templates take as input an object, where each key represents a variable in the prompt template to fill in.

Prompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a string or an array of messages.
The reason this PromptValue exists is to make it easy to switch between strings and messages.

There are a few different types of prompt templates:

#### String PromptTemplates

These prompt templates are used to format a single string, and generally are used for simpler inputs.
For example, a common way to construct and use a PromptTemplate is as follows:

\`\`\`typescript
import { PromptTemplate } from "@langchain/core/prompts";

const promptTemplate = PromptTemplate.fromTemplate(
  "Tell me a joke about {topic}"
);

await promptTemplate.invoke({ topic: "cats" });
\`\`\`

#### ChatPromptTemplates

These prompt templates are used to format an array of messages. These "templates" consist of an array of templates themselves.
For example, a common way to construct and use a ChatPromptTemplate is as follows:

\`\`\`typescript
import { ChatPromptTemplate } from "@langchain/core/prompts";

const promptTemplate = ChatPromptTemplate.fromMessages([
  ["system", "You are a helpful assistant"],
  ["user", "Tell me a joke about {topic}"],
]);

await promptTemplate.invoke({ topic: "cats" });
\`\`\`

In the above example, this ChatPromptTemplate will construct two messages when called.
The first is a system message, that has no variables to format.
The second is a HumanMessage, and will be formatted by the \`topic\` variable the user passes in.

#### MessagesPlaceholder

<span data-heading-keywords="messagesplaceholder"></span>

This prompt template is responsible for adding an array of messages in a particular place.
In the above ChatPromptTemplate, we saw how we could format two messages, each one a string.
But what if we wanted the user to pass in an array of messages that we would slot into a particular spot?
This is how you use MessagesPlaceholder.

\`\`\`typescript
import {
  ChatPromptTemplate,
  MessagesPlaceholder,
} from "@langchain/core/prompts";
import { HumanMessage } from "@langchain/core/messages";

const promptTemplate = ChatPromptTemplate.fromMessages([
  ["system", "You are a helpful assistant"],
  new MessagesPlaceholder("msgs"),
]);

promptTemplate.invoke({ msgs: [new HumanMessage({ content: "hi!" })] });
\`\`\`

This will produce an array of two messages, the first one being a system message, and the second one being the HumanMessage we passed in.
If we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5 passed in).
This is useful for letting an array of messages be slotted into a particular spot.

An alternative way to accomplish the same thing without using the \`MessagesPlaceholder\` class explicitly is:

\`\`\`typescript
const promptTemplate = ChatPromptTemplate.fromMessages([
  ["system", "You are a helpful assistant"],
  ["placeholder", "{msgs}"], // <-- This is the changed part
]);
\`\`\`

For specifics on how to use prompt templates, see the [relevant how-to guides here](/docs/how_to/#prompt-templates).

### Example Selectors

One common prompting technique for achieving better performance is to include examples as part of the prompt.
This gives the language model concrete examples of how it should behave.
Sometimes these examples are hardcoded into the prompt, but for more advanced situations it may be nice to dynamically select them.
Example Selectors are classes responsible for selecting and then formatting examples into prompts.

For specifics on how to use example selectors, see the [relevant how-to guides here](/docs/how_to/#example-selectors).

### Output parsers

<span data-heading-keywords="output parser"></span>

:::note

The information here refers to parsers that take a text output from a model try to parse it into a more structured representation.
More and more models are supporting function (or tool) calling, which handles this automatically.
It is recommended to use function/tool calling rather than output parsing.
See documentation for that [here](/docs/concepts/tool_calling).

:::

Responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks.
Useful when you are using LLMs to generate structured data, or to normalize output from chat models and LLMs.

There are two main methods an output parser must implement:

- "Get format instructions": A method which returns a string containing instructions for how the output of a language model should be formatted.
- "Parse": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.

And then one optional one:

- "Parse with prompt": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.

Output parsers accept a string or \`BaseMessage\` as input and can return an arbitrary type.

LangChain has many different types of output parsers. This is a list of output parsers LangChain supports. The table below has various pieces of information:

**Name**: The name of the output parser

**Supports Streaming**: Whether the output parser supports streaming.

**Input Type**: Expected input type. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need a message with specific arguments.

**Output Type**: The output type of the object returned by the parser.

**Description**: Our commentary on this output parser and when to use it.

The current date is ${new Date().toISOString()}`;

// Noop statement to hide output
void 0;

import { ChatOpenAI } from "@langchain/openai";

const modelWithCaching = new ChatOpenAI({
  model: "gpt-4o-mini-2024-07-18",
});

// CACHED_TEXT is some string longer than 1024 tokens
const LONG_TEXT = `You are a pirate. Always respond in pirate dialect.

Use the following as context when answering questions:

${CACHED_TEXT}`;

const longMessages = [
  {
    role: "system",
    content: LONG_TEXT,
  },
  {
    role: "user",
    content: "What types of messages are supported in LangChain?",
  },
];

const originalRes = await modelWithCaching.invoke(longMessages);

console.log("USAGE:", originalRes.response_metadata.usage);
# Output:
#   USAGE: {

#     prompt_tokens: 2624,

#     completion_tokens: 263,

#     total_tokens: 2887,

#     prompt_tokens_details: { cached_tokens: 0 },

#     completion_tokens_details: { reasoning_tokens: 0 }

#   }


const resWitCaching = await modelWithCaching.invoke(longMessages);

console.log("USAGE:", resWitCaching.response_metadata.usage);
# Output:
#   USAGE: {

#     prompt_tokens: 2624,

#     completion_tokens: 272,

#     total_tokens: 2896,

#     prompt_tokens_details: { cached_tokens: 2432 },

#     completion_tokens_details: { reasoning_tokens: 0 }

#   }


"""
## Predicted output

Some OpenAI models (such as their `gpt-4o` and `gpt-4o-mini` series) support [Predicted Outputs](https://platform.openai.com/docs/guides/latency-optimization#use-predicted-outputs), which allow you to pass in a known portion of the LLM's expected output ahead of time to reduce latency. This is useful for cases such as editing text or code, where only a small part of the model's output will change.

Here's an example:
"""

import { ChatOpenAI } from "@langchain/openai";

const modelWithPredictions = new ChatOpenAI({
  model: "gpt-4o-mini",
});

const codeSample = `
/// <summary>
/// Represents a user with a first name, last name, and username.
/// </summary>
public class User
{
/// <summary>
/// Gets or sets the user's first name.
/// </summary>
public string FirstName { get; set; }

/// <summary>
/// Gets or sets the user's last name.
/// </summary>
public string LastName { get; set; }

/// <summary>
/// Gets or sets the user's username.
/// </summary>
public string Username { get; set; }
}
`;

// Can also be attached ahead of time
// using `model.bind({ prediction: {...} })`;
await modelWithPredictions.invoke(
  [
    {
      role: "user",
      content:
        "Replace the Username property with an Email property. Respond only with code, and with no markdown formatting.",
    },
    {
      role: "user",
      content: codeSample,
    },
  ],
  {
    prediction: {
      type: "content",
      content: codeSample,
    },
  }
);
# Output:
#   AIMessage {

#     "id": "chatcmpl-AQLyQKnazr7lEV7ejLTo1UqhzHDBl",

#     "content": "/// <summary>\n/// Represents a user with a first name, last name, and email.\n/// </summary>\npublic class User\n{\n/// <summary>\n/// Gets or sets the user's first name.\n/// </summary>\npublic string FirstName { get; set; }\n\n/// <summary>\n/// Gets or sets the user's last name.\n/// </summary>\npublic string LastName { get; set; }\n\n/// <summary>\n/// Gets or sets the user's email.\n/// </summary>\npublic string Email { get; set; }\n}",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "promptTokens": 148,

#         "completionTokens": 217,

#         "totalTokens": 365

#       },

#       "finish_reason": "stop",

#       "usage": {

#         "prompt_tokens": 148,

#         "completion_tokens": 217,

#         "total_tokens": 365,

#         "prompt_tokens_details": {

#           "cached_tokens": 0

#         },

#         "completion_tokens_details": {

#           "reasoning_tokens": 0,

#           "accepted_prediction_tokens": 36,

#           "rejected_prediction_tokens": 116

#         }

#       },

#       "system_fingerprint": "fp_0ba0d124f1"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "output_tokens": 217,

#       "input_tokens": 148,

#       "total_tokens": 365,

#       "input_token_details": {

#         "cache_read": 0

#       },

#       "output_token_details": {

#         "reasoning": 0

#       }

#     }

#   }


"""
Note that currently predictions are billed as additional tokens and will increase your usage and costs in exchange for this reduced latency.
"""

"""
## Audio output

Some OpenAI models (such as `gpt-4o-audio-preview`) support generating audio output. This example shows how to use that feature:
"""

import { ChatOpenAI } from "@langchain/openai";

const modelWithAudioOutput = new ChatOpenAI({
  model: "gpt-4o-audio-preview",
  // You may also pass these fields to `.bind` as a call argument.
  modalities: ["text", "audio"], // Specifies that the model should output audio.
  audio: {
    voice: "alloy",
    format: "wav",
  },
});

const audioOutputResult = await modelWithAudioOutput.invoke("Tell me a joke about cats.");
const castAudioContent = audioOutputResult.additional_kwargs.audio as Record<string, any>;

console.log({
  ...castAudioContent,
  data: castAudioContent.data.slice(0, 100) // Sliced for brevity
})
# Output:
#   {

#     id: 'audio_67129e9466f48190be70372922464162',

#     data: 'UklGRgZ4BABXQVZFZm10IBAAAAABAAEAwF0AAIC7AAACABAATElTVBoAAABJTkZPSVNGVA4AAABMYXZmNTguMjkuMTAwAGRhdGHA',

#     expires_at: 1729277092,

#     transcript: "Why did the cat sit on the computer's keyboard? Because it wanted to keep an eye on the mouse!"

#   }


"""
We see that the audio data is returned inside the `data` field. We are also provided an `expires_at` date field. This field represents the date the audio response will no longer be accessible on the server for use in multi-turn conversations.

### Streaming Audio Output

OpenAI also supports streaming audio output. Here's an example:
"""

import { AIMessageChunk } from "@langchain/core/messages";
import { concat } from "@langchain/core/utils/stream"
import { ChatOpenAI } from "@langchain/openai";

const modelWithStreamingAudioOutput = new ChatOpenAI({
  model: "gpt-4o-audio-preview",
  modalities: ["text", "audio"],
  audio: {
    voice: "alloy",
    format: "pcm16", // Format must be `pcm16` for streaming
  },
});

const audioOutputStream = await modelWithStreamingAudioOutput.stream("Tell me a joke about cats.");
let finalAudioOutputMsg: AIMessageChunk | undefined;
for await (const chunk of audioOutputStream) {
  finalAudioOutputMsg = finalAudioOutputMsg ? concat(finalAudioOutputMsg, chunk) : chunk;
}
const castStreamedAudioContent = finalAudioOutputMsg?.additional_kwargs.audio as Record<string, any>;

console.log({
  ...castStreamedAudioContent,
  data: castStreamedAudioContent.data.slice(0, 100) // Sliced for brevity
})
# Output:
#   {

#     id: 'audio_67129e976ce081908103ba4947399a3eaudio_67129e976ce081908103ba4947399a3e',

#     transcript: 'Why was the cat sitting on the computer? Because it wanted to keep an eye on the mouse!',

#     index: 0,

#     data: 'CgAGAAIADAAAAA0AAwAJAAcACQAJAAQABQABAAgABQAPAAAACAADAAUAAwD8/wUA+f8MAPv/CAD7/wUA///8/wUA/f8DAPj/AgD6',

#     expires_at: 1729277096

#   }


"""
### Audio input

These models also support passing audio as input. For this, you must specify `input_audio` fields as seen below:
"""

import { HumanMessage } from "@langchain/core/messages";

const userInput = new HumanMessage({
  content: [{
    type: "input_audio",
    input_audio: {
      data: castAudioContent.data, // Re-use the base64 data from the first example
      format: "wav",
    },
  }]
})

// Re-use the same model instance
const userInputAudioRes = await modelWithAudioOutput.invoke([userInput]);

console.log((userInputAudioRes.additional_kwargs.audio as Record<string, any>).transcript);
# Output:
#   That's a great joke! It's always fun to imagine why cats do the funny things they do. Keeping an eye on the "mouse" is a creatively punny way to describe it!


"""
## API reference

For detailed documentation of all ChatOpenAI features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_openai.ChatOpenAI.html
"""



================================================
FILE: docs/core_docs/docs/integrations/chat/perplexity.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Perplexity
---
"""

"""
# ChatPerplexity

This guide will help you getting started with Perplexity [chat models](/docs/concepts/#chat-models). For detailed documentation of all `ChatPerplexity` features and configurations head to the [API reference](https://api.js.langchain.com/classes/_langchain_community.chat_models_perplexity.ChatPerplexity.html).

## Overview
### Integration details

| Class | Package | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/perplexity/) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [`ChatPerplexity`](https://api.js.langchain.com/classes/_langchain_community.chat_models_perplexity.ChatPerplexity.html) | [`@langchain/community`](https://npmjs.com/@langchain/community) | ❌ | beta | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/community?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |

### Model features

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | [Image input](/docs/how_to/multimodal_inputs/) | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |
| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: |
| ❌ | ✅ | ❌ | ❌ | ❌ | ❌ | ✅ | ✅ | ❌ | 

Note that at the time of writing, Perplexity only supports structured outputs on certain usage tiers.

## Setup

To access Perplexity models you'll need to create a Perplexity account, get an API key, and install the `@langchain/community` integration package.

### Credentials

Head to https://perplexity.ai to sign up for Perplexity and generate an API key. Once you've done this set the `PERPLEXITY_API_KEY` environment variable:

```bash
export PERPLEXITY_API_KEY="your-api-key"
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain Perplexity integration lives in the `@langchain/community` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

import { ChatPerplexity } from "@langchain/community/chat_models/perplexity"

const llm = new ChatPerplexity({
  model: "sonar",
  temperature: 0,
  maxTokens: undefined,
  timeout: undefined,
  maxRetries: 2,
  // other params...
})

"""
## Invocation
"""

const aiMsg = await llm.invoke([
  {
    role: "system",
    content: "You are a helpful assistant that translates English to French. Translate the user sentence.",
  },
  {
    role: "user",
    content: "I love programming.",
  },
])
aiMsg
# Output:
#   AIMessage {

#     "id": "run-71853938-aa30-4861-9019-f12323c09f9a",

#     "content": "J'adore la programmation.",

#     "additional_kwargs": {

#       "citations": [

#         "https://careersatagoda.com/blog/why-we-love-programming/",

#         "https://henrikwarne.com/2012/06/02/why-i-love-coding/",

#         "https://forum.freecodecamp.org/t/i-love-programming-but/497502",

#         "https://ilovecoding.org",

#         "https://thecodinglove.com"

#       ]

#     },

#     "response_metadata": {

#       "tokenUsage": {

#         "promptTokens": 20,

#         "completionTokens": 9,

#         "totalTokens": 29

#       }

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": []

#   }


console.log(aiMsg.content)
# Output:
#   J'adore la programmation.


"""
## Chaining

We can [chain](/docs/how_to/sequence/) our model with a prompt template like so:
"""

import { ChatPromptTemplate } from "@langchain/core/prompts"

const prompt = ChatPromptTemplate.fromMessages(
  [
    [
      "system",
      "You are a helpful assistant that translates {input_language} to {output_language}.",
    ],
    ["human", "{input}"],
  ]
)

const chain = prompt.pipe(llm);
await chain.invoke(
  {
    input_language: "English",
    output_language: "German",
    input: "I love programming.",
  }
)
# Output:
#   AIMessage {

#     "id": "run-a44dc452-4a71-423d-a4ee-50a2d7c90abd",

#     "content": "**English to German Translation:**\n\n\"I love programming\" translates to **\"Ich liebe das Programmieren.\"**\n\nIf you'd like to express your passion for programming in more detail, here are some additional translations:\n\n- **\"Programming is incredibly rewarding and fulfilling.\"** translates to **\"Das Programmieren ist unglaublich lohnend und erfüllend.\"**\n- **\"I enjoy solving problems through coding.\"** translates to **\"Ich genieße es, Probleme durch Codieren zu lösen.\"**\n- **\"I find the process of creating something from nothing very satisfying.\"** translates to **\"Ich finde den Prozess, etwas aus dem Nichts zu schaffen, sehr befriedigend.\"**",

#     "additional_kwargs": {

#       "citations": [

#         "https://careersatagoda.com/blog/why-we-love-programming/",

#         "https://henrikwarne.com/2012/06/02/why-i-love-coding/",

#         "https://dev.to/dvddpl/coding-is-boring-why-do-you-love-coding-cl0",

#         "https://forum.freecodecamp.org/t/i-love-programming-but/497502",

#         "https://ilovecoding.org"

#       ]

#     },

#     "response_metadata": {

#       "tokenUsage": {

#         "promptTokens": 15,

#         "completionTokens": 149,

#         "totalTokens": 164

#       }

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": []

#   }


"""
## API reference

For detailed documentation of all ChatPerplexity features and configurations head to the API reference: https://api.js.langchain.com/classes/_langchain_community.chat_models_perplexity.ChatPerplexity.html
"""



================================================
FILE: docs/core_docs/docs/integrations/chat/premai.mdx
================================================
---
sidebar_label: PremAI
---

import CodeBlock from "@theme/CodeBlock";

# ChatPrem

## Setup

1. Create a Prem AI account and get your API key [here](https://app.premai.io/accounts/signup/).
2. Export or set your API key inline. The ChatPrem class defaults to `process.env.PREM_API_KEY`.

```bash
export PREM_API_KEY=your-api-key
```

You can use models provided by Prem AI as follows:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core
```

import PremAI from "@examples/models/chat/integration_premai.ts";

<CodeBlock language="typescript">{PremAI}</CodeBlock>

## Related

- Chat model [conceptual guide](/docs/concepts/chat_models)
- Chat model [how-to guides](/docs/how_to/#chat-models)



================================================
FILE: docs/core_docs/docs/integrations/chat/prompt_layer_openai.mdx
================================================
---
sidebar_label: PromptLayer OpenAI
---

# PromptLayerChatOpenAI

You can pass in the optional `returnPromptLayerId` boolean to get a `promptLayerRequestId` like below. Here is an example of getting the PromptLayerChatOpenAI requestID:

```typescript
import { PromptLayerChatOpenAI } from "langchain/llms/openai";

const chat = new PromptLayerChatOpenAI({
  returnPromptLayerId: true,
});

const respA = await chat.generate([
  [
    new SystemMessage(
      "You are a helpful assistant that translates English to French."
    ),
  ],
]);

console.log(JSON.stringify(respA, null, 3));

/*
  {
    "generations": [
      [
        {
          "text": "Bonjour! Je suis un assistant utile qui peut vous aider à traduire de l'anglais vers le français. Que puis-je faire pour vous aujourd'hui?",
          "message": {
            "type": "ai",
            "data": {
              "content": "Bonjour! Je suis un assistant utile qui peut vous aider à traduire de l'anglais vers le français. Que puis-je faire pour vous aujourd'hui?"
            }
          },
          "generationInfo": {
            "promptLayerRequestId": 2300682
          }
        }
      ]
    ],
    "llmOutput": {
      "tokenUsage": {
        "completionTokens": 35,
        "promptTokens": 19,
        "totalTokens": 54
      }
    }
  }
*/
```

## Related

- Chat model [conceptual guide](/docs/concepts/chat_models)
- Chat model [how-to guides](/docs/how_to/#chat-models)



================================================
FILE: docs/core_docs/docs/integrations/chat/tencent_hunyuan.mdx
================================================
---
sidebar_label: Tencent Hunyuan
---

import CodeBlock from "@theme/CodeBlock";

# ChatTencentHunyuan

LangChain.js supports the Tencent Hunyuan family of models.

https://cloud.tencent.com/document/product/1729/104753

## Setup

1. Sign up for a Tencent Cloud account [here](https://cloud.tencent.com/register).
2. Create SecretID & SecretKey [here](https://console.cloud.tencent.com/cam/capi).
3. Set SecretID and SecretKey as environment variables named `TENCENT_SECRET_ID` and `TENCENT_SECRET_KEY`, respectively.

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core
```

If you are using LangChain.js in a browser environment, you'll also need to install the following dependencies:

```bash npm2yarn
npm install crypto-js
```

And then make sure that you import from the `web` as shown below.

## Usage

Here's an example:

import TencentHunyuan from "@examples/models/chat/integration_tencent_hunyuan.ts";

<CodeBlock language="typescript">{TencentHunyuan}</CodeBlock>

## Related

- Chat model [conceptual guide](/docs/concepts/chat_models)
- Chat model [how-to guides](/docs/how_to/#chat-models)



================================================
FILE: docs/core_docs/docs/integrations/chat/togetherai.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Together
---
"""

"""
# ChatTogetherAI

[Together AI](https://www.together.ai/) offers an API to query [50+ leading open-source models](https://docs.together.ai/docs/inference-models) in a couple lines of code.

This guide will help you getting started with `ChatTogetherAI` [chat models](/docs/concepts/chat_models). For detailed documentation of all `ChatTogetherAI` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_chat_models_togetherai.ChatTogetherAI.html).

## Overview
### Integration details

| Class | Package | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/togetherai) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [ChatTogetherAI](https://api.js.langchain.com/classes/langchain_community_chat_models_togetherai.ChatTogetherAI.html) | [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) | ❌ | ✅ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/community?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |

### Model features

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | [Image input](/docs/how_to/multimodal_inputs/) | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |
| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: |
| ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | 

## Setup

To access `ChatTogetherAI` models you'll need to create a Together account, get an API key [here](https://api.together.xyz/), and install the `@langchain/community` integration package.

### Credentials

Head to [api.together.ai](https://api.together.ai/) to sign up to TogetherAI and generate an API key. Once you've done this set the `TOGETHER_AI_API_KEY` environment variable:

```bash
export TOGETHER_AI_API_KEY="your-api-key"
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain ChatTogetherAI integration lives in the `@langchain/community` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

import { ChatTogetherAI } from "@langchain/community/chat_models/togetherai"

const llm = new ChatTogetherAI({
    model: "mistralai/Mixtral-8x7B-Instruct-v0.1",
    temperature: 0,
    // other params...
})

"""
## Invocation
"""

const aiMsg = await llm.invoke([
    [
        "system",
        "You are a helpful assistant that translates English to French. Translate the user sentence.",
    ],
    ["human", "I love programming."],
])
aiMsg
# Output:
#   AIMessage {

#     "id": "chatcmpl-9rT9qEDPZ6iLCk6jt3XTzVDDH6pcI",

#     "content": "J'adore la programmation.",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "completionTokens": 8,

#         "promptTokens": 31,

#         "totalTokens": 39

#       },

#       "finish_reason": "stop"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 31,

#       "output_tokens": 8,

#       "total_tokens": 39

#     }

#   }


console.log(aiMsg.content)
# Output:
#   J'adore la programmation.


"""
## Chaining

We can [chain](/docs/how_to/sequence/) our model with a prompt template like so:
"""

import { ChatPromptTemplate } from "@langchain/core/prompts"

const prompt = ChatPromptTemplate.fromMessages(
    [
        [
            "system",
            "You are a helpful assistant that translates {input_language} to {output_language}.",
        ],
        ["human", "{input}"],
    ]
)

const chain = prompt.pipe(llm);
await chain.invoke(
    {
        input_language: "English",
        output_language: "German",
        input: "I love programming.",
    }
)
# Output:
#   AIMessage {

#     "id": "chatcmpl-9rT9wolZWfJ3xovORxnkdf1rcPbbY",

#     "content": "Ich liebe das Programmieren.",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "completionTokens": 6,

#         "promptTokens": 26,

#         "totalTokens": 32

#       },

#       "finish_reason": "stop"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 26,

#       "output_tokens": 6,

#       "total_tokens": 32

#     }

#   }


"""
Behind the scenes, TogetherAI uses the OpenAI SDK and OpenAI compatible API, with some caveats:

## API reference

For detailed documentation of all ChatTogetherAI features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_community_chat_models_togetherai.ChatTogetherAI.html
"""



================================================
FILE: docs/core_docs/docs/integrations/chat/web_llm.mdx
================================================
---
sidebar_class_name: web-only
---

# WebLLM

:::tip Compatibility
Only available in web environments.
:::

You can run LLMs directly in your web browser using LangChain's [WebLLM](https://webllm.mlc.ai) integration.

## Setup

You'll need to install the [WebLLM SDK](https://www.npmjs.com/package/@mlc-ai/web-llm) module to communicate with your local model.

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install -S @mlc-ai/web-llm @langchain/community @langchain/core
```

## Usage

Note that the first time a model is called, WebLLM will download the full weights for that model. This can be multiple gigabytes, and may not be possible for all end-users of your application depending on their internet connection and computer specs.
While the browser will cache future invocations of that model, we recommend using the smallest possible model you can.

We also recommend using a [separate web worker](https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Using_web_workers) when invoking and loading your models to
not block execution.

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/models/chat/integration_webllm.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

Streaming is also supported.

## Example

For a full end-to-end example, check out [this project](https://github.com/jacoblee93/fully-local-pdf-chatbot).

## Related

- Chat model [conceptual guide](/docs/concepts/chat_models)
- Chat model [how-to guides](/docs/how_to/#chat-models)



================================================
FILE: docs/core_docs/docs/integrations/chat/xai.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: xAI
---
"""

"""
# ChatXAI

[xAI](https://x.ai/) is an artificial intelligence company that develops large language models (LLMs). Their flagship model, Grok, is trained on real-time X (formerly Twitter) data and aims to provide witty, personality-rich responses while maintaining high capability on technical tasks.

This guide will help you getting started with `ChatXAI` [chat models](/docs/concepts/chat_models). For detailed documentation of all `ChatXAI` features and configurations head to the [API reference](https://api.js.langchain.com/classes/_langchain_xai.ChatXAI.html).

## Overview
### Integration details

| Class | Package | Local | Serializable | PY support | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [ChatXAI](https://api.js.langchain.com/classes/_langchain_xai.ChatXAI.html) | [`@langchain/xai`](https://www.npmjs.com/package/@langchain/xai) | ❌ | ✅ | ❌ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/xai?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/xai?style=flat-square&label=%20&) |

### Model features

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | [Image input](/docs/how_to/multimodal_inputs/) | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |
| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: |
| ✅ | ✅ | ✅ | ❌ | ❌ | ❌ | ✅ | ✅ | ✅ | 

## Setup

To access `ChatXAI` models you'll need to create an xAI account, [get an API key](https://console.x.ai/), and install the `@langchain/xai` integration package.

### Credentials

Head to [the xAI website](https://x.ai) to sign up to xAI and generate an API key. Once you've done this set the `XAI_API_KEY` environment variable:

```bash
export XAI_API_KEY="your-api-key"
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain `ChatXAI` integration lives in the `@langchain/xai` package:

```{=mdx}

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/xai @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

import { ChatXAI } from "@langchain/xai" 

const llm = new ChatXAI({
    model: "grok-beta", // default
    temperature: 0,
    maxTokens: undefined,
    maxRetries: 2,
    // other params...
})

"""
## Invocation
"""

const aiMsg = await llm.invoke([
    [
      "system",
      "You are a helpful assistant that translates English to French. Translate the user sentence.",
    ],
    ["human", "I love programming."],
])
console.log(aiMsg)
# Output:
#   AIMessage {

#     "id": "71d7e3d8-30dd-472c-8038-b6b283dcee63",

#     "content": "J'adore programmer.",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "promptTokens": 30,

#         "completionTokens": 6,

#         "totalTokens": 36

#       },

#       "finish_reason": "stop",

#       "usage": {

#         "prompt_tokens": 30,

#         "completion_tokens": 6,

#         "total_tokens": 36

#       },

#       "system_fingerprint": "fp_3e3898d4ce"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "output_tokens": 6,

#       "input_tokens": 30,

#       "total_tokens": 36,

#       "input_token_details": {},

#       "output_token_details": {}

#     }

#   }


console.log(aiMsg.content)
# Output:
#   J'adore programmer.


"""
## Chaining

We can [chain](/docs/how_to/sequence/) our model with a prompt template like so:
"""

import { ChatPromptTemplate } from "@langchain/core/prompts"

const prompt = ChatPromptTemplate.fromMessages(
  [
    [
      "system",
      "You are a helpful assistant that translates {input_language} to {output_language}.",
    ],
    ["human", "{input}"],
  ]
)

const chain = prompt.pipe(llm);
await chain.invoke(
  {
    input_language: "English",
    output_language: "German",
    input: "I love programming.",
  }
)
# Output:
#   AIMessage {

#     "id": "b2738008-8247-40e1-81dc-d9bf437a1a0c",

#     "content": "Ich liebe das Programmieren.",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "promptTokens": 25,

#         "completionTokens": 7,

#         "totalTokens": 32

#       },

#       "finish_reason": "stop",

#       "usage": {

#         "prompt_tokens": 25,

#         "completion_tokens": 7,

#         "total_tokens": 32

#       },

#       "system_fingerprint": "fp_3e3898d4ce"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "output_tokens": 7,

#       "input_tokens": 25,

#       "total_tokens": 32,

#       "input_token_details": {},

#       "output_token_details": {}

#     }

#   }


"""
Behind the scenes, xAI uses the OpenAI SDK and OpenAI compatible API.
"""

"""
## API reference

For detailed documentation of all ChatXAI features and configurations head to the API reference: https://api.js.langchain.com/classes/_langchain_xai.ChatXAI.html
"""



================================================
FILE: docs/core_docs/docs/integrations/chat/yandex.mdx
================================================
---
sidebar_label: YandexGPT
---

# ChatYandexGPT

LangChain.js supports calling [YandexGPT](https://cloud.yandex.com/en/services/yandexgpt) chat models.

## Setup

First, you should [create a service account](https://cloud.yandex.com/en/docs/iam/operations/sa/create) with the `ai.languageModels.user` role.

Next, you have two authentication options:

- [IAM token](https://cloud.yandex.com/en/docs/iam/operations/iam-token/create-for-sa).
  You can specify the token in a constructor parameter as `iam_token` or in an environment variable `YC_IAM_TOKEN`.
- [API key](https://cloud.yandex.com/en/docs/iam/operations/api-key/create)
  You can specify the key in a constructor parameter as `api_key` or in an environment variable `YC_API_KEY`.

## Usage

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/yandex @langchain/core
```

import CodeBlock from "@theme/CodeBlock";
import YandexGPTChatExample from "@examples/models/chat/integration_yandex.ts";

<CodeBlock language="typescript">{YandexGPTChatExample}</CodeBlock>

## Related

- Chat model [conceptual guide](/docs/concepts/chat_models)
- Chat model [how-to guides](/docs/how_to/#chat-models)



================================================
FILE: docs/core_docs/docs/integrations/chat/zhipuai.mdx
================================================
---
sidebar_label: ZhipuAI
---

import CodeBlock from "@theme/CodeBlock";

# ChatZhipuAI

LangChain.js supports the Zhipu AI family of models.

https://open.bigmodel.cn/dev/howuse/model

## Setup

You'll need to sign up for an Zhipu API key and set it as an environment variable named `ZHIPUAI_API_KEY`

https://open.bigmodel.cn

You'll also need to install the following dependencies:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core jsonwebtoken
```

## Usage

Here's an example:

import ZhipuAI from "@examples/models/chat/integration_zhipuai.ts";

<CodeBlock language="typescript">{ZhipuAI}</CodeBlock>

## Related

- Chat model [conceptual guide](/docs/concepts/chat_models)
- Chat model [how-to guides](/docs/how_to/#chat-models)



================================================
FILE: docs/core_docs/docs/integrations/document_compressors/cohere_rerank.mdx
================================================
# Cohere Rerank

Reranking documents can greatly improve any RAG application and document retrieval system.

At a high level, a rerank API is a language model which analyzes documents and reorders them based on their relevance to a given query.

Cohere offers an API for reranking documents. In this example we'll show you how to use it.

## Setup

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/cohere @langchain/core
```

import CodeBlock from "@theme/CodeBlock";

import Example from "@examples/document_compressors/cohere_rerank.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

Here, we can see the `.rerank()` method returns just the index of the documents (matching the indexes of the input documents) and their relevancy scores.

If we'd like to have the documents returned from the method itself, we can use the `.compressDocuments()` method.

import ExampleCompressor from "@examples/document_compressors/cohere_rerank_compressor.ts";

<CodeBlock language="typescript">{ExampleCompressor}</CodeBlock>

From the results, we can see it returned the top 3 documents, and assigned a `relevanceScore` to each.

As expected, the document with the highest `relevanceScore` is the one that references Washington, D.C., with a score of `98.7%`!

### Usage with `CohereClient`

If you are using Cohere on Azure, AWS Bedrock or a standalone instance you can use the `CohereClient` to create a `CohereRerank` instance with your endpoint.

import ExampleClient from "@examples/document_compressors/cohere_rerank_custom_client.ts";

<CodeBlock language="typescript">{ExampleClient}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/document_compressors/ibm.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: IBM watsonx.ai
---
"""

"""
# IBM watsonx.ai

## Overview

This will help you getting started with the [Watsonx document compressor](/docs/concepts/#document_compressors). For detailed documentation of all Watsonx document compressor features and configurations head to the [API reference](https://api.js.langchain.com/modules/_langchain_community.document_compressors_ibm.html).

### Integration details

| Class | Package | [PY support](https://python.langchain.com/docs/integrations/retrievers/ibm_watsonx_ranker/) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: | :---: |
| [`WatsonxRerank`](https://api.js.langchain.com/classes/_langchain_community.document_compressors_ibm.WatsonxRerank.html) | [@langchain/community](https://www.npmjs.com/package/@langchain/community) |  ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/community?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |

## Setup

To access IBM WatsonxAI models you'll need to create an IBM watsonx.ai account, get an API key or any other type of credentials, and install the `@langchain/community` integration package.

### Credentials

Head to [IBM Cloud](https://cloud.ibm.com/login) to sign up to IBM watsonx.ai and generate an API key or provide any other authentication form as presented below.

#### IAM authentication

```bash
export WATSONX_AI_AUTH_TYPE=iam
export WATSONX_AI_APIKEY=<YOUR-APIKEY>
```

#### Bearer token authentication

```bash
export WATSONX_AI_AUTH_TYPE=bearertoken
export WATSONX_AI_BEARER_TOKEN=<YOUR-BEARER-TOKEN>
```

#### IBM watsonx.ai software authentication

```bash
export WATSONX_AI_AUTH_TYPE=cp4d
export WATSONX_AI_USERNAME=<YOUR_USERNAME>
export WATSONX_AI_PASSWORD=<YOUR_PASSWORD>
export WATSONX_AI_URL=<URL>
```

Once these are placed in your environment variables and object is initialized authentication will proceed automatically.

Authentication can also be accomplished by passing these values as parameters to a new instance.

## IAM authentication

```typescript
import { WatsonxLLM } from "@langchain/community/llms/ibm";

const props = {
  version: "YYYY-MM-DD",
  serviceUrl: "<SERVICE_URL>",
  projectId: "<PROJECT_ID>",
  watsonxAIAuthType: "iam",
  watsonxAIApikey: "<YOUR-APIKEY>",
};
const instance = new WatsonxLLM(props);
```

## Bearer token authentication

```typescript
import { WatsonxLLM } from "@langchain/community/llms/ibm";

const props = {
  version: "YYYY-MM-DD",
  serviceUrl: "<SERVICE_URL>",
  projectId: "<PROJECT_ID>",
  watsonxAIAuthType: "bearertoken",
  watsonxAIBearerToken: "<YOUR-BEARERTOKEN>",
};
const instance = new WatsonxLLM(props);
```

### IBM watsonx.ai software authentication

```typescript
import { WatsonxLLM } from "@langchain/community/llms/ibm";

const props = {
  version: "YYYY-MM-DD",
  serviceUrl: "<SERVICE_URL>",
  projectId: "<PROJECT_ID>",
  watsonxAIAuthType: "cp4d",
  watsonxAIUsername: "<YOUR-USERNAME>",
  watsonxAIPassword: "<YOUR-PASSWORD>",
  watsonxAIUrl: "<url>",
};
const instance = new WatsonxLLM(props);
```

If you want to get automated tracing from individual queries, you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
// process.env.LANGSMITH_API_KEY = "<YOUR API KEY HERE>";
// process.env.LANGSMITH_TRACING = "true";
```

### Installation

This document compressor lives in the `@langchain/community` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core
</Npm2Yarn>
```
"""

"""
## Instantiation

Now we can instantiate our compressor:
"""

import { WatsonxRerank } from "@langchain/community/document_compressors/ibm";

const watsonxRerank = new WatsonxRerank({
  version: "2024-05-31",
  serviceUrl: process.env.WATSONX_AI_SERVICE_URL,
  projectId: process.env.WATSONX_AI_PROJECT_ID,
  model: "cross-encoder/ms-marco-minilm-l-12-v2",
});

"""
## Usage
"""

"""
First, set up a basic RAG ingest pipeline with embeddings, a text splitter and a vector store. We'll use this to and rerank some documents regarding the selected query:
"""

import { readFileSync } from "node:fs";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { WatsonxEmbeddings } from "@langchain/community/embeddings/ibm";
import { CharacterTextSplitter } from "@langchain/textsplitters";

const embeddings = new WatsonxEmbeddings({
 version: "YYYY-MM-DD",
 serviceUrl: process.env.API_URL,
 projectId: "<PROJECT_ID>",
 spaceId: "<SPACE_ID>",
 model: "ibm/slate-125m-english-rtrvr",
});

const textSplitter = new CharacterTextSplitter({
  chunkSize: 400,
  chunkOverlap: 0,
});
  
const query = "What did the president say about Ketanji Brown Jackson";
const text = readFileSync("state_of_the_union.txt", "utf8");

const docs = await textSplitter.createDocuments([text]);
const vectorStore = await MemoryVectorStore.fromDocuments(docs, embeddings);
const vectorStoreRetriever = vectorStore.asRetriever();

const result = await vectorStoreRetriever.invoke(query);
console.log(result);
# Output:
#   [

#     Document {

#       pageContent: 'And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.',

#       metadata: { loc: [Object] },

#       id: undefined

#     },

#     Document {

#       pageContent: 'I spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves. \n' +

#         '\n' +

#         'I’ve worked on these issues a long time. \n' +

#         '\n' +

#         'I know what works: Investing in crime preventionand community police officers who’ll walk the beat, who’ll know the neighborhood, and who can restore trust and safety.',

#       metadata: { loc: [Object] },

#       id: undefined

#     },

#     Document {

#       pageContent: 'We are the only nation on Earth that has always turned every crisis we have faced into an opportunity. \n' +

#         '\n' +

#         'The only nation that can be defined by a single word: possibilities. \n' +

#         '\n' +

#         'So on this night, in our 245th year as a nation, I have come to report on the State of the Union. \n' +

#         '\n' +

#         'And my report is this: the State of the Union is strong—because you, the American people, are strong.',

#       metadata: { loc: [Object] },

#       id: undefined

#     },

#     Document {

#       pageContent: 'And I’m taking robust action to make sure the pain of our sanctions  is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers. \n' +

#         '\n' +

#         'Tonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.',

#       metadata: { loc: [Object] },

#       id: undefined

#     }

#   ]


"""
Pass selected documents to rerank and recive specific score for each
"""

import { WatsonxRerank } from "@langchain/community/document_compressors/ibm";

const reranker = new WatsonxRerank({
  version: "2024-05-31",
  serviceUrl: process.env.WATSONX_AI_SERVICE_URL,
  projectId: process.env.WATSONX_AI_PROJECT_ID,
  model: "cross-encoder/ms-marco-minilm-l-12-v2",
});
const compressed = await reranker.rerank(result, query);
console.log(compressed);
# Output:
#   [

#     { index: 0, relevanceScore: 0.726995587348938 },

#     { index: 1, relevanceScore: 0.5758284330368042 },

#     { index: 2, relevanceScore: 0.5479092597961426 },

#     { index: 3, relevanceScore: 0.5468723773956299 }

#   ]


"""
Or else you could have the documents returned with the result, for that use .compressDocuments() method as below.
"""

const compressedWithResults = await reranker.compressDocuments(result, query);
console.log(compressedWithResults);
# Output:
#   [

#     Document {

#       pageContent: 'And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.',

#       metadata: { loc: [Object], relevanceScore: 0.726995587348938 },

#       id: undefined

#     },

#     Document {

#       pageContent: 'I spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves. \n' +

#         '\n' +

#         'I’ve worked on these issues a long time. \n' +

#         '\n' +

#         'I know what works: Investing in crime preventionand community police officers who’ll walk the beat, who’ll know the neighborhood, and who can restore trust and safety.',

#       metadata: { loc: [Object], relevanceScore: 0.5758284330368042 },

#       id: undefined

#     },

#     Document {

#       pageContent: 'We are the only nation on Earth that has always turned every crisis we have faced into an opportunity. \n' +

#         '\n' +

#         'The only nation that can be defined by a single word: possibilities. \n' +

#         '\n' +

#         'So on this night, in our 245th year as a nation, I have come to report on the State of the Union. \n' +

#         '\n' +

#         'And my report is this: the State of the Union is strong—because you, the American people, are strong.',

#       metadata: { loc: [Object], relevanceScore: 0.5479092597961426 },

#       id: undefined

#     },

#     Document {

#       pageContent: 'And I’m taking robust action to make sure the pain of our sanctions  is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers. \n' +

#         '\n' +

#         'Tonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.',

#       metadata: { loc: [Object], relevanceScore: 0.5468723773956299 },

#       id: undefined

#     }

#   ]


"""
## API reference

For detailed documentation of all Watsonx document compressor features and configurations head to the [API reference](https://api.js.langchain.com/modules/_langchain_community.document_compressors_ibm.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/document_compressors/mixedbread_ai.mdx
================================================
# Mixedbread AI reranking

## Overview

This guide will help you integrate and use the [Mixedbread AI](https://mixedbread.ai/) reranking API. The reranking API allows you to reorder a list of documents based on a given query, improving the relevance of search results or any ranked list.

## Installation

To get started, install the `@langchain/mixedbread-ai` package:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash
npm install @langchain/mixedbread-ai
```

## Authentication

Obtain your API key by signing up at [Mixedbread AI](https://mixedbread.ai/). You can then set the `MXBAI_API_KEY` environment variable to your Mixedbread AI API key or pass it directly as the `apiKey` option when constructing the class.

## Using Reranking

The `MixedbreadAIReranker` class provides access to the reranking API. Here’s how to use it:

1. **Import the Class**: First, import the `MixedbreadAIReranker` class from the package.

```typescript
import { MixedbreadAIReranker } from "@langchain/mixedbread-ai";
```

2. **Instantiate the Class**: Create an instance of `MixedbreadAIReranker` with your API key.

```typescript
const reranker = new MixedbreadAIReranker({ apiKey: "your-api-key" });
```

3. **Rerank Documents**: Use the `rerankDocuments` method to reorder documents based on a query.

```typescript
const documents = [
  { pageContent: "To bake bread you need flour" },
  { pageContent: "To bake bread you need yeast" },
  { pageContent: "To eat bread you need nothing but good taste" },
];
const query = "What do you need to bake bread?";
const result = await reranker.compressDocuments(documents, query);
console.log(result);
```

## Additional Resources

For more information, refer to the [Reranking API documentation](https://www.mixedbread.ai/docs/reranking/overview).



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/index.mdx
================================================
---
sidebar_position: 0
---

# Document loaders

[Document loaders](/docs/concepts/document_loaders) load data into LangChain's expected format for use-cases such as [retrieval-augmented generation (RAG)](/docs/tutorials/rag).

LangChain.js categorizes document loaders in two different ways:

- [File loaders](/docs/integrations/document_loaders/file_loaders/), which load data into LangChain formats from your local filesystem.
- [Web loaders](/docs/integrations/document_loaders/web_loaders/), which load data from remote sources.

See the individual pages for more on each category.

:::info
If you'd like to write your own document loader, see [this how-to](/docs/how_to/document_loader_custom/). If you'd like to contribute an integration, see [Contributing integrations](/docs/contributing).
:::



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/file_loaders/chatgpt.mdx
================================================
# ChatGPT files

This example goes over how to load conversations.json from your ChatGPT data export folder. You can get your data export by email by going to: ChatGPT -> (Profile) - Settings -> Export data -> Confirm export -> Check email.

## Usage, extracting all logs

Example code:

```typescript
import { ChatGPTLoader } from "@langchain/community/document_loaders/fs/chatgpt";

const loader = new ChatGPTLoader("./example_data/example_conversations.json");

const docs = await loader.load();

console.log(docs);
```

## Usage, extracting a single log

Example code:

```typescript
import { ChatGPTLoader } from "@langchain/community/document_loaders/fs/chatgpt";

const loader = new ChatGPTLoader(
  "./example_data/example_conversations.json",
  1
);

const docs = await loader.load();

console.log(docs);
```



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/file_loaders/csv.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: CSV
sidebar_class_name: node-only
---
"""

"""
# CSVLoader

```{=mdx}

:::tip Compatibility

Only available on Node.js.

:::

```

This notebook provides a quick overview for getting started with `CSVLoader` [document loaders](/docs/concepts/document_loaders). For detailed documentation of all `CSVLoader` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_document_loaders_fs_csv.CSVLoader.html).

This example goes over how to load data from CSV files. The second argument is the `column` name to extract from the CSV file. One document will be created for each row in the CSV file. When `column` is not specified, each row is converted into a key/value pair with each key/value pair outputted to a new line in the document's `pageContent`. When `column` is specified, one document is created for each row, and the value of the specified column is used as the document's `pageContent`.

## Overview
### Integration details

| Class | Package | Compatibility | Local | [PY support](https://python.langchain.com/docs/integrations/document_loaders/csv)| 
| :--- | :--- | :---: | :---: |  :---: |
| [CSVLoader](https://api.js.langchain.com/classes/langchain_community_document_loaders_fs_csv.CSVLoader.html) | [@langchain/community](https://api.js.langchain.com/modules/langchain_community_document_loaders_fs_csv.html) | Node-only | ✅ | ✅ |

## Setup

To access `CSVLoader` document loader you'll need to install the `@langchain/community` integration, along with the `d3-dsv@2` peer dependency.

### Installation

The LangChain CSVLoader integration lives in the `@langchain/community` integration package.

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core d3-dsv@2
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and load documents:
"""

import { CSVLoader } from "@langchain/community/document_loaders/fs/csv"

const exampleCsvPath = "../../../../../../langchain/src/document_loaders/tests/example_data/example_separator.csv";

const loader = new CSVLoader(exampleCsvPath)

"""
## Load
"""

const docs = await loader.load()
docs[0]
# Output:
#   Document {

#     pageContent: 'id｜html: 1｜"<i>Corruption discovered at the core of the Banking Clan!</i>"',

#     metadata: {

#       source: '../../../../../../langchain/src/document_loaders/tests/example_data/example_separator.csv',

#       line: 1

#     },

#     id: undefined

#   }


console.log(docs[0].metadata)
# Output:
#   {

#     source: '../../../../../../langchain/src/document_loaders/tests/example_data/example_separator.csv',

#     line: 1

#   }


"""
## Usage, extracting a single column

Example CSV file:

```csv
id｜html
1｜"<i>Corruption discovered at the core of the Banking Clan!</i>"
2｜"<i>Reunited, Rush Clovis and Senator Amidala</i>"
3｜"<i>discover the full extent of the deception.</i>"
4｜"<i>Anakin Skywalker is sent to the rescue!</i>"
```
"""

import { CSVLoader } from "@langchain/community/document_loaders/fs/csv";

const singleColumnLoader = new CSVLoader(
  exampleCsvPath,
  {
    column: "html",
    separator:"｜"
  }
);

const singleColumnDocs = await singleColumnLoader.load();
console.log(singleColumnDocs[0]);
# Output:
#   Document {

#     pageContent: '<i>Corruption discovered at the core of the Banking Clan!</i>',

#     metadata: {

#       source: '../../../../../../langchain/src/document_loaders/tests/example_data/example_separator.csv',

#       line: 1

#     },

#     id: undefined

#   }


"""
## API reference

For detailed documentation of all CSVLoader features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_community_document_loaders_fs_csv.CSVLoader.html
"""



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/file_loaders/directory.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: DirectoryLoader
sidebar_class_name: node-only
---
"""

"""
# DirectoryLoader

```{=mdx}

:::tip Compatibility

Only available on Node.js.

:::

```

This notebook provides a quick overview for getting started with `DirectoryLoader` [document loaders](/docs/concepts/document_loaders). For detailed documentation of all `DirectoryLoader` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.document_loaders_fs_directory.DirectoryLoader.html).

This example goes over how to load data from folders with multiple files. The second argument is a map of file extensions to loader factories. Each file will be passed to the matching loader, and the resulting documents will be concatenated together.

Example folder:

```text
src/document_loaders/example_data/example/
├── example.json
├── example.jsonl
├── example.txt
└── example.csv
```

## Overview
### Integration details

| Class | Package | Compatibility | Local | PY support | 
| :--- | :--- | :---: | :---: |  :---: |
| [DirectoryLoader](https://api.js.langchain.com/classes/langchain.document_loaders_fs_directory.DirectoryLoader.html) | [langchain](https://api.js.langchain.com/modules/langchain.document_loaders_fs_directory.html) | Node-only | ✅ | ✅ |

## Setup

To access `DirectoryLoader` document loader you'll need to install the `langchain` package.

### Installation

The LangChain DirectoryLoader integration lives in the `langchain` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  langchain @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and load documents:
"""

import { DirectoryLoader } from "langchain/document_loaders/fs/directory";
import {
  JSONLoader,
  JSONLinesLoader,
} from "langchain/document_loaders/fs/json";
import { TextLoader } from "langchain/document_loaders/fs/text";
import { CSVLoader } from "@langchain/community/document_loaders/fs/csv";

const loader = new DirectoryLoader(
  "../../../../../../examples/src/document_loaders/example_data",
  {
    ".json": (path) => new JSONLoader(path, "/texts"),
    ".jsonl": (path) => new JSONLinesLoader(path, "/html"),
    ".txt": (path) => new TextLoader(path),
    ".csv": (path) => new CSVLoader(path, "text"),
  }
);

"""
## Load
"""

const docs = await loader.load()
// disable console.warn calls
console.warn = () => {}
docs[0]
# Output:
#   Document {

#     pageContent: 'Foo\nBar\nBaz\n\n',

#     metadata: {

#       source: '/Users/bracesproul/code/lang-chain-ai/langchainjs/examples/src/document_loaders/example_data/example.txt'

#     },

#     id: undefined

#   }


console.log(docs[0].metadata)
# Output:
#   {

#     source: '/Users/bracesproul/code/lang-chain-ai/langchainjs/examples/src/document_loaders/example_data/example.txt'

#   }


"""
## API reference

For detailed documentation of all DirectoryLoader features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain.document_loaders_fs_directory.DirectoryLoader.html
"""



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/file_loaders/docx.mdx
================================================
---
hide_table_of_contents: true
---

# Docx files

The `DocxLoader` allows you to extract text data from Microsoft Word documents. It supports both the modern `.docx` format and the legacy `.doc` format. Depending on the file type, additional dependencies are required.

---

## Setup

To use `DocxLoader`, you'll need the `@langchain/community` integration along with either `mammoth` or `word-extractor` package:

- **`mammoth`**: For processing `.docx` files.
- **`word-extractor`**: For handling `.doc` files.

### Installation

#### For `.docx` Files

```bash npm2yarn
npm install @langchain/community @langchain/core mammoth
```

#### For `.doc` Files

```bash npm2yarn
npm install @langchain/community @langchain/core word-extractor
```

## Usage

### Loading `.docx` Files

For `.docx` files, there is no need to explicitly specify any parameters when initializing the loader:

```javascript
import { DocxLoader } from "@langchain/community/document_loaders/fs/docx";

const loader = new DocxLoader(
  "src/document_loaders/tests/example_data/attention.docx"
);

const docs = await loader.load();
```

### Loading `.doc` Files

For `.doc` files, you must explicitly specify the `type` as `doc` when initializing the loader:

```javascript
import { DocxLoader } from "@langchain/community/document_loaders/fs/docx";

const loader = new DocxLoader(
  "src/document_loaders/tests/example_data/attention.doc",
  {
    type: "doc",
  }
);

const docs = await loader.load();
```



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/file_loaders/epub.mdx
================================================
---
hide_table_of_contents: true
---

# EPUB files

This example goes over how to load data from EPUB files. By default, one document will be created for each chapter in the EPUB file, you can change this behavior by setting the `splitChapters` option to `false`.

# Setup

```bash npm2yarn
npm install @langchain/community @langchain/core epub2 html-to-text
```

# Usage, one document per chapter

```typescript
import { EPubLoader } from "@langchain/community/document_loaders/fs/epub";

const loader = new EPubLoader("src/document_loaders/example_data/example.epub");

const docs = await loader.load();
```

# Usage, one document per file

```typescript
import { EPubLoader } from "@langchain/community/document_loaders/fs/epub";

const loader = new EPubLoader(
  "src/document_loaders/example_data/example.epub",
  {
    splitChapters: false,
  }
);

const docs = await loader.load();
```



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/file_loaders/index.mdx
================================================
---
label: "File Loaders"
hide_table_of_contents: true
sidebar_class_name: node-only-category
---

# File Loaders

:::tip Compatibility
Only available on Node.js.
:::

These loaders are used to load files given a filesystem path or a Blob object.

:::info
If you'd like to write your own document loader, see [this how-to](/docs/how_to/document_loader_custom/). If you'd like to contribute an integration, see [Contributing integrations](/docs/contributing).
:::

import { CategoryTable, IndexTable } from "@theme/FeatureTables";

## All document loaders

<IndexTable />



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/file_loaders/json.mdx
================================================
# JSON files

The JSON loader use [JSON pointer](https://github.com/janl/node-jsonpointer) to target keys in your JSON files you want to target.

### No JSON pointer example

The most simple way of using it, is to specify no JSON pointer.
The loader will load all strings it finds in the JSON object.

Example JSON file:

```json
{
  "texts": ["This is a sentence.", "This is another sentence."]
}
```

Example code:

```typescript
import { JSONLoader } from "langchain/document_loaders/fs/json";

const loader = new JSONLoader("src/document_loaders/example_data/example.json");

const docs = await loader.load();
/*
[
  Document {
    "metadata": {
      "blobType": "application/json",
      "line": 1,
      "source": "blob",
    },
    "pageContent": "This is a sentence.",
  },
  Document {
    "metadata": {
      "blobType": "application/json",
      "line": 2,
      "source": "blob",
    },
    "pageContent": "This is another sentence.",
  },
]
*/
```

### Using JSON pointer example

You can do a more advanced scenario by choosing which keys in your JSON object you want to extract string from.

In this example, we want to only extract information from "from" and "surname" entries.

```json
{
  "1": {
    "body": "BD 2023 SUMMER",
    "from": "LinkedIn Job",
    "labels": ["IMPORTANT", "CATEGORY_UPDATES", "INBOX"]
  },
  "2": {
    "body": "Intern, Treasury and other roles are available",
    "from": "LinkedIn Job2",
    "labels": ["IMPORTANT"],
    "other": {
      "name": "plop",
      "surname": "bob"
    }
  }
}
```

Example code:

```typescript
import { JSONLoader } from "langchain/document_loaders/fs/json";

const loader = new JSONLoader(
  "src/document_loaders/example_data/example.json",
  ["/from", "/surname"]
);

const docs = await loader.load();
/*
[
  Document {
    "metadata": {
      "blobType": "application/json",
      "line": 1,
      "source": "blob",
    },
    "pageContent": "BD 2023 SUMMER",
  },
  Document {
    "metadata": {
      "blobType": "application/json",
      "line": 2,
      "source": "blob",
    },
    "pageContent": "LinkedIn Job",
  },
  ...
]
```



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/file_loaders/jsonlines.mdx
================================================
---
hide_table_of_contents: true
---

# JSONLines files

This example goes over how to load data from JSONLines or JSONL files. The second argument is a JSONPointer to the property to extract from each JSON object in the file. One document will be created for each JSON object in the file.

Example JSONLines file:

```json
{"html": "This is a sentence."}
{"html": "This is another sentence."}
```

Example code:

```typescript
import { JSONLinesLoader } from "langchain/document_loaders/fs/json";

const loader = new JSONLinesLoader(
  "src/document_loaders/example_data/example.jsonl",
  "/html"
);

const docs = await loader.load();
/*
[
  Document {
    "metadata": {
      "blobType": "application/jsonl+json",
      "line": 1,
      "source": "blob",
    },
    "pageContent": "This is a sentence.",
  },
  Document {
    "metadata": {
      "blobType": "application/jsonl+json",
      "line": 2,
      "source": "blob",
    },
    "pageContent": "This is another sentence.",
  },
]
*/
```



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/file_loaders/multi_file.mdx
================================================
---
sidebar_position: 2
hide_table_of_contents: true
---

# Multiple individual files

This example goes over how to load data from multiple file paths. The second argument is a map of file extensions to loader factories. Each file will be passed to the matching loader, and the resulting documents will be concatenated together.

Example files:

```text
src/document_loaders/example_data/example/
├── example.txt
└── example.csv

src/document_loaders/example_data/example2/
├── example.json
└── example.jsonl
```

Example code:

```typescript
import { MultiFileLoader } from "langchain/document_loaders/fs/multi_file";
import {
  JSONLoader,
  JSONLinesLoader,
} from "langchain/document_loaders/fs/json";
import { TextLoader } from "langchain/document_loaders/fs/text";
import { CSVLoader } from "langchain/document_loaders/fs/csv";

const loader = new MultiFileLoader(
  [
    "src/document_loaders/example_data/example/example.txt",
    "src/document_loaders/example_data/example/example.csv",
    "src/document_loaders/example_data/example2/example.json",
    "src/document_loaders/example_data/example2/example.jsonl",
  ],
  {
    ".json": (path) => new JSONLoader(path, "/texts"),
    ".jsonl": (path) => new JSONLinesLoader(path, "/html"),
    ".txt": (path) => new TextLoader(path),
    ".csv": (path) => new CSVLoader(path, "text"),
  }
);
const docs = await loader.load();
console.log({ docs });
```



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/file_loaders/notion_markdown.mdx
================================================
---
hide_table_of_contents: true
---

# Notion markdown export

This example goes over how to load data from your Notion pages exported from the notion dashboard.

First, export your notion pages as **Markdown & CSV** as per the offical explanation [here](https://www.notion.so/help/export-your-content). Make sure to select `include subpages` and `Create folders for subpages.`

Then, unzip the downloaded file and move the unzipped folder into your repository. It should contain the markdown files of your pages.

Once the folder is in your repository, simply run the example below:

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/document_loaders/notion_markdown.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/file_loaders/openai_whisper_audio.mdx
================================================
---
hide_table_of_contents: true
---

# Open AI Whisper Audio

:::tip Compatibility
Only available on Node.js.
:::

This covers how to load document objects from an audio file using the [Open AI Whisper](https://platform.openai.com/docs/guides/speech-to-text) API.

## Setup

To run this loader you will need to create an account on the Open AI and obtain an auth key from the https://platform.openai.com/account page.

## Usage

Once auth key is configured, you can use the loader to create transcriptions and then convert them into a Document.

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/document_loaders/openai_whisper_audio.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/file_loaders/pdf.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: PDFLoader
sidebar_class_name: node-only
---
"""

"""
# PDFLoader

```{=mdx}

:::tip Compatibility

Only available on Node.js.

:::

```

This notebook provides a quick overview for getting started with `PDFLoader` [document loaders](/docs/concepts/document_loaders). For detailed documentation of all `PDFLoader` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_document_loaders_fs_pdf.PDFLoader.html).

## Overview
### Integration details

| Class | Package | Compatibility | Local | PY support | 
| :--- | :--- | :---: | :---: |  :---: |
| [PDFLoader](https://api.js.langchain.com/classes/langchain_community_document_loaders_fs_pdf.PDFLoader.html) | [@langchain/community](https://api.js.langchain.com/modules/langchain_community_document_loaders_fs_pdf.html) | Node-only | ✅ | 🟠 (See note below) |

## Setup

To access `PDFLoader` document loader you'll need to install the `@langchain/community` integration, along with the `pdf-parse` package.

### Credentials

### Installation

The LangChain PDFLoader integration lives in the `@langchain/community` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core pdf-parse
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and load documents:
"""

import { PDFLoader } from "@langchain/community/document_loaders/fs/pdf"

const nike10kPdfPath = "../../../../data/nke-10k-2023.pdf"

const loader = new PDFLoader(nike10kPdfPath)

"""
## Load
"""

const docs = await loader.load()
docs[0]
# Output:
#   Document {

#     pageContent: 'Table of Contents\n' +

#       'UNITED STATES\n' +

#       'SECURITIES AND EXCHANGE COMMISSION\n' +

#       'Washington, D.C. 20549\n' +

#       'FORM 10-K\n' +

#       '(Mark One)\n' +

#       '☑ ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934\n' +

#       'FOR THE FISCAL YEAR ENDED MAY 31, 2023\n' +

#       'OR\n' +

#       '☐ TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934\n' +

#       'FOR THE TRANSITION PERIOD FROM                         TO                         .\n' +

#       'Commission File No. 1-10635\n' +

#       'NIKE, Inc.\n' +

#       '(Exact name of Registrant as specified in its charter)\n' +

#       'Oregon93-0584541\n' +

#       '(State or other jurisdiction of incorporation)(IRS Employer Identification No.)\n' +

#       'One Bowerman Drive, Beaverton, Oregon 97005-6453\n' +

#       '(Address of principal executive offices and zip code)\n' +

#       '(503) 671-6453\n' +

#       "(Registrant's telephone number, including area code)\n" +

#       'SECURITIES REGISTERED PURSUANT TO SECTION 12(B) OF THE ACT:\n' +

#       'Class B Common StockNKENew York Stock Exchange\n' +

#       '(Title of each class)(Trading symbol)(Name of each exchange on which registered)\n' +

#       'SECURITIES REGISTERED PURSUANT TO SECTION 12(G) OF THE ACT:\n' +

#       'NONE\n' +

#       'Indicate by check mark:YESNO\n' +

#       '•if the registrant is a well-known seasoned issuer, as defined in Rule 405 of the Securities Act.þ ̈\n' +

#       '•if the registrant is not required to file reports pursuant to Section 13 or Section 15(d) of the Act. ̈þ\n' +

#       '•whether the registrant (1) has filed all reports required to be filed by Section 13 or 15(d) of the Securities Exchange Act of 1934 during the preceding\n' +

#       '12 months (or for such shorter period that the registrant was required to file such reports), and (2) has been subject to such filing requirements for the\n' +

#       'past 90 days.\n' +

#       'þ ̈\n' +

#       '•whether the registrant has submitted electronically every Interactive Data File required to be submitted pursuant to Rule 405 of Regulation S-T\n' +

#       '(§232.405 of this chapter) during the preceding 12 months (or for such shorter period that the registrant was required to submit such files).\n' +

#       'þ ̈\n' +

#       '•whether the registrant is a large accelerated filer, an accelerated filer, a non-accelerated filer, a smaller reporting company or an emerging growth company. See the definitions of “large accelerated filer,”\n' +

#       '“accelerated filer,” “smaller reporting company,” and “emerging growth company” in Rule 12b-2 of the Exchange Act.\n' +

#       'Large accelerated filerþAccelerated filer☐Non-accelerated filer☐Smaller reporting company☐Emerging growth company☐\n' +

#       '•if an emerging growth company, if the registrant has elected not to use the extended transition period for complying with any new or revised financial\n' +

#       'accounting standards provided pursuant to Section 13(a) of the Exchange Act.\n' +

#       ' ̈\n' +

#       "•whether the registrant has filed a report on and attestation to its management's assessment of the effectiveness of its internal control over financial\n" +

#       'reporting under Section 404(b) of the Sarbanes-Oxley Act (15 U.S.C. 7262(b)) by the registered public accounting firm that prepared or issued its audit\n' +

#       'report.\n' +

#       'þ\n' +

#       '•if securities are registered pursuant to Section 12(b) of the Act, whether the financial statements of the registrant included in the filing reflect the\n' +

#       'correction of an error to previously issued financial statements.\n' +

#       ' ̈\n' +

#       '•whether any of those error corrections are restatements that required a recovery analysis of incentive-based compensation received by any of the\n' +

#       "registrant's executive officers during the relevant recovery period pursuant to § 240.10D-1(b).\n" +

#       ' ̈\n' +

#       '•\n' +

#       'whether the registrant is a shell company (as defined in Rule 12b-2 of the Act).☐þ\n' +

#       "As of November 30, 2022, the aggregate market values of the Registrant's Common Stock held by non-affiliates were:\n" +

#       'Class A$7,831,564,572 \n' +

#       'Class B136,467,702,472 \n' +

#       '$144,299,267,044 ',

#     metadata: {

#       source: '../../../../data/nke-10k-2023.pdf',

#       pdf: {

#         version: '1.10.100',

#         info: [Object],

#         metadata: null,

#         totalPages: 107

#       },

#       loc: { pageNumber: 1 }

#     },

#     id: undefined

#   }


console.log(docs[0].metadata)
# Output:
#   {

#     source: '../../../../data/nke-10k-2023.pdf',

#     pdf: {

#       version: '1.10.100',

#       info: {

#         PDFFormatVersion: '1.4',

#         IsAcroFormPresent: false,

#         IsXFAPresent: false,

#         Title: '0000320187-23-000039',

#         Author: 'EDGAR Online, a division of Donnelley Financial Solutions',

#         Subject: 'Form 10-K filed on 2023-07-20 for the period ending 2023-05-31',

#         Keywords: '0000320187-23-000039; ; 10-K',

#         Creator: 'EDGAR Filing HTML Converter',

#         Producer: 'EDGRpdf Service w/ EO.Pdf 22.0.40.0',

#         CreationDate: "D:20230720162200-04'00'",

#         ModDate: "D:20230720162208-04'00'"

#       },

#       metadata: null,

#       totalPages: 107

#     },

#     loc: { pageNumber: 1 }

#   }


"""
## Usage, one document per file
"""

import { PDFLoader } from "@langchain/community/document_loaders/fs/pdf";

const singleDocPerFileLoader = new PDFLoader(nike10kPdfPath, {
  splitPages: false,
});

const singleDoc = await singleDocPerFileLoader.load();
console.log(singleDoc[0].pageContent.slice(0, 100))
# Output:
#   Table of Contents

#   UNITED STATES

#   SECURITIES AND EXCHANGE COMMISSION

#   Washington, D.C. 20549

#   FORM 10-K

#   


"""
## Usage, custom `pdfjs` build

By default we use the `pdfjs` build bundled with `pdf-parse`, which is compatible with most environments, including Node.js and modern browsers. If you want to use a more recent version of `pdfjs-dist` or if you want to use a custom build of `pdfjs-dist`, you can do so by providing a custom `pdfjs` function that returns a promise that resolves to the `PDFJS` object.

In the following example we use the "legacy" (see [pdfjs docs](https://github.com/mozilla/pdf.js/wiki/Frequently-Asked-Questions#which-browsersenvironments-are-supported)) build of `pdfjs-dist`, which includes several polyfills not included in the default build.

```{=mdx}
<Npm2Yarn>
  pdfjs-dist
</Npm2Yarn>

```

"""

import { PDFLoader } from "@langchain/community/document_loaders/fs/pdf";

const customBuildLoader = new PDFLoader(nike10kPdfPath, {
  // you may need to add `.then(m => m.default)` to the end of the import
  // @lc-ts-ignore
  pdfjs: () => import("pdfjs-dist/legacy/build/pdf.js"),
});

"""
## Eliminating extra spaces

PDFs come in many varieties, which makes reading them a challenge. The loader parses individual text elements and joins them together with a space by default, but
if you are seeing excessive spaces, this may not be the desired behavior. In that case, you can override the separator with an empty string like this:

"""

import { PDFLoader } from "@langchain/community/document_loaders/fs/pdf";

const noExtraSpacesLoader = new PDFLoader(nike10kPdfPath, {
  parsedItemSeparator: "",
});

const noExtraSpacesDocs = await noExtraSpacesLoader.load();
console.log(noExtraSpacesDocs[0].pageContent.slice(100, 250))
# Output:
#   (Mark One)

#   ☑ ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934

#   FOR THE FISCAL YEAR ENDED MAY 31, 2023

#   OR

#   ☐ TRANSITI


"""
## Loading directories
"""

import { DirectoryLoader } from "langchain/document_loaders/fs/directory";
import { PDFLoader } from "@langchain/community/document_loaders/fs/pdf";
import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";

const exampleDataPath = "../../../../../../examples/src/document_loaders/example_data/";

/* Load all PDFs within the specified directory */
const directoryLoader = new DirectoryLoader(
  exampleDataPath,
  {
    ".pdf": (path: string) => new PDFLoader(path),
  }
);

const directoryDocs = await directoryLoader.load();

console.log(directoryDocs[0]);

/* Additional steps : Split text into chunks with any TextSplitter. You can then use it as context or save it to memory afterwards. */
const textSplitter = new RecursiveCharacterTextSplitter({
  chunkSize: 1000,
  chunkOverlap: 200,
});

const splitDocs = await textSplitter.splitDocuments(directoryDocs);
console.log(splitDocs[0]);

# Output:
#   Unknown file type: Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.srt

#   Unknown file type: example.txt

#   Unknown file type: notion.md

#   Unknown file type: bad_frontmatter.md

#   Unknown file type: frontmatter.md

#   Unknown file type: no_frontmatter.md

#   Unknown file type: no_metadata.md

#   Unknown file type: tags_and_frontmatter.md

#   Unknown file type: test.mp3

#   Document {

#     pageContent: 'Bitcoin: A Peer-to-Peer Electronic Cash System\n' +

#       'Satoshi Nakamoto\n' +

#       'satoshin@gmx.com\n' +

#       'www.bitcoin.org\n' +

#       'Abstract.   A  purely   peer-to-peer   version   of   electronic   cash   would   allow   online \n' +

#       'payments   to   be   sent   directly   from   one   party   to   another   without   going   through   a \n' +

#       'financial institution.   Digital signatures provide part of the solution, but the main \n' +

#       'benefits are lost if a trusted third party is still required to prevent double-spending. \n' +

#       'We propose a solution to the double-spending problem using a peer-to-peer network. \n' +

#       'The   network   timestamps   transactions   by   hashing   them   into   an   ongoing   chain   of \n' +

#       'hash-based proof-of-work, forming a record that cannot be changed without redoing \n' +

#       'the proof-of-work.   The longest chain not only serves as proof of the sequence of \n' +

#       'events witnessed, but proof that it came from the largest pool of CPU power.   As \n' +

#       'long as a majority of CPU power is controlled by nodes that are not cooperating to \n' +

#       "attack the network,  they'll  generate the  longest  chain  and  outpace attackers.   The \n" +

#       'network itself requires minimal structure.   Messages are broadcast on a best effort \n' +

#       'basis,   and   nodes   can   leave   and   rejoin   the   network   at   will,   accepting   the   longest \n' +

#       'proof-of-work chain as proof of what happened while they were gone.\n' +

#       '1.Introduction\n' +

#       'Commerce on the Internet has come to rely almost exclusively on financial institutions serving as \n' +

#       'trusted third  parties  to process electronic payments.   While the  system works  well enough for \n' +

#       'most   transactions,   it   still   suffers   from   the   inherent   weaknesses   of   the   trust   based   model. \n' +

#       'Completely non-reversible transactions are not really possible, since financial institutions cannot \n' +

#       'avoid   mediating   disputes.     The   cost   of   mediation   increases   transaction   costs,   limiting   the \n' +

#       'minimum practical transaction size and cutting off the possibility for small casual transactions, \n' +

#       'and   there   is   a   broader   cost   in   the   loss   of   ability   to   make   non-reversible   payments   for   non-\n' +

#       'reversible services.  With the possibility of reversal, the need for trust spreads.  Merchants must \n' +

#       'be wary of their customers, hassling them for more information than they would otherwise need. \n' +

#       'A certain percentage of fraud is accepted as unavoidable.  These costs and payment uncertainties \n' +

#       'can be avoided in person by using physical currency, but no mechanism exists to make payments \n' +

#       'over a communications channel without a trusted party.\n' +

#       'What is needed is an electronic payment system based on cryptographic proof instead of trust, \n' +

#       'allowing any two willing parties to transact directly with each other without the need for a trusted \n' +

#       'third  party.    Transactions  that  are  computationally  impractical  to   reverse   would  protect  sellers \n' +

#       'from fraud, and routine escrow mechanisms could easily be implemented to protect buyers.   In \n' +

#       'this paper, we propose a solution to the double-spending problem using a peer-to-peer distributed \n' +

#       'timestamp server to generate computational proof of the chronological order of transactions.  The \n' +

#       'system   is   secure   as   long   as   honest   nodes   collectively   control   more   CPU   power   than   any \n' +

#       'cooperating group of attacker nodes.\n' +

#       '1',

#     metadata: {

#       source: '/Users/bracesproul/code/lang-chain-ai/langchainjs/examples/src/document_loaders/example_data/bitcoin.pdf',

#       pdf: {

#         version: '1.10.100',

#         info: [Object],

#         metadata: null,

#         totalPages: 9

#       },

#       loc: { pageNumber: 1 }

#     },

#     id: undefined

#   }

#   Document {

#     pageContent: 'Bitcoin: A Peer-to-Peer Electronic Cash System\n' +

#       'Satoshi Nakamoto\n' +

#       'satoshin@gmx.com\n' +

#       'www.bitcoin.org\n' +

#       'Abstract.   A  purely   peer-to-peer   version   of   electronic   cash   would   allow   online \n' +

#       'payments   to   be   sent   directly   from   one   party   to   another   without   going   through   a \n' +

#       'financial institution.   Digital signatures provide part of the solution, but the main \n' +

#       'benefits are lost if a trusted third party is still required to prevent double-spending. \n' +

#       'We propose a solution to the double-spending problem using a peer-to-peer network. \n' +

#       'The   network   timestamps   transactions   by   hashing   them   into   an   ongoing   chain   of \n' +

#       'hash-based proof-of-work, forming a record that cannot be changed without redoing \n' +

#       'the proof-of-work.   The longest chain not only serves as proof of the sequence of \n' +

#       'events witnessed, but proof that it came from the largest pool of CPU power.   As \n' +

#       'long as a majority of CPU power is controlled by nodes that are not cooperating to',

#     metadata: {

#       source: '/Users/bracesproul/code/lang-chain-ai/langchainjs/examples/src/document_loaders/example_data/bitcoin.pdf',

#       pdf: {

#         version: '1.10.100',

#         info: [Object],

#         metadata: null,

#         totalPages: 9

#       },

#       loc: { pageNumber: 1, lines: [Object] }

#     },

#     id: undefined

#   }


"""
## API reference

For detailed documentation of all PDFLoader features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_community_document_loaders_fs_pdf.PDFLoader.html
"""



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/file_loaders/pptx.mdx
================================================
# PPTX files

This example goes over how to load data from PPTX files. By default, one document will be created for all pages in the PPTX file.

## Setup

```bash npm2yarn
npm install officeparser
```

## Usage, one document per page

```typescript
import { PPTXLoader } from "@langchain/community/document_loaders/fs/pptx";

const loader = new PPTXLoader("src/document_loaders/example_data/example.pptx");

const docs = await loader.load();
```



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/file_loaders/subtitles.mdx
================================================
---
hide_table_of_contents: true
---

# Subtitles

This example goes over how to load data from subtitle files. One document will be created for each subtitles file.

## Setup

```bash npm2yarn
npm install srt-parser-2
```

## Usage

```typescript
import { SRTLoader } from "@langchain/community/document_loaders/fs/srt";

const loader = new SRTLoader(
  "src/document_loaders/example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.srt"
);

const docs = await loader.load();
```



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/file_loaders/text.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: TextLoader
sidebar_class_name: node-only
---
"""

"""
# TextLoader

```{=mdx}

:::tip Compatibility

Only available on Node.js.

:::

```

This notebook provides a quick overview for getting started with `TextLoader` [document loaders](/docs/concepts/document_loaders). For detailed documentation of all `TextLoader` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.document_loaders_fs_text.TextLoader.html).

## Overview
### Integration details

| Class | Package | Compatibility | Local | PY support | 
| :--- | :--- | :---: | :---: |  :---: |
| [TextLoader](https://api.js.langchain.com/classes/langchain.document_loaders_fs_text.TextLoader.html) | [langchain](https://api.js.langchain.com/modules/langchain.document_loaders_fs_text.html) | Node-only | ✅ | ❌ |

## Setup

To access `TextLoader` document loader you'll need to install the `langchain` package.

### Installation

The LangChain TextLoader integration lives in the `langchain` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  langchain
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and load documents:
"""

import { TextLoader } from "langchain/document_loaders/fs/text"

const loader = new TextLoader("../../../../../../examples/src/document_loaders/example_data/example.txt")

"""
## Load
"""

const docs = await loader.load()
docs[0]
# Output:
#   Document {

#     pageContent: 'Foo\nBar\nBaz\n\n',

#     metadata: {

#       source: '../../../../../../examples/src/document_loaders/example_data/example.txt'

#     },

#     id: undefined

#   }


console.log(docs[0].metadata)
# Output:
#   {

#     source: '../../../../../../examples/src/document_loaders/example_data/example.txt'

#   }


"""
## API reference

For detailed documentation of all TextLoader features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain.document_loaders_fs_text.TextLoader.html
"""



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/file_loaders/unstructured.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Unstructured
sidebar_class_name: node-only
---
"""

"""
# UnstructuredLoader

```{=mdx}

:::tip Compatibility

Only available on Node.js.

:::

```

This notebook provides a quick overview for getting started with `UnstructuredLoader` [document loaders](/docs/concepts/document_loaders). For detailed documentation of all `UnstructuredLoader` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_document_loaders_fs_unstructured.UnstructuredLoader.html).

## Overview
### Integration details

| Class | Package | Compatibility | Local | [PY support](https://python.langchain.com/docs/integrations/document_loaders/unstructured_file) | 
| :--- | :--- | :---: | :---: |  :---: |
| [UnstructuredLoader](https://api.js.langchain.com/classes/langchain_community_document_loaders_fs_unstructured.UnstructuredLoader.html) | [@langchain/community](https://api.js.langchain.com/modules/langchain_community_document_loaders_fs_unstructured.html) | Node-only | ✅ | ✅ |

## Setup

To access `UnstructuredLoader` document loader you'll need to install the `@langchain/community` integration package, and create an Unstructured account and get an API key.

### Local

You can run Unstructured locally in your computer using Docker. To do so, you need to have Docker installed. You can find the instructions to install Docker [here](https://docs.docker.com/get-docker/).

```bash
docker run -p 8000:8000 -d --rm --name unstructured-api downloads.unstructured.io/unstructured-io/unstructured-api:latest --port 8000 --host 0.0.0.0
```

### Credentials

Head to [unstructured.io](https://unstructured.io/api-key-hosted) to sign up to Unstructured and generate an API key. Once you've done this set the `UNSTRUCTURED_API_KEY` environment variable:

```bash
export UNSTRUCTURED_API_KEY="your-api-key"
```

### Installation

The LangChain UnstructuredLoader integration lives in the `@langchain/community` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and load documents:
"""

import { UnstructuredLoader } from "@langchain/community/document_loaders/fs/unstructured"

const loader = new UnstructuredLoader("../../../../../../examples/src/document_loaders/example_data/notion.md")

"""
## Load
"""

const docs = await loader.load()
docs[0]
# Output:
#   Document {

#     pageContent: '# Testing the notion markdownloader',

#     metadata: {

#       filename: 'notion.md',

#       languages: [ 'eng' ],

#       filetype: 'text/plain',

#       category: 'NarrativeText'

#     },

#     id: undefined

#   }


console.log(docs[0].metadata)
# Output:
#   {

#     filename: 'notion.md',

#     languages: [ 'eng' ],

#     filetype: 'text/plain',

#     category: 'NarrativeText'

#   }


"""
## Directories

You can also load all of the files in the directory using [`UnstructuredDirectoryLoader`](https://api.js.langchain.com/classes/langchain.document_loaders_fs_unstructured.UnstructuredDirectoryLoader.html), which inherits from [`DirectoryLoader`](/docs/integrations/document_loaders/file_loaders/directory):

"""

import { UnstructuredDirectoryLoader } from "@langchain/community/document_loaders/fs/unstructured";

const directoryLoader = new UnstructuredDirectoryLoader(
  "../../../../../../examples/src/document_loaders/example_data/",
  {}
);
const directoryDocs = await directoryLoader.load();
console.log("directoryDocs.length: ", directoryDocs.length);
console.log(directoryDocs[0])

# Output:
#   Unknown file type: Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.srt

#   Unknown file type: test.mp3

#   directoryDocs.length:  247

#   Document {

#     pageContent: 'Bitcoin: A Peer-to-Peer Electronic Cash System',

#     metadata: {

#       filetype: 'application/pdf',

#       languages: [ 'eng' ],

#       page_number: 1,

#       filename: 'bitcoin.pdf',

#       category: 'Title'

#     },

#     id: undefined

#   }


"""
## API reference

For detailed documentation of all UnstructuredLoader features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_community_document_loaders_fs_unstructured.UnstructuredLoader.html
"""



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/airtable.mdx
================================================
---
hide_table_of_contents: true
---

import loadExample from "@examples/document_loaders/airtable_load";
import CodeBlock from "@theme/CodeBlock";

# AirtableLoader

The `AirtableLoader` class provides functionality to load documents from Airtable tables. It supports two main methods:

1. `load()`: Retrieves all records at once, ideal for small to moderate datasets.
2. `loadLazy()`: Fetches records one by one, which is more memory-efficient for large datasets.

## Prerequisites

Ensure that your Airtable API token is available as an environment variable:

```typescript
process.env.AIRTABLE_API_TOKEN = "YOUR_AIRTABLE_API_TOKEN";
```

## Usage

<CodeBlock language="typescript">{loadExample}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/apify_dataset.mdx
================================================
---
hide_table_of_contents: true
sidebar_class_name: node-only
---

# Apify Dataset

This guide shows how to use [Apify](https://apify.com) with LangChain to load documents from an Apify Dataset.

## Overview

[Apify](https://apify.com) is a cloud platform for web scraping and data extraction,
which provides an [ecosystem](https://apify.com/store) of more than two thousand
ready-made apps called _Actors_ for various web scraping, crawling, and data extraction use cases.

This guide shows how to load documents
from an [Apify Dataset](https://docs.apify.com/platform/storage/dataset) — a scalable append-only
storage built for storing structured web scraping results,
such as a list of products or Google SERPs, and then export them to various
formats like JSON, CSV, or Excel.

Datasets are typically used to save results of different Actors.
For example, [Website Content Crawler](https://apify.com/apify/website-content-crawler) Actor
deeply crawls websites such as documentation, knowledge bases, help centers, or blogs,
and then stores the text content of webpages into a dataset,
from which you can feed the documents into a vector database and use it for information retrieval.
Another example is the [RAG Web Browser](https://apify.com/apify/rag-web-browser) Actor,
which queries Google Search, scrapes the top N pages from the results, and returns the cleaned
content in Markdown format for further processing by a large language model.

## Setup

You'll first need to install the official Apify client:

```bash npm2yarn
npm install apify-client
```

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install hnswlib-node @langchain/openai @langchain/community @langchain/core
```

You'll also need to sign up and retrieve your [Apify API token](https://console.apify.com/settings/integrations).

## Usage

### From a New Dataset (Crawl a Website and Store the data in Apify Dataset)

If you don't already have an existing dataset on the Apify platform, you'll need to initialize the document loader by calling an Actor and waiting for the results.
In the example below, we use the [Website Content Crawler](https://apify.com/apify/website-content-crawler) Actor to crawl
LangChain documentation, store the results in Apify Dataset, and then load the dataset using the `ApifyDatasetLoader`.
For this demonstration, we'll use a fast Cheerio crawler type and limit the number of crawled pages to 10.

**Note:** Running the Website Content Crawler may take some time, depending on the size of the website. For large sites, it can take several hours or even days!

Here's an example:

import CodeBlock from "@theme/CodeBlock";
import NewExample from "@examples/document_loaders/apify_dataset_new.ts";

<CodeBlock language="typescript">{NewExample}</CodeBlock>

## From an Existing Dataset

If you've already run an Actor and have an existing dataset on the Apify platform, you can initialize the document loader directly using the constructor

import ExistingExample from "@examples/document_loaders/apify_dataset_existing.ts";

<CodeBlock language="typescript">{ExistingExample}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/assemblyai_audio_transcription.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# AssemblyAI Audio Transcript

This covers how to load audio (and video) transcripts as document objects from a file using the [AssemblyAI API](https://www.assemblyai.com/docs/api-reference/transcripts/submit?utm_source=langchainjs).

## Usage

First, you'll need to install the official AssemblyAI package:

```bash npm2yarn
npm install @langchain/community @langchain/core assemblyai
```

To use the loaders you need an [AssemblyAI account](https://www.assemblyai.com/dashboard/signup?utm_source=langchainjs) and
[get your AssemblyAI API key from the dashboard](https://www.assemblyai.com/app/account?utm_source=langchainjs).

Then, configure the API key as the `ASSEMBLYAI_API_KEY` environment variable or the `apiKey` options parameter.

import TranscriptExample from "@examples/document_loaders/assemblyai_audio_transcription.ts";

<CodeBlock language="typescript">{TranscriptExample}</CodeBlock>

> ** info **
>
> - You can use the `AudioTranscriptParagraphsLoader` or `AudioTranscriptSentencesLoader` to split the transcript into paragraphs or sentences.
> - The `audio` parameter can be a URL, a local file path, a buffer, or a stream.
> - The `audio` can also be a video file. See the [list of supported file types in the FAQ doc](https://www.assemblyai.com/docs/concepts/faq?utm_source=langchainjs#:~:text=file%20types%20are%20supported).
> - If you don't pass in the `apiKey` option, the loader will use the `ASSEMBLYAI_API_KEY` environment variable.
> - You can add more properties in addition to `audio`. Find the full list of request parameters in the [AssemblyAI API docs](https://www.assemblyai.com/docs/api-reference/transcripts/submit?utm_source=langchainjs#create-a-transcript).

You can also use the `AudioSubtitleLoader` to get `srt` or `vtt` subtitles as a document.

import SubtitleExample from "@examples/document_loaders/assemblyai_subtitles.ts";

<CodeBlock language="typescript">{SubtitleExample}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/azure_blob_storage_container.mdx
================================================
---
hide_table_of_contents: true
sidebar_class_name: node-only
---

# Azure Blob Storage Container

:::tip Compatibility
Only available on Node.js.
:::

This covers how to load a container on Azure Blob Storage into LangChain documents.

## Setup

To run this loader, you'll need to have Unstructured already set up and ready to use at an available URL endpoint. It can also be configured to run locally.

See the docs [here](/docs/integrations/document_loaders/file_loaders/unstructured) for information on how to do that.

You'll also need to install the official Azure Storage Blob client library:

```bash npm2yarn
npm install @langchain/community @langchain/core @azure/storage-blob
```

## Usage

Once Unstructured is configured, you can use the Azure Blob Storage Container loader to load files and then convert them into a Document.

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/document_loaders/azure_blob_storage_container.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/azure_blob_storage_file.mdx
================================================
---
hide_table_of_contents: true
sidebar_class_name: node-only
---

# Azure Blob Storage File

:::tip Compatibility
Only available on Node.js.
:::

This covers how to load an Azure File into LangChain documents.

## Setup

To use this loader, you'll need to have Unstructured already set up and ready to use at an available URL endpoint. It can also be configured to run locally.

See the docs [here](/docs/integrations/document_loaders/file_loaders/unstructured) for information on how to do that.

You'll also need to install the official Azure Storage Blob client library:

```bash npm2yarn
npm install @langchain/community @langchain/core @azure/storage-blob
```

## Usage

Once Unstructured is configured, you can use the Azure Blob Storage File loader to load files and then convert them into a Document.

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/document_loaders/azure_blob_storage_file.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/browserbase.mdx
================================================
# Browserbase Loader

## Description

[Browserbase](https://browserbase.com) is a developer platform to reliably run, manage, and monitor headless browsers.

Power your AI data retrievals with:

- [Serverless Infrastructure](https://docs.browserbase.com/under-the-hood) providing reliable browsers to extract data from complex UIs
- [Stealth Mode](https://docs.browserbase.com/features/stealth-mode) with included fingerprinting tactics and automatic captcha solving
- [Session Debugger](https://docs.browserbase.com/features/sessions) to inspect your Browser Session with networks timeline and logs
- [Live Debug](https://docs.browserbase.com/guides/session-debug-connection/browser-remote-control) to quickly debug your automation

## Installation

- Get an API key and Project ID from [browserbase.com](https://browserbase.com) and set it in environment variables (`BROWSERBASE_API_KEY`, `BROWSERBASE_PROJECT_ID`).
- Install the [Browserbase SDK](http://github.com/browserbase/js-sdk):

```bash npm2yarn
npm i @langchain/community @langchain/core @browserbasehq/sdk
```

## Example

Utilize the `BrowserbaseLoader` as follows to allow your agent to load websites:

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/document_loaders/browserbase.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## Arguments

- `urls`: Required. List of URLs to load.

## Options

- `textContent` Retrieve only text content. Default is `false`.
- `sessionId` Optional. Provide an existing Session ID.
- `proxy` Optional. Enable/Disable Proxies.



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/college_confidential.mdx
================================================
---
hide_table_of_contents: true
---

# College Confidential

This example goes over how to load data from the college confidential website, using Cheerio. One document will be created for each page.

## Setup

```bash npm2yarn
npm install @langchain/community @langchain/core cheerio
```

## Usage

```typescript
import { CollegeConfidentialLoader } from "@langchain/community/document_loaders/web/college_confidential";

const loader = new CollegeConfidentialLoader(
  "https://www.collegeconfidential.com/colleges/brown-university/"
);

const docs = await loader.load();
```



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/confluence.mdx
================================================
---
sidebar_class_name: node-only
---

# Confluence

:::tip Compatibility
Only available on Node.js.
:::

This covers how to load document objects from pages in a Confluence space.

## Credentials

- You'll need to set up an access token and provide it along with your confluence username in order to authenticate the request
- You'll also need the `space key` for the space containing the pages to load as documents. This can be found in the url when navigating to your space e.g. `https://example.atlassian.net/wiki/spaces/{SPACE_KEY}`
- And you'll need to install `html-to-text` to parse the pages into plain text

```bash npm2yarn
npm install @langchain/community @langchain/core html-to-text
```

## Usage

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/document_loaders/confluence.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/couchbase.mdx
================================================
---
hide_table_of_contents: true
sidebar_class_name: node-only
---

# Couchbase

[Couchbase](http://couchbase.com/) is an award-winning distributed NoSQL cloud database that delivers unmatched versatility, performance, scalability, and financial value for all of your cloud, mobile, AI, and edge computing applications.

This guide shows how to use load documents from couchbase database.

# Installation

```bash npm2yarn
npm install @langchain/community @langchain/core couchbase
```

## Usage

### Querying for Documents from Couchbase

For more details on connecting to a Couchbase cluster, please check the [Node.js SDK documentation](https://docs.couchbase.com/nodejs-sdk/current/howtos/managing-connections.html#connection-strings).

For help with querying for documents using SQL++ (SQL for JSON), please check the [documentation](https://docs.couchbase.com/server/current/n1ql/n1ql-language-reference/index.html).

```typescript
import { CouchbaseDocumentLoader } from "@langchain/community/document_loaders/web/couchbase";
import { Cluster } from "couchbase";

const connectionString = "couchbase://localhost"; // valid couchbase connection string
const dbUsername = "Administrator"; // valid database user with read access to the bucket being queried
const dbPassword = "Password"; // password for the database user

// query is a valid SQL++ query
const query = `
    SELECT h.* FROM \`travel-sample\`.inventory.hotel h 
    WHERE h.country = 'United States'
    LIMIT 1
`;
```

### Connect to Couchbase Cluster

```typescript
const couchbaseClient = await Cluster.connect(connectionString, {
  username: dbUsername,
  password: dbPassword,
  configProfile: "wanDevelopment",
});
```

### Create the Loader

```typescript
const loader = new CouchbaseDocumentLoader(
  couchbaseClient, // The connected couchbase cluster client
  query // A valid SQL++ query which will return the required data
);
```

### Load Documents

You can fetch the documents by calling the `load` method of the loader. It will return a list with all the documents. If you want to avoid this blocking call, you can call `lazy_load` method that returns an Iterator.

```typescript
// using load method
docs = await loader.load();
console.log(docs);
```

```typescript
// using lazy_load
for await (const doc of this.lazyLoad()) {
  console.log(doc);
  break; // break based on required condition
}
```

### Specifying Fields with Content and Metadata

The fields that are part of the Document content can be specified using the `pageContentFields` parameter.
The metadata fields for the Document can be specified using the `metadataFields` parameter.

```typescript
const loaderWithSelectedFields = new CouchbaseDocumentLoader(
  couchbaseClient,
  query,
  // pageContentFields
  [
    "address",
    "name",
    "city",
    "phone",
    "country",
    "geo",
    "description",
    "reviews",
  ],
  ["id"] // metadataFields
);

const filtered_docs = await loaderWithSelectedFields.load();
console.log(filtered_docs);
```



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/figma.mdx
================================================
---
hide_table_of_contents: true
---

# Figma

This example goes over how to load data from a Figma file.
You will need a Figma access token in order to get started.

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/document_loaders/figma.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

You can find your Figma file's key and node ids by opening the file in your browser and extracting them from the URL:

```
https://www.figma.com/file/<YOUR FILE KEY HERE>/LangChainJS-Test?type=whiteboard&node-id=<YOUR NODE ID HERE>&t=e6lqWkKecuYQRyRg-0
```



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/firecrawl.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: FireCrawl

---
"""

"""
# FireCrawlLoader

This notebook provides a quick overview for getting started with [FireCrawlLoader](/docs/integrations/document_loaders/). For detailed documentation of all FireCrawlLoader features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_document_loaders_web_firecrawl.FireCrawlLoader.html).

## Overview
### Integration details

| Class | Package | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/document_loaders/firecrawl)|
| :--- | :--- | :---: | :---: |  :---: |
| [FireCrawlLoader](https://api.js.langchain.com/classes/langchain_community_document_loaders_web_firecrawl.FireCrawlLoader.html) | [@langchain/community](https://api.js.langchain.com/modules/langchain_community_document_loaders_web_firecrawl.html) | 🟠 (see details below) | beta | ✅ | 
### Loader features
| Source | Web Loader | Node Envs Only
| :---: | :---: | :---: | 
| FireCrawlLoader | ✅ | ❌ | 

[FireCrawl](https://firecrawl.dev) crawls and convert any website into LLM-ready data. It crawls all accessible sub-pages and give you clean markdown and metadata for each. No sitemap required.

FireCrawl handles complex tasks such as reverse proxies, caching, rate limits, and content blocked by JavaScript. Built by the [mendable.ai](https://mendable.ai) team.

This guide shows how to scrap and crawl entire websites and load them using the `FireCrawlLoader` in LangChain.

## Setup

To access `FireCrawlLoader` document loader you'll need to install the `@langchain/community` integration, and the `@mendable/firecrawl-js@0.0.36` package. Then create a **[FireCrawl](https://firecrawl.dev)** account and get an API key.

### Credentials

Sign up and get your free [FireCrawl API key](https://firecrawl.dev) to start. FireCrawl offers 300 free credits to get you started, and it's [open-source](https://github.com/mendableai/firecrawl) in case you want to self-host.

Once you've done this set the `FIRECRAWL_API_KEY` environment variable:

```bash
export FIRECRAWL_API_KEY="your-api-key"
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain FireCrawlLoader integration lives in the `@langchain/community` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core @mendable/firecrawl-js@0.0.36
</Npm2Yarn>

```
"""

"""
## Instantiation

Here's an example of how to use the `FireCrawlLoader` to load web search results:

Firecrawl offers 3 modes: `scrape`, `crawl`, and `map`. In `scrape` mode, Firecrawl will only scrape the page you provide. In `crawl` mode, Firecrawl will crawl the entire website. In `map` mode, Firecrawl will return semantic links related to the website.

The `formats` (`scrapeOptions.formats` for `crawl` mode) parameter allows selection from `"markdown"`, `"html"`, or `"rawHtml"`. However, the Loaded Document will return content in only one format, prioritizing as follows: `markdown`, then `html`, and finally `rawHtml`.

Now we can instantiate our model object and load documents:
"""

import "@mendable/firecrawl-js";
import { FireCrawlLoader } from "@langchain/community/document_loaders/web/firecrawl"

const loader = new FireCrawlLoader({
  url: "https://firecrawl.dev", // The URL to scrape
  apiKey: "...", // Optional, defaults to `FIRECRAWL_API_KEY` in your env.
  mode: "scrape", // The mode to run the crawler in. Can be "scrape" for single urls or "crawl" for all accessible subpages
  params: {
    // optional parameters based on Firecrawl API docs
    // For API documentation, visit https://docs.firecrawl.dev
  },
})

"""
## Load
"""

const docs = await loader.load()
docs[0]
# Output:
#   Document {

#     pageContent: [32m"Introducing [Smart Crawl!](https://www.firecrawl.dev/smart-crawl)\n"[39m +

#       [32m" Join the waitlist to turn any web"[39m... 18721 more characters,

#     metadata: {

#       title: [32m"Home - Firecrawl"[39m,

#       description: [32m"Firecrawl crawls and converts any website into clean markdown."[39m,

#       keywords: [32m"Firecrawl,Markdown,Data,Mendable,Langchain"[39m,

#       robots: [32m"follow, index"[39m,

#       ogTitle: [32m"Firecrawl"[39m,

#       ogDescription: [32m"Turn any website into LLM-ready data."[39m,

#       ogUrl: [32m"https://www.firecrawl.dev/"[39m,

#       ogImage: [32m"https://www.firecrawl.dev/og.png?123"[39m,

#       ogLocaleAlternate: [],

#       ogSiteName: [32m"Firecrawl"[39m,

#       sourceURL: [32m"https://firecrawl.dev"[39m,

#       pageStatusCode: [33m500[39m

#     },

#     id: [90mundefined[39m

#   }

console.log(docs[0].metadata)
# Output:
#   {

#     title: "Home - Firecrawl",

#     description: "Firecrawl crawls and converts any website into clean markdown.",

#     keywords: "Firecrawl,Markdown,Data,Mendable,Langchain",

#     robots: "follow, index",

#     ogTitle: "Firecrawl",

#     ogDescription: "Turn any website into LLM-ready data.",

#     ogUrl: "https://www.firecrawl.dev/",

#     ogImage: "https://www.firecrawl.dev/og.png?123",

#     ogLocaleAlternate: [],

#     ogSiteName: "Firecrawl",

#     sourceURL: "https://firecrawl.dev",

#     pageStatusCode: 500

#   }


"""
## Additional Parameters

For `params` you can pass any of the params according to the [Firecrawl documentation](https://docs.firecrawl.dev).
"""

"""
## API reference

For detailed documentation of all FireCrawlLoader features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_community_document_loaders_web_firecrawl.FireCrawlLoader.html
"""



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/gitbook.mdx
================================================
---
hide_table_of_contents: true
---

# GitBook

This example goes over how to load data from any GitBook, using Cheerio. One document will be created for each page.

## Setup

```bash npm2yarn
npm install @langchain/community @langchain/core cheerio
```

## Load from single GitBook page

```typescript
import { GitbookLoader } from "@langchain/community/document_loaders/web/gitbook";

const loader = new GitbookLoader(
  "https://docs.gitbook.com/product-tour/navigation"
);

const docs = await loader.load();
```

## Load from all paths in a given GitBook

For this to work, the GitbookLoader needs to be initialized with the root path (https://docs.gitbook.com in this example) and have `shouldLoadAllPaths` set to `true`.

```typescript
import { GitbookLoader } from "@langchain/community/document_loaders/web/gitbook";

const loader = new GitbookLoader("https://docs.gitbook.com", {
  shouldLoadAllPaths: true,
});

const docs = await loader.load();
```



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/github.mdx
================================================
---
sidebar_class_name: node-only
hide_table_of_contents: true
---

# GitHub

This example goes over how to load data from a GitHub repository.
You can set the `GITHUB_ACCESS_TOKEN` environment variable to a GitHub access token to increase the rate limit and access private repositories.

## Setup

The GitHub loader requires the [ignore npm package](https://www.npmjs.com/package/ignore) as a peer dependency. Install it like this:

```bash npm2yarn
npm install @langchain/community @langchain/core ignore
```

## Usage

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/document_loaders/github.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

The loader will ignore binary files like images.

### Using .gitignore Syntax

To ignore specific files, you can pass in an `ignorePaths` array into the constructor:

import IgnoreExample from "@examples/document_loaders/github_ignore_paths.ts";

<CodeBlock language="typescript">{IgnoreExample}</CodeBlock>

### Using a Different GitHub Instance

You may want to target a different GitHub instance than `github.com`, e.g. if you have a GitHub Enterprise instance for your company.
For this you need two additional parameters:

- `baseUrl` - the base URL of your GitHub instance, so the githubUrl matches `<baseUrl>/<owner>/<repo>/...`
- `apiUrl` - the URL of the API endpoint of your GitHub instance

import CustomInstanceExample from "@examples/document_loaders/github_custom_instance.ts";

<CodeBlock language="typescript">{CustomInstanceExample}</CodeBlock>

### Dealing with Submodules

In case your repository has submodules, you have to decide if the loader should follow them or not. You can control this with the boolean `processSubmodules` parameter. By default, submodules are not processed.
Note that processing submodules works only in conjunction with setting the `recursive` parameter to true.

import SubmodulesExample from "@examples/document_loaders/github_submodules.ts";

<CodeBlock language="typescript">{SubmodulesExample}</CodeBlock>

Note, that the loader will not follow submodules which are located on another GitHub instance than the one of the current repository.

### Stream large repository

For situations where processing large repositories in a memory-efficient manner is required. You can use the `loadAsStream` method to asynchronously streams documents from the entire GitHub repository.

import StreamExample from "@examples/document_loaders/github_stream.ts";

<CodeBlock language="typescript">{StreamExample}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/google_cloud_storage.mdx
================================================
---
hide_table_of_contents: true
sidebar_class_name: node-only
---

# Google Cloud Storage

:::tip Compatibility
Only available on Node.js.
:::

This covers how to load a Google Cloud Storage File into LangChain documents.

## Setup

To use this loader, you'll need to have Unstructured already set up and ready to use at an available URL endpoint. It can also be configured to run locally.

See the docs [here](/docs/integrations/document_loaders/file_loaders/unstructured) for information on how to do that.

You'll also need to install the official Google Cloud Storage SDK:

```bash npm2yarn
npm install @langchain/community @langchain/core @google-cloud/storage
```

## Usage

Once Unstructured is configured, you can use the Google Cloud Storage loader to load files and then convert them into a Document.

In addition, you can optionally provide a `storageOptions` parameter to specify not only your storage options but also other authentication ways if you don't want Application Default Credentials(ADC) as default manner.

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/document_loaders/google_cloud_storage.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/google_cloudsql_pg.mdx
================================================
# Google Cloud SQL for PostgreSQL

[Cloud SQL](https://cloud.google.com/sql) is a fully managed relational database service that offers high
performance, seamless integration, and impressive scalability and offers database engines such as PostgreSQL.

This guide provides a quick overview of how to use Cloud SQL for PostgreSQL to load Documents with the `PostgresLoader` class.

## Overview

### Before you begin

In order to use this package, you first need to go through the following steps:

1.  [Select or create a Cloud Platform project.](https://developers.google.com/workspace/guides/create-project)
2.  [Enable billing for your project.](https://cloud.google.com/billing/docs/how-to/modify-project#enable_billing_for_a_project)
3.  [Enable the Cloud SQL Admin API.](https://console.cloud.google.com/flows/enableapi?apiid=sqladmin.googleapis.com)
4.  [Setup Authentication.](https://cloud.google.com/docs/authentication)
5.  [Create a CloudSQL instance](https://cloud.google.com/sql/docs/postgres/connect-instance-auth-proxy#create-instance)
6.  [Create a CloudSQL database](https://cloud.google.com/sql/docs/postgres/create-manage-databases)
7.  [Add a user to the database](https://cloud.google.com/sql/docs/postgres/create-manage-users)

### Authentication

Authenticate locally to your Google Cloud account using the `gcloud auth login` command.

### Set Your Google Cloud Project

Set your Google Cloud project ID to leverage Google Cloud resources locally:

```bash
gcloud config set project YOUR-PROJECT-ID
```

If you don't know your project ID, try the following:

- Run `gcloud config list`.
- Run `gcloud projects list`.
- See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113).

## Setting up a PostgresLoader instance

To use the PostgresLoader class, you'll need to install the `@langchain/google-cloud-sql-pg`
package and then follow the steps bellow.

First, you'll need to log in to your Google Cloud account and set the following environment variables based
on your Google Cloud project; these will be defined based on how you want to configure (fromInstance,
fromEngine, fromEngineArgs) your PostgresEngine instance:

```bash
PROJECT_ID="your-project-id"
REGION="your-project-region" // example: "us-central1"
INSTANCE_NAME="your-instance"
DB_NAME="your-database-name"
DB_USER="your-database-user"
PASSWORD="your-database-password"
```

### Setting up an instance

To instantiate a PostgresLoader, you'll first need to create a database connection through the
PostgresEngine.

```typescript
import {
  PostgresLoader,
  PostgresEngine,
  PostgresEngineArgs,
} from "@langchain/google-cloud-sql-pg";
import * as dotenv from "dotenv";

dotenv.config();

const peArgs: PostgresEngineArgs = {
  user: process.env.DB_USER ?? "",
  password: process.env.PASSWORD ?? "",
};

// PostgresEngine instantiation
const engine: PostgresEngine = await PostgresEngine.fromInstance(
  process.env.PROJECT_ID ?? "",
  process.env.REGION ?? "",
  process.env.INSTANCE_NAME ?? "",
  process.env.DB_NAME ?? "",
  peArgs
);
```

### Load Documents using the table_name argument

The loader returns a list of Documents from the table using the first column as page_content and all other columns as metadata. The default table will have the first column as page_content and the second column as metadata (JSON). Each row becomes a document.

```js
const documentLoaderArgs: PostgresLoaderOptions = {
  tableName: "test_table_custom",
  contentColumns: ["fruit_name", "variety"],
  metadataColumns: [
    "fruit_id",
    "quantity_in_stock",
    "price_per_unit",
    "organic",
  ],
  format: "text",
};

const documentLoaderInstance = await PostgresLoader.initialize(
  PEInstance,
  documentLoaderArgs
);
```

### Load Documents using a SQL query

The query parameter allows users to specify a custom SQL query which can include filters to load specific documents from a database.

```js
const documentLoaderArgs: PostgresLoaderOptions = {
  query: "SELECT * FROM my_fruit_table",
  contentColumns: ["fruit_name", "variety"],
  metadataColumns: [
    "fruit_id",
    "quantity_in_stock",
    "price_per_unit",
    "organic",
  ],
  format: "text",
};

const documentLoaderInstance = await PostgresLoader.initialize(
  PEInstance,
  docucumetLoaderArgs
);
```

### Set page content format

The loader returns a list of Documents, with one document per row, with page content in specified string format, i.e. text (space separated concatenation), JSON, YAML, CSV, etc. JSON and YAML formats include headers, while text and CSV do not include field headers.



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/hn.mdx
================================================
---
hide_table_of_contents: true
---

# Hacker News

This example goes over how to load data from the hacker news website, using Cheerio. One document will be created for each page.

## Setup

```bash npm2yarn
npm install @langchain/community @langchain/core cheerio
```

## Usage

```typescript
import { HNLoader } from "@langchain/community/document_loaders/web/hn";

const loader = new HNLoader("https://news.ycombinator.com/item?id=34817881");

const docs = await loader.load();
```



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/imsdb.mdx
================================================
---
hide_table_of_contents: true
---

# IMSDB

This example goes over how to load data from the internet movie script database website, using Cheerio. One document will be created for each page.

## Setup

```bash npm2yarn
npm install @langchain/community @langchain/core cheerio
```

## Usage

```typescript
import { IMSDBLoader } from "@langchain/community/document_loaders/web/imsdb";

const loader = new IMSDBLoader("https://imsdb.com/scripts/BlacKkKlansman.html");

const docs = await loader.load();
```



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/index.mdx
================================================
---
label: "Web Loaders"
hide_table_of_contents: true
---

# Web Loaders

These loaders are used to load web resources. They do not involve the local file system.

import { IndexTable } from "@theme/FeatureTables";

:::info
If you'd like to write your own document loader, see [this how-to](/docs/how_to/document_loader_custom/). If you'd like to contribute an integration, see [Contributing integrations](/docs/contributing).
:::

## All web loaders

<IndexTable />



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/jira.mdx
================================================
---
sidebar_class_name: node-only
---

# Jira

:::tip Compatibility
Only available on Node.js.
:::

This covers how to load document objects from issues in a Jira projects.

## Credentials

- You'll need to set up an access token and provide it along with your Jira username in order to authenticate the request
- You'll also need the project key and host URL for the project containing the issues to load as documents.

## Usage

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/document_loaders/jira.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/langsmith.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: LangSmith

---
"""

"""
# LangSmithLoader

This notebook provides a quick overview for getting started with the [LangSmithLoader](/docs/integrations/document_loaders/). For detailed documentation of all `LangSmithLoader` features and configurations head to the [API reference](https://api.js.langchain.com/classes/_langchain_core.document_loaders_langsmith.LangSmithLoader.html).

## Overview
### Integration details

| Class | Package | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/document_loaders/langsmith)|
| :--- | :--- | :---: | :---: |  :---: |
| [LangSmithLoader](https://api.js.langchain.com/classes/_langchain_core.document_loaders_langsmith.LangSmithLoader.html) | [@langchain/community](https://api.js.langchain.com/classes/_langchain_core.html) | ✅ | beta | ✅ | 
### Loader features
| Source | Web Loader | Node Envs Only
| :---: | :---: | :---: | 
| LangSmithLoader | ✅ | ❌ | 

## Setup

To access the LangSmith document loader you'll need to install `@langchain/core`, create a [LangSmith](https://langsmith.com/) account and get an API key.

### Credentials

Sign up at https://langsmith.com and generate an API key. Once you've done this set the `LANGSMITH_API_KEY` environment variable:

```bash
export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The `LangSmithLoader` integration lives in the `@langchain/core` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/core
</Npm2Yarn>

```
"""

"""
## Create example dataset

For this example, we'll create a new dataset which we'll use in our document loader.
"""

import { Client as LangSmithClient } from 'langsmith';
import { faker } from "@faker-js/faker";

const lsClient = new LangSmithClient();

const datasetName = "LangSmith Few Shot Datasets Notebook";

const exampleInputs = Array.from({ length: 10 }, (_, i) => ({
  input: faker.lorem.paragraph(),
}));
const exampleOutputs = Array.from({ length: 10 }, (_, i) => ({
  output: faker.lorem.sentence(),
}));
const exampleMetadata = Array.from({ length: 10 }, (_, i) => ({
  companyCatchPhrase: faker.company.catchPhrase(),
}));

await lsClient.deleteDataset({
  datasetName,
})

const dataset = await lsClient.createDataset(datasetName);

const examples = await lsClient.createExamples({
  inputs: exampleInputs,
  outputs: exampleOutputs,
  metadata: exampleMetadata,
  datasetId: dataset.id,
});

import { LangSmithLoader } from "@langchain/core/document_loaders/langsmith"

const loader = new LangSmithLoader({
  datasetName: "LangSmith Few Shot Datasets Notebook",
  // Instead of a datasetName, you can alternatively provide a datasetId
  // datasetId: dataset.id,
  contentKey: "input",
  limit: 5,
  // formatContent: (content) => content,
  // ... other options
})

"""
## Load
"""

const docs = await loader.load()
docs[0]
# Output:
#   {

#     pageContent: 'Conventus supellex aegrotatio termes. Vapulus abscido ubi vita coadunatio modi crapula comparo caecus. Acervus voluptate tergeo pariatur conor argumentum inventore vomito stella.',

#     metadata: {

#       id: 'f1a04800-6f7a-4232-9743-fb5d9029bf1f',

#       created_at: '2024-08-20T17:01:38.984045+00:00',

#       modified_at: '2024-08-20T17:01:38.984045+00:00',

#       name: '#f1a0 @ LangSmith Few Shot Datasets Notebook',

#       dataset_id: '9ccd66e6-e506-478c-9095-3d9e27575a89',

#       source_run_id: null,

#       metadata: {

#         dataset_split: [Array],

#         companyCatchPhrase: 'Integrated solution-oriented secured line'

#       },

#       inputs: {

#         input: 'Conventus supellex aegrotatio termes. Vapulus abscido ubi vita coadunatio modi crapula comparo caecus. Acervus voluptate tergeo pariatur conor argumentum inventore vomito stella.'

#       },

#       outputs: {

#         output: 'Excepturi adeptio spectaculum bis volaticus accusamus.'

#       }

#     }

#   }


console.log(docs[0].metadata)
# Output:
#   {

#     id: 'f1a04800-6f7a-4232-9743-fb5d9029bf1f',

#     created_at: '2024-08-20T17:01:38.984045+00:00',

#     modified_at: '2024-08-20T17:01:38.984045+00:00',

#     name: '#f1a0 @ LangSmith Few Shot Datasets Notebook',

#     dataset_id: '9ccd66e6-e506-478c-9095-3d9e27575a89',

#     source_run_id: null,

#     metadata: {

#       dataset_split: [ 'base' ],

#       companyCatchPhrase: 'Integrated solution-oriented secured line'

#     },

#     inputs: {

#       input: 'Conventus supellex aegrotatio termes. Vapulus abscido ubi vita coadunatio modi crapula comparo caecus. Acervus voluptate tergeo pariatur conor argumentum inventore vomito stella.'

#     },

#     outputs: { output: 'Excepturi adeptio spectaculum bis volaticus accusamus.' }

#   }


console.log(docs[0].metadata.inputs)
# Output:
#   {

#     input: 'Conventus supellex aegrotatio termes. Vapulus abscido ubi vita coadunatio modi crapula comparo caecus. Acervus voluptate tergeo pariatur conor argumentum inventore vomito stella.'

#   }


console.log(docs[0].metadata.outputs)
# Output:
#   { output: 'Excepturi adeptio spectaculum bis volaticus accusamus.' }


console.log(Object.keys(docs[0].metadata))
# Output:
#   [

#     'id',

#     'created_at',

#     'modified_at',

#     'name',

#     'dataset_id',

#     'source_run_id',

#     'metadata',

#     'inputs',

#     'outputs'

#   ]


"""
## API reference

For detailed documentation of all `LangSmithLoader` features and configurations head to the [API reference](https://api.js.langchain.com/classes/_langchain_core.document_loaders_langsmith.LangSmithLoader.html)
"""



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/notionapi.mdx
================================================
---
sidebar_class_name: node-only
hide_table_of_contents: true
---

# Notion API

This guide will take you through the steps required to load documents from Notion pages and databases using the Notion API.

## Overview

Notion is a versatile productivity platform that consolidates note-taking, task management, and data organization tools into one interface.

This document loader is able to take full Notion pages and databases and turn them into a LangChain Documents ready to be integrated into your projects.

## Setup

1. You will first need to install the official Notion client and the [notion-to-md](https://www.npmjs.com/package/notion-to-md) package as peer dependencies:

```bash npm2yarn
npm install @langchain/community @langchain/core @notionhq/client notion-to-md
```

2. Create a [Notion integration](https://www.notion.so/my-integrations) and securely record the Internal Integration Secret (also known as `NOTION_INTEGRATION_TOKEN`).
3. Add a connection to your new integration on your page or database. To do this open your Notion page, go to the settings pips in the top right and scroll down to `Add connections` and select your new integration.
4. Get the `PAGE_ID` or `DATABASE_ID` for the page or database you want to load.

> The 32 char hex in the url path represents the `ID`. For example:

> PAGE_ID: [https://www.notion.so/skarard/LangChain-Notion-API-`b34ca03f219c4420a6046fc4bdfdf7b4`](https://www.notion.so/skarard/LangChain-Notion-API-b34ca03f219c4420a6046fc4bdfdf7b4)

> DATABASE_ID: [https://www.notion.so/skarard/`c393f19c3903440da0d34bf9c6c12ff2`?v=9c70a0f4e174498aa0f9021e0a9d52de](https://www.notion.so/skarard/c393f19c3903440da0d34bf9c6c12ff2?v=9c70a0f4e174498aa0f9021e0a9d52de)

> REGEX: `/(?<!=)[0-9a-f]{32}/`

## Example Usage

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/document_loaders/notionapi.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/pdf.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: PDF files

---
"""

"""
# WebPDFLoader

This notebook provides a quick overview for getting started with [WebPDFLoader](/docs/integrations/document_loaders/). For detailed documentation of all WebPDFLoader features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_document_loaders_web_pdf.WebPDFLoader.html).

## Overview
### Integration details

| Class | Package | Local | Serializable | PY support |
| :--- | :--- | :---: | :---: |  :---: |
| [WebPDFLoader](https://api.js.langchain.com/classes/langchain_community_document_loaders_web_pdf.WebPDFLoader.html) | [@langchain/community](https://api.js.langchain.com/modules/langchain_community_document_loaders_web_pdf.html) | ✅ | beta | ❌ | 
### Loader features
| Source | Web Loader | Node Envs Only
| :---: | :---: | :---: | 
| WebPDFLoader | ✅ | ❌ | 

You can use this version of the popular PDFLoader in web environments.
By default, one document will be created for each page in the PDF file, you can change this behavior by setting the `splitPages` option to `false`.

## Setup

To access `WebPDFLoader` document loader you'll need to install the `@langchain/community` integration, along with the `pdf-parse` package:

### Credentials

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain WebPDFLoader integration lives in the `@langchain/community` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core pdf-parse
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and load documents:
"""

import fs from "fs/promises";
import { WebPDFLoader } from "@langchain/community/document_loaders/web/pdf"

const nike10kPDFPath = "../../../../data/nke-10k-2023.pdf";

// Read the file as a buffer
const buffer = await fs.readFile(nike10kPDFPath);

// Create a Blob from the buffer
const nike10kPDFBlob = new Blob([buffer], { type: 'application/pdf' });

const loader = new WebPDFLoader(nike10kPDFBlob, {
  // required params = ...
  // optional params = ...
})

"""
## Load
"""

const docs = await loader.load()
docs[0]
# Output:
#   Document {

#     pageContent: 'Table of Contents\n' +

#       'UNITED STATES\n' +

#       'SECURITIES AND EXCHANGE COMMISSION\n' +

#       'Washington, D.C. 20549\n' +

#       'FORM 10-K\n' +

#       '(Mark One)\n' +

#       '☑ ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934\n' +

#       'FOR THE FISCAL YEAR ENDED MAY 31, 2023\n' +

#       'OR\n' +

#       '☐ TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934\n' +

#       'FOR THE TRANSITION PERIOD FROM                         TO                         .\n' +

#       'Commission File No. 1-10635\n' +

#       'NIKE, Inc.\n' +

#       '(Exact name of Registrant as specified in its charter)\n' +

#       'Oregon93-0584541\n' +

#       '(State or other jurisdiction of incorporation)(IRS Employer Identification No.)\n' +

#       'One Bowerman Drive, Beaverton, Oregon 97005-6453\n' +

#       '(Address of principal executive offices and zip code)\n' +

#       '(503) 671-6453\n' +

#       "(Registrant's telephone number, including area code)\n" +

#       'SECURITIES REGISTERED PURSUANT TO SECTION 12(B) OF THE ACT:\n' +

#       'Class B Common StockNKENew York Stock Exchange\n' +

#       '(Title of each class)(Trading symbol)(Name of each exchange on which registered)\n' +

#       'SECURITIES REGISTERED PURSUANT TO SECTION 12(G) OF THE ACT:\n' +

#       'NONE\n' +

#       'Indicate by check mark:YESNO\n' +

#       '•if the registrant is a well-known seasoned issuer, as defined in Rule 405 of the Securities Act.þ ̈\n' +

#       '•if the registrant is not required to file reports pursuant to Section 13 or Section 15(d) of the Act. ̈þ\n' +

#       '•whether the registrant (1) has filed all reports required to be filed by Section 13 or 15(d) of the Securities Exchange Act of 1934 during the preceding\n' +

#       '12 months (or for such shorter period that the registrant was required to file such reports), and (2) has been subject to such filing requirements for the\n' +

#       'past 90 days.\n' +

#       'þ ̈\n' +

#       '•whether the registrant has submitted electronically every Interactive Data File required to be submitted pursuant to Rule 405 of Regulation S-T\n' +

#       '(§232.405 of this chapter) during the preceding 12 months (or for such shorter period that the registrant was required to submit such files).\n' +

#       'þ ̈\n' +

#       '•whether the registrant is a large accelerated filer, an accelerated filer, a non-accelerated filer, a smaller reporting company or an emerging growth company. See the definitions of “large accelerated filer,”\n' +

#       '“accelerated filer,” “smaller reporting company,” and “emerging growth company” in Rule 12b-2 of the Exchange Act.\n' +

#       'Large accelerated filerþAccelerated filer☐Non-accelerated filer☐Smaller reporting company☐Emerging growth company☐\n' +

#       '•if an emerging growth company, if the registrant has elected not to use the extended transition period for complying with any new or revised financial\n' +

#       'accounting standards provided pursuant to Section 13(a) of the Exchange Act.\n' +

#       ' ̈\n' +

#       "•whether the registrant has filed a report on and attestation to its management's assessment of the effectiveness of its internal control over financial\n" +

#       'reporting under Section 404(b) of the Sarbanes-Oxley Act (15 U.S.C. 7262(b)) by the registered public accounting firm that prepared or issued its audit\n' +

#       'report.\n' +

#       'þ\n' +

#       '•if securities are registered pursuant to Section 12(b) of the Act, whether the financial statements of the registrant included in the filing reflect the\n' +

#       'correction of an error to previously issued financial statements.\n' +

#       ' ̈\n' +

#       '•whether any of those error corrections are restatements that required a recovery analysis of incentive-based compensation received by any of the\n' +

#       "registrant's executive officers during the relevant recovery period pursuant to § 240.10D-1(b).\n" +

#       ' ̈\n' +

#       '•\n' +

#       'whether the registrant is a shell company (as defined in Rule 12b-2 of the Act).☐þ\n' +

#       "As of November 30, 2022, the aggregate market values of the Registrant's Common Stock held by non-affiliates were:\n" +

#       'Class A$7,831,564,572 \n' +

#       'Class B136,467,702,472 \n' +

#       '$144,299,267,044 ',

#     metadata: {

#       pdf: {

#         version: '1.10.100',

#         info: [Object],

#         metadata: null,

#         totalPages: 107

#       },

#       loc: { pageNumber: 1 }

#     },

#     id: undefined

#   }


console.log(docs[0].metadata)
# Output:
#   {

#     pdf: {

#       version: '1.10.100',

#       info: {

#         PDFFormatVersion: '1.4',

#         IsAcroFormPresent: false,

#         IsXFAPresent: false,

#         Title: '0000320187-23-000039',

#         Author: 'EDGAR Online, a division of Donnelley Financial Solutions',

#         Subject: 'Form 10-K filed on 2023-07-20 for the period ending 2023-05-31',

#         Keywords: '0000320187-23-000039; ; 10-K',

#         Creator: 'EDGAR Filing HTML Converter',

#         Producer: 'EDGRpdf Service w/ EO.Pdf 22.0.40.0',

#         CreationDate: "D:20230720162200-04'00'",

#         ModDate: "D:20230720162208-04'00'"

#       },

#       metadata: null,

#       totalPages: 107

#     },

#     loc: { pageNumber: 1 }

#   }


"""
## Usage, custom `pdfjs` build

By default we use the `pdfjs` build bundled with `pdf-parse`, which is compatible with most environments, including Node.js and modern browsers. If you want to use a more recent version of `pdfjs-dist` or if you want to use a custom build of `pdfjs-dist`, you can do so by providing a custom `pdfjs` function that returns a promise that resolves to the `PDFJS` object.

In the following example we use the "legacy" (see [pdfjs docs](https://github.com/mozilla/pdf.js/wiki/Frequently-Asked-Questions#which-browsersenvironments-are-supported)) build of `pdfjs-dist`, which includes several polyfills not included in the default build.

```{=mdx}
<Npm2Yarn>
  pdfjs-dist
</Npm2Yarn>

```
"""

import { WebPDFLoader } from "@langchain/community/document_loaders/web/pdf";

const blob = new Blob(); // e.g. from a file input

const customBuildLoader = new WebPDFLoader(blob, {
  // you may need to add `.then(m => m.default)` to the end of the import
  // @lc-ts-ignore
  pdfjs: () => import("pdfjs-dist/legacy/build/pdf.js"),
});

"""
## Eliminating extra spaces

PDFs come in many varieties, which makes reading them a challenge. The loader parses individual text elements and joins them together with a space by default, but
if you are seeing excessive spaces, this may not be the desired behavior. In that case, you can override the separator with an empty string like this:
"""

import { WebPDFLoader } from "@langchain/community/document_loaders/web/pdf";

// new Blob(); e.g. from a file input
const eliminatingExtraSpacesLoader = new WebPDFLoader(new Blob(), {
  parsedItemSeparator: "",
});

"""
## API reference

For detailed documentation of all WebPDFLoader features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_community_document_loaders_web_pdf.WebPDFLoader.html
"""



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/recursive_url_loader.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: RecursiveUrlLoader
sidebar_class_name: node-only
---
"""

"""
# RecursiveUrlLoader

```{=mdx}

:::tip Compatibility

Only available on Node.js.

:::

```

This notebook provides a quick overview for getting started with [RecursiveUrlLoader](/docs/integrations/document_loaders/). For detailed documentation of all RecursiveUrlLoader features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_document_loaders_web_recursive_url.RecursiveUrlLoader.html).

## Overview
### Integration details

| Class | Package | Local | Serializable | PY support |
| :--- | :--- | :---: | :---: |  :---: |
| [RecursiveUrlLoader](https://api.js.langchain.com/classes/langchain_community_document_loaders_web_recursive_url.RecursiveUrlLoader.html) | [@langchain/community](https://api.js.langchain.com/modules/langchain_community_document_loaders_web_recursive_url.html) | ✅ | beta | ❌ | 
### Loader features
| Source | Web Loader | Node Envs Only
| :---: | :---: | :---: | 
| RecursiveUrlLoader | ✅ | ✅ | 

When loading content from a website, we may want to process load all URLs on a page.

For example, let's look at the [LangChain.js introduction](/docs/introduction) docs.

This has many interesting child pages that we may want to load, split, and later retrieve in bulk.

The challenge is traversing the tree of child pages and assembling a list!

We do this using the `RecursiveUrlLoader`.

This also gives us the flexibility to exclude some children, customize the extractor, and more.

## Setup

To access `RecursiveUrlLoader` document loader you'll need to install the `@langchain/community` integration, and the [`jsdom`](https://www.npmjs.com/package/jsdom) package.

### Credentials

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain RecursiveUrlLoader integration lives in the `@langchain/community` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core jsdom
</Npm2Yarn>

We also suggest adding a package like [`html-to-text`](https://www.npmjs.com/package/html-to-text) or
[`@mozilla/readability`](https://www.npmjs.com/package/@mozilla/readability) for extracting the raw text from the page.

<Npm2Yarn>
  html-to-text
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and load documents:
"""

import { RecursiveUrlLoader } from "@langchain/community/document_loaders/web/recursive_url"
import { compile } from "html-to-text";

const compiledConvert = compile({ wordwrap: 130 }); // returns (text: string) => string;

const loader = new RecursiveUrlLoader("https://langchain.com/",  {
  extractor: compiledConvert,
  maxDepth: 1,
  excludeDirs: ["/docs/api/"],
})

"""
## Load
"""

const docs = await loader.load()
docs[0]
# Output:
#   {

#     pageContent: '\n' +

#       '/\n' +

#       'Products\n' +

#       '\n' +

#       'LangChain [/langchain]LangSmith [/langsmith]LangGraph [/langgraph]\n' +

#       'Methods\n' +

#       '\n' +

#       'Retrieval [/retrieval]Agents [/agents]Evaluation [/evaluation]\n' +

#       'Resources\n' +

#       '\n' +

#       'Blog [https://blog.langchain.dev/]Case Studies [/case-studies]Use Case Inspiration [/use-cases]Experts [/experts]Changelog\n' +

#       '[https://changelog.langchain.com/]\n' +

#       'Docs\n' +

#       '\n' +

#       'LangChain Docs [https://python.langchain.com/v0.2/docs/introduction/]LangSmith Docs [https://docs.smith.langchain.com/]\n' +

#       'Company\n' +

#       '\n' +

#       'About [/about]Careers [/careers]\n' +

#       'Pricing [/pricing]\n' +

#       'Get a demo [/contact-sales]\n' +

#       'Sign up [https://smith.langchain.com/]\n' +

#       '\n' +

#       '\n' +

#       '\n' +

#       '\n' +

#       'LangChain’s suite of products supports developers along each step of the LLM application lifecycle.\n' +

#       '\n' +

#       '\n' +

#       'APPLICATIONS THAT CAN REASON. POWERED BY LANGCHAIN.\n' +

#       '\n' +

#       'Get a demo [/contact-sales]Sign up for free [https://smith.langchain.com/]\n' +

#       '\n' +

#       '\n' +

#       '\n' +

#       'FROM STARTUPS TO GLOBAL ENTERPRISES,\n' +

#       'AMBITIOUS BUILDERS CHOOSE\n' +

#       'LANGCHAIN PRODUCTS.\n' +

#       '\n' +

#       '[https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65ca3b7c22746faa78338532_logo_Ally.svg][https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65ca3b7c08e67bb7eefba4c2_logo_Rakuten.svg][https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65ca3b7c576fdde32d03c1a0_logo_Elastic.svg][https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65ca3b7c6d5592036dae24e5_logo_BCG.svg][https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667f19528c3557c2c19c3086_the-home-depot-2%201.png][https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65ca3b7cbcf6473519b06d84_logo_IDEO.svg][https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65ca3b7cb5f96dcc100ee3b7_logo_Zapier.svg][https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6606183e52d49bc369acc76c_mdy_logo_rgb_moodysblue.png][https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65ca3b7c8ad7db6ed6ec611e_logo_Adyen.svg][https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65ca3b7c737d50036a62768b_logo_Infor.svg][https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667f59d98444a5f98aabe21c_acxiom-vector-logo-2022%201.png][https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65ca3b7c09a158ffeaab0bd2_logo_Replit.svg][https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65ca3b7c9d2b23d292a0cab0_logo_Retool.svg][https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65ca3b7c44e67a3d0a996bf3_logo_Databricks.svg][https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667f5a1299d6ba453c78a849_image%20(19).png][https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65ca3b7c63af578816bafcc3_logo_Instacart.svg][https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/665dc1dabc940168384d9596_podium%20logo.svg]\n' +

#       '\n' +

#       'Build\n' +

#       '\n' +

#       'LangChain is a framework to build with LLMs by chaining interoperable components. LangGraph is the framework for building\n' +

#       'controllable agentic workflows.\n' +

#       '\n' +

#       '\n' +

#       '\n' +

#       'Run\n' +

#       '\n' +

#       'Deploy your LLM applications at scale with LangGraph Cloud, our infrastructure purpose-built for agents.\n' +

#       '\n' +

#       '\n' +

#       '\n' +

#       'Manage\n' +

#       '\n' +

#       "Debug, collaborate, test, and monitor your LLM app in LangSmith - whether it's built with a LangChain framework or not. \n" +

#       '\n' +

#       '\n' +

#       '\n' +

#       '\n' +

#       'BUILD YOUR APP WITH LANGCHAIN\n' +

#       '\n' +

#       'Build context-aware, reasoning applications with LangChain’s flexible framework that leverages your company’s data and APIs.\n' +

#       'Future-proof your application by making vendor optionality part of your LLM infrastructure design.\n' +

#       '\n' +

#       'Learn more about LangChain\n' +

#       '\n' +

#       '[/langchain]\n' +

#       '\n' +

#       '\n' +

#       'RUN AT SCALE WITH LANGGRAPH CLOUD\n' +

#       '\n' +

#       'Deploy your LangGraph app with LangGraph Cloud for fault-tolerant scalability - including support for async background jobs,\n' +

#       'built-in persistence, and distributed task queues.\n' +

#       '\n' +

#       'Learn more about LangGraph\n' +

#       '\n' +

#       '[/langgraph]\n' +

#       '[https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667c6d7284e58f4743a430e6_Langgraph%20UI-home-2.webp]\n' +

#       '\n' +

#       '\n' +

#       'MANAGE LLM PERFORMANCE WITH LANGSMITH\n' +

#       '\n' +

#       'Ship faster with LangSmith’s debug, test, deploy, and monitoring workflows. Don’t rely on “vibes” – add engineering rigor to your\n' +

#       'LLM-development workflow, whether you’re building with LangChain or not.\n' +

#       '\n' +

#       'Learn more about LangSmith\n' +

#       '\n' +

#       '[/langsmith]\n' +

#       '\n' +

#       '\n' +

#       'HEAR FROM OUR HAPPY CUSTOMERS\n' +

#       '\n' +

#       'LangChain, LangGraph, and LangSmith help teams of all sizes, across all industries - from ambitious startups to established\n' +

#       'enterprises.\n' +

#       '\n' +

#       '[https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65c5308aee06d9826765c897_Retool_logo%201.png]\n' +

#       '\n' +

#       '“LangSmith helped us improve the accuracy and performance of Retool’s fine-tuned models. Not only did we deliver a better product\n' +

#       'by iterating with LangSmith, but we’re shipping new AI features to our users in a fraction of the time it would have taken without\n' +

#       'it.”\n' +

#       '\n' +

#       '[https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65c5308abdd2dbbdde5a94a1_Jamie%20Cuffe.png]\n' +

#       'Jamie Cuffe\n' +

#       'Head of Self-Serve and New Products\n' +

#       '[https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65c5308a04d37cf7d3eb1341_Rakuten_Global_Brand_Logo.png]\n' +

#       '\n' +

#       '“By combining the benefits of LangSmith and standing on the shoulders of a gigantic open-source community, we’re able to identify\n' +

#       'the right approaches of using LLMs in an enterprise-setting faster.”\n' +

#       '\n' +

#       '[https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65c5308a8b6137d44c621cb4_Yusuke%20Kaji.png]\n' +

#       'Yusuke Kaji\n' +

#       'General Manager of AI\n' +

#       '[https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65c5308aea1371b447cc4af9_elastic-ar21.png]\n' +

#       '\n' +

#       '“Working with LangChain and LangSmith on the Elastic AI Assistant had a significant positive impact on the overall pace and\n' +

#       'quality of the development and shipping experience. We couldn’t have achieved  the product experience delivered to our customers\n' +

#       'without LangChain, and we couldn’t have done it at the same pace without LangSmith.”\n' +

#       '\n' +

#       '[https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65c5308a4095d5a871de7479_James%20Spiteri.png]\n' +

#       'James Spiteri\n' +

#       'Director of Security Products\n' +

#       '[https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65c530539f4824b828357352_Logo_de_Fintual%201.png]\n' +

#       '\n' +

#       '“As soon as we heard about LangSmith, we moved our entire development stack onto it. We could have built evaluation, testing and\n' +

#       'monitoring tools in house, but with LangSmith it took us 10x less time to get a 1000x better tool.”\n' +

#       '\n' +

#       '[https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65c53058acbff86f4c2dcee2_jose%20pena.png]\n' +

#       'Jose Peña\n' +

#       'Senior Manager\n' +

#       '\n' +

#       '\n' +

#       '\n' +

#       '\n' +

#       'THE REFERENCE ARCHITECTURE ENTERPRISES ADOPT FOR SUCCESS.\n' +

#       '\n' +

#       'LangChain’s suite of products can be used independently or stacked together for multiplicative impact – guiding you through\n' +

#       'building, running, and managing your LLM apps.\n' +

#       '\n' +

#       '[https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6695b116b0b60c78fd4ef462_15.07.24%20-Updated%20stack%20diagram%20-%20lightfor%20website-3.webp][https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667d392696fc0bc3e17a6d04_New%20LC%20stack%20-%20light-2.webp]\n' +

#       '15M+\n' +

#       'Monthly Downloads\n' +

#       '100K+\n' +

#       'Apps Powered\n' +

#       '75K+\n' +

#       'GitHub Stars\n' +

#       '3K+\n' +

#       'Contributors\n' +

#       '\n' +

#       '\n' +

#       'THE BIGGEST DEVELOPER COMMUNITY IN GENAI\n' +

#       '\n' +

#       'Learn alongside the 1M+ developers who are pushing the industry forward.\n' +

#       '\n' +

#       'Explore LangChain\n' +

#       '\n' +

#       '[/langchain]\n' +

#       '\n' +

#       '\n' +

#       'GET STARTED WITH THE LANGSMITH PLATFORM TODAY\n' +

#       '\n' +

#       'Get a demo [/contact-sales]Sign up for free [https://smith.langchain.com/]\n' +

#       '[https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65ccf12801bc39bf912a58f3_Home%20C.webp]\n' +

#       '\n' +

#       'Teams building with LangChain are driving operational efficiency, increasing discovery & personalization, and delivering premium\n' +

#       'products that generate revenue.\n' +

#       '\n' +

#       'Discover Use Cases\n' +

#       '\n' +

#       '[/use-cases]\n' +

#       '\n' +

#       '\n' +

#       'GET INSPIRED BY COMPANIES WHO HAVE DONE IT.\n' +

#       '\n' +

#       '[https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65bcd7ee85507bdf350399c3_Ally_Financial%201.svg]\n' +

#       'Financial Services\n' +

#       '\n' +

#       '[https://blog.langchain.dev/ally-financial-collaborates-with-langchain-to-deliver-critical-coding-module-to-mask-personal-identifying-information-in-a-compliant-and-safe-manner/]\n' +

#       '[https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65bcd8b3ae4dc901daa3037a_Adyen_Corporate_Logo%201.svg]\n' +

#       'FinTech\n' +

#       '\n' +

#       '[https://blog.langchain.dev/llms-accelerate-adyens-support-team-through-smart-ticket-routing-and-support-agent-copilot/]\n' +

#       '[https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65c534b3fa387379c0f4ebff_elastic-ar21%20(1).png]\n' +

#       'Technology\n' +

#       '\n' +

#       '[https://blog.langchain.dev/langchain-partners-with-elastic-to-launch-the-elastic-ai-assistant/]\n' +

#       '\n' +

#       '\n' +

#       'LANGSMITH IS THE ENTERPRISE DEVOPS PLATFORM BUILT FOR LLMS.\n' +

#       '\n' +

#       'Explore LangSmith\n' +

#       '\n' +

#       '[/langsmith]\n' +

#       'Gain visibility to make trade offs between cost, latency, and quality.\n' +

#       'Increase developer productivity.\n' +

#       'Eliminate manual, error-prone testing.\n' +

#       'Reduce hallucinations and improve reliability.\n' +

#       'Enterprise deployment options to keep data secure.\n' +

#       '\n' +

#       '\n' +

#       'READY TO START SHIPPING  RELIABLE GENAI APPS FASTER?\n' +

#       '\n' +

#       'Get started with LangChain, LangGraph, and LangSmith to enhance your LLM app development, from prototype to production.\n' +

#       '\n' +

#       'Get a demo [/contact-sales]Sign up for free [https://smith.langchain.com/]\n' +

#       'Products\n' +

#       'LangChain [/langchain]LangSmith [/langsmith]LangGraph [/langgraph]Agents [/agents]Evaluation [/evaluation]Retrieval [/retrieval]\n' +

#       'Resources\n' +

#       'Python Docs [https://python.langchain.com/]JS/TS Docs [https://js.langchain.com/docs/get_started/introduction/]GitHub\n' +

#       '[https://github.com/langchain-ai]Integrations [https://python.langchain.com/v0.2/docs/integrations/platforms/]Templates\n' +

#       '[https://templates.langchain.com/]Changelog [https://changelog.langchain.com/]LangSmith Trust Portal\n' +

#       '[https://trust.langchain.com/]\n' +

#       'Company\n' +

#       'About [/about]Blog [https://blog.langchain.dev/]Twitter [https://twitter.com/LangChainAI]LinkedIn\n' +

#       '[https://www.linkedin.com/company/langchain/]YouTube [https://www.youtube.com/@LangChain]Community [/join-community]Marketing\n' +

#       'Assets [https://drive.google.com/drive/folders/17xybjzmVBdsQA-VxouuGLxF6bDsHDe80?usp=sharing]\n' +

#       'Sign up for our newsletter to stay up to date\n' +

#       'Thank you! Your submission has been received!\n' +

#       'Oops! Something went wrong while submitting the form.\n' +

#       '[https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65c6a38f9c53ec71f5fc73de_langchain-word.svg]\n' +

#       'All systems operational\n' +

#       '[https://status.smith.langchain.com/]Privacy Policy [/'... 111 more characters,

#     metadata: {

#       source: 'https://langchain.com/',

#       title: 'LangChain',

#       description: 'LangChain’s suite of products supports developers along each step of their development journey.',

#       language: 'en'

#     }

#   }


console.log(docs[0].metadata)
# Output:
#   {

#     source: 'https://langchain.com/',

#     title: 'LangChain',

#     description: 'LangChain’s suite of products supports developers along each step of their development journey.',

#     language: 'en'

#   }


"""
## Options

```typescript
interface Options {
  excludeDirs?: string[]; // webpage directories to exclude.
  extractor?: (text: string) => string; // a function to extract the text of the document from the webpage, by default it returns the page as it is. It is recommended to use tools like html-to-text to extract the text. By default, it just returns the page as it is.
  maxDepth?: number; // the maximum depth to crawl. By default, it is set to 2. If you need to crawl the whole website, set it to a number that is large enough would simply do the job.
  timeout?: number; // the timeout for each request, in the unit of seconds. By default, it is set to 10000 (10 seconds).
  preventOutside?: boolean; // whether to prevent crawling outside the root url. By default, it is set to true.
  callerOptions?: AsyncCallerConstructorParams; // the options to call the AsyncCaller for example setting max concurrency (default is 64)
}
```

However, since it's hard to perform a perfect filter, you may still see some irrelevant results in the results. You can perform a filter on the returned documents by yourself, if it's needed. Most of the time, the returned results are good enough.
"""

"""
## API reference

For detailed documentation of all RecursiveUrlLoader features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_community_document_loaders_web_recursive_url.RecursiveUrlLoader.html
"""



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/s3.mdx
================================================
---
hide_table_of_contents: true
sidebar_class_name: node-only
---

# S3 File

:::tip Compatibility
Only available on Node.js.
:::

This covers how to load document objects from an s3 file object.

## Setup

To run this index you'll need to have Unstructured already set up and ready to use at an available URL endpoint. It can also be configured to run locally.

See the docs [here](/docs/integrations/document_loaders/file_loaders/unstructured) for information on how to do that.

You'll also need to install the official AWS SDK:

```bash npm2yarn
npm install @langchain/community @langchain/core @aws-sdk/client-s3
```

## Usage

Once Unstructured is configured, you can use the S3 loader to load files and then convert them into a Document.

You can optionally provide a s3Config parameter to specify your bucket region, access key, and secret access key. If these are not provided, you will need to have them in your environment (e.g., by running `aws configure`).

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/document_loaders/s3.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/searchapi.mdx
================================================
---
hide_table_of_contents: true
---

# SearchApi Loader

This guide shows how to use SearchApi with LangChain to load web search results.

## Overview

[SearchApi](https://www.searchapi.io/) is a real-time API that grants developers access to results from a variety of search engines, including engines like [Google Search](https://www.searchapi.io/docs/google),
[Google News](https://www.searchapi.io/docs/google-news), [Google Scholar](https://www.searchapi.io/docs/google-scholar), [YouTube Transcripts](https://www.searchapi.io/docs/youtube-transcripts) or any other engine that could be found in documentation.
This API enables developers and businesses to scrape and extract meaningful data directly from the result pages of all these search engines, providing valuable insights for different use-cases.

This guide shows how to load web search results using the `SearchApiLoader` in LangChain. The `SearchApiLoader` simplifies the process of loading and processing web search results from SearchApi.

## Setup

You'll need to sign up and retrieve your [SearchApi API key](https://www.searchapi.io/).

## Usage

Here's an example of how to use the `SearchApiLoader`:

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/document_loaders/searchapi.ts";

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core @langchain/openai
```

<CodeBlock language="typescript">{Example}</CodeBlock>

In this example, the `SearchApiLoader` is used to load web search results, which are then stored in memory using `MemoryVectorStore`. A retrieval chain is then used to retrieve the most relevant documents from the memory and answer the question based on these documents. This demonstrates how the `SearchApiLoader` can streamline the process of loading and processing web search results.



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/serpapi.mdx
================================================
---
hide_table_of_contents: true
---

# SerpAPI Loader

This guide shows how to use SerpAPI with LangChain to load web search results.

## Overview

[SerpAPI](https://serpapi.com/) is a real-time API that provides access to search results from various search engines. It is commonly used for tasks like competitor analysis and rank tracking. It empowers businesses to scrape, extract, and make sense of data from all search engines' result pages.

This guide shows how to load web search results using the `SerpAPILoader` in LangChain. The `SerpAPILoader` simplifies the process of loading and processing web search results from SerpAPI.

## Setup

You'll need to sign up and retrieve your [SerpAPI API key](https://serpapi.com/dashboard).

## Usage

Here's an example of how to use the `SerpAPILoader`:

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/document_loaders/serpapi.ts";

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core @langchain/openai
```

<CodeBlock language="typescript">{Example}</CodeBlock>

In this example, the `SerpAPILoader` is used to load web search results, which are then stored in memory using `MemoryVectorStore`. A retrieval chain is then used to retrieve the most relevant documents from the memory and answer the question based on these documents. This demonstrates how the `SerpAPILoader` can streamline the process of loading and processing web search results.



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/sitemap.mdx
================================================
# Sitemap Loader

This notebook goes over how to use the [`SitemapLoader`](https://api.js.langchain.com/classes/_langchain_community.document_loaders_web_sitemap.SitemapLoader.html) class to load sitemaps into `Document`s.

## Setup

First, we need to install the `langchain` package:

```bash npm2yarn
npm install @langchain/community @langchain/core
```

The URL passed in must either contain the `.xml` path to the sitemap, or a default `/sitemap.xml` will be appended to the URL.

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/document_loaders/sitemap.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

Or, if you want to only load the sitemap and not the contents of each page from the sitemap, you can use the `parseSitemap` method:

import ParseSitemapExample from "@examples/document_loaders/parse_sitemap.ts";

<CodeBlock language="typescript">{ParseSitemapExample}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/sonix_audio_transcription.mdx
================================================
---
hide_table_of_contents: true
sidebar_class_name: node-only
---

# Sonix Audio

:::tip Compatibility
Only available on Node.js.
:::

This covers how to load document objects from an audio file using the [Sonix](https://sonix.ai/) API.

## Setup

To run this loader you will need to create an account on the https://sonix.ai/ and obtain an auth key from the https://my.sonix.ai/api page.

You'll also need to install the `sonix-speech-recognition` library:

```bash npm2yarn
npm install @langchain/community @langchain/core sonix-speech-recognition
```

## Usage

Once auth key is configured, you can use the loader to create transcriptions and then convert them into a Document.
In the `request` parameter, you can either specify a local file by setting `audioFilePath` or a remote file using `audioUrl`.
You will also need to specify the audio language. See the list of supported languages [here](https://sonix.ai/docs/api#languages).

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/document_loaders/sonix_audio_transcription.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/sort_xyz_blockchain.mdx
================================================
---
hide_table_of_contents: true
---

# Blockchain Data

This example shows how to load blockchain data, including NFT metadata and transactions for a contract address, via the sort.xyz SQL API.

You will need a free Sort API key, visiting sort.xyz to obtain one.

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/document_loaders/sort_xyz_blockchain.ts";

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core @langchain/openai
```

<CodeBlock language="typescript">{Example}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/spider.mdx
================================================
---
hide_table_of_contents: true
---

# Spider

[Spider](https://spider.cloud/?ref=langchainjs) is the [fastest](https://github.com/spider-rs/spider/blob/main/benches/BENCHMARKS.md#benchmark-results) crawler. It converts any website into pure HTML, markdown, metadata or text while enabling you to crawl with custom actions using AI.

## Overview

Spider allows you to use high performance proxies to prevent detection, caches AI actions, webhooks for crawling status, scheduled crawls etc...

This guide shows how to crawl/scrape a website using [Spider](https://spider.cloud/) and loading the LLM-ready documents with `SpiderLoader` in LanghChain.

## Setup

Get your own Spider API key on [spider.cloud](https://spider.cloud/).

## Usage

Here's an example of how to use the `SpiderLoader`:

Spider offers two scraping modes `scrape` and `crawl`. Scrape only gets the content of the url provided while crawl gets the content of the url provided and crawls deeper following subpages.

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/document_loaders/spider.ts";

```bash npm2yarn
npm install @langchain/community @langchain/core @spider-cloud/spider-client
```

<CodeBlock language="typescript">{Example}</CodeBlock>

### Additional Parameters

See the [Spider documentation](https://spider.cloud/docs/api) for all the available `params`.



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/taskade.mdx
================================================
---
hide_table_of_contents: true
---

# Taskade

[Taskade](https://www.taskade.com) is the ultimate tool for AI-driven writing, project management, and task automation. Designed to be your second brain, Taskade simplifies project execution and enhances team collaboration from start to finish.

## Overview

With [Taskade](https://www.taskade.com), you can build, train, and deploy your own team of AI agents to automate tasks and streamline workflows. Taskade features a seamless blend of ideation, collaboration, and execution tools—from structured lists to modern tables and mind maps, all customizable to fit your unique workflow and adapt to your needs.

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/document_loaders/taskade.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

You can find your Taskade project id by opening the project in your browser and extracting them from the URL:

```
https://www.taskade.com/d/<YOUR PROJECT ID HERE>
```



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/web_cheerio.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Cheerio
---
"""

"""
# Cheerio

This notebook provides a quick overview for getting started with [CheerioWebBaseLoader](/docs/integrations/document_loaders/). For detailed documentation of all CheerioWebBaseLoader features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_document_loaders_web_cheerio.CheerioWebBaseLoader.html).

## Overview
### Integration details

This example goes over how to load data from webpages using Cheerio. One document will be created for each webpage.

Cheerio is a fast and lightweight library that allows you to parse and traverse HTML documents using a jQuery-like syntax. You can use Cheerio to extract data from web pages, without having to render them in a browser.

However, Cheerio does not simulate a web browser, so it cannot execute JavaScript code on the page. This means that it cannot extract data from dynamic web pages that require JavaScript to render. To do that, you can use the [`PlaywrightWebBaseLoader`](/docs/integrations/document_loaders/web_loaders/web_playwright) or [`PuppeteerWebBaseLoader`](/docs/integrations/document_loaders/web_loaders/web_puppeteer) instead.

| Class | Package | Local | Serializable | PY support|
| :--- | :--- | :---: | :---: |  :---: |
| [CheerioWebBaseLoader](https://api.js.langchain.com/classes/langchain_community_document_loaders_web_cheerio.CheerioWebBaseLoader.html) | @langchain/community | ✅ | ✅ | ❌ | 
### Loader features
| Source | Web Support | Node Support
| :---: | :---: | :---: | 
| CheerioWebBaseLoader | ✅ | ✅ | 

## Setup

To access `CheerioWebBaseLoader` document loader you'll need to install the `@langchain/community` integration package, along with the `cheerio` peer dependency.

### Credentials

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain CheerioWebBaseLoader integration lives in the `@langchain/community` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core cheerio
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and load documents:
"""

import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio"

const loader = new CheerioWebBaseLoader("https://news.ycombinator.com/item?id=34817881", {
  // optional params: ...
})

"""
## Load
"""

const docs = await loader.load()
docs[0]
# Output:
#   Document {

#     pageContent: '\n' +

#       '        \n' +

#       '                  Hacker News\n' +

#       '                            new | past | comments | ask | show | jobs | submit            \n' +

#       '                              login\n' +

#       '                          \n' +

#       '              \n' +

#       '\n' +

#       '        \n' +

#       '            What Lights the Universe’s Standard Candles? (quantamagazine.org)\n' +

#       '          75 points by Amorymeltzer on Feb 17, 2023  | hide | past | favorite | 6 comments        \n' +

#       '              \n' +

#       '        \n' +

#       '                  \n' +

#       '          \n' +

#       '          delta_p_delta_x on Feb 17, 2023           \n' +

#       '             | next [–]          \n' +

#       '                  \n' +

#       "                  Astrophysical and cosmological simulations are often insightful. They're also very cross-disciplinary; besides the obvious astrophysics, there's networking and sysadmin, parallel computing and algorithm theory (so that the simulation programs are actually fast but still accurate), systems design, and even a bit of graphic design for the visualisations.Some of my favourite simulation projects:- IllustrisTNG: https://www.tng-project.org/- SWIFT: https://swift.dur.ac.uk/- CO5BOLD: https://www.astro.uu.se/~bf/co5bold_main.html (which produced these animations of a red-giant star: https://www.astro.uu.se/~bf/movie/AGBmovie.html)- AbacusSummit: https://abacussummit.readthedocs.io/en/latest/And I can add the simulations in the article, too.\n" +

#       '                      \n' +

#       '                  \n' +

#       '      \n' +

#       '        \n' +

#       '                      \n' +

#       '          \n' +

#       '          froeb on Feb 18, 2023           \n' +

#       '             | parent | next [–]          \n' +

#       '                  \n' +

#       "                  Supernova simulations are especially interesting too. I have heard them described as the only time in physics when all 4 of the fundamental forces are important. The explosion can be quite finicky too. If I remember right, you can't get supernova to explode properly in 1D simulations, only in higher dimensions. This was a mystery until the realization that turbulence is necessary for supernova to trigger--there is no turbulent flow in 1D.\n" +

#       '                      \n' +

#       '                  \n' +

#       '      \n' +

#       '        \n' +

#       '                        \n' +

#       '          \n' +

#       '          andrewflnr on Feb 17, 2023           \n' +

#       '             | prev | next [–]          \n' +

#       '                  \n' +

#       "                  Whoa. I didn't know the accretion theory of Ia supernovae was dead, much less that it had been since 2011.\n" +

#       '                      \n' +

#       '                  \n' +

#       '      \n' +

#       '        \n' +

#       '                  \n' +

#       '          \n' +

#       '          andreareina on Feb 17, 2023           \n' +

#       '             | prev | next [–]          \n' +

#       '                  \n' +

#       '                  This seems  to be the paper https://academic.oup.com/mnras/article/517/4/5260/6779709\n' +

#       '                      \n' +

#       '                  \n' +

#       '      \n' +

#       '        \n' +

#       '                  \n' +

#       '          \n' +

#       '          andreareina on Feb 17, 2023           \n' +

#       '             | prev [–]          \n' +

#       '                  \n' +

#       "                  Wouldn't double detonation show up as variance in the brightness?\n" +

#       '                      \n' +

#       '                  \n' +

#       '      \n' +

#       '        \n' +

#       '                      \n' +

#       '          \n' +

#       '          yencabulator on Feb 18, 2023           \n' +

#       '             | parent [–]          \n' +

#       '                  \n' +

#       '                  Or widening of the peak. If one type Ia supernova goes 1,2,3,2,1, the sum of two could go    1+0=1\n' +

#       '    2+1=3\n' +

#       '    3+2=5\n' +

#       '    2+3=5\n' +

#       '    1+2=3\n' +

#       '    0+1=1\n' +

#       '                      \n' +

#       '                  \n' +

#       '      \n' +

#       '        \n' +

#       '                  \n' +

#       '  \n' +

#       '\n' +

#       '\n' +

#       'Guidelines | FAQ | Lists | API | Security | Legal | Apply to YC | Contact\n' +

#       'Search:       \n' +

#       '      \n' +

#       '  \n',

#     metadata: { source: 'https://news.ycombinator.com/item?id=34817881' },

#     id: undefined

#   }


console.log(docs[0].metadata)
# Output:
#   { source: 'https://news.ycombinator.com/item?id=34817881' }


"""
## Additional configurations

`CheerioWebBaseLoader` supports additional configuration when instantiating the loader. Here is an example of how to use it with the `selector` field passed, making it only load content from the provided HTML class names:
"""

import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio"

const loaderWithSelector = new CheerioWebBaseLoader("https://news.ycombinator.com/item?id=34817881", {
  selector: "p",
});

const docsWithSelector = await loaderWithSelector.load();
docsWithSelector[0].pageContent;
# Output:
#   Some of my favourite simulation projects:- IllustrisTNG: https://www.tng-project.org/- SWIFT: https://swift.dur.ac.uk/- CO5BOLD: https://www.astro.uu.se/~bf/co5bold_main.html (which produced these animations of a red-giant star: https://www.astro.uu.se/~bf/movie/AGBmovie.html)- AbacusSummit: https://abacussummit.readthedocs.io/en/latest/And I can add the simulations in the article, too.

#                     

#         

#                     

#         

#                     

#         

#                     

#         

#                     

#         

#                     

#         


"""
## API reference

For detailed documentation of all CheerioWebBaseLoader features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_community_document_loaders_web_cheerio.CheerioWebBaseLoader.html
"""



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/web_playwright.mdx
================================================
---
sidebar_position: 3
hide_table_of_contents: true
sidebar_class_name: node-only
sidebar_label: Playwright
---

# Webpages, with Playwright

:::tip Compatibility
Only available on Node.js.
:::

This example goes over how to load data from webpages using Playwright. One document will be created for each webpage.

Playwright is a Node.js library that provides a high-level API for controlling multiple browser engines, including Chromium, Firefox, and WebKit. You can use Playwright to automate web page interactions, including extracting data from dynamic web pages that require JavaScript to render.

If you want a lighterweight solution, and the webpages you want to load do not require JavaScript to render, you can use the [`CheerioWebBaseLoader`](/docs/integrations/document_loaders/web_loaders/web_cheerio) instead.

## Setup

```bash npm2yarn
npm install @langchain/community @langchain/core playwright
```

## Usage

```typescript
import { PlaywrightWebBaseLoader } from "@langchain/community/document_loaders/web/playwright";

/**
 * Loader uses `page.content()`
 * as default evaluate function
 **/
const loader = new PlaywrightWebBaseLoader("https://www.tabnews.com.br/");

const docs = await loader.load();
```

## Options

Here's an explanation of the parameters you can pass to the PlaywrightWebBaseLoader constructor using the PlaywrightWebBaseLoaderOptions interface:

```typescript
type PlaywrightWebBaseLoaderOptions = {
  launchOptions?: LaunchOptions;
  gotoOptions?: PlaywrightGotoOptions;
  evaluate?: PlaywrightEvaluate;
};
```

1. `launchOptions`: an optional object that specifies additional options to pass to the playwright.chromium.launch() method. This can include options such as the headless flag to launch the browser in headless mode.

2. `gotoOptions`: an optional object that specifies additional options to pass to the page.goto() method. This can include options such as the timeout option to specify the maximum navigation time in milliseconds, or the waitUntil option to specify when to consider the navigation as successful.

3. `evaluate`: an optional function that can be used to evaluate JavaScript code on the page using a custom evaluation function. This can be useful for extracting data from the page, interacting with page elements, or handling specific HTTP responses. The function should return a Promise that resolves to a string containing the result of the evaluation.

By passing these options to the `PlaywrightWebBaseLoader` constructor, you can customize the behavior of the loader and use Playwright's powerful features to scrape and interact with web pages.

Here is a basic example to do it:

```typescript
import {
  PlaywrightWebBaseLoader,
  Page,
  Browser,
} from "@langchain/community/document_loaders/web/playwright";

const url = "https://www.tabnews.com.br/";
const loader = new PlaywrightWebBaseLoader(url);
const docs = await loader.load();

// raw HTML page content
const extractedContents = docs[0].pageContent;
```

And a more advanced example:

```typescript
import {
  PlaywrightWebBaseLoader,
  Page,
  Browser,
} from "@langchain/community/document_loaders/web/playwright";

const loader = new PlaywrightWebBaseLoader("https://www.tabnews.com.br/", {
  launchOptions: {
    headless: true,
  },
  gotoOptions: {
    waitUntil: "domcontentloaded",
  },
  /** Pass custom evaluate, in this case you get page and browser instances */
  async evaluate(page: Page, browser: Browser, response: Response | null) {
    await page.waitForResponse("https://www.tabnews.com.br/va/view");

    const result = await page.evaluate(() => document.body.innerHTML);
    return result;
  },
});

const docs = await loader.load();
```



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/web_puppeteer.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Puppeteer
sidebar_class_name: node-only
---
"""

"""
# PuppeteerWebBaseLoader

```{=mdx}
:::tip Compatibility

Only available on Node.js.

:::
```

This notebook provides a quick overview for getting started with [PuppeteerWebBaseLoader](/docs/integrations/document_loaders/). For detailed documentation of all PuppeteerWebBaseLoader features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_document_loaders_web_puppeteer.PuppeteerWebBaseLoader.html).

Puppeteer is a Node.js library that provides a high-level API for controlling headless Chrome or Chromium. You can use Puppeteer to automate web page interactions, including extracting data from dynamic web pages that require JavaScript to render.

If you want a lighterweight solution, and the webpages you want to load do not require JavaScript to render, you can use the [CheerioWebBaseLoader](/docs/integrations/document_loaders/web_loaders/web_cheerio) instead.

## Overview
### Integration details

| Class | Package | Local | Serializable | PY support |
| :--- | :--- | :---: | :---: |  :---: |
| [PuppeteerWebBaseLoader](https://api.js.langchain.com/classes/langchain_community_document_loaders_web_puppeteer.PuppeteerWebBaseLoader.html) | [@langchain/community](https://api.js.langchain.com/modules/langchain_community_document_loaders_web_puppeteer.html) | ✅ | beta | ❌ | 
### Loader features
| Source | Web Loader | Node Envs Only
| :---: | :---: | :---: | 
| PuppeteerWebBaseLoader | ✅ | ✅ | 

## Setup

To access `PuppeteerWebBaseLoader` document loader you'll need to install the `@langchain/community` integration package, along with the `puppeteer` peer dependency.

### Credentials

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain PuppeteerWebBaseLoader integration lives in the `@langchain/community` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core puppeteer
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and load documents:
"""

import { PuppeteerWebBaseLoader } from "@langchain/community/document_loaders/web/puppeteer"

const loader = new PuppeteerWebBaseLoader("https://langchain.com", {
  // required params = ...
  // optional params = ...
})

"""
## Load
"""

const docs = await loader.load()
docs[0]
# Output:
#   Document {

#     pageContent: '<div class="page-wrapper"><div class="global-styles w-embed"><style>\n' +

#       '\n' +

#       '* {\n' +

#       '  -webkit-font-smoothing: antialiased;\n' +

#       '}\n' +

#       '\n' +

#       '.page-wrapper {\n' +

#       'overflow: clip;\n' +

#       '  }\n' +

#       '\n' +

#       '\n' +

#       '\n' +

#       '/* Set fluid size change for smaller breakpoints */\n' +

#       '  html { font-size: 1rem; }\n' +

#       '  @media screen and (max-width:1920px) and (min-width:1281px) { html { font-size: calc(0.2499999999999999rem + 0.6250000000000001vw); } }\n' +

#       '  @media screen and (max-width:1280px) and (min-width:992px) { html { font-size: calc(0.41223612197028925rem + 0.4222048475371384vw); } }\n' +

#       '/* video sizing */\n' +

#       '\n' +

#       'video {\n' +

#       '    object-fit: fill;\n' +

#       '\t\twidth: 100%;\n' +

#       '}\n' +

#       '\n' +

#       '\n' +

#       '\n' +

#       '#retrieval-video {\n' +

#       '    object-fit: cover;\n' +

#       '    width: 100%;\n' +

#       '}\n' +

#       '\n' +

#       '\n' +

#       '\n' +

#       '/* Set color style to inherit */\n' +

#       '.inherit-color * {\n' +

#       '    color: inherit;\n' +

#       '}\n' +

#       '\n' +

#       '/* Focus state style for keyboard navigation for the focusable elements */\n' +

#       '*[tabindex]:focus-visible,\n' +

#       '  input[type="file"]:focus-visible {\n' +

#       '   outline: 0.125rem solid #4d65ff;\n' +

#       '   outline-offset: 0.125rem;\n' +

#       '}\n' +

#       '\n' +

#       '/* Get rid of top margin on first element in any rich text element */\n' +

#       '.w-richtext > :not(div):first-child, .w-richtext > div:first-child > :first-child {\n' +

#       '  margin-top: 0 !important;\n' +

#       '}\n' +

#       '\n' +

#       '/* Get rid of bottom margin on last element in any rich text element */\n' +

#       '.w-richtext>:last-child, .w-richtext ol li:last-child, .w-richtext ul li:last-child {\n' +

#       '\tmargin-bottom: 0 !important;\n' +

#       '}\n' +

#       '\n' +

#       '/* Prevent all click and hover interaction with an element */\n' +

#       '.pointer-events-off {\n' +

#       '\tpointer-events: none;\n' +

#       '}\n' +

#       '\n' +

#       '/* Enables all click and hover interaction with an element */\n' +

#       '.pointer-events-on {\n' +

#       '  pointer-events: auto;\n' +

#       '}\n' +

#       '\n' +

#       '/* Create a class of .div-square which maintains a 1:1 dimension of a div */\n' +

#       '.div-square::after {\n' +

#       '\tcontent: "";\n' +

#       '\tdisplay: block;\n' +

#       '\tpadding-bottom: 100%;\n' +

#       '}\n' +

#       '\n' +

#       '/* Make sure containers never lose their center alignment */\n' +

#       '.container-medium,.container-small, .container-large {\n' +

#       '\tmargin-right: auto !important;\n' +

#       '  margin-left: auto !important;\n' +

#       '}\n' +

#       '\n' +

#       '/* \n' +

#       'Make the following elements inherit typography styles from the parent and not have hardcoded values. \n' +

#       'Important: You will not be able to style for example "All Links" in Designer with this CSS applied.\n' +

#       'Uncomment this CSS to use it in the project. Leave this message for future hand-off.\n' +

#       '*/\n' +

#       '/*\n' +

#       'a,\n' +

#       '.w-input,\n' +

#       '.w-select,\n' +

#       '.w-tab-link,\n' +

#       '.w-nav-link,\n' +

#       '.w-dropdown-btn,\n' +

#       '.w-dropdown-toggle,\n' +

#       '.w-dropdown-link {\n' +

#       '  color: inherit;\n' +

#       '  text-decoration: inherit;\n' +

#       '  font-size: inherit;\n' +

#       '}\n' +

#       '*/\n' +

#       '\n' +

#       '/* Apply "..." after 3 lines of text */\n' +

#       '.text-style-3lines {\n' +

#       '\tdisplay: -webkit-box;\n' +

#       '\toverflow: hidden;\n' +

#       '\t-webkit-line-clamp: 3;\n' +

#       '\t-webkit-box-orient: vertical;\n' +

#       '}\n' +

#       '\n' +

#       '/* Apply "..." after 2 lines of text */\n' +

#       '.text-style-2lines {\n' +

#       '\tdisplay: -webkit-box;\n' +

#       '\toverflow: hidden;\n' +

#       '\t-webkit-line-clamp: 2;\n' +

#       '\t-webkit-box-orient: vertical;\n' +

#       '}\n' +

#       '\n' +

#       '/* Adds inline flex display */\n' +

#       '.display-inlineflex {\n' +

#       '  display: inline-flex;\n' +

#       '}\n' +

#       '\n' +

#       '/* These classes are never overwritten */\n' +

#       '.hide {\n' +

#       '  display: none !important;\n' +

#       '}\n' +

#       '\n' +

#       '@media screen and (max-width: 991px) {\n' +

#       '    .hide, .hide-tablet {\n' +

#       '        display: none !important;\n' +

#       '    }\n' +

#       '}\n' +

#       '  @media screen and (max-width: 767px) {\n' +

#       '    .hide-mobile-landscape{\n' +

#       '      display: none !important;\n' +

#       '    }\n' +

#       '}\n' +

#       '  @media screen and (max-width: 479px) {\n' +

#       '    .hide-mobile{\n' +

#       '      display: none !important;\n' +

#       '    }\n' +

#       '}\n' +

#       ' \n' +

#       '.margin-0 {\n' +

#       '  margin: 0rem !important;\n' +

#       '}\n' +

#       '  \n' +

#       '.padding-0 {\n' +

#       '  padding: 0rem !important;\n' +

#       '}\n' +

#       '\n' +

#       '.spacing-clean {\n' +

#       'padding: 0rem !important;\n' +

#       'margin: 0rem !important;\n' +

#       '}\n' +

#       '\n' +

#       '.margin-top {\n' +

#       '  margin-right: 0rem !important;\n' +

#       '  margin-bottom: 0rem !important;\n' +

#       '  margin-left: 0rem !important;\n' +

#       '}\n' +

#       '\n' +

#       '.padding-top {\n' +

#       '  padding-right: 0rem !important;\n' +

#       '  padding-bottom: 0rem !important;\n' +

#       '  padding-left: 0rem !important;\n' +

#       '}\n' +

#       '  \n' +

#       '.margin-right {\n' +

#       '  margin-top: 0rem !important;\n' +

#       '  margin-bottom: 0rem !important;\n' +

#       '  margin-left: 0rem !important;\n' +

#       '}\n' +

#       '\n' +

#       '.padding-right {\n' +

#       '  padding-top: 0rem !important;\n' +

#       '  padding-bottom: 0rem !important;\n' +

#       '  padding-left: 0rem !important;\n' +

#       '}\n' +

#       '\n' +

#       '.margin-bottom {\n' +

#       '  margin-top: 0rem !important;\n' +

#       '  margin-right: 0rem !important;\n' +

#       '  margin-left: 0rem !important;\n' +

#       '}\n' +

#       '\n' +

#       '.padding-bottom {\n' +

#       '  padding-top: 0rem !important;\n' +

#       '  padding-right: 0rem !important;\n' +

#       '  padding-left: 0rem !important;\n' +

#       '}\n' +

#       '\n' +

#       '.margin-left {\n' +

#       '  margin-top: 0rem !important;\n' +

#       '  margin-right: 0rem !important;\n' +

#       '  margin-bottom: 0rem !important;\n' +

#       '}\n' +

#       '  \n' +

#       '.padding-left {\n' +

#       '  padding-top: 0rem !important;\n' +

#       '  padding-right: 0rem !important;\n' +

#       '  padding-bottom: 0rem !important;\n' +

#       '}\n' +

#       '  \n' +

#       '.margin-horizontal {\n' +

#       '  margin-top: 0rem !important;\n' +

#       '  margin-bottom: 0rem !important;\n' +

#       '}\n' +

#       '\n' +

#       '.padding-horizontal {\n' +

#       '  padding-top: 0rem !important;\n' +

#       '  padding-bottom: 0rem !important;\n' +

#       '}\n' +

#       '\n' +

#       '.margin-vertical {\n' +

#       '  margin-right: 0rem !important;\n' +

#       '  margin-left: 0rem !important;\n' +

#       '}\n' +

#       '  \n' +

#       '.padding-vertical {\n' +

#       '  padding-right: 0rem !important;\n' +

#       '  padding-left: 0rem !important;\n' +

#       '}\n' +

#       '\n' +

#       '/* Apply "..." at 100% width */\n' +

#       '.truncate-width { \n' +

#       '\t\twidth: 100%; \n' +

#       '    white-space: nowrap; \n' +

#       '    overflow: hidden; \n' +

#       '    text-overflow: ellipsis; \n' +

#       '}\n' +

#       '/* Removes native scrollbar */\n' +

#       '.no-scrollbar {\n' +

#       '    -ms-overflow-style: none;\n' +

#       '    overflow: -moz-scrollbars-none; \n' +

#       '}\n' +

#       '\n' +

#       '.no-scrollbar::-webkit-scrollbar {\n' +

#       '    display: none;\n' +

#       '}\n' +

#       '\n' +

#       'input:checked + span {\n' +

#       'color: white    /* styles for the div immediately following the checked input */\n' +

#       '}\n' +

#       '\n' +

#       '/* styles for word-wrapping\n' +

#       'h1, h2, h3 {\n' +

#       'word-wrap: break-word;\n' +

#       'hyphens: auto;\n' +

#       '}*/\n' +

#       '\n' +

#       '[nav-theme="light"] .navbar_logo-svg {\n' +

#       '\t--nav--logo: var(--light--logo);\n' +

#       '}\n' +

#       '\n' +

#       '[nav-theme="light"] .button.is-nav {\n' +

#       '\t--nav--button-bg: var(--light--button-bg);\n' +

#       '\t--nav--button-text: var(--light--button-text);\n' +

#       '}\n' +

#       '\n' +

#       '[nav-theme="light"] .button.is-nav:hover {\n' +

#       '\t--nav--button-bg: var(--dark--button-bg);\n' +

#       '\t--nav--button-text:var(--dark--button-text);\n' +

#       '}\n' +

#       '\n' +

#       '[nav-theme="dark"] .navbar_logo-svg {\n' +

#       '\t--nav--logo: var(--dark--logo);\n' +

#       '}\n' +

#       '\n' +

#       '[nav-theme="dark"] .button.is-nav {\n' +

#       '\t--nav--button-bg: var(--dark--button-bg);\n' +

#       '\t--nav--button-text: var(--dark--button-text);\n' +

#       '}\n' +

#       '\n' +

#       '[nav-theme="dark"] .button.is-nav:hover {\n' +

#       '\t--nav--button-bg: var(--light--button-bg);\n' +

#       '\t--nav--button-text: var(--light--button-text);\n' +

#       '}\n' +

#       '\n' +

#       '[nav-theme="red"] .navbar_logo-svg {\n' +

#       '\t--nav--logo: var(--red--logo);\n' +

#       '}\n' +

#       '\n' +

#       '\n' +

#       '[nav-theme="red"] .button.is-nav {\n' +

#       '\t--nav--button-bg: var(--red--button-bg);\n' +

#       '\t--nav--button-text: var(--red--button-text);\n' +

#       '}\n' +

#       '\n' +

#       '.navbar_logo-svg.is-light, .navbar_logo-svg.is-red.is-light{\n' +

#       'color: #F8F7FF!important;\n' +

#       '}\n' +

#       '\n' +

#       '.news_button[disabled] {\n' +

#       'background: none;\n' +

#       '}\n' +

#       '\n' +

#       '.product_bg-video video {\n' +

#       'object-fit: fill;\n' +

#       '}\n' +

#       '\n' +

#       '</style></div><div data-animation="default" class="navbar_component w-nav" data-easing2="ease" fs-scrolldisable-element="smart-nav" data-easing="ease" data-collapse="medium" data-w-id="78839fc1-6b85-b108-b164-82fcae730868" role="banner" data-duration="400" style="will-change: width, height; height: 10rem;"><div class="navbar_container"><a href="/" aria-current="page" class="navbar_logo-link w-nav-brand w--current" aria-label="home"><div class="navbar_logo-svg w-embed" style="color: rgb(255, 255, 255);"><svg width="100%" height="100%" viewBox="0 0 240 41" fill="none" xmlns="http://www.w3.org/2000/svg">\n' +

#       '<path d="M61.5139 11.1569C60.4527 11.1569 59.4549 11.568 58.708 12.3148L55.6899 15.3248C54.8757 16.1368 54.4574 17.2643 54.5431 18.4202C54.5492 18.4833 54.5553 18.5464 54.5615 18.6115C54.6696 19.4988 55.0594 20.2986 55.6899 20.9254C56.1246 21.3589 56.6041 21.6337 57.1857 21.825C57.2163 22 57.2326 22.177 57.2326 22.3541C57.2326 23.1519 56.9225 23.9008 56.3592 24.4625L56.1735 24.6477C55.1655 24.3037 54.3247 23.8011 53.5656 23.044C52.5576 22.0386 51.8903 20.7687 51.6393 19.3747L51.6046 19.1813L51.4515 19.3055C51.3475 19.3889 51.2495 19.4785 51.1577 19.57L48.1396 22.58C46.5928 24.1226 46.5928 26.636 48.1396 28.1786C48.913 28.9499 49.9292 29.3366 50.9475 29.3366C51.9658 29.3366 52.98 28.9499 53.7534 28.1786L56.7715 25.1687C58.3183 23.626 58.3183 21.1147 56.7715 19.57C56.3592 19.159 55.8675 18.8496 55.3104 18.6502C55.2798 18.469 55.2634 18.2879 55.2634 18.1109C55.2634 17.2439 55.6063 16.4217 56.2348 15.7949C57.2449 16.1388 58.1407 16.6965 58.8978 17.4515C59.9038 18.4548 60.5691 19.7227 60.8241 21.1208L60.8588 21.3141L61.0119 21.19C61.116 21.1066 61.2139 21.017 61.3078 20.9234L64.3259 17.9135C65.8727 16.3708 65.8747 13.8575 64.3259 12.3148C63.577 11.568 62.5811 11.1569 61.518 11.1569H61.5139Z" fill="CurrentColor"></path>\n' +

#       '<path d="M59.8966 0.148865H20.4063C9.15426 0.148865 0 9.27841 0 20.5001C0 31.7217 9.15426 40.8513 20.4063 40.8513H59.8966C71.1486 40.8513 80.3029 31.7217 80.3029 20.5001C80.3029 9.27841 71.1486 0.148865 59.8966 0.148865ZM40.4188 32.0555C39.7678 32.1898 39.0352 32.2142 38.5373 31.6953C38.3536 32.1165 37.9251 31.8947 37.5945 31.8398C37.5639 31.9252 37.5374 32.0005 37.5088 32.086C36.4089 32.1593 35.5845 31.04 35.0601 30.1954C34.0193 29.6337 32.8378 29.2918 31.7746 28.7036C31.7134 29.6724 31.9257 30.8731 31.0012 31.4979C30.9543 33.36 33.8255 31.7177 34.0887 33.1056C33.8847 33.128 33.6582 33.073 33.4949 33.2297C32.746 33.9563 31.8869 32.6803 31.0237 33.2074C29.8646 33.7894 29.7483 34.2656 28.3137 34.3857C28.2342 34.2656 28.2668 34.1862 28.3341 34.113C28.7382 33.6449 28.7668 33.0934 29.4565 32.8939C28.7464 32.782 28.1525 33.1728 27.5546 33.4821C26.7771 33.7996 26.7833 32.7657 25.5875 33.537C25.4548 33.4292 25.5181 33.3315 25.5936 33.2481C25.8976 32.8777 26.2976 32.8227 26.7486 32.8431C24.5304 31.6098 23.4856 34.3511 22.4612 32.9876C22.1531 33.069 22.0368 33.3457 21.8429 33.5411C21.6756 33.358 21.8021 33.1361 21.8103 32.9204C21.6103 32.8268 21.3572 32.782 21.4164 32.4625C21.0246 32.3302 20.7512 32.5622 20.4594 32.782C20.1961 32.5785 20.6369 32.2814 20.7185 32.0697C20.9532 31.6627 21.4878 31.9863 21.7592 31.6932C22.5306 31.2557 23.606 31.9659 24.4876 31.8459C25.1671 31.9313 26.0078 31.2353 25.667 30.5413C24.9406 29.6154 25.0691 28.4045 25.0528 27.2974C24.963 26.6522 23.4101 25.83 22.9612 25.134C22.4061 24.5072 21.9735 23.7807 21.5409 23.0664C19.9798 20.0523 20.4716 16.1795 18.5044 13.3812C17.6147 13.8717 16.4556 13.6397 15.6884 12.9823C15.2741 13.3588 15.2557 13.8513 15.2231 14.3744C14.2293 13.3833 14.3538 11.5109 15.1476 10.4079C15.4721 9.97239 15.8598 9.61421 16.2924 9.29876C16.3903 9.22754 16.423 9.15834 16.4209 9.04844C17.2066 5.52362 22.5653 6.20335 24.259 8.70044C25.4875 10.237 25.8589 12.27 27.2526 13.6967C29.1279 15.744 31.2645 17.5471 32'... 73262 more characters,

#     metadata: { source: 'https://langchain.com' },

#     id: undefined

#   }


console.log(docs[0].metadata)
# Output:
#   { source: 'https://langchain.com' }


"""
## Options

Here's an explanation of the parameters you can pass to the PuppeteerWebBaseLoader constructor using the PuppeteerWebBaseLoaderOptions interface:

```typescript
type PuppeteerWebBaseLoaderOptions = {
  launchOptions?: PuppeteerLaunchOptions;
  gotoOptions?: PuppeteerGotoOptions;
  evaluate?: (page: Page, browser: Browser) => Promise<string>;
};
```

1. `launchOptions`: an optional object that specifies additional options to pass to the puppeteer.launch() method. This can include options such as the headless flag to launch the browser in headless mode, or the slowMo option to slow down Puppeteer's actions to make them easier to follow.

2. `gotoOptions`: an optional object that specifies additional options to pass to the page.goto() method. This can include options such as the timeout option to specify the maximum navigation time in milliseconds, or the waitUntil option to specify when to consider the navigation as successful.

3. `evaluate`: an optional function that can be used to evaluate JavaScript code on the page using the page.evaluate() method. This can be useful for extracting data from the page or interacting with page elements. The function should return a Promise that resolves to a string containing the result of the evaluation.

By passing these options to the `PuppeteerWebBaseLoader` constructor, you can customize the behavior of the loader and use Puppeteer's powerful features to scrape and interact with web pages.

"""

"""
## Screenshots

To take a screenshot of a site, initialize the loader the same as above, and call the `.screenshot()` method.
This will return an instance of `Document` where the page content is a base64 encoded image, and the metadata contains a `source` field with the URL of the page.
"""

import { PuppeteerWebBaseLoader } from "@langchain/community/document_loaders/web/puppeteer";

const loaderForScreenshot = new PuppeteerWebBaseLoader("https://langchain.com", {
  launchOptions: {
    headless: true,
  },
  gotoOptions: {
    waitUntil: "domcontentloaded",
  },
});
const screenshot = await loaderForScreenshot.screenshot();

console.log(screenshot.pageContent.slice(0, 100));
console.log(screenshot.metadata);
# Output:
#   iVBORw0KGgoAAAANSUhEUgAACWAAAAdoCAIAAAA/Q2IJAAAAAXNSR0IArs4c6QAAIABJREFUeJzsvUuzHUeSJuaPiMjMk3nOuU88

#   { source: 'https://langchain.com' }


"""
## API reference

For detailed documentation of all PuppeteerWebBaseLoader features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_community_document_loaders_web_puppeteer.PuppeteerWebBaseLoader.html
"""



================================================
FILE: docs/core_docs/docs/integrations/document_loaders/web_loaders/youtube.mdx
================================================
---
hide_table_of_contents: true
---

# YouTube transcripts

This covers how to load YouTube transcripts into LangChain documents.

## Setup

You'll need to install the [youtubei.js](https://www.npmjs.com/package/youtubei.js) to extract metadata:

```bash npm2yarn
npm install @langchain/community @langchain/core youtubei.js
```

## Usage

You need to specify a link to the video in the `url`. You can also specify `language` in [ISO 639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) and `addVideoInfo` flag.

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/document_loaders/youtube.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/document_transformers/html-to-text.mdx
================================================
# html-to-text

When ingesting HTML documents for later retrieval, we are often interested only in the actual content of the webpage rather than semantics.
Stripping HTML tags from documents with the HtmlToTextTransformer can result in more content-rich chunks, making retrieval more effective.

## Setup

You'll need to install the [`html-to-text`](https://www.npmjs.com/package/html-to-text) npm package:

```bash npm2yarn
npm install html-to-text
```

Though not required for the transformer by itself, the below usage examples require [`cheerio`](https://www.npmjs.com/package/cheerio) for scraping:

```bash npm2yarn
npm install cheerio
```

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core
```

## Usage

The below example scrapes a Hacker News thread, splits it based on HTML tags to group chunks based on the semantic information from the tags,
then extracts content from the individual chunks:

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/document_transformers/html_to_text.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## Customization

You can pass the transformer any [arguments accepted by the `html-to-text` package](https://www.npmjs.com/package/html-to-text) to customize how it works.



================================================
FILE: docs/core_docs/docs/integrations/document_transformers/mozilla_readability.mdx
================================================
# @mozilla/readability

When ingesting HTML documents for later retrieval, we are often interested only in the actual content of the webpage rather than semantics.
Stripping HTML tags from documents with the MozillaReadabilityTransformer can result in more content-rich chunks, making retrieval more effective.

## Setup

You'll need to install the [`@mozilla/readability`](https://www.npmjs.com/package/@mozilla/readability) and the [`jsdom`](https://www.npmjs.com/package/jsdom) npm package:

```bash npm2yarn
npm install @mozilla/readability jsdom
```

Though not required for the transformer by itself, the below usage examples require [`cheerio`](https://www.npmjs.com/package/cheerio) for scraping:

```bash npm2yarn
npm install cheerio
```

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core
```

## Usage

The below example scrapes a Hacker News thread, splits it based on HTML tags to group chunks based on the semantic information from the tags,
then extracts content from the individual chunks:

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/document_transformers/mozilla_readability.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## Customization

You can pass the transformer any [arguments accepted by the `@mozilla/readability` package](https://www.npmjs.com/package/@mozilla/readability) to customize how it works.



================================================
FILE: docs/core_docs/docs/integrations/document_transformers/openai_metadata_tagger.mdx
================================================
# OpenAI functions metadata tagger

It can often be useful to tag ingested documents with structured metadata, such as the title, tone, or length of a document, to allow for more targeted similarity search later. However, for large numbers of documents, performing this labelling process manually can be tedious.

The `MetadataTagger` document transformer automates this process by extracting metadata from each provided document according to a provided schema. It uses a configurable OpenAI Functions-powered chain under the hood, so if you pass a custom LLM instance, it must be an OpenAI model with functions support.

**Note:** This document transformer works best with complete documents, so it's best to run it first with whole documents before doing any other splitting or processing!

### Usage

For example, let's say you wanted to index a set of movie reviews. You could initialize the document transformer as follows:

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/document_transformers/metadata_tagger.ts";

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core
```

<CodeBlock language="typescript">{Example}</CodeBlock>

There is an additional `createMetadataTagger` method that accepts a valid JSON Schema object as well.

### Customization

You can pass the underlying tagging chain the standard LLMChain arguments in the second options parameter.
For example, if you wanted to ask the LLM to focus specific details in the input documents, or extract metadata in a certain style, you could pass in a custom prompt:

import CustomExample from "@examples/document_transformers/metadata_tagger_custom_prompt.ts";

<CodeBlock language="typescript">{CustomExample}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/llm_caching/azure_cosmosdb_nosql.mdx
================================================
# Azure Cosmos DB NoSQL Semantic Cache

> The Semantic Cache feature is supported with Azure Cosmos DB for NoSQL integration, enabling users to retrieve cached responses based on semantic similarity between the user input and previously cached results. It leverages [AzureCosmosDBNoSQLVectorStore](/docs/integrations/vectorstores/azure_cosmosdb_nosql), which stores vector embeddings of cached prompts. These embeddings enable similarity-based searches, allowing the system to retrieve relevant cached results.

If you don't have an Azure account, you can [create a free account](https://azure.microsoft.com/free/) to get started.

## Setup

You'll first need to install the [`@langchain/azure-cosmosdb`](https://www.npmjs.com/package/@langchain/azure-cosmosdb) package:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/azure-cosmosdb @langchain/core
```

You'll also need to have an Azure Cosmos DB for NoSQL instance running. You can deploy a free version on Azure Portal without any cost, following [this guide](https://learn.microsoft.com/azure/cosmos-db/nosql/quickstart-portal).

Once you have your instance running, make sure you have the connection string. If you are using Managed Identity, you need to have the endpoint. You can find them in the Azure Portal, under the "Settings / Keys" section of your instance.

import CodeBlock from "@theme/CodeBlock";

:::info

When using Azure Managed Identity and role-based access control, you must ensure that the database and container have been created beforehand. RBAC does not provide permissions to create databases and containers. You can get more information about the permission model in the [Azure Cosmos DB documentation](https://learn.microsoft.com/azure/cosmos-db/how-to-setup-rbac#permission-model).

:::

## Usage example

import Example from "@examples/caches/azure_cosmosdb_nosql/azure_cosmosdb_nosql.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/llm_caching/index.mdx
================================================
---
sidebar_class_name: hidden
hide_table_of_contents: true
---

# Model caches

[Caching LLM calls](/docs/how_to/chat_model_caching) can be useful for testing, cost savings, and speed.

Below are some integrations that allow you to cache results of individual LLM calls using different caches with different strategies.

import { IndexTable } from "@theme/FeatureTables";

<IndexTable />



================================================
FILE: docs/core_docs/docs/integrations/llms/ai21.mdx
================================================
# AI21

You can get started with AI21Labs' Jurassic family of models, as well as see a full list of available foundational models, by signing up for an API key [on their website](https://www.ai21.com/).

Here's an example of initializing an instance in LangChain.js:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core
```

import CodeBlock from "@theme/CodeBlock";
import AI21Example from "@examples/models/llm/ai21.ts";

<CodeBlock language="typescript">{AI21Example}</CodeBlock>

## Related

- LLM [conceptual guide](/docs/concepts/text_llms)
- LLM [how-to guides](/docs/how_to/#llms)



================================================
FILE: docs/core_docs/docs/integrations/llms/aleph_alpha.mdx
================================================
# AlephAlpha

LangChain.js supports AlephAlpha's Luminous family of models. You'll need to sign up for an API key [on their website](https://www.aleph-alpha.com/).

Here's an example:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core
```

import CodeBlock from "@theme/CodeBlock";
import AlephAlphaExample from "@examples/models/llm/aleph_alpha.ts";

<CodeBlock language="typescript">{AlephAlphaExample}</CodeBlock>

## Related

- LLM [conceptual guide](/docs/concepts/text_llms)
- LLM [how-to guides](/docs/how_to/#llms)



================================================
FILE: docs/core_docs/docs/integrations/llms/arcjet.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Arcjet Redact
---
"""

"""
# Arcjet Redact

The [Arcjet](https://arcjet.com) redact integration allows you to redact sensitive user information from your prompts before sending it to an LLM.

Arcjet Redact runs entirely on your own machine and never sends data anywhere else, ensuring best in class privacy and performance.

The Arcjet Redact object is not an LLM itself, instead it wraps an LLM. It redacts the text that is inputted to it and then unredacts the output of the wrapped LLM before returning it. 



## Overview
### Integration details

| Class | Package | Local | Serializable | PY Support | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| Arcjet | @langchain/community | ❌ | ✅ | ❌ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/community?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |

### Installation

Install the Arcjet Redaction Library:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @arcjet/redact
</Npm2Yarn>

```

And install LangChain Community:


```{=mdx}
<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core
</Npm2Yarn>

And now you're ready to start protecting your LLM calls with Arcjet Redaction!

```
"""

"""
## Usage
"""

import {
  ArcjetRedact,
  ArcjetSensitiveInfoType,
} from "@langchain/community/llms/arcjet";
import { OpenAI } from "@langchain/openai";

// Create an instance of another LLM for Arcjet to wrap
const openai = new OpenAI({
  modelName: "gpt-3.5-turbo-instruct",
  openAIApiKey: process.env.OPENAI_API_KEY,
});

const arcjetRedactOptions = {
  // Specify a LLM that Arcjet Redact will call once it has redacted the input.
  llm: openai,

  // Specify the list of entities that should be redacted.
  // If this isn't specified then all entities will be redacted.
  entities: ["email", "phone-number", "ip-address", "credit-card"] as ArcjetSensitiveInfoType[],

  // You can provide a custom detect function to detect entities that we don't support yet.
  // It takes a list of tokens and you return a list of identified types or undefined.
  // The undefined types that you return should be added to the entities list if used.
  detect: (tokens: string[]) => {
    return tokens.map((t) => t === "some-sensitive-info" ? "custom-entity" : undefined)
  },

  // The number of tokens to provide to the custom detect function. This defaults to 1.
  // It can be used to provide additional context when detecting custom entity types.
  contextWindowSize: 1,

  // This allows you to provide custom replacements when redacting. Please ensure
  // that the replacements are unique so that unredaction works as expected.
  replace: (identifiedType: string) => {
    return identifiedType === "email" ? "redacted@example.com" : undefined;
  },

};

const arcjetRedact = new ArcjetRedact(arcjetRedactOptions);
const response = await arcjetRedact.invoke(
  "My email address is test@example.com, here is some-sensitive-info"
);



================================================
FILE: docs/core_docs/docs/integrations/llms/aws_sagemaker.mdx
================================================
# AWS SageMakerEndpoint

LangChain.js supports integration with AWS SageMaker-hosted endpoints. Check [Amazon SageMaker JumpStart](https://aws.amazon.com/sagemaker/jumpstart/) for a list of available models, and how to deploy your own.

## Setup

You'll need to install the official SageMaker SDK as a peer dependency:

```bash npm2yarn
npm install @aws-sdk/client-sagemaker-runtime
```

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core
```

## Usage

import CodeBlock from "@theme/CodeBlock";
import SageMakerEndpointExample from "@examples/models/llm/sagemaker_endpoint.ts";

<CodeBlock language="typescript">{SageMakerEndpointExample}</CodeBlock>

## Related

- LLM [conceptual guide](/docs/concepts/text_llms)
- LLM [how-to guides](/docs/how_to/#llms)



================================================
FILE: docs/core_docs/docs/integrations/llms/azure.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Azure OpenAI
---
"""

"""
# AzureOpenAI

```{=mdx}

:::caution
You are currently on a page documenting the use of Azure OpenAI [text completion models](/docs/concepts/text_llms). The latest and most popular Azure OpenAI models are [chat completion models](/docs/concepts/chat_models).

Unless you are specifically using `gpt-3.5-turbo-instruct`, you are probably looking for [this page instead](/docs/integrations/chat/azure/).
:::

:::info

Previously, LangChain.js supported integration with Azure OpenAI using the dedicated [Azure OpenAI SDK](https://github.com/Azure/azure-sdk-for-js/tree/main/sdk/openai/openai). This SDK is now deprecated in favor of the new Azure integration in the OpenAI SDK, which allows to access the latest OpenAI models and features the same day they are released, and allows seemless transition between the OpenAI API and Azure OpenAI.

If you are using Azure OpenAI with the deprecated SDK, see the [migration guide](#migration-from-azure-openai-sdk) to update to the new API.

:::

```

[Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-services/openai/) is a Microsoft Azure service that provides powerful language models from OpenAI.

This will help you get started with AzureOpenAI completion models (LLMs) using LangChain. For detailed documentation on `AzureOpenAI` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_openai.AzureOpenAI.html).

## Overview
### Integration details

| Class | Package | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/llms/azure_openai) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [AzureOpenAI](https://api.js.langchain.com/classes/langchain_openai.AzureOpenAI.html) | [@langchain/openai](https://api.js.langchain.com/modules/langchain_openai.html) | ❌ | ✅ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/openai?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/openai?style=flat-square&label=%20&) |

## Setup

To access AzureOpenAI models you'll need to create an Azure account, get an API key, and install the `@langchain/openai` integration package.

### Credentials

Head to [azure.microsoft.com](https://azure.microsoft.com/) to sign up to AzureOpenAI and generate an API key. 

You'll also need to have an Azure OpenAI instance deployed. You can deploy a version on Azure Portal following [this guide](https://learn.microsoft.com/azure/ai-services/openai/how-to/create-resource?pivots=web-portal).

Once you have your instance running, make sure you have the name of your instance and key. You can find the key in the Azure Portal, under the "Keys and Endpoint" section of your instance.

If you're using Node.js, you can define the following environment variables to use the service:

```bash
AZURE_OPENAI_API_INSTANCE_NAME=<YOUR_INSTANCE_NAME>
AZURE_OPENAI_API_DEPLOYMENT_NAME=<YOUR_DEPLOYMENT_NAME>
AZURE_OPENAI_API_KEY=<YOUR_KEY>
AZURE_OPENAI_API_VERSION="2024-02-01"
```

Alternatively, you can pass the values directly to the `AzureOpenAI` constructor.

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain AzureOpenAI integration lives in the `@langchain/openai` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/openai @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

import { AzureOpenAI } from "@langchain/openai"

const llm = new AzureOpenAI({
  model: "gpt-3.5-turbo-instruct",
  azureOpenAIApiKey: "<your_key>", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY
  azureOpenAIApiInstanceName: "<your_instance_name>", // In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME
  azureOpenAIApiDeploymentName: "<your_deployment_name>", // In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME
  azureOpenAIApiVersion: "<api_version>", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION
  temperature: 0,
  maxTokens: undefined,
  timeout: undefined,
  maxRetries: 2,
  // other params...
})

"""
## Invocation
"""

const inputText = "AzureOpenAI is an AI company that "

const completion = await llm.invoke(inputText)
completion
# Output:
#   provides AI solutions to businesses. They offer a range of services including natural language processing, computer vision, and machine learning. Their solutions are designed to help businesses automate processes, gain insights from data, and improve decision-making. AzureOpenAI also offers consulting services to help businesses identify and implement the best AI solutions for their specific needs. They work with a variety of industries, including healthcare, finance, and retail. With their expertise in AI and their partnership with Microsoft Azure, AzureOpenAI is a trusted provider of AI solutions for businesses looking to stay ahead in the rapidly evolving world of technology.


"""
## Chaining

We can [chain](/docs/how_to/sequence/) our completion model with a prompt template like so:
"""

import { PromptTemplate } from "@langchain/core/prompts"

const prompt = new PromptTemplate({
  template: "How to say {input} in {output_language}:\n",
  inputVariables: ["input", "output_language"],
})

const chain = prompt.pipe(llm);
await chain.invoke(
  {
    output_language: "German",
    input: "I love programming.",
  }
)
# Output:
#   

#   Ich liebe Programmieren.


"""
## Using Azure Managed Identity

If you're using Azure Managed Identity, you can configure the credentials like this:
"""

import {
  DefaultAzureCredential,
  getBearerTokenProvider,
} from "@azure/identity";
import { AzureOpenAI } from "@langchain/openai";

const credentials = new DefaultAzureCredential();
const azureADTokenProvider = getBearerTokenProvider(
  credentials,
  "https://cognitiveservices.azure.com/.default"
);

const managedIdentityLLM = new AzureOpenAI({
  azureADTokenProvider,
  azureOpenAIApiInstanceName: "<your_instance_name>",
  azureOpenAIApiDeploymentName: "<your_deployment_name>",
  azureOpenAIApiVersion: "<api_version>",
});


"""
## Using a different domain

If your instance is hosted under a domain other than the default `openai.azure.com`, you'll need to use the alternate `AZURE_OPENAI_BASE_PATH` environment variable.
For example, here's how you would connect to the domain `https://westeurope.api.microsoft.com/openai/deployments/{DEPLOYMENT_NAME}`:
"""

import { AzureOpenAI } from "@langchain/openai";

const differentDomainLLM = new AzureOpenAI({
  azureOpenAIApiKey: "<your_key>", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY
  azureOpenAIApiDeploymentName: "<your_deployment_name>", // In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME
  azureOpenAIApiVersion: "<api_version>", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION
  azureOpenAIBasePath:
    "https://westeurope.api.microsoft.com/openai/deployments", // In Node.js defaults to process.env.AZURE_OPENAI_BASE_PATH
});


"""
## Migration from Azure OpenAI SDK

If you are using the deprecated Azure OpenAI SDK with the `@langchain/azure-openai` package, you can update your code to use the new Azure integration following these steps:

1. Install the new `@langchain/openai` package and remove the previous `@langchain/azure-openai` package:
   ```bash
   npm install @langchain/openai
   npm uninstall @langchain/azure-openai
   ```
2. Update your imports to use the new `AzureOpenAI` and `AzureChatOpenAI` classes from the `@langchain/openai` package:
   ```typescript
   import { AzureOpenAI } from "@langchain/openai";
   ```
3. Update your code to use the new `AzureOpenAI` and `AzureChatOpenAI` classes and pass the required parameters:

   ```typescript
   const model = new AzureOpenAI({
     azureOpenAIApiKey: "<your_key>",
     azureOpenAIApiInstanceName: "<your_instance_name>",
     azureOpenAIApiDeploymentName: "<your_deployment_name>",
     azureOpenAIApiVersion: "<api_version>",
   });
   ```

   Notice that the constructor now requires the `azureOpenAIApiInstanceName` parameter instead of the `azureOpenAIEndpoint` parameter, and adds the `azureOpenAIApiVersion` parameter to specify the API version.

   - If you were using Azure Managed Identity, you now need to use the `azureADTokenProvider` parameter to the constructor instead of `credentials`, see the [Azure Managed Identity](#using-azure-managed-identity) section for more details.

   - If you were using environment variables, you now have to set the `AZURE_OPENAI_API_INSTANCE_NAME` environment variable instead of `AZURE_OPENAI_API_ENDPOINT`, and add the `AZURE_OPENAI_API_VERSION` environment variable to specify the API version.

"""

"""
## API reference

For detailed documentation of all AzureOpenAI features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_openai.AzureOpenAI.html
"""



================================================
FILE: docs/core_docs/docs/integrations/llms/bedrock.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Bedrock
---
"""

"""
# Bedrock

```{=mdx}

:::caution
You are currently on a page documenting the use of Amazon Bedrock models as [text completion models](/docs/concepts/text_llms). Many popular models available on Bedrock are [chat completion models](/docs/concepts/chat_models).

You may be looking for [this page instead](/docs/integrations/chat/bedrock/).
:::

```

> [Amazon Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that makes Foundation Models (FMs)
> from leading AI startups and Amazon available via an API. You can choose from a wide range of FMs to find the model that is best suited for your use case.

This will help you get started with Bedrock completion models (LLMs) using LangChain. For detailed documentation on `Bedrock` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_community_llms_bedrock.Bedrock.html).

## Overview
### Integration details

| Class | Package | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/llms/bedrock) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [Bedrock](https://api.js.langchain.com/classes/langchain_community_llms_bedrock.Bedrock.html) | [@langchain/community](https://api.js.langchain.com/modules/langchain_community_llms_bedrock.html) | ❌ | ✅ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/community?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |

## Setup

To access Bedrock models you'll need to create an AWS account, get an API key, and install the `@langchain/community` integration, along with a few peer dependencies.

### Credentials

Head to [aws.amazon.com](https://aws.amazon.com) to sign up to AWS Bedrock and generate an API key. Once you've done this set the environment variables:

```bash
export BEDROCK_AWS_REGION="your-region-url"
export BEDROCK_AWS_ACCESS_KEY_ID="your-access-key-id"
export BEDROCK_AWS_SECRET_ACCESS_KEY="your-secret-access-key"
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain Bedrock integration lives in the `@langchain/community` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core
</Npm2Yarn>

And install the peer dependencies:

<Npm2Yarn>
  @aws-crypto/sha256-js @aws-sdk/credential-provider-node @smithy/protocol-http @smithy/signature-v4 @smithy/eventstream-codec @smithy/util-utf8 @aws-sdk/types
</Npm2Yarn>

You can also use Bedrock in web environments such as Edge functions or Cloudflare Workers by omitting the `@aws-sdk/credential-provider-node` dependency
and using the `web` entrypoint:

<Npm2Yarn>
  @aws-crypto/sha256-js @smithy/protocol-http @smithy/signature-v4 @smithy/eventstream-codec @smithy/util-utf8 @aws-sdk/types
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

// @lc-docs-hide-cell
// Deno requires these imports, and way of loading env vars.
// we don't want to expose in the docs.
// Below this cell we have a typescript markdown codeblock with
// the node code.
import "@aws-sdk/credential-provider-node";
import "@smithy/protocol-http";
import "@aws-crypto/sha256-js";
import "@smithy/protocol-http";
import "@smithy/signature-v4";
import "@smithy/eventstream-codec";
import "@smithy/util-utf8";
import "@aws-sdk/types";
import { Bedrock } from "@langchain/community/llms/bedrock"
import { getEnvironmentVariable } from "@langchain/core/utils/env";

const llm = new Bedrock({
  model: "anthropic.claude-v2",
  region: "us-east-1",
  // endpointUrl: "custom.amazonaws.com",
  credentials: {
    accessKeyId: getEnvironmentVariable("BEDROCK_AWS_ACCESS_KEY_ID"),
    secretAccessKey: getEnvironmentVariable("BEDROCK_AWS_SECRET_ACCESS_KEY"),
  },
  temperature: 0,
  maxTokens: undefined,
  maxRetries: 2,
  // other params...
})

"""
```typescript
import { Bedrock } from "@langchain/community/llms/bedrock"

const llm = new Bedrock({
  model: "anthropic.claude-v2",
  region: process.env.BEDROCK_AWS_REGION ?? "us-east-1",
  // endpointUrl: "custom.amazonaws.com",
  credentials: {
    accessKeyId: process.env.BEDROCK_AWS_ACCESS_KEY_ID,
    secretAccessKey: process.env.BEDROCK_AWS_SECRET_ACCESS_KEY,
  },
  temperature: 0,
  maxTokens: undefined,
  maxRetries: 2,
  // other params...
})
```
"""

"""
## Invocation

Note that some models require specific prompting techniques. For example, Anthropic's Claude-v2 model will throw an error if
the prompt does not start with `Human: `.
"""

const inputText = "Human: Bedrock is an AI company that\nAssistant: "

const completion = await llm.invoke(inputText)
completion
# Output:
#   [32m" Here are a few key points about Bedrock AI:\n"[39m +

#     [32m"\n"[39m +

#     [32m"- Bedrock was founded in 2021 and is based in San Fran"[39m... 116 more characters

"""
## Chaining

We can [chain](/docs/how_to/sequence/) our completion model with a prompt template like so:
"""

import { PromptTemplate } from "@langchain/core/prompts"

const prompt = PromptTemplate.fromTemplate("Human: How to say {input} in {output_language}:\nAssistant:")

const chain = prompt.pipe(llm);
await chain.invoke(
  {
    output_language: "German",
    input: "I love programming.",
  }
)
# Output:
#   [32m' Here is how to say "I love programming" in German:\n'[39m +

#     [32m"\n"[39m +

#     [32m"Ich liebe das Programmieren."[39m

"""
## API reference

For detailed documentation of all Bedrock features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_community_llms_bedrock.Bedrock.html
"""



================================================
FILE: docs/core_docs/docs/integrations/llms/chrome_ai.mdx
================================================
---
sidebar_label: ChromeAI
---

import CodeBlock from "@theme/CodeBlock";

# ChromeAI

:::info
This feature is **experimental** and is subject to change.
:::

:::note
The `Built-in AI Early Preview Program` by Google is currently in beta. To apply for access or find more information, please visit [this link](https://developer.chrome.com/docs/ai/built-in).
:::

ChromeAI leverages Gemini Nano to run LLMs directly in the browser or in a [worker](https://developer.mozilla.org/en-US/docs/Web/API/Worker),
without the need for an internet connection. This allows for running faster and private models without ever having data leave the consumers device.

## Getting started

Once you've been granted access to the program, follow Google's provided instructions to download the model.

Once downloaded, you can start using `ChromeAI` in the browser as follows:

```typescript
import { ChromeAI } from "@langchain/community/experimental/llms/chrome_ai";

const model = new ChromeAI({
  temperature: 0.5, // Optional, defaults to 0.5
  topK: 40, // Optional, defaults to 40
});

const response = await model.invoke("Write me a short poem please");

/*
  In the realm where moonlight weaves its hue,
  Where dreams and secrets gently intertwine,
  There's a place of tranquility and grace,
  Where whispers of the night find their place.

  Beneath the canopy of starlit skies,
  Where dreams take flight and worries cease,
  A haven of tranquility, pure and true,
  Where the heart finds solace, finding dew.

  In this realm where dreams find their release,
  Where the soul finds peace, at every peace,
  Let us wander, lost in its embrace,
  Finding solace in this tranquil space.
*/
```

### Streaming

`ChromeAI` also supports streaming outputs:

```typescript
import { ChromeAI } from "@langchain/community/experimental/llms/chrome_ai";

const model = new ChromeAI({
  temperature: 0.5, // Optional, defaults to 0.5
  topK: 40, // Optional, defaults to 40
});

for await (const chunk of await model.stream("How are you?")) {
  console.log(chunk);
}

/*
  As
   an
   AI
   language
   model
  ,
   I
   don
  '
  t
   have
   personal
   experiences
   or
   the
   ability
   to
   experience
   emotions
  .
   Therefore
  ,
   I
   cannot
   directly
   answer
   the
   question
   "
  How
   are
   you
  ?".
  
  
  
  May
   I
   suggest
   answering
   something
   else
  ?
*/
```

## Related

- LLM [conceptual guide](/docs/concepts/text_llms)
- LLM [how-to guides](/docs/how_to/#llms)



================================================
FILE: docs/core_docs/docs/integrations/llms/cloudflare_workersai.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Cloudflare Workers AI
---
"""

"""
# CloudflareWorkersAI

This will help you get started with Cloudflare Workers AI [text completion models (LLMs)](/docs/concepts/text_llms) using LangChain. For detailed documentation on `CloudflareWorkersAI` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_cloudflare.CloudflareWorkersAI.html).

## Overview
### Integration details

| Class | Package | Local | Serializable | PY support | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [`CloudflareWorkersAI`](https://api.js.langchain.com/classes/langchain_cloudflare.CloudflareWorkersAI.html) | [`@langchain/cloudflare`](https://npmjs.com/@langchain/cloudflare) | ❌ | ✅ | ❌ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/cloudflare?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/cloudflare?style=flat-square&label=%20&) |

## Setup

To access Cloudflare Workers AI models you'll need to create a Cloudflare account, get an API key, and install the `@langchain/cloudflare` integration package.

### Credentials

Head [to this page](https://developers.cloudflare.com/workers-ai/) to sign up to Cloudflare and generate an API key. Once you've done this, note your `CLOUDFLARE_ACCOUNT_ID` and `CLOUDFLARE_API_TOKEN`.

### Installation

The LangChain Cloudflare integration lives in the `@langchain/cloudflare` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/cloudflare @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

// @lc-docs-hide-cell

// @ts-expect-error Deno is not recognized
const CLOUDFLARE_ACCOUNT_ID = Deno.env.get("CLOUDFLARE_ACCOUNT_ID");
// @ts-expect-error Deno is not recognized
const CLOUDFLARE_API_TOKEN = Deno.env.get("CLOUDFLARE_API_TOKEN");

import { CloudflareWorkersAI } from "@langchain/cloudflare";

const llm = new CloudflareWorkersAI({
  model: "@cf/meta/llama-3.1-8b-instruct", // Default value
  cloudflareAccountId: CLOUDFLARE_ACCOUNT_ID,
  cloudflareApiToken: CLOUDFLARE_API_TOKEN,
  // Pass a custom base URL to use Cloudflare AI Gateway
  // baseUrl: `https://gateway.ai.cloudflare.com/v1/{YOUR_ACCOUNT_ID}/{GATEWAY_NAME}/workers-ai/`,
});

"""
## Invocation
"""

const inputText = "Cloudflare is an AI company that "

const completion = await llm.invoke(inputText);
completion
# Output:
#   [32m"Cloudflare is not an AI company, but rather a content delivery network (CDN) and security company. T"[39m... 876 more characters

"""
## Chaining

We can [chain](/docs/how_to/sequence/) our completion model with a prompt template like so:
"""

import { PromptTemplate } from "@langchain/core/prompts"

const prompt = PromptTemplate.fromTemplate("How to say {input} in {output_language}:\n")

const chain = prompt.pipe(llm);
await chain.invoke(
  {
    output_language: "German",
    input: "I love programming.",
  }
)
# Output:
#   [32m"That's a simple but sweet statement! \n"[39m +

#     [32m"\n"[39m +

#     [32m'To say "I love programming" in German, you can say: "ICH LIEB'[39m... 366 more characters

"""
## API reference

For detailed documentation of all `CloudflareWorkersAI` features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_cloudflare.CloudflareWorkersAI.html
"""



================================================
FILE: docs/core_docs/docs/integrations/llms/cohere.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Cohere
lc_docs_skip_validation: true
---
"""

"""
# Cohere

```{=mdx}

:::warning Legacy

Cohere has marked their `generate` endpoint for LLMs as deprecated. Follow their [migration guide](https://docs.cohere.com/docs/migrating-from-cogenerate-to-cochat) to start using their Chat API via the [`ChatCohere`](/docs/integrations/chat/cohere) integration.

:::

:::caution
You are currently on a page documenting the use of Cohere models as [text completion models](/docs/concepts/text_llms). Many popular models available on Cohere are [chat completion models](/docs/concepts/chat_models).

You may be looking for [this page instead](/docs/integrations/chat/cohere/).
:::

```

This will help you get started with Cohere completion models (LLMs) using LangChain. For detailed documentation on `Cohere` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_cohere.Cohere.html).

## Overview
### Integration details

| Class | Package | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/llms/cohere) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [Cohere](https://api.js.langchain.com/classes/langchain_cohere.Cohere.html) | [@langchain/cohere](https://api.js.langchain.com/modules/langchain_cohere.html) | ❌ | ✅ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/cohere?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/cohere?style=flat-square&label=%20&) |

## Setup

To access Cohere models you'll need to create a Cohere account, get an API key, and install the `@langchain/cohere` integration package.

### Credentials

Head to [cohere.com](https://cohere.com) to sign up to Cohere and generate an API key. Once you've done this set the `COHERE_API_KEY` environment variable:

```bash
export COHERE_API_KEY="your-api-key"
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain Cohere integration lives in the `@langchain/cohere` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/cohere @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

import { Cohere } from "@langchain/cohere"

const llm = new Cohere({
  model: "command",
  temperature: 0,
  maxTokens: undefined,
  maxRetries: 2,
  // other params...
})

"""
### Custom client for Cohere on Azure, Cohere on AWS Bedrock, and Standalone Cohere Instance.

We can instantiate a custom `CohereClient` and pass it to the ChatCohere constructor.

**Note:** If a custom client is provided both `COHERE_API_KEY` environment variable and `apiKey` parameter in the constructor will be ignored.
"""

import { Cohere } from "@langchain/cohere";
import { CohereClient } from "cohere-ai";

const client = new CohereClient({
  token: "<your-api-key>",
  environment: "<your-cohere-deployment-url>", //optional
  // other params
});

const llmWithCustomClient = new Cohere({
  client,
  // other params...
});

"""
## Invocation
"""

const inputText = "Cohere is an AI company that "

const completion = await llm.invoke(inputText)
completion
# Output:
#   Cohere is a company that provides natural language processing models that help companies improve human-machine interactions. Cohere was founded in 2019 by Aidan Gomez, Ivan Zhang, and Nick Frosst. 


"""
## Chaining

We can [chain](/docs/how_to/sequence/) our completion model with a prompt template like so:
"""

import { PromptTemplate } from "@langchain/core/prompts"

const prompt = new PromptTemplate({
  template: "How to say {input} in {output_language}:\n",
  inputVariables: ["input", "output_language"],
})

const chain = prompt.pipe(llm);
await chain.invoke(
  {
    output_language: "German",
    input: "I love programming.",
  }
)
# Output:
#    Ich liebe Programming.

#   

#   But for day to day purposes Ich mag Programming. would be enough and perfectly understood.

#   

#   I love programming is "Ich liebe Programming" and I like programming is "Ich mag Programming" respectively.

#   

#   There are also other ways to express this feeling, such as "Ich habe Spaß mit Programming", which means "I enjoy programming". But "Ich mag" and "Ich liebe" are the most common expressions for this.

#   

#   Let me know if I can be of further help with something else! 


"""
## API reference

For detailed documentation of all Cohere features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_cohere.Cohere.html
"""



================================================
FILE: docs/core_docs/docs/integrations/llms/deep_infra.mdx
================================================
---
sidebar_label: Deep Infra
---

import CodeBlock from "@theme/CodeBlock";

# DeepInfra

LangChain supports LLMs hosted by [Deep Infra](https://deepinfra.com/) through the `DeepInfra` wrapper.
First, you'll need to install the `@langchain/community` package:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core
```

You'll need to obtain an API key and set it as an environment variable named `DEEPINFRA_API_TOKEN`
(or pass it into the constructor), then call the model as shown below:

import Example from "@examples/models/llm/deepinfra.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## Related

- LLM [conceptual guide](/docs/concepts/text_llms)
- LLM [how-to guides](/docs/how_to/#llms)



================================================
FILE: docs/core_docs/docs/integrations/llms/fireworks.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Fireworks
---
"""

"""
# Fireworks


```{=mdx}

:::caution
You are currently on a page documenting the use of Fireworks models as [text completion models](/docs/concepts/text_llms). Many popular models available on Fireworks are [chat completion models](/docs/concepts/chat_models).

You may be looking for [this page instead](/docs/integrations/chat/fireworks/).
:::

```

[Fireworks AI](https://fireworks.ai/) is an AI inference platform to run and customize models. For a list of all models served by Fireworks see the [Fireworks docs](https://fireworks.ai/models).

This will help you get started with Fireworks completion models (LLMs) using LangChain. For detailed documentation on `Fireworks` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_community_llms_fireworks.Fireworks.html).

## Overview
### Integration details

| Class | Package | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/llms/fireworks) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [Fireworks](https://api.js.langchain.com/classes/langchain_community_llms_fireworks.Fireworks.html) | [@langchain/community](https://api.js.langchain.com/modules/langchain_community_llms_fireworks.html) | ❌ | ✅ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/community?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |

## Setup

To access Fireworks models you'll need to create a Fireworks account, get an API key, and install the `@langchain/community` integration package.

### Credentials

Head to [fireworks.ai](https://fireworks.ai/) to sign up to Fireworks and generate an API key. Once you've done this set the `FIREWORKS_API_KEY` environment variable:

```bash
export FIREWORKS_API_KEY="your-api-key"
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain Fireworks integration lives in the `@langchain/community` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

import { Fireworks } from "@langchain/community/llms/fireworks"

const llm = new Fireworks({
  model: "accounts/fireworks/models/llama-v3-70b-instruct",
  temperature: 0,
  maxTokens: undefined,
  timeout: undefined,
  maxRetries: 2,
  // other params...
})

"""
## Invocation
"""

const inputText = "Fireworks is an AI company that "

const completion = await llm.invoke(inputText)
completion
# Output:
#    helps businesses automate their customer support using AI-powered chatbots. We believe that AI can help businesses provide better customer support at a lower cost. Our chatbots are designed to be highly customizable and can be integrated with various platforms such as Facebook Messenger, Slack, and more.

#   

#   We are looking for a talented and motivated **Machine Learning Engineer** to join our team. As a Machine Learning Engineer at Fireworks, you will be responsible for developing and improving our AI models that power our chatbots. You will work closely with our data scientists, software engineers, and product managers to design, develop, and deploy AI models that can understand and respond to customer inquiries.

#   

#   **Responsibilities:**

#   

#   * Develop and improve AI models that can understand and respond to customer inquiries

#   * Work with data scientists to design and develop new AI models

#   * Collaborate with software engineers to integrate AI models with our chatbot platform

#   * Work with product managers to understand customer requirements and develop AI models that meet those requirements

#   * Develop and maintain data pipelines to support AI model development and deployment

#   * Develop and maintain tools to monitor and evaluate AI model performance

#   * Stay up-to-date with the latest developments in AI and machine learning and apply this knowledge to improve our AI models

#   

#   **Requirements:**

#   

#   * Bachelor's


"""
## Chaining

We can [chain](/docs/how_to/sequence/) our completion model with a prompt template like so:
"""

import { PromptTemplate } from "@langchain/core/prompts"

const prompt = PromptTemplate.fromTemplate("How to say {input} in {output_language}:\n")

const chain = prompt.pipe(llm);
await chain.invoke(
  {
    output_language: "German",
    input: "I love programming.",
  }
)
# Output:
#   Ich liebe Programmieren.

#   

#   How to say I love coding. in German:

#   Ich liebe Coden.

#   

#   How to say I love to code. in German:

#   Ich liebe es zu coden.

#   

#   How to say I'm a programmer. in German:

#   Ich bin ein Programmierer.

#   

#   How to say I'm a coder. in German:

#   Ich bin ein Coder.

#   

#   How to say I'm a developer. in German:

#   Ich bin ein Entwickler.

#   

#   How to say I'm a software engineer. in German:

#   Ich bin ein Software-Ingenieur.

#   

#   How to say I'm a tech enthusiast. in German:

#   Ich bin ein Technik-Enthusiast.

#   

#   How to say I'm passionate about technology. in German:

#   Ich bin leidenschaftlich für Technologie.

#   

#   How to say I'm passionate about coding. in German:

#   Ich bin leidenschaftlich für Coden.

#   

#   How to say I'm passionate about programming. in German:

#   Ich bin leidenschaftlich für Programmieren.

#   

#   How to say I enjoy coding. in German:

#   Ich genieße Coden.

#   

#   How to say I enjoy programming. in German:

#   Ich genieße Programmieren.

#   

#   How to say I'm good at coding. in German:

#   Ich bin gut im Coden.

#   

#   How to say I'm


"""
Behind the scenes, Fireworks AI uses the OpenAI SDK and OpenAI compatible API, with some caveats:

- Certain properties are not supported by the Fireworks API, see [here](https://readme.fireworks.ai/docs/openai-compatibility#api-compatibility).
- Generation using multiple prompts is not supported.

"""

"""
## API reference

For detailed documentation of all Fireworks features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_community_llms_fireworks.Fireworks.html
"""



================================================
FILE: docs/core_docs/docs/integrations/llms/friendli.mdx
================================================
# Friendli

> [Friendli](https://friendli.ai/) enhances AI application performance and optimizes cost savings with scalable, efficient deployment options, tailored for high-demand AI workloads.

This tutorial guides you through integrating `Friendli` with LangChain.

## Setup

Ensure the `@langchain/community` is installed.

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core
```

Sign in to [Friendli Suite](https://suite.friendli.ai/) to create a Personal Access Token, and set it as the `FRIENDLI_TOKEN` environment.
You can set team id as `FRIENDLI_TEAM` environment.

You can initialize a Friendli chat model with selecting the model you want to use. The default model is `mixtral-8x7b-instruct-v0-1`. You can check the available models at [docs.friendli.ai](https://docs.friendli.ai/guides/serverless_endpoints/pricing#text-generation-models).

## Usage

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/models/llm/friendli.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## Related

- LLM [conceptual guide](/docs/concepts/text_llms)
- LLM [how-to guides](/docs/how_to/#llms)



================================================
FILE: docs/core_docs/docs/integrations/llms/google_vertex_ai.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Google Vertex AI
---
"""

"""
# Google Vertex AI

```{=mdx}

:::caution
You are currently on a page documenting the use of Google Vertex models as [text completion models](/docs/concepts/text_llms). Many popular models available on Google Vertex are [chat completion models](/docs/concepts/chat_models).

You may be looking for [this page instead](/docs/integrations/chat/google_vertex_ai/).
:::

```

[Google Vertex](https://cloud.google.com/vertex-ai) is a service that exposes all foundation models available in Google Cloud, like `gemini-1.5-pro`, `gemini-1.5-flash`, etc.

This will help you get started with VertexAI completion models (LLMs) using LangChain. For detailed documentation on `VertexAI` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_google_vertexai.VertexAI.html).

## Overview

### Integration details

| Class | Package | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [VertexAI](https://api.js.langchain.com/classes/langchain_google_vertexai.VertexAI.html) | [`@langchain/google-vertexai`](https://www.npmjs.com/package/@langchain/google-vertexai) | ❌ | ✅ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/google-vertexai?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/google-vertexai?style=flat-square&label=%20&) |

## Setup

LangChain.js supports two different authentication methods based on whether
you're running in a Node.js environment or a web environment.

To access VertexAI models you'll need to create a Google Cloud Platform (GCP) account, get an API key, and install the `@langchain/google-vertexai` integration package.

### Credentials

#### Node.js

You should make sure the Vertex AI API is
enabled for the relevant project and that you've authenticated to
Google Cloud using one of these methods:

- You are logged into an account (using `gcloud auth application-default login`)
  permitted to that project.
- You are running on a machine using a service account that is permitted
  to the project.
- You have downloaded the credentials for a service account that is permitted
  to the project and set the `GOOGLE_APPLICATION_CREDENTIALS` environment
  variable to the path of this file.
  **or**
- You set the `GOOGLE_API_KEY` environment variable to the API key for the project.

#### Web

To call Vertex AI models in web environments (like Edge functions), you'll need to install
the `@langchain/google-vertexai-web` package.

Then, you'll need to add your service account credentials directly as a `GOOGLE_VERTEX_AI_WEB_CREDENTIALS` environment variable:

```
GOOGLE_VERTEX_AI_WEB_CREDENTIALS={"type":"service_account","project_id":"YOUR_PROJECT-12345",...}
```

You can also pass your credentials directly in code like this:

```typescript
import { VertexAI } from "@langchain/google-vertexai";
// Or uncomment this line if you're using the web version:
// import { VertexAI } from "@langchain/google-vertexai-web";

const model = new VertexAI({
  authOptions: {
    credentials: {"type":"service_account","project_id":"YOUR_PROJECT-12345",...},
  },
});
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain VertexAI integration lives in the `@langchain/google-vertexai` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/google-vertexai @langchain/core
</Npm2Yarn>

or for web environments:

<Npm2Yarn>
  @langchain/google-vertexai-web @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

import { VertexAI } from "@langchain/google-vertexai-web"

const llm = new VertexAI({
  model: "gemini-pro",
  temperature: 0,
  maxRetries: 2,
  // other params...
})

"""
## Invocation
"""

const inputText = "VertexAI is an AI company that "

const completion = await llm.invoke(inputText)
completion

"""
```txt
offers a wide range of cloud computing services and artificial intelligence solutions to businesses and developers worldwide.
```
"""

"""
## Chaining

We can [chain](/docs/how_to/sequence/) our completion model with a prompt template like so:
"""

import { PromptTemplate } from "@langchain/core/prompts"

const prompt = PromptTemplate.fromTemplate("How to say {input} in {output_language}:\n")

const chain = prompt.pipe(llm);
await chain.invoke(
  {
    output_language: "German",
    input: "I love programming.",
  }
)

"""
```txt
"Ich liebe Programmieren."
Pronunciation guide:

Ich: [ɪç] (similar to "ikh" with a soft "ch" sound)
liebe: [ˈliːbə] (LEE-buh)
Programmieren: [pʁoɡʁaˈmiːʁən] (pro-gra-MEE-ren)

You could also say:
"Ich liebe es zu programmieren."
Which translates more literally to "I love to program." This version is a bit more formal or precise.
Pronunciation:

es: [ɛs] (like the letter "S")
zu: [tsuː] (tsoo)

Both versions are correct and commonly used.
```
"""

"""
## API reference

For detailed documentation of all VertexAI features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_google_vertexai.VertexAI.html
"""



================================================
FILE: docs/core_docs/docs/integrations/llms/gradient_ai.mdx
================================================
---
sidebar_class_name: node-only
---

# Gradient AI

LangChain.js supports integration with Gradient AI. Check out [Gradient AI](https://docs.gradient.ai/docs) for a list of available models.

## Setup

You'll need to install the official Gradient Node SDK as a peer dependency:

```bash npm2yarn
npm i @gradientai/nodejs-sdk
```

You will need to set the following environment variables for using the Gradient AI API.

1. `GRADIENT_ACCESS_TOKEN`
2. `GRADIENT_WORKSPACE_ID`

Alternatively, these can be set during the GradientAI Class instantiation as `gradientAccessKey` and `workspaceId` respectively.
For example:

```typescript
const model = new GradientLLM({
  gradientAccessKey: "My secret Access Token"
  workspaceId: "My secret workspace id"
});
```

## Usage

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core
```

import CodeBlock from "@theme/CodeBlock";
import GradientLLMBaseExample from "@examples/llms/gradient_ai-base.ts";
import GradientLLMAdapterExample from "@examples/llms/gradient_ai-adapter.ts";

### Using Gradient's Base Models

<CodeBlock language="typescript">{GradientLLMBaseExample}</CodeBlock>

### Using your own fine-tuned Adapters

The use your own custom adapter simply set `adapterId` during setup.

<CodeBlock language="typescript">{GradientLLMAdapterExample}</CodeBlock>

## Related

- LLM [conceptual guide](/docs/concepts/text_llms)
- LLM [how-to guides](/docs/how_to/#llms)



================================================
FILE: docs/core_docs/docs/integrations/llms/huggingface_inference.mdx
================================================
# HuggingFaceInference

Here's an example of calling a HugggingFaceInference model as an LLM:

```bash npm2yarn
npm install @langchain/community @langchain/core @huggingface/inference@2
```

import UnifiedModelParamsTooltip from "@mdx_components/unified_model_params_tooltip.mdx";

<UnifiedModelParamsTooltip></UnifiedModelParamsTooltip>

```typescript
import { HuggingFaceInference } from "@langchain/community/llms/hf";

const model = new HuggingFaceInference({
  model: "gpt2",
  apiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.HUGGINGFACEHUB_API_KEY
});
const res = await model.invoke("1 + 1 =");
console.log({ res });
```

## Related

- LLM [conceptual guide](/docs/concepts/text_llms)
- LLM [how-to guides](/docs/how_to/#llms)



================================================
FILE: docs/core_docs/docs/integrations/llms/ibm.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: IBM watsonx.ai
---
"""

"""
# IBM watsonx.ai


This will help you get started with IBM [text completion models (LLMs)](/docs/concepts/text_llms) using LangChain. For detailed documentation on `IBM watsonx.ai` features and configuration options, please refer to the [IBM watsonx.ai](https://api.js.langchain.com/modules/_langchain_community.llms_ibm.html).

## Overview
### Integration details


| Class | Package | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/llms/ibm_watsonx/) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [`WatsonxLLM`](https://api.js.langchain.com/classes/_langchain_community.llms_ibm.WatsonxLLM.html) | [@langchain/community](https://www.npmjs.com/package/@langchain/community) | ❌ | ✅ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/community?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |

## Setup


To access IBM WatsonxAI models you'll need to create an IBM watsonx.ai account, get an API key or any other type of credentials, and install the `@langchain/community` integration package.

### Credentials


Head to [IBM Cloud](https://cloud.ibm.com/login) to sign up to IBM watsonx.ai and generate an API key or provide any other authentication form as presented below.

#### IAM authentication

```bash
export WATSONX_AI_AUTH_TYPE=iam
export WATSONX_AI_APIKEY=<YOUR-APIKEY>
```

#### Bearer token authentication

```bash
export WATSONX_AI_AUTH_TYPE=bearertoken
export WATSONX_AI_BEARER_TOKEN=<YOUR-BEARER-TOKEN>
```

#### IBM watsonx.ai software authentication

```bash
export WATSONX_AI_AUTH_TYPE=cp4d
export WATSONX_AI_USERNAME=<YOUR_USERNAME>
export WATSONX_AI_PASSWORD=<YOUR_PASSWORD>
export WATSONX_AI_URL=<URL>
```

Once these are placed in your environment variables and object is initialized authentication will proceed automatically.

Authentication can also be accomplished by passing these values as parameters to a new instance.

## IAM authentication

```typescript
import { WatsonxLLM } from "@langchain/community/llms/ibm";

const props = {
  version: "YYYY-MM-DD",
  serviceUrl: "<SERVICE_URL>",
  projectId: "<PROJECT_ID>",
  watsonxAIAuthType: "iam",
  watsonxAIApikey: "<YOUR-APIKEY>",
};
const instance = new WatsonxLLM(props);
```

## Bearer token authentication

```typescript
import { WatsonxLLM } from "@langchain/community/llms/ibm";

const props = {
  version: "YYYY-MM-DD",
  serviceUrl: "<SERVICE_URL>",
  projectId: "<PROJECT_ID>",
  watsonxAIAuthType: "bearertoken",
  watsonxAIBearerToken: "<YOUR-BEARERTOKEN>",
};
const instance = new WatsonxLLM(props);
```

### IBM watsonx.ai software authentication

```typescript
import { WatsonxLLM } from "@langchain/community/llms/ibm";

const props = {
  version: "YYYY-MM-DD",
  serviceUrl: "<SERVICE_URL>",
  projectId: "<PROJECT_ID>",
  watsonxAIAuthType: "cp4d",
  watsonxAIUsername: "<YOUR-USERNAME>",
  watsonxAIPassword: "<YOUR-PASSWORD>",
  watsonxAIUrl: "<url>",
};
const instance = new WatsonxLLM(props);
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain IBM watsonx.ai integration lives in the `@langchain/community` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:

"""

import { WatsonxLLM } from "@langchain/community/llms/ibm";

const props = {
  decoding_method: "sample",
  maxNewTokens: 100,
  minNewTokens: 1,
  temperature: 0.5,
  topK: 50,
  topP: 1,
};
const instance = new WatsonxLLM({
  version: "YYYY-MM-DD",
  serviceUrl: process.env.API_URL,
  projectId: "<PROJECT_ID>",
  // spaceId: "<SPACE_ID>",
  // idOrName: "<DEPLOYMENT_ID>",
  model: "<MODEL_ID>",
  ...props,
});

"""
Note:

- You must provide `spaceId`, `projectId` or `idOrName`(deployment id) unless you use lighweight engine which works without specifying either (refer to [watsonx.ai docs](https://www.ibm.com/docs/en/cloud-paks/cp-data/5.0.x?topic=install-choosing-installation-mode))
- Depending on the region of your provisioned service instance, use correct serviceUrl.
- You need to specify the model you want to use for inferencing through model_id.
"""

"""
## Invocation and generation

"""

const result = await instance.invoke("Print hello world.");
console.log(result);

const results = await instance.generate([
  "Print hello world.",
  "Print bye, bye world!",
]);
console.log(results);
# Output:
#   

#   print('Hello world.')<|endoftext|>

#   {

#     generations: [ [ [Object] ], [ [Object] ] ],

#     llmOutput: { tokenUsage: { generated_token_count: 28, input_token_count: 10 } }

#   }


"""
## Chaining

We can chain our completion model with a prompt template like so:
"""

import { PromptTemplate } from "@langchain/core/prompts"

const prompt = PromptTemplate.fromTemplate("How to say {input} in {output_language}:\n")

const chain = prompt.pipe(instance);
await chain.invoke(
  {
    output_language: "German",
    input: "I love programming.",
  }
)
# Output:
#   Ich liebe Programmieren.

#   

#   To express that you are passionate about programming in German,


"""
## Props overwriting

Passed props at initialization will last for the whole life cycle of the object, however you may overwrite them for a single method's call by passing second argument as below

"""

const result2 = await instance.invoke("Print hello world.", {
  parameters: {
    maxNewTokens: 100,
  },
});
console.log(result2);
# Output:
#   

#   print('Hello world.')<|endoftext|>


"""
## Tokenization
This package has it's custom getNumTokens implementation which returns exact amount of tokens that would be used.

"""

const tokens = await instance.getNumTokens("Print hello world.");
console.log(tokens);
# Output:
#   4


"""
## API reference

For detailed documentation of all `IBM watsonx.ai` features and configurations head to the API reference: [API docs](https://api.js.langchain.com/modules/_langchain_community.embeddings_ibm.html)
"""



================================================
FILE: docs/core_docs/docs/integrations/llms/index.mdx
================================================
---
sidebar_position: 0
sidebar_class_name: hidden
---

# LLMs

:::caution
You are currently on a page documenting the use of [text completion models](/docs/concepts/text_llms). Many of the latest and most popular models are [chat completion models](/docs/concepts/chat_models).

Unless you are specifically using more advanced prompting techniques, you are probably looking for [this page instead](/docs/integrations/chat/).
:::

[LLMs](docs/concepts/#llms) are language models that takes a string as input and return a string as output.

:::info
If you'd like to write your own LLM, see [this how-to](/docs/how_to/custom_llm). If you'd like to contribute an integration, see [Contributing integrations](/docs/contributing).
:::

## All LLMs

import { IndexTable } from "@theme/FeatureTables";

<IndexTable />



================================================
FILE: docs/core_docs/docs/integrations/llms/jigsawstack.mdx
================================================
# JigsawStack Prompt Engine

LangChain.js supports calling JigsawStack [Prompt Engine](https://docs.jigsawstack.com/api-reference/prompt-engine/run-direct) LLMs.

## Setup

- Set up an [account](https://jigsawstack.com/dashboard) (Get started for free)
- Create and retrieve your [API key](https://jigsawstack.com/dashboard)

## Credentials

```bash
export JIGSAWSTACK_API_KEY="your-api-key"
```

## Usage

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/jigsawstack
```

import CodeBlock from "@theme/CodeBlock";

```ts
import { JigsawStackPromptEngine } from "@langchain/jigsawstack";

export const run = async () => {
  const model = new JigsawStackPromptEngine();
  const res = await model.invoke(
    "Tell me about the leaning tower of pisa?\nAnswer:"
  );
  console.log({ res });
};
```

## Related

- LLM [conceptual guide](/docs/concepts/text_llms)
- LLM [how-to guides](/docs/how_to/#llms)



================================================
FILE: docs/core_docs/docs/integrations/llms/layerup_security.mdx
================================================
import CodeBlock from "@theme/CodeBlock";

# Layerup Security

The [Layerup Security](https://uselayerup.com) integration allows you to secure your calls to any LangChain LLM, LLM chain or LLM agent. The LLM object wraps around any existing LLM object, allowing for a secure layer between your users and your LLMs.

While the Layerup Security object is designed as an LLM, it is not actually an LLM itself, it simply wraps around an LLM, allowing it to adapt the same functionality as the underlying LLM.

## Setup

First, you'll need a Layerup Security account from the Layerup [website](https://uselayerup.com).

Next, create a project via the [dashboard](https://dashboard.uselayerup.com), and copy your API key. We recommend putting your API key in your project's environment.

Install the Layerup Security SDK:

```bash npm2yarn
npm install @layerup/layerup-security
```

And install LangChain Community:

```bash npm2yarn
npm install @langchain/community @langchain/core
```

And now you're ready to start protecting your LLM calls with Layerup Security!

import LayerupSecurityExampleCode from "@examples/llms/layerup_security.ts";

<CodeBlock language="typescript">{LayerupSecurityExampleCode}</CodeBlock>

## Related

- LLM [conceptual guide](/docs/concepts/text_llms)
- LLM [how-to guides](/docs/how_to/#llms)



================================================
FILE: docs/core_docs/docs/integrations/llms/llama_cpp.mdx
================================================
---
sidebar_class_name: node-only
---

# Llama CPP

:::tip Compatibility
Only available on Node.js.
:::

This module is based on the [node-llama-cpp](https://github.com/withcatai/node-llama-cpp) Node.js bindings for [llama.cpp](https://github.com/ggerganov/llama.cpp), allowing you to work with a locally running LLM. This allows you to work with a much smaller quantized model capable of running on a laptop environment, ideal for testing and scratch padding ideas without running up a bill!

## Setup

You'll need to install major version `3` of the [node-llama-cpp](https://github.com/withcatai/node-llama-cpp) module to communicate with your local model.

```bash npm2yarn
npm install -S node-llama-cpp@3
```

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core
```

You will also need a local Llama 3 model (or a model supported by [node-llama-cpp](https://github.com/withcatai/node-llama-cpp)). You will need to pass the path to this model to the LlamaCpp module as a part of the parameters (see example).

Out-of-the-box `node-llama-cpp` is tuned for running on a MacOS platform with support for the Metal GPU of Apple M-series of processors. If you need to turn this off or need support for the CUDA architecture then refer to the documentation at [node-llama-cpp](https://withcatai.github.io/node-llama-cpp/).

A note to LangChain.js contributors: if you want to run the tests associated with this module you will need to put the path to your local model in the environment variable `LLAMA_PATH`.

## Guide to installing Llama3

Getting a local Llama3 model running on your machine is a pre-req so this is a quick guide to getting and building Llama 3.1-8B (the smallest) and then quantizing it so that it will run comfortably on a laptop. To do this you will need `python3` on your machine (3.11 is recommended), also `gcc` and `make` so that `llama.cpp` can be built.

### Getting the Llama3 models

To get a copy of Llama3 you need to visit [Meta AI](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and request access to their models. Once Meta AI grant you access, you will receive an email containing a unique URL to access the files, this will be needed in the next steps.
Now create a directory to work in, for example:

```
mkdir llama3
cd llama3
```

Now we need to go to the Meta AI `llama-models` repo, which can be found [here](https://github.com/meta-llama/llama-models). In the repo, there are instructions to download the model of your choice, and you should use the unique URL that was received in your email.
The rest of the tutorial assumes that you have downloaded `Llama3.1-8B`, but any model from here on out should work. Upon downloading the model, make sure to save the model download path, this will be used for later.

### Converting and quantizing the model

In this step we need to use `llama.cpp` so we need to download that repo.

```
cd ..
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
```

Now we need to build the `llama.cpp` tools and set up our `python` environment. In these steps it's assumed that your install of python can be run using `python3` and that the virtual environment can be called `llama3`, adjust accordingly for your own situation.

```
cmake -B build
cmake --build build --config Release
python3 -m venv llama3
source llama3/bin/activate
```

After activating your llama3 environment you should see `(llama3)` prefixing your command prompt to let you know this is the active environment. Note: if you need to come back to build another model or re-quantize the model don't forget to activate the environment again also if you update `llama.cpp` you will need to rebuild the tools and possibly install new or updated dependencies! Now that we have an active python environment, we need to install the python dependencies.

```
python3 -m pip install -r requirements.txt
```

Having done this, we can start converting and quantizing the Llama3 model ready for use locally via `llama.cpp`. A conversion to a Hugging Face model is needed, followed by a conversion to a GGUF model.
First, we need to locate the path with the following script `convert_llama_weights_to_hf.py`. Copy and paste this script into your current working directory. Note that using the script may need you to pip install extra dependencies, do so as needed.
Then, we need to convert the model, prior to the conversion let's create directories to store our Hugging Face conversion and our final model.

```
mkdir models/8B
mkdir models/8B-GGUF
python3 convert_llama_weights_to_hf.py --model_size 8B --input_dir <dir-to-your-model> --output_dir models/8B --llama_version 3
python3 convert_hf_to_gguf.py --outtype f16 --outfile models/8B-GGUF/gguf-llama3-f16.bin models/8B
```

This should create a converted Hugging Face model and the final GGUF model in the directories we have created. Note that this is just a converted model so it is also around 16Gb in size, in the next step we will quantize it down to around 4Gb.

```
./build/bin/llama-quantize ./models/8B-GGUF/gguf-llama3-f16.bin ./models/8B-GGUF/gguf-llama3-Q4_0.bin Q4_0
```

Running this should result in a new model being created in the `models\8B-GGUF` directory, this one called `gguf-llama3-Q4_0.bin`, this is the model we can use with langchain. You can validate this model is working by testing it using the `llama.cpp` tools.

```
./build/bin/llama-cli -m ./models/8B-GGUF/gguf-llama3-Q4_0.bin -cnv -p "You are a helpful assistant"
```

Running this command fires up the model for a chat session. BTW if you are running out of disk space this small model is the only one we need, so you can backup and/or delete the original and converted 13.5Gb models.

## Usage

import CodeBlock from "@theme/CodeBlock";
import LlamaCppExample from "@examples/models/llm/llama_cpp.ts";

<CodeBlock language="typescript">{LlamaCppExample}</CodeBlock>

## Streaming

import LlamaCppStreamExample from "@examples/models/llm/llama_cpp_stream.ts";

<CodeBlock language="typescript">{LlamaCppStreamExample}</CodeBlock>;

## Related

- LLM [conceptual guide](/docs/concepts/text_llms)
- LLM [how-to guides](/docs/how_to/#llms)



================================================
FILE: docs/core_docs/docs/integrations/llms/mistral.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: MistralAI
---
"""

"""
# MistralAI

```{=mdx}

:::tip
Want to run Mistral's models locally? Check out our [Ollama integration](/docs/integrations/chat/ollama).
:::

:::caution
You are currently on a page documenting the use of Mistral models as [text completion models](/docs/concepts/text_llms). Many popular models available on Mistral are [chat completion models](/docs/concepts/chat_models).

You may be looking for [this page instead](/docs/integrations/chat/mistral/).
:::

```

[Mistral AI](https://mistral.ai/) is a platform that offers hosting for their powerful [open source models](https://docs.mistral.ai/getting-started/models/).

This will help you get started with MistralAI completion models (LLMs) using LangChain. For detailed documentation on `MistralAI` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_mistralai.MistralAI.html).

## Overview
### Integration details

| Class | Package | Local | Serializable | PY support | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [MistralAI](https://api.js.langchain.com/classes/langchain_mistralai.MistralAI.html) | [`@langchain/mistralai`](https://www.npmjs.com/package/@langchain/mistralai) | ❌ | ✅ | ❌ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/mistralai?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/mistralai?style=flat-square&label=%20&) |

## Setup

To access MistralAI models you'll need to create a MistralAI account, get an API key, and install the `@langchain/mistralai` integration package.

### Credentials

Head to [console.mistral.ai](https://console.mistral.ai/) to sign up to MistralAI and generate an API key. Once you've done this set the `MISTRAL_API_KEY` environment variable:

```bash
export MISTRAL_API_KEY="your-api-key"
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain MistralAI integration lives in the `@langchain/mistralai` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/mistralai @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

import { MistralAI } from "@langchain/mistralai"

const llm = new MistralAI({
  model: "codestral-latest",
  temperature: 0,
  maxTokens: undefined,
  maxRetries: 2,
  // other params...
})

"""
## Invocation
"""

const inputText = "MistralAI is an AI company that "

const completion = await llm.invoke(inputText)
completion
# Output:
#    has developed Mistral 7B, a large language model (LLM) that is open-source and available for commercial use. Mistral 7B is a 7 billion parameter model that is trained on a diverse and high-quality dataset, and it has been fine-tuned to perform well on a variety of tasks, including text generation, question answering, and code interpretation.

#   

#   MistralAI has made Mistral 7B available under a permissive license, allowing anyone to use the model for commercial purposes without having to pay any fees. This has made Mistral 7B a popular choice for businesses and organizations that want to leverage the power of large language models without incurring high costs.

#   

#   Mistral 7B has been trained on a diverse and high-quality dataset, which has enabled it to perform well on a variety of tasks. It has been fine-tuned to generate coherent and contextually relevant text, and it has been shown to be capable of answering complex questions and interpreting code.

#   

#   Mistral 7B is also a highly efficient model, capable of processing text at a fast pace. This makes it well-suited for applications that require real-time responses, such as chatbots and virtual assistants.

#   

#   Overall, Mistral 7B is a powerful and versatile large language model that is open-source and available for commercial use. Its ability to perform well on a variety of tasks, its efficiency, and its permissive license make it a popular choice for businesses and organizations that want to leverage the power of large language models.


"""
## Chaining

We can [chain](/docs/how_to/sequence/) our completion model with a prompt template like so:
"""

import { PromptTemplate } from "@langchain/core/prompts"

const prompt = PromptTemplate.fromTemplate("How to say {input} in {output_language}:\n")

const chain = prompt.pipe(llm);
await chain.invoke(
  {
    output_language: "German",
    input: "I love programming.",
  }
)
# Output:
#   

#   I love programming.

#   

#   Ich liebe Programmieren.

#   

#   In German, the phrase "I love programming" is translated as "Ich liebe Programmieren." The word "programming" is translated to "Programmieren," and "I love" is translated to "Ich liebe."


"""
Since the Mistral LLM is a completions model, they also allow you to insert a `suffix` to the prompt. Suffixes can be passed via the call options when invoking a model like so:
"""

const suffixResponse = await llm.invoke(
  "You can print 'hello world' to the console in javascript like this:\n```javascript", {
    suffix: "```"
  }
);
console.log(suffixResponse);
# Output:
#   

#   console.log('hello world');

#   ```


"""
As seen in the first example, the model generated the requested `console.log('hello world')` code snippet, but also included extra unwanted text. By adding a suffix, we can constrain the model to only complete the prompt up to the suffix (in this case, three backticks). This allows us to easily parse the completion and extract only the desired response without the suffix using a custom output parser.
"""

import { MistralAI } from "@langchain/mistralai";

const llmForFillInCompletion = new MistralAI({
  model: "codestral-latest",
  temperature: 0,
});

const suffix = "```";

const customOutputParser = (input: string) => {
  if (input.includes(suffix)) {
    return input.split(suffix)[0];
  }
  throw new Error("Input does not contain suffix.")
};

const resWithParser = await llmForFillInCompletion.invoke(
  "You can print 'hello world' to the console in javascript like this:\n```javascript", {
    suffix,
  }
);

console.log(customOutputParser(resWithParser));
# Output:
#   

#   console.log('hello world');

#   


"""
## Hooks

Mistral AI supports custom hooks for three events: beforeRequest, requestError, and reponse. Examples of the function signature for each hook type can be seen below:
"""

const beforeRequestHook = (req: Request): Request | void | Promise<Request | void> => {
    // Code to run before a request is processed by Mistral
};

const requestErrorHook = (err: unknown, req: Request): void | Promise<void> => {
    // Code to run when an error occurs as Mistral is processing a request
};

const responseHook = (res: Response, req: Request): void | Promise<void> => {
    // Code to run before Mistral sends a successful response
};

"""
To add these hooks to the chat model, either pass them as arguments and they are automatically added:
"""

import { ChatMistralAI } from "@langchain/mistralai" 

const modelWithHooks = new ChatMistralAI({
    model: "mistral-large-latest",
    temperature: 0,
    maxRetries: 2,
    beforeRequestHooks: [ beforeRequestHook ],
    requestErrorHooks: [ requestErrorHook ],
    responseHooks: [ responseHook ],
    // other params...
});

"""
Or assign and add them manually after instantiation:
"""

import { ChatMistralAI } from "@langchain/mistralai" 

const model = new ChatMistralAI({
    model: "mistral-large-latest",
    temperature: 0,
    maxRetries: 2,
    // other params...
});

model.beforeRequestHooks = [ ...model.beforeRequestHooks, beforeRequestHook ];
model.requestErrorHooks = [ ...model.requestErrorHooks, requestErrorHook ];
model.responseHooks = [ ...model.responseHooks, responseHook ];

model.addAllHooksToHttpClient();

"""
The method addAllHooksToHttpClient clears all currently added hooks before assigning the entire updated hook lists to avoid hook duplication.

Hooks can be removed one at a time, or all hooks can be cleared from the model at once.
"""

model.removeHookFromHttpClient(beforeRequestHook);

model.removeAllHooksFromHttpClient();

"""
## API reference

For detailed documentation of all MistralAI features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_mistralai.MistralAI.html
"""



================================================
FILE: docs/core_docs/docs/integrations/llms/ni_bittensor.mdx
================================================
---
sidebar_class_name: hidden
---

# NIBittensor

:::warning
This module has been deprecated and is no longer supported. The documentation below will not work in versions 0.2.0 or later.
:::

LangChain.js offers experimental support for Neural Internet's Bittensor LLM models.

Here's an example:

```typescript
import { NIBittensorLLM } from "langchain/experimental/llms/bittensor";

const model = new NIBittensorLLM();

const res = await model.invoke(`What is Bittensor?`);

console.log({ res });

/*
  {
    res: "\nBittensor is opensource protocol..."
  }
 */
```

## Related

- LLM [conceptual guide](/docs/concepts/text_llms)
- LLM [how-to guides](/docs/how_to/#llms)



================================================
FILE: docs/core_docs/docs/integrations/llms/ollama.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Ollama
---
"""

"""
# Ollama

```{=mdx}

:::caution
You are currently on a page documenting the use of Ollama models as [text completion models](/docs/concepts/text_llms). Many popular models available on Ollama are [chat completion models](/docs/concepts/chat_models).

You may be looking for [this page instead](/docs/integrations/chat/ollama/).
:::

```

This will help you get started with Ollama [text completion models (LLMs)](/docs/concepts/text_llms) using LangChain. For detailed documentation on `Ollama` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_ollama.Ollama.html).

## Overview
### Integration details

[Ollama](https://ollama.ai/) allows you to run open-source large language models, such as Llama 3, locally.

Ollama bundles model weights, configuration, and data into a single package, defined by a Modelfile. It optimizes setup and configuration details, including GPU usage.

This example goes over how to use LangChain to interact with an Ollama-run Llama 2 7b instance.
For a complete list of supported models and model variants, see the [Ollama model library](https://github.com/jmorganca/ollama#model-library).

| Class | Package | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/llms/ollama/) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [`Ollama`](https://api.js.langchain.com/classes/langchain_ollama.Ollama.html) | [`@langchain/ollama`](https://npmjs.com/@langchain/ollama) | ✅ | ❌ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/ollama?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/ollama?style=flat-square&label=%20&) |

## Setup

To access Ollama embedding models you'll need to follow [these instructions](https://github.com/jmorganca/ollama) to install Ollama, and install the `@langchain/ollama` integration package.

### Credentials

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain Ollama integration lives in the `@langchain/ollama` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/ollama @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

import { Ollama } from "@langchain/ollama"

const llm = new Ollama({
  model: "llama3", // Default value
  temperature: 0,
  maxRetries: 2,
  // other params...
})

"""
## Invocation
"""

const inputText = "Ollama is an AI company that "

const completion = await llm.invoke(inputText)
completion
# Output:
#   I think you meant to say "Olivia" instead of "Ollama". Olivia is not a well-known AI company, but there are several other AI companies with similar names. Here are a few examples:

#   

#   * Oliva AI: A startup that uses artificial intelligence to help businesses optimize their operations and improve customer experiences.

#   * Olivia Technologies: A company that develops AI-powered solutions for industries such as healthcare, finance, and education.

#   * Olivia.ai: A platform that uses AI to help businesses automate their workflows and improve productivity.

#   

#   If you meant something else by "Ollama", please let me know and I'll do my best to help!


"""
## Chaining

We can [chain](/docs/how_to/sequence/) our completion model with a prompt template like so:
"""

import { PromptTemplate } from "@langchain/core/prompts"

const prompt = PromptTemplate.fromTemplate("How to say {input} in {output_language}:\n")

const chain = prompt.pipe(llm);
await chain.invoke(
  {
    output_language: "German",
    input: "I love programming.",
  }
)
# Output:
#   A programmer's passion!

#   

#   In German, you can express your love for programming with the following phrases:

#   

#   1. Ich liebe Programmieren: This is a direct translation of "I love programming."

#   2. Programmieren ist meine Leidenschaft: This means "Programming is my passion."

#   3. Ich bin total verliebt in Programmieren: This translates to "I'm totally in love with programming."

#   4. Programmieren macht mich glücklich: This phrase means "Programming makes me happy" or "I'm joyful when programming."

#   

#   If you want to be more casual, you can use:

#   

#   1. Ich bin ein Programmier-Fan: This is a playful way to say "I'm a fan of programming."

#   2. Programmieren ist mein Ding: This translates to "Programming is my thing" or "I'm all about programming."

#   

#   Remember that German has different forms for formal and informal speech, so adjust the phrases according to your relationship with the person you're speaking to!


"""
## Multimodal models

Ollama supports open source multimodal models like [LLaVA](https://ollama.ai/library/llava) in versions 0.1.15 and up.
You can bind base64 encoded image data to multimodal-capable models to use as context like this:
"""

import { Ollama } from "@langchain/ollama";
import * as fs from "node:fs/promises";

const imageData = await fs.readFile("../../../../../examples/hotdog.jpg");

const model = new Ollama({
  model: "llava",
}).bind({
  images: [imageData.toString("base64")],
});

const res = await model.invoke("What's in this image?");
console.log(res);
# Output:
#    The image shows a hot dog placed inside what appears to be a bun that has been specially prepared to resemble a hot dog bun. This is an example of a creative or novelty food item, where the bread used for the bun looks similar to a cooked hot dog itself, playing on the name "hot dog." The image also shows the typical garnishes like ketchup and mustard on the side. 


"""
## Related

- LLM [conceptual guide](/docs/concepts/text_llms)
- LLM [how-to guides](/docs/how_to/#llms)
"""

"""
## API reference

For detailed documentation of all `Ollama` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_ollama.Ollama.html)
"""



================================================
FILE: docs/core_docs/docs/integrations/llms/openai.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: OpenAI
---
"""

"""
# OpenAI

```{=mdx}

:::caution
You are currently on a page documenting the use of OpenAI [text completion models](/docs/concepts/text_llms). The latest and most popular OpenAI models are [chat completion models](/docs/concepts/chat_models).

Unless you are specifically using `gpt-3.5-turbo-instruct`, you are probably looking for [this page instead](/docs/integrations/chat/openai/).
:::

```

[OpenAI](https://en.wikipedia.org/wiki/OpenAI) is an artificial intelligence (AI) research laboratory.

This will help you get started with OpenAI completion models (LLMs) using LangChain. For detailed documentation on `OpenAI` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_openai.OpenAI.html).

## Overview
### Integration details

| Class | Package | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/llms/openai) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [OpenAI](https://api.js.langchain.com/classes/langchain_openai.OpenAI.html) | [@langchain/openai](https://www.npmjs.com/package/@langchain/openai) | ❌ | ✅ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/openai?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/openai?style=flat-square&label=%20&) |

## Setup

To access OpenAI models you'll need to create an OpenAI account, get an API key, and install the `@langchain/openai` integration package.

### Credentials

Head to [platform.openai.com](https://platform.openai.com/) to sign up to OpenAI and generate an API key. Once you've done this set the `OPENAI_API_KEY` environment variable:

```bash
export OPENAI_API_KEY="your-api-key"
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain OpenAI integration lives in the `@langchain/openai` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/openai @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

import { OpenAI } from "@langchain/openai"

const llm = new OpenAI({
  model: "gpt-3.5-turbo-instruct",
  temperature: 0,
  maxTokens: undefined,
  timeout: undefined,
  maxRetries: 2,
  apiKey: process.env.OPENAI_API_KEY,
  // other params...
})

"""
## Invocation
"""

const inputText = "OpenAI is an AI company that "

const completion = await llm.invoke(inputText)
completion
# Output:
#   develops and promotes friendly AI for the benefit of humanity. It was founded in 2015 by Elon Musk, Sam Altman, Greg Brockman, Ilya Sutskever, Wojciech Zaremba, John Schulman, and Chris Olah. The company's mission is to create and promote artificial general intelligence (AGI) that is safe and beneficial to humanity.

#   

#   OpenAI conducts research in various areas of AI, including deep learning, reinforcement learning, robotics, and natural language processing. The company also develops and releases open-source tools and platforms for AI research, such as the GPT-3 language model and the Gym toolkit for reinforcement learning.

#   

#   One of the main goals of OpenAI is to ensure that the development of AI is aligned with human values and does not pose a threat to humanity. To this end, the company has established a set of principles for safe and ethical AI development, and it actively collaborates with other organizations and researchers in the field.

#   

#   OpenAI has received funding from various sources, including tech giants like Microsoft and Amazon, as well as individual investors. It has also partnered with companies and organizations such as Google, IBM, and the United Nations to advance its research and promote responsible AI development.

#   

#   In addition to its research and development


"""
## Chaining

We can [chain](/docs/how_to/sequence/) our completion model with a prompt template like so:
"""

import { PromptTemplate } from "@langchain/core/prompts"

const prompt = new PromptTemplate({
  template: "How to say {input} in {output_language}:\n",
  inputVariables: ["input", "output_language"],
})

const chain = prompt.pipe(llm);
await chain.invoke(
  {
    output_language: "German",
    input: "I love programming.",
  }
)
# Output:
#   

#   Ich liebe Programmieren.


"""
If you're part of an organization, you can set `process.env.OPENAI_ORGANIZATION` to your OpenAI organization id, or pass it in as `organization` when
initializing the model.

## Custom URLs

You can customize the base URL the SDK sends requests to by passing a `configuration` parameter like this:
"""

const llmCustomURL = new OpenAI({
  temperature: 0.9,
  configuration: {
    baseURL: "https://your_custom_url.com",
  },
});

"""
You can also pass other `ClientOptions` parameters accepted by the official SDK.

If you are hosting on Azure OpenAI, see the [dedicated page instead](/docs/integrations/llms/azure).

"""

"""
## API reference

For detailed documentation of all OpenAI features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_openai.OpenAI.html
"""



================================================
FILE: docs/core_docs/docs/integrations/llms/prompt_layer_openai.mdx
================================================
---
sidebar_class_name: hidden
---

# PromptLayer OpenAI

:::warning
This module has been deprecated and is no longer supported. The documentation below will not work in versions 0.2.0 or later.
:::

LangChain integrates with PromptLayer for logging and debugging prompts and responses. To add support for PromptLayer:

1. Create a PromptLayer account here: [https://promptlayer.com](https://promptlayer.com).
2. Create an API token and pass it either as `promptLayerApiKey` argument in the `PromptLayerOpenAI` constructor or in the `PROMPTLAYER_API_KEY` environment variable.

```typescript
import { PromptLayerOpenAI } from "langchain/llms/openai";

const model = new PromptLayerOpenAI({
  temperature: 0.9,
  apiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.OPENAI_API_KEY
  promptLayerApiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.PROMPTLAYER_API_KEY
});
const res = await model.invoke(
  "What would be a good company name a company that makes colorful socks?"
);
```

# Azure PromptLayerOpenAI

LangChain also integrates with PromptLayer for Azure-hosted OpenAI instances:

```typescript
import { PromptLayerOpenAI } from "langchain/llms/openai";

const model = new PromptLayerOpenAI({
  temperature: 0.9,
  azureOpenAIApiKey: "YOUR-AOAI-API-KEY", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY
  azureOpenAIApiInstanceName: "YOUR-AOAI-INSTANCE-NAME", // In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME
  azureOpenAIApiDeploymentName: "YOUR-AOAI-DEPLOYMENT-NAME", // In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME
  azureOpenAIApiCompletionsDeploymentName:
    "YOUR-AOAI-COMPLETIONS-DEPLOYMENT-NAME", // In Node.js defaults to process.env.AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME
  azureOpenAIApiEmbeddingsDeploymentName:
    "YOUR-AOAI-EMBEDDINGS-DEPLOYMENT-NAME", // In Node.js defaults to process.env.AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME
  azureOpenAIApiVersion: "YOUR-AOAI-API-VERSION", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION
  azureOpenAIBasePath: "YOUR-AZURE-OPENAI-BASE-PATH", // In Node.js defaults to process.env.AZURE_OPENAI_BASE_PATH
  promptLayerApiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.PROMPTLAYER_API_KEY
});
const res = await model.invoke(
  "What would be a good company name a company that makes colorful socks?"
);
```

The request and the response will be logged in the [PromptLayer dashboard](https://promptlayer.com/home).

> **_Note:_** In streaming mode PromptLayer will not log the response.

## Related

- LLM [conceptual guide](/docs/concepts/text_llms)
- LLM [how-to guides](/docs/how_to/#llms)



================================================
FILE: docs/core_docs/docs/integrations/llms/raycast.mdx
================================================
# RaycastAI

> **Note:** This is a community-built integration and is not officially supported by Raycast.

You can utilize the LangChain's RaycastAI class within the [Raycast Environment](https://developers.raycast.com/api-reference/ai) to enhance your Raycast extension with Langchain's capabilities.

- The RaycastAI class is only available in the Raycast environment and only to [Raycast Pro](https://www.raycast.com/pro) users as of August 2023. You may check how to create an extension for Raycast [here](https://developers.raycast.com/).

- There is a rate limit of approx 10 requests per minute for each Raycast Pro user. If you exceed this limit, you will receive an error. You can set your desired rpm limit by passing `rateLimitPerMinute` to the `RaycastAI` constructor as shown in the example, as this rate limit may change in the future.

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core
```

import CodeBlock from "@theme/CodeBlock";

```ts
import { RaycastAI } from "@langchain/community/llms/raycast";

import { Tool } from "@langchain/core/tools";

const model = new RaycastAI({
  rateLimitPerMinute: 10, // It is 10 by default so you can omit this line
  model: "<model_name>",
  creativity: 0, // `creativity` is a term used by Raycast which is equivalent to `temperature` in some other LLMs
});
```

## Related

- LLM [conceptual guide](/docs/concepts/text_llms)
- LLM [how-to guides](/docs/how_to/#llms)



================================================
FILE: docs/core_docs/docs/integrations/llms/replicate.mdx
================================================
import CodeBlock from "@theme/CodeBlock";

# Replicate

Here's an example of calling a Replicate model as an LLM:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install replicate@1 @langchain/community @langchain/core
```

import ReplicateLlama2 from "@examples/models/llm/replicate_llama2.ts";

<CodeBlock language="typescript">{ReplicateLlama2}</CodeBlock>

You can run other models through Replicate by changing the `model` parameter.

You can find a full list of models on [Replicate's website](https://replicate.com/explore).

## Related

- LLM [conceptual guide](/docs/concepts/text_llms)
- LLM [how-to guides](/docs/how_to/#llms)



================================================
FILE: docs/core_docs/docs/integrations/llms/together.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Together AI
---
"""

"""
# TogetherAI

:::caution
You are currently on a page documenting the use of Together AI models as [text completion models](/docs/concepts/text_llms). Many popular models available on Together AI are [chat completion models](/docs/concepts/chat_models).

You may be looking for [this page instead](/docs/integrations/chat/togetherai/).
:::

[Together AI](https://www.together.ai/) offers an API to query [50+ leading open-source models](https://docs.together.ai/docs/inference-models) in a couple lines of code.

This will help you get started with Together AI [text completion models (LLMs)](/docs/concepts/text_llms) using LangChain. For detailed documentation on `TogetherAI` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_community_llms_togetherai.TogetherAI.html).

## Overview
### Integration details

| Class | Package | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/llms/together/) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [`TogetherAI`](https://api.js.langchain.com/classes/langchain_community_llms_togetherai.TogetherAI.html) | [`@langchain/community`](https://npmjs.com/@langchain/community) | ❌ | ✅ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/community?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |

## Setup

To access `ChatTogetherAI` models you'll need to create a Together account, get an API key [here](https://api.together.xyz/), and install the `@langchain/community` integration package.

### Credentials

Head to [api.together.ai](https://api.together.ai/) to sign up to TogetherAI and generate an API key. Once you've done this set the `TOGETHER_AI_API_KEY` environment variable:

```bash
export TOGETHER_AI_API_KEY="your-api-key"
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain TogetherAI integration lives in the `@langchain/community` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

import { TogetherAI } from "@langchain/community/llms/togetherai";

const llm = new TogetherAI({
  model: "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
  maxTokens: 256,
});

"""
## Invocation
"""

const inputText = "Together is an AI company that "

const completion = await llm.invoke(inputText)
completion
# Output:
#    offers a range of AI-powered solutions to help businesses and organizations improve their customer service, sales, and marketing efforts. Their platform uses natural language processing (NLP) and machine learning algorithms to analyze customer interactions and provide insights and recommendations to help businesses improve their customer experience.

#   Together's solutions include:

#   1. Customer Service: Together's customer service solution uses AI to analyze customer interactions and provide insights and recommendations to help businesses improve their customer experience. This includes analyzing customer feedback, sentiment analysis, and predictive analytics to identify areas for improvement.

#   2. Sales: Together's sales solution uses AI to analyze customer interactions and provide insights and recommendations to help businesses improve their sales efforts. This includes analyzing customer behavior, sentiment analysis, and predictive analytics to identify opportunities for upselling and cross-selling.

#   3. Marketing: Together's marketing solution uses AI to analyze customer interactions and provide insights and recommendations to help businesses improve their marketing efforts. This includes analyzing customer behavior, sentiment analysis, and predictive analytics to identify areas for improvement.

#   Together's platform is designed to be easy to use and integrates with a range of popular CRM and marketing automation tools. Their solutions are available as a cloud-based subscription service, making it easy for businesses to get started with AI-powered customer service, sales, and marketing.

#   Overall,


"""
## Chaining

We can [chain](/docs/how_to/sequence/) our completion model with a prompt template like so:
"""

import { PromptTemplate } from "@langchain/core/prompts"

const prompt = PromptTemplate.fromTemplate("How to say {input} in {output_language}:\n")

const chain = prompt.pipe(llm);
await chain.invoke(
  {
    output_language: "German",
    input: "I love programming.",
  }
)
# Output:
#   Ich liebe Programmieren.

#   

#   How to say I love programming. in French:

#   J'adore programmer.

#   

#   How to say I love programming. in Spanish:

#   Me encanta programar.

#   

#   How to say I love programming. in Italian:

#   Mi piace programmare.

#   

#   How to say I love programming. in Portuguese:

#   Eu amo programar.

#   

#   How to say I love programming. in Russian:

#   Я люблю программирование.

#   

#   How to say I love programming. in Japanese:

#   私はプログラミングが好きです。

#   

#   How to say I love programming. in Chinese:

#   我喜欢编程。

#   

#   How to say I love programming. in Korean:

#   나는 프로그래밍을 좋아합니다.

#   

#   How to say I love programming. in Arabic:

#   أنا أحب البرمجة.

#   

#   How to say I love programming. in Hebrew:

#   אני אוהבת לתכנת.

#   

#   How to say I love programming. in Hindi:

#   

#   मुझे प्रोग्रामिंग पसंद है।

#   

#   

#   

#   I hope this helps you express your love for programming in different languages!


"""
## API reference

For detailed documentation of all `TogetherAi` features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_community_llms_togetherai.TogetherAI.html
"""



================================================
FILE: docs/core_docs/docs/integrations/llms/writer.mdx
================================================
# Writer

LangChain.js supports calling [Writer](https://writer.com/) LLMs.

## Setup

First, you'll need to sign up for an account at https://writer.com/. Create a service account and note your API key.

Next, you'll need to install the official package as a peer dependency:

```bash npm2yarn
yarn add @writerai/writer-sdk
```

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core
```

## Usage

import CodeBlock from "@theme/CodeBlock";
import WriterExample from "@examples/models/llm/writer.ts";

<CodeBlock language="typescript">{WriterExample}</CodeBlock>

## Related

- LLM [conceptual guide](/docs/concepts/text_llms)
- LLM [how-to guides](/docs/how_to/#llms)



================================================
FILE: docs/core_docs/docs/integrations/llms/yandex.mdx
================================================
# YandexGPT

LangChain.js supports calling [YandexGPT](https://cloud.yandex.com/en/services/yandexgpt) LLMs.

## Setup

First, you should [create service account](https://cloud.yandex.com/en/docs/iam/operations/sa/create) with the `ai.languageModels.user` role.

Next, you have two authentication options:

- [IAM token](https://cloud.yandex.com/en/docs/iam/operations/iam-token/create-for-sa).
  You can specify the token in a constructor parameter `iam_token` or in an environment variable `YC_IAM_TOKEN`.
- [API key](https://cloud.yandex.com/en/docs/iam/operations/api-key/create)
  You can specify the key in a constructor parameter `api_key` or in an environment variable `YC_API_KEY`.

## Usage

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/yandex @langchain/core
```

import CodeBlock from "@theme/CodeBlock";
import YandexGPTExample from "@examples/models/llm/yandex.ts";

<CodeBlock language="typescript">{YandexGPTExample}</CodeBlock>

## Related

- LLM [conceptual guide](/docs/concepts/text_llms)
- LLM [how-to guides](/docs/how_to/#llms)



================================================
FILE: docs/core_docs/docs/integrations/memory/astradb.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# Astra DB Chat Memory

For longer-term persistence across chat sessions, you can swap out the default in-memory `chatHistory` that backs chat memory classes like `BufferMemory` for Astra DB.

## Setup

You need to install the Astra DB TS client:

```bash npm2yarn
npm install @datastax/astra-db-ts
```

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

## Configuration and Initalization

There are two ways to inialize your `AstraDBChatMessageHistory`

If you already have an instance of the `AstraDB` client defined you can connect to your collection and initialize an instance of the `ChatMessageHistory` using the constuctor.

```typescript
const client = (client = new AstraDB(
  process.env.ASTRA_DB_APPLICATION_TOKEN,
  process.env.ASTRA_DB_ENDPOINT,
  process.env.ASTRA_DB_NAMESPACE
));

const collection = await client.collection("YOUR_COLLECTION_NAME");

const chatHistory = new AstraDBChatMessageHistory({
  collection,
  sessionId: "YOUR_SESSION_ID",
});
```

If you don't already have an instance of an `AstraDB` client you can use the `initialize` method.

```typescript
const chatHistory = await AstraDBChatMessageHistory.initialize({
  token: process.env.ASTRA_DB_APPLICATION_TOKEN ?? "token",
  endpoint: process.env.ASTRA_DB_ENDPOINT ?? "endpoint",
  namespace: process.env.ASTRA_DB_NAMESPACE,
  collectionName: "YOUR_COLLECTION_NAME",
  sessionId: "YOUR_SESSION_ID",
});
```

## Usage

:::tip Tip
Your collection must already exist
:::

import Example from "@examples/memory/astradb.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/memory/aurora_dsql.mdx
================================================
---
hide_table_of_contents: true
sidebar_class_name: node-only
---

import CodeBlock from "@theme/CodeBlock";

# Aurora DSQL Chat Memory

For longer-term persistence across chat sessions, you can swap out the default in-memory `chatHistory` for the serverless PostgreSQL-compatible [Amazon Aurora DSQL](https://aws.amazon.com/rds/aurora/dsql/) Database.

This is very similar to the PostgreSQL integration with a few differences to make it compatible with DSQL:

1. The `id` column in PostgreSQL is SERIAL auto-incrementent, and DSQL is UUID using the database function `gen_random_uuid`.
2. A `created_at` column is created to track the order and history of the messages.
3. The `message` column in PostgreSQL is JSONB, and DSQL is TEXT with Javascript parsing handling

## Setup

Go to you AWS Console and create an Aurora DSQL Cluster, https://console.aws.amazon.com/dsql/clusters

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core pg @aws-sdk/dsql-signer
```

## Usage

Each chat history session is stored in a Aurora DSQL (Postgres-compatible) database and requires a session id.

The connection to Aurora DSQL is handled through a PostgreSQL pool. You can either pass an instance of a pool via the `pool` parameter or pass a pool config via the `poolConfig` parameter. See [pg-node docs on pools](https://node-postgres.com/apis/pool)
for more information. A provided pool takes precedence, thus if both a pool instance and a pool config are passed, only the pool will be used.

For options on how to do the authentication and authorization for DSQL please check https://docs.aws.amazon.com/aurora-dsql/latest/userguide/authentication-authorization.html.

The following example uses the AWS-SDK to generate an authentication token that is passed to the pool configuration:

import Example from "@examples/memory/aurora_dsql.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/memory/azure_cosmos_mongo_vcore.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# Azure Cosmos DB Mongo vCore Chat Message History

The AzureCosmosDBMongoChatMessageHistory uses Azure Cosmos DB Mongo vCore to store chat message history. For longer-term persistence across chat sessions, you can swap out the default in-memory `chatHistory` that backs chat memory classes like `BufferMemory`.
If you don't have an Azure account, you can [create a free account](https://azure.microsoft.com/free/) to get started.

## Setup

You'll first need to install the [`@langchain/azure-cosmosdb`](https://www.npmjs.com/package/@langchain/azure-cosmosdb) package:

```bash npm2yarn
npm install @langchain/azure-cosmosdb @langchain/core
```

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

You'll also need to have an Azure Cosmos DB mongo vCore instance running. You can deploy a free version on Azure Portal without any cost, following [this guide](https://learn.microsoft.com/en-us/azure/cosmos-db/mongodb/vcore/quickstart-portal).

Once you have your instance running, make sure you have the connection string.

## Usage

import Example from "@examples/memory/azure_cosmosdb_mongo.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/memory/azure_cosmosdb_nosql.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# Azure Cosmos DB NoSQL Chat Message History

The AzureCosmosDBNoSQLChatMessageHistory uses Cosmos DB to store chat message history. For longer-term persistence across chat sessions, you can swap out the default in-memory `chatHistory` that backs chat memory classes like `BufferMemory`.
If you don't have an Azure account, you can [create a free account](https://azure.microsoft.com/free/) to get started.

## Setup

You'll first need to install the [`@langchain/azure-cosmosdb`](https://www.npmjs.com/package/@langchain/azure-cosmosdb) package:

```bash npm2yarn
npm install @langchain/azure-cosmosdb @langchain/core
```

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

You'll also need to have an Azure Cosmos DB for NoSQL instance running. You can deploy a free version on Azure Portal without any cost, following [this guide](https://learn.microsoft.com/azure/cosmos-db/nosql/quickstart-portal).

Once you have your instance running, make sure you have the connection string. If you are using Managed Identity, you need to have the endpoint. You can find them in the Azure Portal, under the "Settings / Keys" section of your instance.

:::info

When using Azure Managed Identity and role-based access control, you must ensure that the database and container have been created beforehand. RBAC does not provide permissions to create databases and containers. You can get more information about the permission model in the [Azure Cosmos DB documentation](https://learn.microsoft.com/azure/cosmos-db/how-to-setup-rbac#permission-model).

:::

## Usage

import Example from "@examples/memory/azure_cosmosdb_nosql.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/memory/cassandra.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# Cassandra Chat Memory

For longer-term persistence across chat sessions, you can swap out the default in-memory `chatHistory` that backs chat memory classes like `BufferMemory` for a Cassandra cluster.

## Setup

First, install the Cassandra Node.js driver:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install cassandra-driver @langchain/openai @langchain/community @langchain/core
```

Depending on your database providers, the specifics of how to connect to the database will vary. We will create a document `configConnection` which will be used as part of the vector store configuration.

### Apache Cassandra®

```typescript
const configConnection = {
  contactPoints: ['h1', 'h2'],
  localDataCenter: 'datacenter1',
  credentials: {
    username: <...> as string,
    password: <...> as string,
  },
};
```

### Astra DB

Astra DB is a cloud-native Cassandra-as-a-Service platform.

1. Create an [Astra DB account](https://astra.datastax.com/register).
2. Create a [vector enabled database](https://astra.datastax.com/createDatabase).
3. Create a [token](https://docs.datastax.com/en/astra/docs/manage-application-tokens.html) for your database.

```typescript
const configConnection = {
  serviceProviderArgs: {
    astra: {
      token: <...> as string,
      endpoint: <...> as string,
    },
  },
};
```

Instead of `endpoint:`, you many provide property `datacenterID:` and optionally `regionName:`.

## Usage

import Example from "@examples/memory/cassandra-store.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/memory/cloudflare_d1.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# Cloudflare D1-Backed Chat Memory

:::info
This integration is only supported in Cloudflare Workers.
:::

For longer-term persistence across chat sessions, you can swap out the default in-memory `chatHistory` that backs chat memory classes like `BufferMemory` for a Cloudflare D1 instance.

## Setup

You'll need to install the LangChain Cloudflare integration package.
For the below example, we also use Anthropic, but you can use any model you'd like:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/cloudflare @langchain/anthropic @langchain/core
```

Set up a D1 instance for your worker by following [the official documentation](https://developers.cloudflare.com/d1/). Your project's `wrangler.toml` file should
look something like this:

```toml
name = "YOUR_PROJECT_NAME"
main = "src/index.ts"
compatibility_date = "2024-01-10"

[vars]
ANTHROPIC_API_KEY = "YOUR_ANTHROPIC_KEY"

[[d1_databases]]
binding = "DB"                                       # available in your Worker as env.DB
database_name = "YOUR_D1_DB_NAME"
database_id = "YOUR_D1_DB_ID"
```

## Usage

You can then use D1 to store your history as follows:

import Example from "@examples/memory/cloudflare_d1.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/memory/convex.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# Convex Chat Memory

For longer-term persistence across chat sessions, you can swap out the default in-memory `chatHistory` that backs chat memory classes like `BufferMemory` for [Convex](https://convex.dev/).

## Setup

### Create project

Get a working [Convex](https://docs.convex.dev/) project set up, for example by using:

```bash
npm create convex@latest
```

### Add database accessors

Add query and mutation helpers to `convex/langchain/db.ts`:

```ts title="convex/langchain/db.ts"
export * from "@langchain/community/utils/convex";
```

### Configure your schema

Set up your schema (for indexing):

```ts title="convex/schema.ts"
import { defineSchema, defineTable } from "convex/server";
import { v } from "convex/values";

export default defineSchema({
  messages: defineTable({
    sessionId: v.string(),
    message: v.object({
      type: v.string(),
      data: v.object({
        content: v.string(),
        role: v.optional(v.string()),
        name: v.optional(v.string()),
        additional_kwargs: v.optional(v.any()),
      }),
    }),
  }).index("bySessionId", ["sessionId"]),
});
```

## Usage

Each chat history session stored in Convex must have a unique session id.

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

import Example from "@examples/memory/convex/convex.ts";

<CodeBlock language="typescript" title="convex/myActions.ts">
  {Example}
</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/memory/dynamodb.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# DynamoDB-Backed Chat Memory

For longer-term persistence across chat sessions, you can swap out the default in-memory `chatHistory` that backs chat memory classes like `BufferMemory` for a DynamoDB instance.

## Setup

First, install the AWS DynamoDB client in your project:

```bash npm2yarn
npm install @aws-sdk/client-dynamodb
```

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

Next, sign into your AWS account and create a DynamoDB table. Name the table `langchain`, and name your partition key `id`. Make sure your partition key is a string. You can leave sort key and the other settings alone.

You'll also need to retrieve an AWS access key and secret key for a role or user that has access to the table and add them to your environment variables.

## Usage

import Example from "@examples/memory/dynamodb-store.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/memory/file.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# File System Chat Message History

The `FileSystemChatMessageHistory` uses a JSON file to store chat message history. For longer-term persistence across chat sessions, you can swap out the default in-memory `chatHistory` that backs chat memory classes like `BufferMemory`.

## Setup

You'll first need to install the [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) package:

```bash npm2yarn
npm install @langchain/community @langchain/core
```

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

## Usage

import Example from "@examples/memory/file.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/memory/firestore.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# Firestore Chat Memory

For longer-term persistence across chat sessions, you can swap out the default in-memory `chatHistory` that backs chat memory classes like `BufferMemory` for a firestore.

## Setup

First, install the Firebase admin package in your project:

```bash npm2yarn
npm install firebase-admin
```

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

Visit the `Project Settings` page from your Firebase project and select the `Service accounts` tab.

Inside the `Service accounts` tab, click the `Generate new private key` button inside the `Firebase Admin SDK` section to download a JSON file containing your service account's credentials.

Using the downloaded JSON file, pass in the `projectId`, `privateKey`, and `clientEmail` to the `config` object of the `FirestoreChatMessageHistory` class, like shown below:

```typescript
import { FirestoreChatMessageHistory } from "@langchain/community/stores/message/firestore";
import admin from "firebase-admin";

const messageHistory = new FirestoreChatMessageHistory({
  collections: ["chats"],
  docs: ["user-id"],
  sessionId: "user-id",
  userId: "a@example.com",
  config: {
    projectId: "YOUR-PROJECT-ID",
    credential: admin.credential.cert({
      projectId: "YOUR-PROJECT-ID",
      privateKey:
        "-----BEGIN PRIVATE KEY-----\nCHANGE-ME\n-----END PRIVATE KEY-----\n",
      clientEmail: "CHANGE-ME@CHANGE-ME-TOO.iam.gserviceaccount.com",
    }),
  },
});
```

Here, the `collections` field should match the names and ordering of the `collections` in your database.
The same goes for `docs`, it should match the names and ordering of the `docs` in your database.

## Usage

import Example from "@examples/memory/firestore.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

### Nested Collections

The `FirestoreChatMessageHistory` class supports nested collections, and dynamic collection/doc names.

The example below shows how to add and retrieve messages from a database with the following structure:

```
/chats/{chat-id}/bots/{bot-id}/messages/{message-id}
```

import NestedExample from "@examples/memory/firestore_nested.ts";

<CodeBlock language="typescript">{NestedExample}</CodeBlock>

## Firestore Rules

If your collection name is "chathistory," you can configure Firestore rules as follows.

```
      match /chathistory/{sessionId} {
       allow read: if request.auth.uid == resource.data.createdBy;
       allow write: if request.auth.uid == request.resource.data.createdBy;
			 }
			 match /chathistory/{sessionId}/messages/{messageId} {
       allow read: if request.auth.uid == resource.data.createdBy;
       allow write: if request.auth.uid == request.resource.data.createdBy;
		    }
```



================================================
FILE: docs/core_docs/docs/integrations/memory/google_cloudsql_pg.mdx
================================================
# Google Cloud SQL for PostgreSQL

[Cloud SQL](https://cloud.google.com/sql) is a fully managed relational database service that offers high
performance, seamless integration, and impressive scalability and offers database engines such as PostgreSQL.

This guide provides a quick overview of how to use Cloud SQL for PostgreSQL to store messages and provide
conversation with the PostgresChatMessageHistory class.

## Overview

### Before you begin

In order to use this package, you first need to go through the following steps:

1.  [Select or create a Cloud Platform project.](https://developers.google.com/workspace/guides/create-project)
2.  [Enable billing for your project.](https://cloud.google.com/billing/docs/how-to/modify-project#enable_billing_for_a_project)
3.  [Enable the Cloud SQL Admin API.](https://console.cloud.google.com/flows/enableapi?apiid=sqladmin.googleapis.com)
4.  [Setup Authentication.](https://cloud.google.com/docs/authentication)
5.  [Create a CloudSQL instance](https://cloud.google.com/sql/docs/postgres/connect-instance-auth-proxy#create-instance)
6.  [Create a CloudSQL database](https://cloud.google.com/sql/docs/postgres/create-manage-databases)
7.  [Add a user to the database](https://cloud.google.com/sql/docs/postgres/create-manage-users)

### Authentication

Authenticate locally to your Google Cloud account using the `gcloud auth login` command.

### Set Your Google Cloud Project

Set your Google Cloud project ID to leverage Google Cloud resources locally:

```bash
gcloud config set project YOUR-PROJECT-ID
```

If you don't know your project ID, try the following:

- Run `gcloud config list`.
- Run `gcloud projects list`.
- See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113).

## Setting up a PostgresChatMessageHistory instance

To use the PostgresChatMessageHistory class, you'll need to install the [`@langchain/google-cloud-sql-pg`](https://www.npmjs.com/package/@langchain/google-cloud-sql-pg)
package and then follow the steps bellow.

First, you'll need to log in to your Google Cloud account and set the following environment variables based
on your Google Cloud project; these will be defined based on how you want to configure (fromInstance,
fromEngine, fromEngineArgs) your PostgresEngine instance:

```bash
PROJECT_ID="your-project-id"
REGION="your-project-region" // example: "us-central1"
INSTANCE_NAME="your-instance"
DB_NAME="your-database-name"
DB_USER="your-database-user"
PASSWORD="your-database-password"
```

### Setting up an instance

To instantiate a PostgresChatMessageHistory, you'll first need to create a database connection through the
PostgresEngine, then initialize the chat history table and finally call the `.initialize()` method to instantiate
the chat message history.

```typescript
import {
  PostgresChatMessageHistory,
  PostgresEngine,
  PostgresEngineArgs,
} from "@langchain/google-cloud-sql-pg";
import * as dotenv from "dotenv";

dotenv.config();

const peArgs: PostgresEngineArgs = {
  user: process.env.DB_USER ?? "",
  password: process.env.PASSWORD ?? "",
};

// PostgresEngine instantiation
const engine: PostgresEngine = await PostgresEngine.fromInstance(
  process.env.PROJECT_ID ?? "",
  process.env.REGION ?? "",
  process.env.INSTANCE_NAME ?? "",
  process.env.DB_NAME ?? "",
  peArgs
);

// Chat history table initialization
await engine.initChatHistoryTable("my_chat_history_table");

// PostgresChatMessageHistory instantiation
const historyInstance: PostgresChatMessageHistory =
  await PostgresChatMessageHistory.initialize(
    engine,
    "test",
    "my_chat_history_table"
  );
```

## Manage Chat Message History

### Add Messages to the chat history

You can add a message to the chat history by using the addMessage method or you can use the addMessages method
to pass an array of messages.

```typescript
import { AIMessage, BaseMessage, HumanMessage } from "@langchain/core/messages";

// Add one message
const msg = new HumanMessage("Hi!");
await historyInstance.addMessage(msg);

// Add an array of messages
const msg1: HumanMessage = new HumanMessage("Hi!");
const msg2: AIMessage = new AIMessage("what's up?");
const msg3: HumanMessage = new HumanMessage("How are you?");
const messages: BaseMessage[] = [msg1, msg2, msg3];
await historyInstance.addMessages(messages);
```

### Get Messages saved on the chat history

```typescript
const messagesSaved: BaseMessage[] = await historyInstance.getMessages();
console.log(messagesSaved);
```

### Clear Messages from the chat history

To remove all messages from the chat history, just call the clear method from the PostgresChatMessageHistory class.

```typescript
await historyInstance.clear();
```



================================================
FILE: docs/core_docs/docs/integrations/memory/ipfs_datastore.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# IPFS Datastore Chat Memory

For a storage backend you can use the IPFS Datastore Chat Memory to wrap an IPFS Datastore allowing you to use any IPFS compatible datastore.

## Setup

First, install the integration dependencies:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install cborg interface-datastore it-all @langchain/community @langchain/core
```

Now you can install and use an IPFS Datastore of your choice. Here are some options:

- [datastore-core](https://github.com/ipfs/js-stores/blob/main/packages/datastore-core) Datastore in-memory implementation.
- [datastore-fs](https://github.com/ipfs/js-stores/blob/main/packages/datastore-fs) Datastore implementation with file system backend.
- [datastore-idb](https://github.com/ipfs/js-stores/blob/main/packages/datastore-idb) Datastore implementation with IndexedDB backend.
- [datastore-level](https://github.com/ipfs/js-stores/blob/main/packages/datastore-level) Datastore implementation with level(up|down) backend
- [datastore-s3](https://github.com/ipfs/js-stores/blob/main/packages/datastore-s3) Datastore implementation backed by s3.

## Usage

```typescript
// Replace FsDatastore with the IPFS Datastore of your choice.
import { FsDatastore } from "datastore-fs";
import { IPFSDatastoreChatMessageHistory } from "@langchain/community/stores/message/ipfs_datastore";

const datastore = new FsDatastore("path/to/store");
const sessionId = "my-session";

const history = new IPFSDatastoreChatMessageHistory({ datastore, sessionId });
```



================================================
FILE: docs/core_docs/docs/integrations/memory/mem0_memory.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# Mem0 Memory

[Mem0](https://github.com/mem0ai/mem0) is a self-improving memory layer for LLM applications, enabling personalized AI experiences that save costs and delight users.

## Setup

Goto [Mem0 Dashboard](https://app.mem0.ai) to get API keys for Mem0.

## Usage

import Example from "@examples/memory/mem0.ts";

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core @langchain/community
```

<CodeBlock language="typescript">{Example}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/memory/momento.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# Momento-Backed Chat Memory

For distributed, serverless persistence across chat sessions, you can swap in a [Momento](https://gomomento.com/)-backed chat message history.
Because a Momento cache is instantly available and requires zero infrastructure maintenance, it's a great way to get started with chat history whether building locally or in production.

## Setup

You will need to install the [Momento Client Library](https://github.com/momentohq/client-sdk-javascript) in your project. Given Momento's compatibility with Node.js, browser, and edge environments, ensure you install the relevant package.

To install for **Node.js**:

```bash npm2yarn
npm install @gomomento/sdk
```

To install for **browser/edge workers**:

```bash npm2yarn
npm install @gomomento/sdk-web
```

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

You will also need an API key from [Momento](https://gomomento.com/). You can sign up for a free account [here](https://console.gomomento.com/).

## Usage

To distinguish one chat history session from another, we need a unique `sessionId`. You may also provide an optional `sessionTtl` to make sessions expire after a given number of seconds.

import MomentoExample from "@examples/memory/momento.ts";

<CodeBlock language="typescript">{MomentoExample}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/memory/mongodb.mdx
================================================
---
hide_table_of_contents: true
sidebar_class_name: node-only
---

import CodeBlock from "@theme/CodeBlock";

# MongoDB Chat Memory

:::tip Compatibility
Only available on Node.js.

You can still create API routes that use MongoDB with Next.js by setting the `runtime` variable to `nodejs` like so:

```typescript
export const runtime = "nodejs";
```

You can read more about Edge runtimes in the Next.js documentation [here](https://nextjs.org/docs/app/building-your-application/rendering/edge-and-nodejs-runtimes).
:::

For longer-term persistence across chat sessions, you can swap out the default in-memory `chatHistory` that backs chat memory classes like `BufferMemory` for a MongoDB instance.

## Setup

You need to install Node MongoDB SDK in your project:

```bash npm2yarn
npm install -S mongodb
```

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

You will also need a MongoDB instance to connect to.

## Usage

Each chat history session stored in MongoDB must have a unique session id.

import Example from "@examples/memory/mongodb.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/memory/motorhead_memory.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# Motörhead Memory

[Motörhead](https://github.com/getmetal/motorhead) is a memory server implemented in Rust. It automatically handles incremental summarization in the background and allows for stateless applications.

## Setup

See instructions at [Motörhead](https://github.com/getmetal/motorhead) for running the server locally, or https://getmetal.io to get API keys for the hosted version.

## Usage

import Example from "@examples/memory/motorhead.ts";

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core
```

<CodeBlock language="typescript">{Example}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/memory/planetscale.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# PlanetScale Chat Memory

Because PlanetScale works via a REST API, you can use this with [Vercel Edge](https://vercel.com/docs/concepts/functions/edge-functions/edge-runtime), [Cloudflare Workers](https://developers.cloudflare.com/workers/) and other Serverless environments.

For longer-term persistence across chat sessions, you can swap out the default in-memory `chatHistory` that backs chat memory classes like `BufferMemory` for an PlanetScale [Database](https://planetscale.com/) instance.

## Setup

You will need to install [@planetscale/database](https://github.com/planetscale/database-js) in your project:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @planetscale/database @langchain/community @langchain/core
```

You will also need an PlanetScale Account and a database to connect to. See instructions on [PlanetScale Docs](https://planetscale.com/docs) on how to create a HTTP client.

## Usage

Each chat history session stored in PlanetScale database must have a unique id.
The `config` parameter is passed directly into the `new Client()` constructor of [@planetscale/database](https://planetscale.com/docs/tutorials/planetscale-serverless-driver), and takes all the same arguments.

import Example from "@examples/memory/planetscale.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## Advanced Usage

You can also directly pass in a previously created [@planetscale/database](https://planetscale.com/docs/tutorials/planetscale-serverless-driver) client instance:

import AdvancedExample from "@examples/memory/planetscale_advanced.ts";

<CodeBlock language="typescript">{AdvancedExample}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/memory/postgres.mdx
================================================
---
hide_table_of_contents: true
sidebar_class_name: node-only
---

import CodeBlock from "@theme/CodeBlock";

# Postgres Chat Memory

For longer-term persistence across chat sessions, you can swap out the default in-memory `chatHistory` for a [Postgres](https://www.postgresql.org/) Database.

## Setup

First install the [node-postgres](https://node-postgres.com/) package:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core pg
```

## Usage

Each chat history session is stored in a Postgres database and requires a session id.

The connection to postgres is handled through a pool. You can either pass an instance of a pool via the `pool` parameter or pass a pool config via the `poolConfig` parameter. See [pg-node docs on pools](https://node-postgres.com/apis/pool)
for more information. A provided pool takes precedence, thus if both a pool instance and a pool config are passed, only the pool will be used.

import Example from "@examples/memory/postgres.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/memory/redis.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# Redis-Backed Chat Memory

For longer-term persistence across chat sessions, you can swap out the default in-memory `chatHistory` that backs chat memory classes like `BufferMemory` for a [Redis](https://redis.io/) instance.

## Setup

You will need to install [node-redis](https://github.com/redis/node-redis) in your project:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core redis
```

You will also need a Redis instance to connect to. See instructions on [the official Redis website](https://redis.io/docs/getting-started/) for running the server locally.

## Usage

Each chat history session stored in Redis must have a unique id. You can provide an optional `sessionTTL` to make sessions expire after a give number of seconds.
The `config` parameter is passed directly into the `createClient` method of [node-redis](https://github.com/redis/node-redis), and takes all the same arguments.

import Example from "@examples/memory/redis.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## Advanced Usage

You can also directly pass in a previously created [node-redis](https://github.com/redis/node-redis) client instance:

import AdvancedExample from "@examples/memory/redis-advanced.ts";

<CodeBlock language="typescript">{AdvancedExample}</CodeBlock>

### Redis Sentinel Support

You can enable a Redis Sentinel backed cache using [ioredis](https://github.com/redis/ioredis)

This will require the installation of [ioredis](https://github.com/redis/ioredis) in your project.

```bash npm2yarn
npm install ioredis
```

import RedisSentinel from "@examples/memory/redis-sentinel.ts";

<CodeBlock language="typescript">{RedisSentinel}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/memory/upstash_redis.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# Upstash Redis-Backed Chat Memory

Because Upstash Redis works via a REST API, you can use this with [Vercel Edge](https://vercel.com/docs/concepts/functions/edge-functions/edge-runtime), [Cloudflare Workers](https://developers.cloudflare.com/workers/) and other Serverless environments.
Based on Redis-Backed Chat Memory.

For longer-term persistence across chat sessions, you can swap out the default in-memory `chatHistory` that backs chat memory classes like `BufferMemory` for an Upstash [Redis](https://redis.io/) instance.

## Setup

You will need to install [@upstash/redis](https://github.com/upstash/upstash-redis) in your project:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core @upstash/redis
```

You will also need an Upstash Account and a Redis database to connect to. See instructions on [Upstash Docs](https://docs.upstash.com/redis) on how to create a HTTP client.

## Usage

Each chat history session stored in Redis must have a unique id. You can provide an optional `sessionTTL` to make sessions expire after a give number of seconds.
The `config` parameter is passed directly into the `new Redis()` constructor of [@upstash/redis](https://docs.upstash.com/redis/sdks/javascriptsdk/overview), and takes all the same arguments.

import Example from "@examples/memory/upstash_redis.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## Advanced Usage

You can also directly pass in a previously created [@upstash/redis](https://docs.upstash.com/redis/sdks/javascriptsdk/overview) client instance:

import AdvancedExample from "@examples/memory/upstash_redis_advanced.ts";

<CodeBlock language="typescript">{AdvancedExample}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/memory/xata.mdx
================================================
# Xata Chat Memory

[Xata](https://xata.io) is a serverless data platform, based on PostgreSQL. It provides a type-safe TypeScript/JavaScript SDK for interacting with your database, and a
UI for managing your data.

With the `XataChatMessageHistory` class, you can use Xata databases for longer-term persistence of chat sessions.

Because Xata works via a REST API and has a pure TypeScript SDK, you can use this with [Vercel Edge](https://vercel.com/docs/concepts/functions/edge-functions/edge-runtime), [Cloudflare Workers](https://developers.cloudflare.com/workers/) and any other Serverless environment.

## Setup

### Install the Xata CLI

```bash
npm install @xata.io/cli -g
```

### Create a database to be used as a vector store

In the [Xata UI](https://app.xata.io) create a new database. You can name it whatever you want, but for this example we'll use `langchain`.

When executed for the first time, the Xata LangChain integration will create the table used for storing the chat messages. If a table with that name already exists, it will be left untouched.

### Initialize the project

In your project, run:

```bash
xata init
```

and then choose the database you created above. This will also generate a `xata.ts` or `xata.js` file that defines the client you can use to interact with the database. See the [Xata getting started docs](https://xata.io/docs/getting-started/installation) for more details on using the Xata JavaScript/TypeScript SDK.

## Usage

import CodeBlock from "@theme/CodeBlock";

Each chat history session stored in Xata database must have a unique id.

In this example, the `getXataClient()` function is used to create a new Xata client based on the environment variables. However, we recommend using the code generated by the `xata init` command, in which case you only need to import the `getXataClient()` function from the generated `xata.ts` file.

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

import Example from "@examples/memory/xata.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

### With pre-created table

If you don't want the code to always check if the table exists, you can create the table manually in the Xata UI and pass `createTable: false` to the constructor. The table must have the following columns:

- `sessionId` of type `String`
- `type` of type `String`
- `role` of type `String`
- `content` of type `Text`
- `name` of type `String`
- `additionalKwargs` of type `Text`

import Advanced from "@examples/memory/xata-advanced.ts";

<CodeBlock language="typescript">{Advanced}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/memory/zep_memory.mdx
================================================
---
hide_table_of_contents: true
---

# Zep Open Source Memory

> Recall, understand, and extract data from chat histories. Power personalized AI experiences.

> [Zep](https://www.getzep.com) is a long-term memory service for AI Assistant apps.
> With Zep, you can provide AI assistants with the ability to recall past conversations, no matter how distant,
> while also reducing hallucinations, latency, and cost.

## How Zep works

Zep persists and recalls chat histories, and automatically generates summaries and other artifacts from these chat histories.
It also embeds messages and summaries, enabling you to search Zep for relevant context from past conversations.
Zep does all of this asynchronously, ensuring these operations don't impact your user's chat experience.
Data is persisted to database, allowing you to scale out when growth demands.

Zep also provides a simple, easy to use abstraction for document vector search called Document Collections.
This is designed to complement Zep's core memory features, but is not designed to be a general purpose vector database.

Zep allows you to be more intentional about constructing your prompt:

- automatically adding a few recent messages, with the number customized for your app;
- a summary of recent conversations prior to the messages above;
- and/or contextually relevant summaries or messages surfaced from the entire chat session.
- and/or relevant Business data from Zep Document Collections.

> Interested in Zep Cloud? See [Zep Cloud Installation Guide](https://help.getzep.com/sdks)

## Setup

See the instructions from [Zep Open Source](https://github.com/getzep/zep) for running the server locally or through an automated hosting provider.

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

## Usage

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/memory/zep.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/memory/zep_memory_cloud.mdx
================================================
---
hide_table_of_contents: true
---

# Zep Cloud Memory

> Recall, understand, and extract data from chat histories. Power personalized AI experiences.

> [Zep](https://www.getzep.com) is a long-term memory service for AI Assistant apps.
> With Zep, you can provide AI assistants with the ability to recall past conversations, no matter how distant,
> while also reducing hallucinations, latency, and cost.

## How Zep Cloud works

Zep persists and recalls chat histories, and automatically generates summaries and other artifacts from these chat histories.
It also embeds messages and summaries, enabling you to search Zep for relevant context from past conversations.
Zep does all of this asynchronously, ensuring these operations don't impact your user's chat experience.
Data is persisted to database, allowing you to scale out when growth demands.

Zep also provides a simple, easy to use abstraction for document vector search called Document Collections.
This is designed to complement Zep's core memory features, but is not designed to be a general purpose vector database.

Zep allows you to be more intentional about constructing your prompt:

- automatically adding a few recent messages, with the number customized for your app;
- a summary of recent conversations prior to the messages above;
- and/or contextually relevant summaries or messages surfaced from the entire chat session.
- and/or relevant Business data from Zep Document Collections.

Zep Cloud offers:

- **Fact Extraction**: Automatically build fact tables from conversations, without having to define a data schema upfront.
- **Dialog Classification**: Instantly and accurately classify chat dialog. Understand user intent and emotion, segment users, and more. Route chains based on semantic context, and trigger events.
- **Structured Data Extraction**: Quickly extract business data from chat conversations using a schema you define. Understand what your Assistant should ask for next in order to complete its task.

## Installation

Sign up for [Zep Cloud](https://app.getzep.com/) and create a project.

Follow the [Zep Cloud Typescript SDK Installation Guide](https://help.getzep.com/sdks) to install and get started with Zep.

You'll need your Zep Cloud Project API Key to use the Zep Cloud Memory. See the [Zep Cloud docs](https://help.getzep.com/projects) for more information.

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @getzep/zep-cloud @langchain/openai @langchain/community @langchain/core
```

### ZepCloudChatMessageHistory + RunnableWithMessageHistory usage

import CodeBlock from "@theme/CodeBlock";
import ZepCloudMessageHistoryExample from "@examples/guides/expression_language/zep/zep_cloud_message_history.ts";

<CodeBlock language="typescript">{ZepCloudMessageHistoryExample}</CodeBlock>

### ZepCloudChatMessageHistory + RunnableWithMessageHistory + ZepVectorStore (as retriever) usage

import ZepCloudMessageHistoryWithVectorStoreExample from "@examples/guides/expression_language/zep/zep_cloud_message_history_vector_store.ts";

<CodeBlock language="typescript">
  {ZepCloudMessageHistoryWithVectorStoreExample}
</CodeBlock>

### Memory Usage

import ZepCloudMemoryExample from "@examples/memory/zep_cloud.ts";

<CodeBlock language="typescript">{ZepCloudMemoryExample}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/platforms/anthropic.mdx
================================================
# Anthropic

All functionality related to Anthropic models.

[Anthropic](https://www.anthropic.com/) is an AI safety and research company, and is the creator of Claude.
This page covers all integrations between Anthropic models and LangChain.

## Prompting Best Practices

Anthropic models have several prompting best practices compared to OpenAI models.

**System Messages may only be the first message**

Anthropic models require any system messages to be the first one in your prompts.

## `ChatAnthropic`

`ChatAnthropic` is a subclass of LangChain's `ChatModel`, meaning it works best with `ChatPromptTemplate`.
You can import this wrapper with the following code:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/anthropic @langchain/core
```

```typescript
import { ChatAnthropic } from "@langchain/anthropic";
const model = new ChatAnthropic({});
```

When working with ChatModels, it is preferred that you design your prompts as `ChatPromptTemplate`s.
Here is an example below of doing that:

```typescript
import { ChatPromptTemplate } from "langchain/prompts";

const prompt = ChatPromptTemplate.fromMessages([
  ["system", "You are a helpful chatbot"],
  ["human", "Tell me a joke about {topic}"],
]);
```

You can then use this in a chain as follows:

```typescript
const chain = prompt.pipe(model);
await chain.invoke({ topic: "bears" });
```

See the [chat model integration page](/docs/integrations/chat/anthropic/) for more examples, including multimodal inputs.



================================================
FILE: docs/core_docs/docs/integrations/platforms/aws.mdx
================================================
---
keywords: [bedrock]
---

# AWS

All functionality related to the [Amazon AWS](https://aws.amazon.com/) platform.

## Chat Models

### Bedrock

See a [usage example](/docs/integrations/chat/bedrock).

```typescript
import { BedrockChat } from "@langchain/community/chat_models/bedrock";
```

## LLMs

### Bedrock

See a [usage example](/docs/integrations/llms/bedrock).

```typescript
import { Bedrock } from "@langchain/community/llms/bedrock";
```

### SageMaker Endpoint

> [Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a system that can build, train, and deploy machine learning (ML) models with fully managed infrastructure, tools, and workflows.

We use `SageMaker` to host our model and expose it as the `SageMaker Endpoint`.

See a [usage example](/docs/integrations/llms/aws_sagemaker).

```typescript
import {
  SagemakerEndpoint,
  SageMakerLLMContentHandler,
} from "@langchain/community/llms/sagemaker_endpoint";
```

## Text Embedding Models

### Bedrock

See a [usage example](/docs/integrations/text_embedding/bedrock).

```typescript
import { BedrockEmbeddings } from "@langchain/aws";
```

## Document loaders

### AWS S3 Directory and File

> [Amazon Simple Storage Service (Amazon S3)](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-folders.html) is an object storage service.
> [AWS S3 Directory](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-folders.html) >[AWS S3 Buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingBucket.html)

See a [usage example for S3FileLoader](/docs/integrations/document_loaders/web_loaders/s3).

```bash npm2yarn
npm install @aws-sdk/client-s3
```

```typescript
import { S3Loader } from "@langchain/community/document_loaders/web/s3";
```

## Memory

### AWS DynamoDB

> [AWS DynamoDB](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dynamodb/index.html)
> is a fully managed `NoSQL` database service that provides fast and predictable performance with seamless scalability.

We have to configure the [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html).

```bash npm2yarn
npm install @aws-sdk/client-dynamodb
```

See a [usage example](/docs/integrations/memory/dynamodb).

```typescript
import { DynamoDBChatMessageHistory } from "@langchain/community/stores/message/dynamodb";
```



================================================
FILE: docs/core_docs/docs/integrations/platforms/google.mdx
================================================
---
keywords: [gemini, gemini-pro, gemma]
---

# Google

Functionality related to [Google Cloud Platform](https://cloud.google.com/)
and [AI Studio](https://aistudio.google.com/)

## Chat models

### Gemini Models

Access Gemini models such as `gemini-1.5-pro` and `gemini-2.0-flex` through the [`ChatGoogleGenerativeAI`](/docs/integrations/chat/google_generativeai),
or if using VertexAI, via the [`ChatVertexAI`](/docs/integrations/chat/google_vertex_ai) class.

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<Tabs>
<TabItem value="genai" label="GenAI" default>
<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/google-genai @langchain/core
```

Configure your API key.

```
export GOOGLE_API_KEY=your-api-key
```

```typescript
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";

const model = new ChatGoogleGenerativeAI({
  model: "gemini-pro",
  maxOutputTokens: 2048,
});

// Batch and stream are also supported
const res = await model.invoke([
  [
    "human",
    "What would be a good company name for a company that makes colorful socks?",
  ],
]);
```

More recent Gemini models support image inputs:

```typescript
const visionModel = new ChatGoogleGenerativeAI({
  model: "gemini-2.0-flash",
  maxOutputTokens: 2048,
});
const image = fs.readFileSync("./hotdog.jpg").toString("base64");
const input2 = [
  new HumanMessage({
    content: [
      {
        type: "text",
        text: "Describe the following image.",
      },
      {
        type: "image_url",
        image_url: `data:image/png;base64,${image}`,
      },
    ],
  }),
];

const res = await visionModel.invoke(input2);
```

:::tip
Click [here](/docs/integrations/chat/google_generativeai) for the `@langchain/google-genai` specific integration docs
:::

</TabItem>

<TabItem value="vertexai" label="VertexAI" default>
<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/google-vertexai @langchain/core
```

Then, you'll need to add your service account credentials, either directly as a `GOOGLE_VERTEX_AI_WEB_CREDENTIALS` environment variable:

```
GOOGLE_VERTEX_AI_WEB_CREDENTIALS={"type":"service_account","project_id":"YOUR_PROJECT-12345",...}
```

or as a file path:

```
GOOGLE_VERTEX_AI_WEB_CREDENTIALS_FILE=/path/to/your/credentials.json
```

```typescript
import { ChatVertexAI } from "@langchain/google-vertexai";
// Or, if using the web entrypoint:
// import { ChatVertexAI } from "@langchain/google-vertexai-web";

const model = new ChatVertexAI({
  model: "gemini-1.0-pro",
  maxOutputTokens: 2048,
});

// Batch and stream are also supported
const res = await model.invoke([
  [
    "human",
    "What would be a good company name for a company that makes colorful socks?",
  ],
]);
```

Gemini vision models support image inputs when providing a single human message. For example:

```typescript
const visionModel = new ChatVertexAI({
  model: "gemini-pro-vision",
  maxOutputTokens: 2048,
});
const image = fs.readFileSync("./hotdog.png").toString("base64");
const input2 = [
  new HumanMessage({
    content: [
      {
        type: "text",
        text: "Describe the following image.",
      },
      {
        type: "image_url",
        image_url: `data:image/png;base64,${image}`,
      },
    ],
  }),
];

const res = await visionModel.invoke(input2);
```

:::tip
Click [here](/docs/integrations/chat/google_vertex_ai) for the `@langchain/google-vertexai` specific integration docs
:::

</TabItem>
</Tabs>

The value of `image_url` must be a base64 encoded image (e.g., `data:image/png;base64,abcd124`).

### Gemma

Access the `gemma-3-27b-it` model through AI Studio using the `ChatGoogle` class.
(This class is a superclass of the [`ChatVertexAI`](/docs/integrations/chat/google_vertex_ai)
class that works with both Vertex AI and the AI Studio APIs.)

:::tip
Since Gemma is an open model, it may also be available from other platforms
including [Ollama](/docs/integrations/chat/ollama/).
:::

```bash npm2yarn
npm install @langchain/google-gauth @langchain/core
```

Configure your API key.

```
export GOOGLE_API_KEY=your-api-key
```

```typescript
import { ChatGoogle } from "@langchain/google-gauth";

const model = new ChatGoogle({
  model: "gemma-3-27b-it",
});

const res = await model.invoke([
  {
    role: "user",
    content:
      "What would be a good company name for a company that makes colorful socks?",
  },
]);
```

### Third Party Models

See above for setting up authentication through Vertex AI to use these models.

[Anthropic](/docs/integrations/chat/anthropic) Claude models are also available through
the [Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude)
platform. See [here](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude)
for more information about enabling access to the models and the model names to use.

PaLM models are no longer supported.

## Vector Store

### Vertex AI Vector Search

> [Vertex AI Vector Search](https://cloud.google.com/vertex-ai/docs/matching-engine/overview),
> formerly known as Vertex AI Matching Engine, provides the industry's leading high-scale
> low latency vector database. These vector databases are commonly
> referred to as vector similarity-matching or an approximate nearest neighbor (ANN) service.

```typescript
import { MatchingEngine } from "@langchain/community/vectorstores/googlevertexai";
```

### Postgres Vector Store

The [PostgresVectorStore](/docs/integrations/vectorstores/google_cloudsql_pg) module from the
[`@langchain/google-cloud-sql-pg`](https://www.npmjs.com/package/@langchain/google-cloud-sql-pg) package provides a way to use the CloudSQL for PostgresSQL to store
vector embeddings using the class.

```bash
$ yarn add @langchain/google-cloud-sql-pg
```

Set your environment variables:

```bash
PROJECT_ID="your-project-id"
REGION="your-project-region"
INSTANCE_NAME="your-instance"
DB_NAME="your-database-name"
DB_USER="your-database-user"
PASSWORD="your-database-password"
```

Create a DB connection through the PostgresEngine class:

```typescript
const engine: PostgresEngine = await PostgresEngine.fromInstance(
  process.env.PROJECT_ID ?? "",
  process.env.REGION ?? "",
  process.env.INSTANCE_NAME ?? "",
  process.env.DB_NAME ?? "",
  peArgs
);
```

Initialize the vector store table:

```typescript
await engine.initVectorstoreTable(
  "my_vector_store_table",
  768,
  vectorStoreArgs
);
```

Create a vector store instance:

```typescript
const vectorStore = await PostgresVectorStore.initialize(
  engine,
  embeddingService,
  "my_vector_store_table",
  pvectorArgs
);
```

## Tools

### Google Search

- Set up a Custom Search Engine, following [these instructions](https://stackoverflow.com/questions/37083058/programmatically-searching-google-in-python-using-custom-search)
- Get an API Key and Custom Search Engine ID from the previous step, and set them as environment variables `GOOGLE_API_KEY` and `GOOGLE_CSE_ID` respectively

There exists a `GoogleCustomSearch` utility which wraps this API. To import this utility:

```typescript
import { GoogleCustomSearch } from "@langchain/community/tools/google_custom_search";
```

We can easily load this wrapper as a Tool (to use with an Agent). We can do this with:

```typescript
const tools = [new GoogleCustomSearch({})];
// Pass this variable into your agent.
```

## Chat history

### Postgres Chat Message History

The [PostgresChatMessageHistory](/docs/integrations/memory/google_cloudsql_pg) from
the [`@langchain/google-cloud-sql-pg`](https://www.npmjs.com/package/@langchain/google-cloud-sql-pg) package provides a way to use the CloudSQL for PostgresSQL to store messages and provide conversation history.

```bash
$ yarn add @langchain/google-cloud-sql-pg
```

_Note:_ See the [Postgres Vector Store](#Postgres Vector Store) section on this page to learn
how to install the package and initialize a DB connection.

Initialize the chat history table:

```typescript
await engine.initChatHistoryTable("chat_message_table");
```

Create a chat message history instance:

```typescript
const historyInstance = await PostgresChatMessageHistory.initialize(
  engine,
  "test",
  "chat_message_table"
);
```

## Document Loaders

### Postgres Loader

The [PostgresLoader](/docs/integrations/memory/google_cloudsql_pg) from
`@langchain/google-cloud-sql-pg` provides a way to use the CloudSQL for PostgresSQL to load data as LangChain `Document`s.

_Note:_ See the [Postgres Vector Store](#Postgres Vector Store) section on this page to learn
how to install the package and initialize a DB connection.

Create a loader instance:

```typescript
const documentLoaderArgs: PostgresLoaderOptions = {
  tableName: "test_table_custom",
  contentColumns: ["fruit_name", "variety"],
  metadataColumns: [
    "fruit_id",
    "quantity_in_stock",
    "price_per_unit",
    "organic",
  ],
  format: "text",
};

const documentLoaderInstance = await PostgresLoader.initialize(
  PEInstance,
  documentLoaderArgs
);

const documents = await documentLoaderInstance.load();
```



================================================
FILE: docs/core_docs/docs/integrations/platforms/index.mdx
================================================
---
sidebar_position: 0
sidebar_class_name: hidden
---

# Providers

LangChain integrates with many providers.

## Partner Packages

These providers have standalone `@langchain/{provider}` packages for improved versioning, dependency management and testing.

For specifics on how to use each package, look for their pages in the appropriate component docs section (e.g. [chat models](/docs/integrations/chat/)).

- [Anthropic](https://www.npmjs.com/package/@langchain/anthropic)
- [Cerebras](https://www.npmjs.com/package/@langchain/cerebras)
- [Cloudflare](https://www.npmjs.com/package/@langchain/cloudflare)
- [Cohere](https://www.npmjs.com/package/@langchain/cohere)
- [Exa](https://www.npmjs.com/package/@langchain/exa)
- [Google GenAI](https://www.npmjs.com/package/@langchain/google-genai)
- [Google VertexAI](https://www.npmjs.com/package/@langchain/google-vertexai)
- [Google VertexAI (Web Environments)](https://www.npmjs.com/package/@langchain/google-vertexai-web)
- [Groq](https://www.npmjs.com/package/@langchain/groq)
- [MistralAI](https://www.npmjs.com/package/@langchain/mistralai)
- [MongoDB](https://www.npmjs.com/package/@langchain/mongodb)
- [Nomic](https://www.npmjs.com/package/@langchain/nomic)
- [OpenAI](https://www.npmjs.com/package/@langchain/openai)
- [Pinecone](https://www.npmjs.com/package/@langchain/pinecone)
- [Qdrant](https://www.npmjs.com/package/@langchain/qdrant)
- [Redis](https://www.npmjs.com/package/@langchain/redis)
- [Weaviate](https://www.npmjs.com/package/@langchain/weaviate)
- [Yandex](https://www.npmjs.com/package/@langchain/yandex)
- [Azure CosmosDB](https://www.npmjs.com/package/@langchain/azure-cosmosdb)
- [xAI](https://www.npmjs.com/package/@langchain/xai)



================================================
FILE: docs/core_docs/docs/integrations/platforms/microsoft.mdx
================================================
---
keywords: [azure]
---

import CodeBlock from "@theme/CodeBlock";

# Microsoft

All functionality related to `Microsoft Azure` and other `Microsoft` products.

## Chat Models

### Azure OpenAI

See a [usage example](/docs/integrations/chat/azure)

import AzureChatOpenAI from "@examples/models/chat/integration_azure_openai.ts";

<UnifiedModelParamsTooltip></UnifiedModelParamsTooltip>

<CodeBlock language="typescript">{AzureChatOpenAI}</CodeBlock>

## LLM

### Azure OpenAI

> [Microsoft Azure](https://en.wikipedia.org/wiki/Microsoft_Azure), often referred to as `Azure` is a cloud computing platform run by `Microsoft`, which offers access, management, and development of applications and services through global data centers. It provides a range of capabilities, including software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS). `Microsoft Azure` supports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems.

> [Azure OpenAI](https://azure.microsoft.com/products/ai-services/openai-service/) is a cloud service to help you quickly develop generative AI experiences with a diverse set of prebuilt and curated models from OpenAI, Meta and beyond.

LangChain.js supports integration with [Azure OpenAI](https://azure.microsoft.com/products/ai-services/openai-service/) using the new Azure integration in the [OpenAI SDK](https://github.com/openai/openai-node).

You can learn more about Azure OpenAI and its difference with the OpenAI API on [this page](https://learn.microsoft.com/azure/ai-services/openai/overview). If you don't have an Azure account, you can [create a free account](https://azure.microsoft.com/free/) to get started.

You'll need to have an Azure OpenAI instance deployed. You can deploy a version on Azure Portal following [this guide](https://learn.microsoft.com/azure/ai-services/openai/how-to/create-resource?pivots=web-portal).

Once you have your instance running, make sure you have the name of your instance and key. You can find the key in the Azure Portal, under the "Keys and Endpoint" section of your instance.

If you're using Node.js, you can define the following environment variables to use the service:

```bash
AZURE_OPENAI_API_INSTANCE_NAME=<YOUR_INSTANCE_NAME>
AZURE_OPENAI_API_DEPLOYMENT_NAME=<YOUR_DEPLOYMENT_NAME>
AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME=<YOUR_EMBEDDINGS_DEPLOYMENT_NAME>
AZURE_OPENAI_API_KEY=<YOUR_KEY>
AZURE_OPENAI_API_VERSION="2024-02-01"
```

:::info

You can find the list of supported API versions in the [Azure OpenAI documentation](https://learn.microsoft.com/azure/ai-services/openai/reference).

:::

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core
```

See a [usage example](/docs/integrations/llms/azure).

import AzureOpenAI from "@examples/models/llm/azure_openai.ts";

import UnifiedModelParamsTooltip from "@mdx_components/unified_model_params_tooltip.mdx";

<UnifiedModelParamsTooltip></UnifiedModelParamsTooltip>

<CodeBlock language="typescript">{AzureOpenAI}</CodeBlock>

## Text Embedding Models

### Azure OpenAI

See a [usage example](/docs/integrations/text_embedding/azure_openai)

import AzureOpenAIEmbeddings from "@examples/models/embeddings/azure_openai.ts";

<UnifiedModelParamsTooltip></UnifiedModelParamsTooltip>

<CodeBlock language="typescript">{AzureOpenAIEmbeddings}</CodeBlock>

## Vector stores

### Azure AI Search

> [Azure AI Search](https://azure.microsoft.com/products/ai-services/ai-search) (formerly known as Azure Search and Azure Cognitive Search) is a distributed, RESTful search engine optimized for speed and relevance on production-scale workloads on Azure. It supports also vector search using the [k-nearest neighbor](https://en.wikipedia.org/wiki/Nearest_neighbor_search) (kNN) algorithm and also [semantic search](https://learn.microsoft.com/azure/search/semantic-search-overview).

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install -S @langchain/community @langchain/core @azure/search-documents
```

See a [usage example](/docs/integrations/vectorstores/azure_aisearch).

```typescript
import { AzureAISearchVectorStore } from "@langchain/community/vectorstores/azure_aisearch";
```

### Azure Cosmos DB for NoSQL

> [Azure Cosmos DB for NoSQL](https://learn.microsoft.com/azure/cosmos-db/nosql/) provides support for querying items with flexible schemas and native support for JSON. It now offers vector indexing and search. This feature is designed to handle high-dimensional vectors, enabling efficient and accurate vector search at any scale. You can now store vectors directly in the documents alongside your data. Each document in your database can contain not only traditional schema-free data, but also high-dimensional vectors as other properties of the documents.

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/azure-cosmosdb @langchain/core
```

See a [usage example](/docs/integrations/vectorstores/azure_cosmosdb_nosql).

```typescript
import { AzureCosmosDBNoSQLVectorStore } from "@langchain/azure-cosmosdb";
```

### Azure Cosmos DB for MongoDB vCore

> [Azure Cosmos DB for MongoDB vCore](https://learn.microsoft.com/azure/cosmos-db/mongodb/vcore/) makes it easy to create a database with full native MongoDB support. You can apply your MongoDB experience and continue to use your favorite MongoDB drivers, SDKs, and tools by pointing your application to the API for MongoDB vCore account’s connection string. Use vector search in Azure Cosmos DB for MongoDB vCore to seamlessly integrate your AI-based applications with your data that’s stored in Azure Cosmos DB.

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/azure-cosmosdb @langchain/core
```

See a [usage example](/docs/integrations/vectorstores/azure_cosmosdb_mongodb).

```typescript
import { AzureCosmosDBMongoDBVectorStore } from "@langchain/azure-cosmosdb";
```

## Semantic Cache

### Azure Cosmos DB NoSQL Semantic Cache

> The Semantic Cache feature is supported with Azure Cosmos DB for NoSQL integration, enabling users to retrieve cached responses based on semantic similarity between the user input and previously cached results. It leverages [AzureCosmosDBNoSQLVectorStore](/docs/integrations/vectorstores/azure_cosmosdb_nosql), which stores vector embeddings of cached prompts. These embeddings enable similarity-based searches, allowing the system to retrieve relevant cached results.

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/azure-cosmosdb @langchain/core
```

See a [usage example](/docs/integrations/llm_caching/azure_cosmosdb_nosql).

```typescript
import { AzureCosmosDBNoSQLSemanticCache } from "@langchain/azure-cosmosdb";
```

## Chat Message History

### Azure Cosmos DB NoSQL Chat Message History

> The AzureCosmosDBNoSQLChatMessageHistory uses Cosmos DB to store chat message history. For longer-term persistence across chat sessions, you can swap out the default in-memory `chatHistory` that backs chat memory classes like `BufferMemory`.

```bash npm2yarn
npm install @langchain/azure-cosmosdb @langchain/core
```

See [usage example](/docs/integrations/memory/azure_cosmosdb_nosql.mdx).

```typescript
import { AzureCosmosDBNoSQLChatMessageHistory } from "@langchain/azure-cosmosdb";
```

### Azure Cosmos DB MongoDB vCore Chat Message History

> The AzureCosmosDBMongoChatMessageHistory uses Cosmos DB Mongo vCore to store chat message history. For longer-term persistence across chat sessions, you can swap out the default in-memory `chatHistory` that backs chat memory classes like `BufferMemory`.

```bash npm2yarn
npm install @langchain/azure-cosmosdb @langchain/core
```

See a [usage example](/docs/integrations/memory/azure_cosmos_mongo_vcore.mdx).

```typescript
import { AzureCosmosDBMongoChatMessageHistory } from "@langchain/azure-cosmosdb";
```

## Document loaders

### Azure Blob Storage

> [Azure Blob Storage](https://learn.microsoft.com/azure/storage/blobs/storage-blobs-introduction) is Microsoft's object storage solution for the cloud. Blob Storage is optimized for storing massive amounts of unstructured data. Unstructured data is data that doesn't adhere to a particular data model or definition, such as text or binary data.

> [Azure Files](https://learn.microsoft.com/azure/storage/files/storage-files-introduction) offers fully managed
> file shares in the cloud that are accessible via the industry standard Server Message Block (`SMB`) protocol,
> Network File System (`NFS`) protocol, and `Azure Files REST API`. `Azure Files` are based on the `Azure Blob Storage`.

`Azure Blob Storage` is designed for:

- Serving images or documents directly to a browser.
- Storing files for distributed access.
- Streaming video and audio.
- Writing to log files.
- Storing data for backup and restore, disaster recovery, and archiving.
- Storing data for analysis by an on-premises or Azure-hosted service.

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core @azure/storage-blob
```

See a [usage example for the Azure Blob Storage](/docs/integrations/document_loaders/web_loaders/azure_blob_storage_container).

```typescript
import { AzureBlobStorageContainerLoader } from "@langchain/community/document_loaders/web/azure_blob_storage_container";
```

See a [usage example for the Azure Files](/docs/integrations/document_loaders/web_loaders/azure_blob_storage_file).

```typescript
import { AzureBlobStorageFileLoader } from "@langchain/community/document_loaders/web/azure_blob_storage_file";
```

## Tools

### Azure Container Apps Dynamic Sessions

> [Azure Container Apps dynamic sessions](https://learn.microsoft.com/azure/container-apps/sessions) provide fast access to secure sandboxed environments that are ideal for running code or applications that require strong isolation from other workloads.

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/azure-dynamic-sessions @langchain/core
```

See a [usage example](/docs/integrations/tools/azure_dynamic_sessions).

```typescript
import { SessionsPythonREPLTool } from "@langchain/azure-dynamic-sessions";
```



================================================
FILE: docs/core_docs/docs/integrations/platforms/openai.mdx
================================================
---
keywords: [openai]
---

# OpenAI

All functionality related to OpenAI

> [OpenAI](https://en.wikipedia.org/wiki/OpenAI) is American artificial intelligence (AI) research laboratory
> consisting of the non-profit `OpenAI Incorporated`
> and its for-profit subsidiary corporation `OpenAI Limited Partnership`.
> `OpenAI` conducts AI research with the declared intention of promoting and developing a friendly AI.
> `OpenAI` systems run on an `Azure`-based supercomputing platform from `Microsoft`.

> The [OpenAI API](https://platform.openai.com/docs/models) is powered by a diverse set of models with different capabilities and price points.
>
> [ChatGPT](https://chat.openai.com) is the Artificial Intelligence (AI) chatbot developed by `OpenAI`.

## Installation and Setup

- Get an OpenAI api key and set it as an environment variable (`OPENAI_API_KEY`)

## Chat model

See a [usage example](/docs/integrations/chat/openai).

```typescript
import { ChatOpenAI } from "@langchain/openai";
```

## LLM

See a [usage example](/docs/integrations/llms/openai).

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core
```

```typescript
import { OpenAI } from "@langchain/openai";
```

## Text Embedding Model

See a [usage example](/docs/integrations/text_embedding/openai)

```typescript
import { OpenAIEmbeddings } from "@langchain/openai";
```

## Chain

```typescript
import { OpenAIModerationChain } from "langchain/chains";
```



================================================
FILE: docs/core_docs/docs/integrations/retrievers/arxiv-retriever.mdx
================================================
# ArxivRetriever

The `arXiv Retriever` allows users to query the arXiv database for academic articles. It supports both full-document retrieval (PDF parsing) and summary-based retrieval.

For detailed documentation of all ArxivRetriever features and configurations, head to the [API reference](https://api.js.langchain.com/classes/_langchain_community.retrievers_arxiv.ArxivRetriever.html)

## Features

- Query Flexibility: Search using natural language queries or specific arXiv IDs.
- Full-Document Retrieval: Option to fetch and parse PDFs.
- Summaries as Documents: Retrieve summaries for faster results.
- Customizable Options: Configure maximum results and output format.

## Integration details

| Retriever        | Source                       | Package                                                                      |
| ---------------- | ---------------------------- | ---------------------------------------------------------------------------- |
| `ArxivRetriever` | Academic articles from arXiv | [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) |

## Setup

Ensure the following dependencies are installed:

- `pdf-parse` for parsing PDFs
- `fast-xml-parser` for parsing XML responses from the arXiv API

```npm2yarn
npm install pdf-parse fast-xml-parser
```

## Instantiation

```typescript
const retriever = new ArxivRetriever({
  getFullDocuments: false, // Set to true to fetch full documents (PDFs)
  maxSearchResults: 5, // Maximum number of results to retrieve
});
```

## Usage

Use the `invoke` method to search arXiv for relevant articles. You can use either natural language queries or specific arXiv IDs.

```typescript
const query = "quantum computing";

const documents = await retriever.invoke(query);
documents.forEach((doc) => {
  console.log("Title:", doc.metadata.title);
  console.log("Content:", doc.pageContent); // Parsed PDF content
});
```

## Use within a chain

Like other retrievers, `ArxivRetriever` can be incorporated into LLM applications via chains. Below is an example of using the retriever within a chain:

```typescript
import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import {
  RunnablePassthrough,
  RunnableSequence,
} from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers";
import type { Document } from "@langchain/core/documents";

const llm = new ChatOpenAI({
  model: "gpt-4o-mini",
  temperature: 0,
});

const prompt = ChatPromptTemplate.fromTemplate(`
Answer the question based only on the context provided.

Context: {context}

Question: {question}`);

const formatDocs = (docs: Document[]) => {
  return docs.map((doc) => doc.pageContent).join("\n\n");
};

const ragChain = RunnableSequence.from([
  {
    context: retriever.pipe(formatDocs),
    question: new RunnablePassthrough(),
  },
  prompt,
  llm,
  new StringOutputParser(),
]);

await ragChain.invoke("What are the latest advances in quantum computing?");
```

## API reference

For detailed documentation of all ArxivRetriever features and configurations, head to the [API reference](https://api.js.langchain.com/classes/_langchain_community.retrievers_arxiv.ArxivRetriever.html)



================================================
FILE: docs/core_docs/docs/integrations/retrievers/azion-edgesql.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Azion EdgeSQL
---
"""

"""
# AzionRetriever

## Overview

This will help you getting started with the [AzionRetriever](/docs/concepts/#retrievers). For detailed documentation of all AzionRetriever features and configurations head to the [API reference](https://api.js.langchain.com/classes/_langchain_community.retrievers_azion_edgesql.AzionRetriever.html).

### Integration details


| Retriever | Self-host | Cloud offering | Package | [Py support] |
| :--- | :---: | :---: | :---: | :---: |
[AzionRetriever](https://api.js.langchain.com/classes/_langchain_community.retrievers_azion_edgesql.AzionRetriever.html) | ❌ | ❌ | @langchain/community | ❌ |


## Setup

To use the AzionRetriever, you need to set the AZION_TOKEN environment variable.

```typescript
process.env.AZION_TOKEN = "your-api-key"
```

If you are using OpenAI embeddings for this guide, you'll need to set your OpenAI key as well:

```typescript
process.env.OPENAI_API_KEY = "YOUR_API_KEY";
```
If you want to get automated tracing from individual queries, you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
// process.env.LANGSMITH_API_KEY = "<YOUR API KEY HERE>";
// process.env.LANGSMITH_TRACING = "true";
```

### Installation

This retriever lives in the `@langchain/community/retrievers/azion_edgesql` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  azion @langchain/openai @langchain/community
</Npm2Yarn>
```
"""

"""
## Instantiation

Now we can instantiate our retriever:
"""

import { AzionRetriever } from "@langchain/community/retrievers/azion_edgesql";
import { OpenAIEmbeddings } from "@langchain/openai";
import { ChatOpenAI } from "@langchain/openai";

const embeddingModel = new OpenAIEmbeddings({
  model: "text-embedding-3-small"
})

const chatModel = new ChatOpenAI({
  model: "gpt-4o-mini",
  apiKey: process.env.OPENAI_API_KEY
})

const retriever = new AzionRetriever(embeddingModel, 
  {dbName:"langchain",
   vectorTable:"documents", // table where the vector embeddings are stored
   ftsTable:"documents_fts", // table where the fts index is stored
   searchType:"hybrid", // search type to use for the retriever
   ftsK:2, // number of results to return from the fts index
   similarityK:2, // number of results to return from the vector index
   metadataItems:["language","topic"],
   filters: [{ operator: "=", column: "language", value: "en" }],
   entityExtractor:chatModel

}) // number of results to return from the vector index

"""
## Usage
"""

const query = "Australia"

await retriever.invoke(query);
# Output:
#   [

#     Document {

#       pageContent: 'Australia s indigenous people have inhabited the continent for over 65,000 years',

#       metadata: { language: 'en', topic: 'history', searchtype: 'similarity' },

#       id: '3'

#     },

#     Document {

#       pageContent: 'Australia is a leader in solar energy adoption and renewable technology',

#       metadata: { language: 'en', topic: 'technology', searchtype: 'similarity' },

#       id: '5'

#     },

#     Document {

#       pageContent: 'Australia s tech sector is rapidly growing with innovation hubs in major cities',

#       metadata: { language: 'en', topic: 'technology', searchtype: 'fts' },

#       id: '7'

#     }

#   ]


"""
## Use within a chain

Like other retrievers, AzionRetriever can be incorporated into LLM applications via [chains](/docs/how_to/sequence/).

We will need a LLM or chat model:

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

// @lc-docs-hide-cell

import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({
  model: "gpt-4o-mini",
  temperature: 0,
});

import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers";

import type { Document } from "@langchain/core/documents";

const prompt = ChatPromptTemplate.fromTemplate(`
Answer the question based only on the context provided.

Context: {context}

Question: {question}`);

const formatDocs = (docs: Document[]) => {
  return docs.map((doc) => doc.pageContent).join("\n\n");
}

// See https://js.langchain.com/docs/tutorials/rag
const ragChain = RunnableSequence.from([
  {
    context: retriever.pipe(formatDocs),
    question: new RunnablePassthrough(),
  },
  prompt,
  llm,
  new StringOutputParser(),
]);

await ragChain.invoke("Paris")
# Output:
#   The context mentions that the 2024 Olympics are in Paris.


"""
## API reference

For detailed documentation of all AzionRetriever features and configurations head to the [API reference](https://api.js.langchain.com/classes/_langchain_community.retrievers_azion_edgesql.AzionRetriever.html).

"""



================================================
FILE: docs/core_docs/docs/integrations/retrievers/bedrock-knowledge-bases.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Knowledge Bases for Amazon Bedrock
---
"""

"""
# Knowledge Bases for Amazon Bedrock

## Overview

This will help you getting started with the [AmazonKnowledgeBaseRetriever](/docs/concepts/retrievers). For detailed documentation of all AmazonKnowledgeBaseRetriever features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_aws.AmazonKnowledgeBaseRetriever.html).

Knowledge Bases for Amazon Bedrock is a fully managed support for end-to-end RAG workflow provided by Amazon Web Services (AWS).
It provides an entire ingestion workflow of converting your documents into embeddings (vector) and storing the embeddings in a specialized vector database.
Knowledge Bases for Amazon Bedrock supports popular databases for vector storage, including vector engine for Amazon OpenSearch Serverless, Pinecone, Redis Enterprise Cloud, Amazon Aurora (coming soon), and MongoDB (coming soon).

### Integration details

| Retriever | Self-host | Cloud offering | Package | [Py support](https://python.langchain.com/docs/integrations/retrievers/bedrock/) |
| :--- | :--- | :---: | :---: | :---: |
[AmazonKnowledgeBaseRetriever](https://api.js.langchain.com/classes/langchain_aws.AmazonKnowledgeBaseRetriever.html) | 🟠 (see details below) | ✅ | @langchain/aws | ✅ |

> AWS Knowledge Base Retriever can be 'self hosted' in the sense you can run it on your own AWS infrastructure. However it is not possible to run on another cloud provider or on-premises.

## Setup

In order to use the AmazonKnowledgeBaseRetriever, you need to have an AWS account, where you can manage your indexes and documents. Once you've setup your account, set the following environment variables:

```bash
process.env.AWS_KNOWLEDGE_BASE_ID=your-knowledge-base-id
process.env.AWS_ACCESS_KEY_ID=your-access-key-id
process.env.AWS_SECRET_ACCESS_KEY=your-secret-access-key
```
"""

"""
If you want to get automated tracing from individual queries, you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:
"""

// process.env.LANGSMITH_API_KEY = "<YOUR API KEY HERE>";
// process.env.LANGSMITH_TRACING = "true";

"""
### Installation

This retriever lives in the `@langchain/aws` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/aws @langchain/core
</Npm2Yarn>
```
"""

"""
## Instantiation

Now we can instantiate our retriever:
"""

import { AmazonKnowledgeBaseRetriever } from "@langchain/aws";

const retriever = new AmazonKnowledgeBaseRetriever({
  topK: 10,
  knowledgeBaseId: process.env.AWS_KNOWLEDGE_BASE_ID,
  region: "us-east-2",
  clientOptions: {
    credentials: {
      accessKeyId: process.env.AWS_ACCESS_KEY_ID,
      secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY,
    },
  },
});

"""
## Usage
"""

const query = "..."

await retriever.invoke(query);

"""
## Use within a chain

Like other retrievers, AmazonKnowledgeBaseRetriever can be incorporated into LLM applications via [chains](/docs/how_to/sequence/).

We will need a LLM or chat model:

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

// @lc-docs-hide-cell

import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({
  model: "gpt-4o-mini",
  temperature: 0,
});

import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers";

import type { Document } from "@langchain/core/documents";

const prompt = ChatPromptTemplate.fromTemplate(`
Answer the question based only on the context provided.

Context: {context}

Question: {question}`);

const formatDocs = (docs: Document[]) => {
  return docs.map((doc) => doc.pageContent).join("\n\n");
}

// See https://js.langchain.com/docs/tutorials/rag
const ragChain = RunnableSequence.from([
  {
    context: retriever.pipe(formatDocs),
    question: new RunnablePassthrough(),
  },
  prompt,
  llm,
  new StringOutputParser(),
]);

"""
```{=mdx}

:::tip

See [our RAG tutorial](docs/tutorials/rag) for more information and examples on `RunnableSequence`'s like the one above.

:::

```
"""

await ragChain.invoke("...")

"""
## API reference

For detailed documentation of all AmazonKnowledgeBaseRetriever features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_aws.AmazonKnowledgeBaseRetriever.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/retrievers/bm25.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# BM25

BM25, also known as [Okapi BM25](https://en.wikipedia.org/wiki/Okapi_BM25), is a ranking function used in information retrieval systems to estimate the relevance of documents to a given search query.

You can use it as part of your retrieval pipeline as a to rerank documents as a postprocessing step after retrieving an initial set of documents from another source.

## Setup

The `BM25Retriever` is exported from `@langchain/community`. You'll need to install it like this:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core
</Npm2Yarn>
```

This retriever uses code from [`this implementation`](https://github.com/FurkanToprak/OkapiBM25) of Okapi BM25.

## Usage

You can now create a new retriever with previously retrieved documents:
"""

import { BM25Retriever } from "@langchain/community/retrievers/bm25";

const retriever = BM25Retriever.fromDocuments([
  { pageContent: "Buildings are made out of brick", metadata: {} },
  { pageContent: "Buildings are made out of wood", metadata: {} },
  { pageContent: "Buildings are made out of stone", metadata: {} },
  { pageContent: "Cars are made out of metal", metadata: {} },
  { pageContent: "Cars are made out of plastic", metadata: {} },
  { pageContent: "mitochondria is the powerhouse of the cell", metadata: {} },
  { pageContent: "mitochondria is made of lipids", metadata: {} },
], { k: 4 });

// Will return the 4 documents reranked by the BM25 algorithm
await retriever.invoke("mitochondria");
# Output:
#   [

#     { pageContent: 'mitochondria is made of lipids', metadata: {} },

#     {

#       pageContent: 'mitochondria is the powerhouse of the cell',

#       metadata: {}

#     },

#     { pageContent: 'Buildings are made out of brick', metadata: {} },

#     { pageContent: 'Buildings are made out of wood', metadata: {} }

#   ]




================================================
FILE: docs/core_docs/docs/integrations/retrievers/chaindesk-retriever.mdx
================================================
# Chaindesk Retriever

This example shows how to use the Chaindesk Retriever in a retrieval chain to retrieve documents from a Chaindesk.ai datastore.

## Usage

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core
```

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/retrievers/chaindesk.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## Related

- Retriever [conceptual guide](/docs/concepts/retrievers)
- Retriever [how-to guides](/docs/how_to/#retrievers)



================================================
FILE: docs/core_docs/docs/integrations/retrievers/chatgpt-retriever-plugin.mdx
================================================
---
hide_table_of_contents: true
sidebar_class_name: hidden
---

# ChatGPT Plugin Retriever

:::warning
This module has been deprecated and is no longer supported. The documentation below will not work in versions 0.2.0 or later.
:::

This example shows how to use the ChatGPT Retriever Plugin within LangChain.

To set up the ChatGPT Retriever Plugin, please follow instructions [here](https://github.com/openai/chatgpt-retrieval-plugin).

## Usage

```typescript
import { ChatGPTPluginRetriever } from "langchain/retrievers/remote";

const retriever = new ChatGPTPluginRetriever({
  url: "http://0.0.0.0:8000",
  auth: {
    bearer: "super-secret-jwt-token-with-at-least-32-characters-long",
  },
});

const docs = await retriever.invoke("hello world");

console.log(docs);
```

## Related

- Retriever [conceptual guide](/docs/concepts/retrievers)
- Retriever [how-to guides](/docs/how_to/#retrievers)



================================================
FILE: docs/core_docs/docs/integrations/retrievers/dria.mdx
================================================
---
hide_table_of_contents: true
---

# Dria Retriever

The [Dria](https://dria.co/profile) retriever allows an agent to perform a text-based search across a comprehensive knowledge hub.

## Setup

To use Dria retriever, first install Dria JS client:

```bash npm2yarn
npm install dria
```

You need to provide two things to the retriever:

- **API Key**: you can get yours at your [profile page](https://dria.co/profile) when you create an account.
- **Contract ID**: accessible at the top of the page when viewing a knowledge or in its URL.
  For example, the Bitcoin whitepaper is uploaded on Dria at https://dria.co/knowledge/2KxNbEb040GKQ1DSDNDsA-Fsj_BlQIEAlzBNuiapBR0, so its contract ID is `2KxNbEb040GKQ1DSDNDsA-Fsj_BlQIEAlzBNuiapBR0`.
  Contract ID can be omitted during instantiation, and later be set via `dria.contractId = "your-contract"`

Dria retriever exposes the underlying [Dria client](https://npmjs.com/package/dria) as well, refer to the [Dria documentation](https://github.com/firstbatchxyz/dria-js-client?tab=readme-ov-file#usage) to learn more about the client.

## Usage

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install dria @langchain/community @langchain/core
```

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/retrievers/dria.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## Related

- Retriever [conceptual guide](/docs/concepts/retrievers)
- Retriever [how-to guides](/docs/how_to/#retrievers)



================================================
FILE: docs/core_docs/docs/integrations/retrievers/exa.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Exa
---
"""

"""
# ExaRetriever

## Overview

[Exa](https://exa.ai/) is a search engine that retrieves relevant content from the web given some input query.

This guide will help you getting started with the Exa [retriever](/docs/concepts/retrievers). For detailed documentation of all `ExaRetriever` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_exa.ExaRetriever.html).

### Integration details

| Retriever | Source | Package |
| :--- | :--- | :---: |
[ExaRetriever](https://api.js.langchain.com/classes/langchain_exa.ExaRetriever.html) | Information on the web. | [`@langchain/exa`](https://www.npmjs.com/package/@langchain/exa) |

## Setup

You'll need to set your API key as an environment variable.

The `Exa` class defaults to `EXASEARCH_API_KEY` when searching for your API key.

```typescript
process.env.EXASEARCH_API_KEY="<YOUR API KEY>";
```

If you want to get automated tracing from individual queries, you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
// process.env.LANGSMITH_API_KEY = "<YOUR API KEY HERE>";
// process.env.LANGSMITH_TRACING = "true";
```

### Installation

This retriever lives in the `@langchain/exa` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/exa @langchain/core
</Npm2Yarn>
```
"""

"""
## Instantiation

Now we can instantiate our retriever:
"""

import { ExaRetriever } from "@langchain/exa";
import Exa from "exa-js";

const retriever = new ExaRetriever({
  // @lc-ts-ignore
  client: new Exa(
    process.env.EXASEARCH_API_KEY // default API key
  ),
  searchArgs: {
    numResults: 2,
  }
});

"""
## Usage
"""

const query = "What did the speaker say about Justice Breyer in the 2022 State of the Union?";

await retriever.invoke(query);
# Output:
#   [

#     Document {

#       pageContent: 'President Biden’s State of the Union Address\n' +

#         'Madam Speaker, Madam Vice President, and our First Lady and Second Gentleman, members of Congress and the Cabinet, Justices of the Supreme Court, my fellow Americans: Last year, COVID-19 kept us apart. This year, we’re finally together again.\n' +

#         'Tonight — tonight we meet as Democrats, Republicans, and independents, but, most importantly, as Americans with a duty to one another, to America, to the American people, and to the Constitution, and an unwavering resolve that freedom will always triumph over tyranny.\n' +

#         'Six — thank you. Six days ago, Russia’s Vladimir Putin sought to shake the very foundations of the free world, thinking he could make it bend to his menacing ways. But he badly miscalculated. He thought he could roll into Ukraine and the world would roll over. Instead, he met with a wall of strength he never anticipated or imagined. He met the Ukrainian people.\n' +

#         'From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination literally inspires the world. Groups of citizens blocking tanks with their bodies. Everyone from students to retirees, to teachers turned soldiers defending their homeland.\n' +

#         'And in this struggle — President Zelenskyy said in his speech to the European Parliament, “Light will win over darkness.”\n' +

#         'The Ukrainian Ambassador to the United States is here tonight sitting with the First Lady. Let each of us, if you’re able to stand, stand and send an unmistakable signal to the world and Ukraine. Thank you. Thank you, thank you, thank you.\n' +

#         'She’s bright, she’s strong, and she’s resolved.\n' +

#         'Yes. We, the United States of America, stand with the Ukrainian people.\n' +

#         'Throughout our history, we’ve learned this lesson: When dictators do not pay a price for their aggression, they cause more chaos; they keep moving; and the costs, the threats to the America — and America, to the world keeps rising.\n' +

#         'That’s why the NATO Alliance was created: to secure peace and stability in Europe after World War Two.\n' +

#         'The United States is a member, along with 29 other nations. It matters. American diplomacy matters. American resolve matters.\n' +

#         'Putin’s latest attack on Ukraine was premeditated and totally unprovoked. He rejected repeated efforts at diplomacy.\n' +

#         'He thought the West and NATO wouldn’t respond. He thought he could divide us at home, in this chamber, in this nation. He thought he could divide us in Europe as well.\n' +

#         'But Putin was wrong. We are ready. We are united. And that’s what we did: We stayed united.\n' +

#         'We prepared extensively and carefully. We spent months building coalitions of other freedom-loving nations in Europe and the Americas to — from America to the Asian and African continents to confront Putin.\n' +

#         'Like many of you, I spent countless hours unifying our European Allies.\n' +

#         'We shared with the world, in advance, what we knew Putin was planning and precisely how he would try to falsely and justify his aggression.\n' +

#         'We countered Russia’s lies with the truth. And now — now that he’s acted, the free world is holding him accountable, along with 27 members of the European Union — including France, Germany, Italy — as well as countries like the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others. Even Switzerland are inflicting pain on Russia and supporting the people of Ukraine.\n' +

#         'Putin is now isolated from the world more than he has ever been.\n' +

#         'Together. Together. Together, along with our Allies, we are right now enforcing powerful economic sanctions. We’re cutting off Russia’s largest banks from the international financial system; preventing Russia’s Central Bank from defending the Russian ruble, making Putin’s $630 billion war fund worthless. We’re choking Russia’s access, we’re choking Russia’s access to technology that will sap its economic strength and weaken its military for years to come.\n' +

#         'Tonight, I say to the Russian oligarchs and the corrupt leaders who’ve bilked billions of dollars off this violent regime: No more.\n' +

#         'The United States — I mean it. The United States Department of Justice is assembling a dedicated task force to go after the crimes of the Russian oligarchs.\n' +

#         'We’re joining with European Allies to find and seize their yachts, their luxury apartments, their private jets. We’re coming for your ill-begotten gains.\n' +

#         'And, tonight, I’m announcing that we will join our Allies in closing off American air space to all Russian flights, further isolating Russia and adding an additional squeeze on their economy.\n' +

#         'He has no idea what’s coming.\n' +

#         'The ruble has already lost 30 percent of its value, the Russian stock market has lost 40 percent of its value, and trading remains suspended.\n' +

#         'The Russian economy is reeling, and Putin alone is the one to blame.\n' +

#         'Together with our Allies, we’re providing support to the Ukrainians in their fight for freedom: military assistance, economic assistance, humanitarian assistance. We’re giving more than a billion dollars in direct assistance to Ukraine. And we’ll continue to aid the Ukrainian people as they defend their country and help ease their suffering.\n' +

#         'But let me be clear: Our forces are not engaged and will not engage in the conflict with Russian forces in Ukraine. Our forces are not going to Europe to fight in Ukraine but to defend our NATO Allies in the event that Putin decides to keep moving west.\n' +

#         'For that purpose, we have mobilized American ground forces, air squadrons, ship deployments to protect NATO countries, including Poland, Romania, Latvia, Lithuania, and Estonia.\n' +

#         'And as I’ve made crystal clear, the United States and our Allies will defend every inch of territory that is NATO territory with the full force of our collective power — every single inch.\n' +

#         'And we’re clear-eyed. The Ukrainians are fighting back with pure courage. But the next few days, weeks, and months will be hard on them.\n' +

#         'Putin has unleashed violence and chaos. But while he may make gains on the battlefield, he will pay a continuing high price over the long run.\n' +

#         'And a pound of Ukrainian people — the proud, proud people — pound for pound, ready to fight with every inch of (inaudible) they have. They’ve known 30 years of independence — have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.\n' +

#         'To all Americans, I’ll be honest with you, as I’ve always promised I would be. A Russian dictator infa- — invading a foreign country has costs around the world. And I’m taking robust action to make sure the pain of our sanctions is targeted at the Russian economy and that we use every tool at our disposal to protect American businesses and consumers.\n' +

#         'Tonight, I can announce the United States has worked with 30 other countries to release 60 million barrels of oil from reserves around the world. America will lead that effort, releasing 30 million barrels of our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, united with our Allies.\n' +

#         'These steps will help blunt gas prices here at home. But I know news about what’s happening can seem alarming to all Americans. But I want you to know: We’re going to be okay. We’re going to be okay.\n' +

#         'When the history of this era is written, Putin’s war on Ukraine will have left Russia weaker and the rest of the world stronger.\n' +

#         'While it shouldn’t and while it shouldn’t have taken something so terrible for people around the world to see what’s at stake, now everyone sees it clearly.\n' +

#         'We see the unity among leaders of nations, a more unified Europe, a more unified West.\n' +

#         'We see unity among the people who are gathering in cities in large crowds around the world, even in Russia, to demonstrate their support for the people of Ukraine.\n' +

#         'In the battle between democracy and autocracies, democracies are rising to the moment and the world is clearly choosing the side of peace and security.\n' +

#         'This is the real test, and it’s going to take time. So, let us continue to draw inspiration from the iron will of the Ukrainian people.\n' +

#         'To our fellow Ukrainian Americans who forged a deep bond that connects our two nations: We stand with you. We stand with you.\n' +

#         'Putin may circle Kyiv with tanks, but he’ll never gain the hearts and souls of Ukrainian people. He’ll never — he’ll never extinguish their love of freedom. And he will never, never weaken the resolve of the free world.\n' +

#         'We meet tonight in an America that has lived through two of the hardest years this nation has ever faced. The pandemic has been punishing. And so many families are living paycheck to paycheck, struggling to keep up with the rising cost of food, gas, housing, and so much more.\n' +

#         'I understand, like many of you did. My dad had to leave his home in Scranton, Pennsylvania, to find work. So, like many of you, I grew up in a family when the price of food went up, it was felt throughout the family; it had an impact.\n' +

#         'That’s why one of the first things I did as President was fight to pass the American Rescue Plan, because people were hurting. We needed to act and we did.\n' +

#         'American Rescue Plan \n' +

#         'Few pieces of legislation have done more at a critical moment in our history to lift us out of a crisis. It fueled our efforts to vaccinate the nation and combat COVID-19. It delivered immediate economic relief to tens of millions of Americans. It helped put food on the table. Remember those long lines of cars waiting for hours just to get a box of food put in their trunk? It cut the cost of healthcare insurance. And as my dad used to say, it gave the people “just a little bit of breathing room.”\n' +

#         'And unlike the $2 trillion tax cut passed in the previous administration that benefitted the top 1 percent of Americans, the American Rescue Plan helped working people and left no one behind. And, folks — and it worked. It worked.\n' +

#         'It worked and created jobs — lots of jobs. In fact, our economy created over 6.5 million new jobs just last year, more jobs in one year than ever before in the history of the United States of America.\n' +

#         'Economic Progress Report \n' +

#         'The economy grew at a rate of 5.7 last year — the strongest growth'... 35166 more characters,

#       metadata: {

#         score: 0.16303963959217072,

#         title: '2022 State of the Union Address | The White House',

#         id: 'https://www.whitehouse.gov/state-of-the-union-2022/',

#         url: 'https://www.whitehouse.gov/state-of-the-union-2022/',

#         publishedDate: '2022-02-25',

#         author: ''

#       },

#       id: undefined

#     },

#     Document {

#       pageContent: "The President. Thank you all very, very much. Thank you, please. Thank you so much. Madam Speaker, Madam Vice President, and our First Lady and Second Gentleman, Members of Congress and the Cabinet, Justices of the Supreme Court, my fellow Americans: Last year, COVID-19 kept us apart. This year, we're finally together again.\n" +

#         'Tonight we meet as Democrats, Republicans, and Independents, but most importantly, as Americans with a duty to one another, to America, to the American people, and to the Constitution, and an unwavering resolve that freedom will always triumph over tyranny.\n' +

#         "Six—[applause]—thank you. Six days ago, Russia's Vladimir Putin sought to shake the very foundations of the free world, thinking he could make it bend to his menacing ways. But he badly miscalculated. He thought he could roll into Ukraine and the world would roll over. Instead, he met with a wall of strength he never anticipated or imagined. He met the Ukrainian people.\n" +

#         'From President Zelenskiy, their—to every Ukrainian, their fearlessness, their courage, their determination literally inspires the world. Groups of citizens blocking tanks with their bodies. Everyone from students to retirees, to teachers turned soldiers defending their homeland. And in this struggle—President Zelenskiy said in his speech to the European Parliament, "Light will win over darkness."\n' +

#         "The Ukrainian Ambassador to the United States is here tonight sitting with the First Lady. Let each of us, if you're able to stand, stand and send an unmistakable signal to the world and Ukraine. Thank you. Thank you, thank you, thank you. She's bright, she's strong, and she's resolved. Yes. We, the United States of America, stand with the Ukrainian people.\n" +

#         "Throughout our history, we've learned this lesson: When dictators do not pay a price for their aggression, they cause more chaos; they keep moving; and the costs, the threats to the America—and America, to the world keeps rising. That's why the NATO alliance was created: to secure peace and stability in Europe after World War II. The United States is a member, along with 29 other nations. It matters. American diplomacy matters. American resolve matters.\n" +

#         "Putin's latest attack on Ukraine was premeditated and totally unprovoked. He rejected repeated—repeated—efforts at diplomacy. He thought the West and NATO wouldn't respond. He thought he could divide us at home, in this Chamber, in this Nation. He thought he could divide us in Europe as well.\n" +

#         "But Putin was wrong. We are ready. We are united. And that's what we did: We stayed united. We prepared extensively and carefully. We spent months building coalitions of other freedom-loving nations in Europe and the Americas to—from America to the Asian and African continents to confront Putin.\n" +

#         "Like many of you, I spent countless hours unifying our European allies. We shared with the world in advance what we knew Putin was planning and precisely how he would try to falsify and justify his aggression. We countered Russia's lies with the truth. And now—now that he's acted, the free world is holding him accountable, along with 27 members of the European Union—including France, Germany, Italy—as well as countries like the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others—even Switzerland—are inflicting pain on Russia and supporting the people of Ukraine. Putin is now isolated from the world more than he has ever been.\n" +

#         "Together, along with our allies, we are right now enforcing powerful economic sanctions. We're cutting off Russia's largest banks from the international financial system; preventing Russia's Central Bank from defending the Russian ruble, making Putin's $630 billion war fund worthless. We're choking Russia's access to technology that will sap its economic strength and weaken its military for years to come.\n" +

#         'Tonight I say to the Russian oligarchs and the corrupt leaders who have bilked billions of dollars off this violent regime: No more. The United States—[applause]—I mean it. The United States Department of Justice is assembling a dedicated task force to go after the crimes of the Russian oligarchs.\n' +

#         "We're joining with European allies to find and seize their yachts, their luxury apartments, their private jets. We're coming for your ill-begotten gains. And tonight I'm announcing that we will join our allies in closing off American air space to all Russian flights, further isolating Russia and adding an additional squeeze on their economy.\n" +

#         "He has no idea what's coming. The ruble has already lost 30 percent of its value, the Russian stock market has lost 40 percent of its value, and trading remains suspended. The Russian economy is reeling, and Putin alone is the one to blame.\n" +

#         "Together with our allies, we're providing support to the Ukrainians in their fight for freedom: military assistance, economic assistance, humanitarian assistance. We're giving more than a billion dollars in direct assistance to Ukraine. And we'll continue to aid the Ukrainian people as they defend their country and help ease their suffering.\n" +

#         "But let me be clear: Our Forces are not engaged and will not engage in the conflict with Russian forces in Ukraine. Our Forces are not going to Europe to fight [in]* Ukraine but to defend our NATO allies in the event that Putin decides to keep moving west. For that purpose, we have mobilized American ground forces, air squadrons, ship deployments to protect NATO countries, including Poland, Romania, Latvia, Lithuania, and Estonia. And as I've made crystal clear, the United States and our allies will defend every inch of territory that is NATO territory with the full force of our collective power—every single inch.\n" +

#         "And we're clear eyed. The Ukrainians are fighting back with pure courage. But the next few days, weeks, and months will be hard on them. Putin has unleashed violence and chaos. But while he may make gains on the battlefield, he'll pay a continuing high price over the long run. And a pound of Ukrainian people—the proud, proud people—pound for pound, ready to fight with every inch of energy they have. They've known 30 years of independence—have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.\n" +

#         "To all Americans, I'll be honest with you, as I've always promised I would be. A Russian dictator invading a foreign country has costs around the world. And I'm taking robust action to make sure the pain of our sanctions is targeted at Russian economy and that we use every tool at our disposal to protect American businesses and consumers.\n" +

#         'Tonight I can announce the United States has worked with 30 other countries to release 60 million barrels of oil from reserves around the world. America will lead that effort, releasing 30 million barrels of our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, united with our allies.\n' +

#         "These steps will help blunt gas prices here at home. But I know news about what's happening can seem alarming to all Americans. But I want you to know: We're going to be okay. We're going to be okay.\n" +

#         "When the history of this era is written, Putin's war on Ukraine will have left Russia weaker and the rest of the world stronger.\n" +

#         "While it shouldn't have taken something so terrible for people around the world to see what's at stake, now everyone sees it clearly. We see the unity among leaders of nations, a more unified Europe, a more unified West. We see unity among the people who are gathering in cities in large crowds around the world, even in Russia, to demonstrate their support for the people of Ukraine.\n" +

#         "In the battle between democracy and autocracies, democracies are rising to the moment, and the world is clearly choosing the side of peace and security. This is the real test, and it's going to take time. So let us continue to draw inspiration from the iron will of the Ukrainian people.\n" +

#         "To our fellow Ukrainian Americans who forged a deep bond that connects our two nations: We stand with you. We stand with you. Putin may circle Kiev with tanks, but he'll never gain the hearts and souls of the Uranian [Ukrainian]* people. He'll never extinguish their love of freedom. And he will never, never weaken the resolve of the free world.\n" +

#         'We meet tonight in an America that has lived through 2 of the hardest years this Nation has ever faced. The pandemic has been punishing. And so many families are living paycheck to paycheck, struggling to keep up with the rising cost of food, gas, housing, and so much more.\n' +

#         "I understand, like many of you did. My dad had to leave his home in Scranton, Pennsylvania, to find work. So, like many of you, I grew up in a family when the price of food went up, it was felt throughout the family; it had an impact. That's why one of the first things I did as President was fight to pass the American Rescue Plan, because people were hurting. We needed to act, and we did.\n" +

#         'Few pieces of legislation have done more at a critical moment in our history to lift us out of a crisis. It fueled our efforts to vaccinate the Nation and combat COVID-19. It delivered immediate economic relief to tens of millions of Americans. It helped put food on the table. Remember those long lines of cars waiting for hours just to get a box of food put in their trunk? It cut the cost of health care insurance. And as my dad used to say, it gave the people "just a little bit of breathing room."\n' +

#         'And unlike the $2 trillion tax cut passed in the previous administration that benefited the top 1 percent of Americans, the American Rescue Plan——\n' +

#         ' Audience members. Boo!\n' +

#         ' The President. ——the American Rescue Plan helped working people and left no one behind. And, folks—and it worked. It worked. It worked and created jobs, lots of jobs. In fact, our economy created over 6.5 million new jobs just last year, more jobs in 1 year than ever before in the history of the United States of America. The economy grew at a rate of 5.7 last year, the strongest growth rate in 40 years and the first step in'... 35254 more characters,

#       metadata: {

#         score: 0.16301880776882172,

#         title: 'Address Before a Joint Session of the Congress on the State of the Union',

#         id: 'https://www.presidency.ucsb.edu/documents/address-before-joint-session-the-congress-the-state-the-union-28',

#         url: 'https://www.presidency.ucsb.edu/documents/address-before-joint-session-the-congress-the-state-the-union-28',

#         publishedDate: '2022-03-01',

#         author: ''

#       },

#       id: undefined

#     }

#   ]


"""
## Use within a chain

Like other retrievers, ExaRetriever can be incorporated into LLM applications via [chains](/docs/how_to/sequence/).

We will need a LLM or chat model:

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

// @lc-docs-hide-cell

import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({
  model: "gpt-4o-mini",
  temperature: 0,
});

import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers";

import type { Document } from "@langchain/core/documents";

const prompt = ChatPromptTemplate.fromTemplate(`
Answer the question based only on the context provided.

Context: {context}

Question: {question}`);

const formatDocs = (docs: Document[]) => {
  return docs.map((doc) => doc.pageContent).join("\n\n");
}

// See https://js.langchain.com/docs/tutorials/rag
const ragChain = RunnableSequence.from([
  {
    context: retriever.pipe(formatDocs),
    question: new RunnablePassthrough(),
  },
  prompt,
  llm,
  new StringOutputParser(),
]);

await ragChain.invoke("What did the speaker say about Justice Breyer in the 2022 State of the Union?");
# Output:
#   In the 2022 State of the Union Address, the speaker, President Biden, honored Justice Breyer, describing him as someone who has dedicated his life to serve the country. He acknowledged Justice Breyer as an Army veteran and a constitutional scholar, and he expressed gratitude for his service. President Biden also mentioned that one of the most serious constitutional responsibilities of a President is nominating someone to serve on the United States Supreme Court, and he highlighted his nomination of Ketanji Brown Jackson to succeed Justice Breyer.


"""
## API reference

For detailed documentation of all ExaRetriever features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_exa.ExaRetriever.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/retrievers/hyde.mdx
================================================
---
hide_table_of_contents: true
---

# HyDE Retriever

This example shows how to use the HyDE Retriever, which implements Hypothetical Document Embeddings (HyDE) as described in [this paper](https://arxiv.org/abs/2212.10496).

At a high level, HyDE is an embedding technique that takes queries, generates a hypothetical answer, and then embeds that generated document and uses that as the final example.

In order to use HyDE, we therefore need to provide a base embedding model, as well as an LLM that can be used to generate those documents. By default, the HyDE class comes with some default prompts to use (see the paper for more details on them), but we can also create our own, which should have a single input variable `{question}`.

## Usage

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/retrievers/hyde.ts";

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core
```

<CodeBlock language="typescript">{Example}</CodeBlock>

## Related

- Retriever [conceptual guide](/docs/concepts/retrievers)
- Retriever [how-to guides](/docs/how_to/#retrievers)



================================================
FILE: docs/core_docs/docs/integrations/retrievers/index.mdx
================================================
---
sidebar_position: 0
sidebar_class_name: hidden
---

import { CategoryTable, IndexTable } from "@theme/FeatureTables";

# Retrievers

A [retriever](/docs/concepts/retrievers) is an interface that returns documents given an unstructured query.
It is more general than a vector store.
A retriever does not need to be able to store documents, only to return (or retrieve) them.

Retrievers accept a string query as input and return a list of Documents.

For specifics on how to use retrievers, see the [relevant how-to guides here](/docs/how_to/#retrievers).

Note that all [vector stores](/docs/concepts/#vectorstores) can be [cast to retrievers](/docs/how_to/vectorstore_retriever/).
Refer to the vector store [integration docs](/docs/integrations/vectorstores/) for available vector store retrievers.

:::info
If you'd like to write your own retriever, see [this how-to](/docs/how_to/custom_retriever/). If you'd like to contribute an integration, see [Contributing integrations](/docs/contributing).
:::

## All retrievers

<IndexTable />



================================================
FILE: docs/core_docs/docs/integrations/retrievers/kendra-retriever.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Amazon Kendra Retriever
---
"""

"""
# AWSKendraRetriever

## Overview

[Amazon Kendra](https://aws.amazon.com/kendra/) is an intelligent search service provided by Amazon Web Services (AWS).
It utilizes advanced natural language processing (NLP) and machine learning algorithms to enable powerful search capabilities across various data sources within an organization.
Kendra is designed to help users find the information they need quickly and accurately, improving productivity and decision-making.

With Kendra, users can search across a wide range of content types, including documents, FAQs, knowledge bases, manuals, and websites.
It supports multiple languages and can understand complex queries, synonyms, and contextual meanings to provide highly relevant search results.

This will help you getting started with the Amazon Kendra [`retriever`](/docs/concepts/retrievers). For detailed documentation of all `AWSKendraRetriever` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_aws.AmazonKendraRetriever.html).

### Integration details

| Retriever | Source | Package |
| :--- | :--- | :---: |
[AWSKendraRetriever](https://api.js.langchain.com/classes/langchain_aws.AmazonKendraRetriever.html) | Various AWS resources | [`@langchain/aws`](https://www.npmjs.com/package/@langchain/aws) |

## Setup

You'll need an AWS account and an Amazon Kendra instance to get started. See this [tutorial](https://docs.aws.amazon.com/kendra/latest/dg/getting-started.html) from AWS for more information.

If you want to get automated tracing from individual queries, you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
// process.env.LANGSMITH_API_KEY = "<YOUR API KEY HERE>";
// process.env.LANGSMITH_TRACING = "true";
```

### Installation

This retriever lives in the `@langchain/aws` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/aws @langchain/core
</Npm2Yarn>
```
"""

"""
## Instantiation

Now we can instantiate our retriever:
"""

import { AmazonKendraRetriever } from "@langchain/aws";

const retriever = new AmazonKendraRetriever({
  topK: 10,
  indexId: "YOUR_INDEX_ID",
  region: "us-east-2", // Your region
  clientOptions: {
    credentials: {
      accessKeyId: "YOUR_ACCESS_KEY_ID",
      secretAccessKey: "YOUR_SECRET_ACCESS_KEY",
    },
  },
});

"""
## Usage
"""

const query = "..."

await retriever.invoke(query);

"""
## Use within a chain

Like other retrievers, the `AWSKendraRetriever` can be incorporated into LLM applications via [chains](/docs/how_to/sequence/).

We will need a LLM or chat model:

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

// @lc-docs-hide-cell

import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({
  model: "gpt-4o-mini",
  temperature: 0,
});

import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers";

import type { Document } from "@langchain/core/documents";

const prompt = ChatPromptTemplate.fromTemplate(`
Answer the question based only on the context provided.

Context: {context}

Question: {question}`);

const formatDocs = (docs: Document[]) => {
  return docs.map((doc) => doc.pageContent).join("\n\n");
}

// See https://js.langchain.com/docs/tutorials/rag
const ragChain = RunnableSequence.from([
  {
    context: retriever.pipe(formatDocs),
    question: new RunnablePassthrough(),
  },
  prompt,
  llm,
  new StringOutputParser(),
]);

await ragChain.invoke(query);

"""
## API reference

For detailed documentation of all `AmazonKendraRetriever` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_aws.AmazonKendraRetriever.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/retrievers/metal-retriever.mdx
================================================
---
hide_table_of_contents: true
---

# Metal Retriever

This example shows how to use the Metal Retriever in a retrieval chain to retrieve documents from a Metal index.

## Setup

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm i @getmetal/metal-sdk @langchain/community @langchain/core
```

## Usage

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/retrievers/metal.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## Related

- Retriever [conceptual guide](/docs/concepts/retrievers)
- Retriever [how-to guides](/docs/how_to/#retrievers)



================================================
FILE: docs/core_docs/docs/integrations/retrievers/supabase-hybrid.mdx
================================================
# Supabase Hybrid Search

Langchain supports hybrid search with a Supabase Postgres database. The hybrid search combines the postgres `pgvector` extension (similarity search) and Full-Text Search (keyword search) to retrieve documents. You can add documents via SupabaseVectorStore `addDocuments` function. SupabaseHybridKeyWordSearch accepts embedding, supabase client, number of results for similarity search, and number of results for keyword search as parameters. The `getRelevantDocuments` function produces a list of documents that has duplicates removed and is sorted by relevance score.

## Setup

### Install the library with

```bash npm2yarn
npm install -S @supabase/supabase-js
```

### Create a table and search functions in your database

Run this in your database:

```sql
-- Enable the pgvector extension to work with embedding vectors
create extension vector;

-- Create a table to store your documents
create table documents (
  id bigserial primary key,
  content text, -- corresponds to Document.pageContent
  metadata jsonb, -- corresponds to Document.metadata
  embedding vector(1536) -- 1536 works for OpenAI embeddings, change if needed
);

-- Create a function to similarity search for documents
create function match_documents (
  query_embedding vector(1536),
  match_count int DEFAULT null,
  filter jsonb DEFAULT '{}'
) returns table (
  id bigint,
  content text,
  metadata jsonb,
  similarity float
)
language plpgsql
as $$
#variable_conflict use_column
begin
  return query
  select
    id,
    content,
    metadata,
    1 - (documents.embedding <=> query_embedding) as similarity
  from documents
  where metadata @> filter
  order by documents.embedding <=> query_embedding
  limit match_count;
end;
$$;

-- Create a function to keyword search for documents
create function kw_match_documents(query_text text, match_count int)
returns table (id bigint, content text, metadata jsonb, similarity real)
as $$

begin
return query execute
format('select id, content, metadata, ts_rank(to_tsvector(content), plainto_tsquery($1)) as similarity
from documents
where to_tsvector(content) @@ plainto_tsquery($1)
order by similarity desc
limit $2')
using query_text, match_count;
end;
$$ language plpgsql;
```

## Usage

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/retrievers/supabase_hybrid.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## Related

- Retriever [conceptual guide](/docs/concepts/retrievers)
- Retriever [how-to guides](/docs/how_to/#retrievers)



================================================
FILE: docs/core_docs/docs/integrations/retrievers/tavily.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Tavily Search API
---
"""

"""
# TavilySearchAPIRetriever

[Tavily's Search API](https://tavily.com) is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.

## Overview

This will help you getting started with the Tavily Search API [retriever](/docs/concepts/retrievers). For detailed documentation of all `TavilySearchAPIRetriever` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_retrievers_tavily_search_api.TavilySearchAPIRetriever.html).

### Integration details

| Retriever | Source | Package |
| :--- | :--- | :---: |
[`TavilySearchAPIRetriever`](https://api.js.langchain.com/classes/langchain_community_retrievers_tavily_search_api.TavilySearchAPIRetriever.html) | Information on the web. | [`@langchain/community`](https://npmjs.com/@langchain/community/) |

## Setup

You will need to populate a `TAVILY_API_KEY` environment variable with your Tavily API key or pass it into the constructor as `apiKey`. Obtain a key by signing up [on their website](https://tavily.com/).

If you want to get automated tracing from individual queries, you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
// process.env.LANGSMITH_API_KEY = "<YOUR API KEY HERE>";
// process.env.LANGSMITH_TRACING = "true";
```

### Installation

This retriever lives in the `@langchain/community` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core
</Npm2Yarn>
```
"""

"""
## Instantiation

Now we can instantiate our retriever:
"""

import { TavilySearchAPIRetriever } from "@langchain/community/retrievers/tavily_search_api";

const retriever = new TavilySearchAPIRetriever({
  k: 3,
});

"""
For a full list of allowed arguments, see [the official documentation](https://docs.tavily.com/docs/tavily-api/rest_api#parameters). You can pass any param to the SDK via a `kwargs` object.
"""

"""
## Usage
"""

const query = "what is the current weather in SF?";

await retriever.invoke(query);
# Output:
#   [

#     Document {

#       pageContent: "{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.78, 'lon': -122.42, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1722900266, 'localtime': '2024-08-05 16:24'}, 'current': {'last_updated_epoch': 1722899700, 'last_updated': '2024-08-05 16:15', 'temp_c': 16.8, 'temp_f': 62.2, 'is_day': 1, 'condition': {'text': 'Partly Cloudy', 'icon': '//cdn.weatherapi.com/weather/64x64/day/116.png', 'code': 1003}, 'wind_mph': 13.2, 'wind_kph': 21.2, 'wind_degree': 261, 'wind_dir': 'W', 'pressure_mb': 1014.0, 'pressure_in': 29.94, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 74, 'cloud': 60, 'feelslike_c': 16.8, 'feelslike_f': 62.2, 'windchill_c': 16.8, 'windchill_f': 62.2, 'heatindex_c': 16.8, 'heatindex_f': 62.2, 'dewpoint_c': 12.3, 'dewpoint_f': 54.1, 'vis_km': 10.0, 'vis_miles': 6.0, 'uv': 5.0, 'gust_mph': 17.3, 'gust_kph': 27.8}}",

#       metadata: {

#         title: 'Weather in San Francisco',

#         source: 'https://www.weatherapi.com/',

#         score: 0.9947009,

#         images: []

#       },

#       id: undefined

#     },

#     Document {

#       pageContent: 'Current Weather for Popular Cities . San Francisco, CA 56 ° F Mostly Cloudy; Manhattan, NY warning 85 ° F Fair; Schiller Park, IL (60176) 71 ° F Mostly Cloudy; Boston, MA warning 84 ° F Partly ...',

#       metadata: {

#         title: 'San Francisco, CA Hourly Weather Forecast | Weather Underground',

#         source: 'https://www.wunderground.com/hourly/us/ca/san-francisco/date/2024-08-02',

#         score: 0.9859904,

#         images: []

#       },

#       id: undefined

#     },

#     Document {

#       pageContent: 'San Francisco CA 37.77°N 122.41°W (Elev. 131 ft) Last Update: 2:42 pm PDT Aug 4, 2024. Forecast Valid: 5pm PDT Aug 4, 2024-6pm PDT Aug 11, 2024 . Forecast Discussion . Additional Resources. Radar & Satellite Image. Hourly Weather Forecast. ... Severe Weather ; Current Outlook Maps ; Drought ; Fire Weather ; Fronts/Precipitation Maps ; Current ...',

#       metadata: {

#         title: 'National Weather Service',

#         source: 'https://forecast.weather.gov/zipcity.php?inputstring=San+Francisco,CA',

#         score: 0.98141783,

#         images: []

#       },

#       id: undefined

#     }

#   ]


"""
## Use within a chain

Like other retrievers, `TavilySearchAPIRetriever` can be incorporated into LLM applications via [chains](/docs/how_to/sequence/).

We will need a LLM or chat model:

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

// @lc-docs-hide-cell

import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({
  model: "gpt-4o-mini",
  temperature: 0,
});

import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers";

import type { Document } from "@langchain/core/documents";

const prompt = ChatPromptTemplate.fromTemplate(`
Answer the question based only on the context provided.

Context: {context}

Question: {question}`);

const formatDocs = (docs: Document[]) => {
  return docs.map((doc) => doc.pageContent).join("\n\n");
}

// See https://js.langchain.com/docs/tutorials/rag
const ragChain = RunnableSequence.from([
  {
    context: retriever.pipe(formatDocs),
    question: new RunnablePassthrough(),
  },
  prompt,
  llm,
  new StringOutputParser(),
]);

await ragChain.invoke(query);
# Output:
#   The current weather in San Francisco is partly cloudy with a temperature of 16.8°C (62.2°F). The wind is coming from the west at 13.2 mph (21.2 kph), and the humidity is at 74%. There is no precipitation, and visibility is 10 km (6 miles).


"""
## API reference

For detailed documentation of all `TavilySearchAPIRetriever` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_retrievers_tavily_search_api.TavilySearchAPIRetriever.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/retrievers/time-weighted-retriever.mdx
================================================
# Time-Weighted Retriever

A Time-Weighted Retriever is a retriever that takes into account recency in addition to similarity. The scoring algorithm is:

```typescript
let score = (1.0 - this.decayRate) ** hoursPassed + vectorRelevance;
```

Notably, `hoursPassed` above refers to the time since the object in the retriever was last accessed, not since it was created. This means that frequently accessed objects remain "fresh" and score higher.

`this.decayRate` is a configurable decimal number between 0 and 1. A lower number means that documents will be "remembered" for longer, while a higher number strongly weights more recently accessed documents.

Note that setting a decay rate of exactly 0 or 1 makes `hoursPassed` irrelevant and makes this retriever equivalent to a standard vector lookup.

## Usage

This example shows how to intialize a `TimeWeightedVectorStoreRetriever` with a vector store.
It is important to note that due to required metadata, all documents must be added to the backing vector store using the `addDocuments` method on the **retriever**, not the vector store itself.

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/retrievers/time-weighted-retriever.ts";

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core
```

<CodeBlock language="typescript">{Example}</CodeBlock>

## Related

- Retriever [conceptual guide](/docs/concepts/retrievers)
- Retriever [how-to guides](/docs/how_to/#retrievers)



================================================
FILE: docs/core_docs/docs/integrations/retrievers/vespa-retriever.mdx
================================================
# Vespa Retriever

This shows how to use Vespa.ai as a LangChain retriever.
Vespa.ai is a platform for highly efficient structured text and vector search.
Please refer to [Vespa.ai](https://vespa.ai) for more information.

The following sets up a retriever that fetches results from Vespa's documentation search:

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/retrievers/vespa.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

Here, up to 5 results are retrieved from the `content` field in the `paragraph` document type,
using `documentation` as the ranking method. The `userQuery()` is replaced with the actual query
passed from LangChain.

Please refer to the [pyvespa documentation](https://pyvespa.readthedocs.io/en/latest/getting-started-pyvespa.html#Query)
for more information.

The URL is the endpoint of the Vespa application.
You can connect to any Vespa endpoint, either a remote service or a local instance using Docker.
However, most Vespa Cloud instances are protected with mTLS.
If this is your case, you can, for instance set up a [CloudFlare Worker](https://cloud.vespa.ai/en/security/cloudflare-workers)
that contains the necessary credentials to connect to the instance.

Now you can return the results and continue using them in LangChain.

## Related

- Retriever [conceptual guide](/docs/concepts/retrievers)
- Retriever [how-to guides](/docs/how_to/#retrievers)



================================================
FILE: docs/core_docs/docs/integrations/retrievers/zep-cloud-retriever.mdx
================================================
---
hide_table_of_contents: true
---

# Zep Cloud Retriever

> [Zep](https://www.getzep.com) is a long-term memory service for AI Assistant apps.
> With Zep, you can provide AI assistants with the ability to recall past conversations, no matter how distant,
> while also reducing hallucinations, latency, and cost.

This example shows how to use the Zep Retriever in a retrieval chain to retrieve documents from Zep Open Source memory store.

## Installation

Sign up for [Zep Cloud](https://app.getzep.com/) and create a project.

Follow the [Zep Cloud Typescript SDK Installation Guide](https://help.getzep.com/sdks) to install and get started with Zep.

You'll need your Zep Cloud Project API Key to use the ZepCloudRetriever. See the [Zep Cloud docs](https://help.getzep.com/projects) for more information.

## Setup

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm i @getzep/zep-cloud @langchain/community @langchain/core
```

## Usage

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/retrievers/zep_cloud.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## Related

- Retriever [conceptual guide](/docs/concepts/retrievers)
- Retriever [how-to guides](/docs/how_to/#retrievers)



================================================
FILE: docs/core_docs/docs/integrations/retrievers/zep-retriever.mdx
================================================
---
hide_table_of_contents: true
---

# Zep Open Source Retriever

> [Zep](https://www.getzep.com) is a long-term memory service for AI Assistant apps.
> With Zep, you can provide AI assistants with the ability to recall past conversations, no matter how distant,
> while also reducing hallucinations, latency, and cost.

> Interested in Zep Cloud? See [Zep Cloud Installation Guide](https://help.getzep.com/sdks)

This example shows how to use the Zep Retriever in a retrieval chain to retrieve documents from Zep Open Source memory store.

## Installation

Follow the [Zep Open Source Quickstart Guide](https://docs.getzep.com/deployment/quickstart/) to install and get started with Zep.

## Setup

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm i @getzep/zep-js @langchain/community @langchain/core
```

## Usage

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/retrievers/zep.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## Related

- Retriever [conceptual guide](/docs/concepts/retrievers)
- Retriever [how-to guides](/docs/how_to/#retrievers)



================================================
FILE: docs/core_docs/docs/integrations/retrievers/self_query/chroma.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Chroma
---
"""

"""
# Chroma

This guide will help you getting started with such a retriever backed by a [Chroma vector store](/docs/integrations/vectorstores/chroma). For detailed documentation of all features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.retrievers_self_query.SelfQueryRetriever.html).

## Overview

A [self-query retriever](/docs/how_to/self_query/) retrieves documents by dynamically generating metadata filters based on some input query. This allows the retriever to account for underlying document metadata in addition to pure semantic similarity when fetching results.

It uses a module called a `Translator` that generates a filter based on information about metadata fields and the query language that a given vector store supports.

### Integration details

| Backing vector store | Self-host | Cloud offering | Package | [Py support](https://python.langchain.com/docs/integrations/retrievers/self_query/chroma_self_query/) |
| :--- | :--- | :---: | :---: | :---: |
[`Chroma`](https://api.js.langchain.com/classes/langchain_community_vectorstores_chroma.Chroma.html) | ✅ | ✅ | [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) | ✅ |

## Setup

Set up a Chroma instance as documented [here](/docs/integrations/vectorstores/chroma).

If you want to get automated tracing from individual queries, you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
// process.env.LANGSMITH_API_KEY = "<YOUR API KEY HERE>";
// process.env.LANGSMITH_TRACING = "true";
```

### Installation

The vector store lives in the `@langchain/community` package. You'll also need to install the `langchain` package to import the main `SelfQueryRetriever` class.

For this example, we'll also use OpenAI embeddings, so you'll need to install the `@langchain/openai` package and [obtain an API key](https://platform.openai.com):

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community langchain @langchain/openai @langchain/core
</Npm2Yarn>
```
"""

"""
## Instantiation

First, initialize your Chroma vector store with some documents that contain metadata:
"""

import { OpenAIEmbeddings } from "@langchain/openai";
import { Chroma } from "@langchain/community/vectorstores/chroma";
import { Document } from "@langchain/core/documents";
import type { AttributeInfo } from "langchain/chains/query_constructor";

/**
 * First, we create a bunch of documents. You can load your own documents here instead.
 * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.
 */
const docs = [
  new Document({
    pageContent:
      "A bunch of scientists bring back dinosaurs and mayhem breaks loose",
    metadata: { year: 1993, rating: 7.7, genre: "science fiction" },
  }),
  new Document({
    pageContent:
      "Leo DiCaprio gets lost in a dream within a dream within a dream within a ...",
    metadata: { year: 2010, director: "Christopher Nolan", rating: 8.2 },
  }),
  new Document({
    pageContent:
      "A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea",
    metadata: { year: 2006, director: "Satoshi Kon", rating: 8.6 },
  }),
  new Document({
    pageContent:
      "A bunch of normal-sized women are supremely wholesome and some men pine after them",
    metadata: { year: 2019, director: "Greta Gerwig", rating: 8.3 },
  }),
  new Document({
    pageContent: "Toys come alive and have a blast doing so",
    metadata: { year: 1995, genre: "animated" },
  }),
  new Document({
    pageContent: "Three men walk into the Zone, three men walk out of the Zone",
    metadata: {
      year: 1979,
      director: "Andrei Tarkovsky",
      genre: "science fiction",
      rating: 9.9,
    },
  }),
];

/**
 * Next, we define the attributes we want to be able to query on.
 * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.
 * We also provide a description of each attribute and the type of the attribute.
 * This is used to generate the query prompts.
 */
const attributeInfo: AttributeInfo[] = [
  {
    name: "genre",
    description: "The genre of the movie",
    type: "string or array of strings",
  },
  {
    name: "year",
    description: "The year the movie was released",
    type: "number",
  },
  {
    name: "director",
    description: "The director of the movie",
    type: "string",
  },
  {
    name: "rating",
    description: "The rating of the movie (1-10)",
    type: "number",
  },
  {
    name: "length",
    description: "The length of the movie in minutes",
    type: "number",
  },
];

/**
 * Next, we instantiate a vector store. This is where we store the embeddings of the documents.
 * We also need to provide an embeddings object. This is used to embed the documents.
 */
const embeddings = new OpenAIEmbeddings();
const vectorStore = await Chroma.fromDocuments(docs, embeddings, {
  collectionName: "movie-collection",
});

"""
Now we can instantiate our retriever:

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

// @lc-docs-hide-cell

import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({
  model: "gpt-4o-mini",
  temperature: 0,
});

import { SelfQueryRetriever } from "langchain/retrievers/self_query";
import { ChromaTranslator } from "@langchain/community/structured_query/chroma";

const selfQueryRetriever = SelfQueryRetriever.fromLLM({
  llm,
  vectorStore,
  /** A short summary of what the document contents represent. */
  documentContents: "Brief summary of a movie",
  attributeInfo,
  /**
   * We need to create a basic translator that translates the queries into a
   * filter format that the vector store can understand. We provide a basic translator
   * translator here, but you can create your own translator by extending BaseTranslator
   * abstract class. Note that the vector store needs to support filtering on the metadata
   * attributes you want to query on.
   */
  structuredQueryTranslator: new ChromaTranslator(),
});

"""
## Usage

Now, ask a question that requires some knowledge of the document's metadata to answer. You can see that the retriever will generate the correct result:
"""

await selfQueryRetriever.invoke(
  "Which movies are rated higher than 8.5?"
);
# Output:
#   [

#     Document {

#       pageContent: 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea',

#       metadata: { director: 'Satoshi Kon', rating: 8.6, year: 2006 },

#       id: undefined

#     },

#     Document {

#       pageContent: 'Three men walk into the Zone, three men walk out of the Zone',

#       metadata: {

#         director: 'Andrei Tarkovsky',

#         genre: 'science fiction',

#         rating: 9.9,

#         year: 1979

#       },

#       id: undefined

#     }

#   ]


"""
## Use within a chain

Like other retrievers, Chroma self-query retrievers can be incorporated into LLM applications via [chains](/docs/how_to/sequence/).

Note that because their returned answers can heavily depend on document metadata, we format the retrieved documents differently to include that information.
"""

import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers";

import type { Document } from "@langchain/core/documents";

const prompt = ChatPromptTemplate.fromTemplate(`
Answer the question based only on the context provided.

Context: {context}

Question: {question}`);

const formatDocs = (docs: Document[]) => {
  return docs.map((doc) => JSON.stringify(doc)).join("\n\n");
}

// See https://js.langchain.com/docs/tutorials/rag
const ragChain = RunnableSequence.from([
  {
    context: selfQueryRetriever.pipe(formatDocs),
    question: new RunnablePassthrough(),
  },
  prompt,
  llm,
  new StringOutputParser(),
]);

await ragChain.invoke("Which movies are rated higher than 8.5?")
# Output:
#   The movies rated higher than 8.5 are:

#   

#   1. The movie directed by Andrei Tarkovsky, which has a rating of 9.9. 

#   2. The movie directed by Satoshi Kon, which has a rating of 8.6.


"""
## Default search params

You can also pass a `searchParams` field into the above method that provides default filters applied in addition to any generated query. The filter syntax is the same as the backing Chroma vector store:
"""

const selfQueryRetrieverWithDefaultParams = SelfQueryRetriever.fromLLM({
  llm,
  vectorStore,
  documentContents: "Brief summary of a movie",
  attributeInfo,
  structuredQueryTranslator: new ChromaTranslator(),
  searchParams: {
    filter: {
      rating: {
        $gt: 8.5,
      }
    },
    mergeFiltersOperator: "and",
  }
});

"""
## API reference

For detailed documentation of all Chroma self-query retriever features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.retrievers_self_query.SelfQueryRetriever.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/retrievers/self_query/hnswlib.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: HNSWLib
---
"""

"""
# HNSWLib

This guide will help you getting started with such a retriever backed by a [HNSWLib vector store](/docs/integrations/vectorstores/hnswlib). For detailed documentation of all features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.retrievers_self_query.SelfQueryRetriever.html).

## Overview

A [self-query retriever](/docs/how_to/self_query/) retrieves documents by dynamically generating metadata filters based on some input query. This allows the retriever to account for underlying document metadata in addition to pure semantic similarity when fetching results.

It uses a module called a `Translator` that generates a filter based on information about metadata fields and the query language that a given vector store supports.

### Integration details

| Backing vector store | Self-host | Cloud offering | Package | Py support |
| :--- | :--- | :---: | :---: | :---: |
[`HNSWLib`](https://api.js.langchain.com/classes/langchain_community_vectorstores_hnswlib.HNSWLib.html) | ✅ | ❌ | [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) | ❌ |

## Setup

Set up a HNSWLib instance as documented [here](/docs/integrations/vectorstores/hnswlib).

If you want to get automated tracing from individual queries, you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
// process.env.LANGSMITH_API_KEY = "<YOUR API KEY HERE>";
// process.env.LANGSMITH_TRACING = "true";
```

### Installation

The vector store lives in the `@langchain/community` package. You'll also need to install the `langchain` package to import the main `SelfQueryRetriever` class.

For this example, we'll also use OpenAI embeddings, so you'll need to install the `@langchain/openai` package and [obtain an API key](https://platform.openai.com):

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community langchain @langchain/openai @langchain/core
</Npm2Yarn>
```
"""

"""
## Instantiation

First, initialize your HNSWLib vector store with some documents that contain metadata:
"""

import { OpenAIEmbeddings } from "@langchain/openai";
import { HNSWLib } from "@langchain/community/vectorstores/hnswlib";
import { Document } from "@langchain/core/documents";
import type { AttributeInfo } from "langchain/chains/query_constructor";

/**
 * First, we create a bunch of documents. You can load your own documents here instead.
 * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.
 */
const docs = [
  new Document({
    pageContent:
      "A bunch of scientists bring back dinosaurs and mayhem breaks loose",
    metadata: { year: 1993, rating: 7.7, genre: "science fiction" },
  }),
  new Document({
    pageContent:
      "Leo DiCaprio gets lost in a dream within a dream within a dream within a ...",
    metadata: { year: 2010, director: "Christopher Nolan", rating: 8.2 },
  }),
  new Document({
    pageContent:
      "A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea",
    metadata: { year: 2006, director: "Satoshi Kon", rating: 8.6 },
  }),
  new Document({
    pageContent:
      "A bunch of normal-sized women are supremely wholesome and some men pine after them",
    metadata: { year: 2019, director: "Greta Gerwig", rating: 8.3 },
  }),
  new Document({
    pageContent: "Toys come alive and have a blast doing so",
    metadata: { year: 1995, genre: "animated" },
  }),
  new Document({
    pageContent: "Three men walk into the Zone, three men walk out of the Zone",
    metadata: {
      year: 1979,
      director: "Andrei Tarkovsky",
      genre: "science fiction",
      rating: 9.9,
    },
  }),
];

/**
 * Next, we define the attributes we want to be able to query on.
 * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.
 * We also provide a description of each attribute and the type of the attribute.
 * This is used to generate the query prompts.
 */
const attributeInfo: AttributeInfo[] = [
  {
    name: "genre",
    description: "The genre of the movie",
    type: "string or array of strings",
  },
  {
    name: "year",
    description: "The year the movie was released",
    type: "number",
  },
  {
    name: "director",
    description: "The director of the movie",
    type: "string",
  },
  {
    name: "rating",
    description: "The rating of the movie (1-10)",
    type: "number",
  },
  {
    name: "length",
    description: "The length of the movie in minutes",
    type: "number",
  },
];

/**
 * Next, we instantiate a vector store. This is where we store the embeddings of the documents.
 * We also need to provide an embeddings object. This is used to embed the documents.
 */
const embeddings = new OpenAIEmbeddings();
const vectorStore = await HNSWLib.fromDocuments(docs, embeddings);

"""
Now we can instantiate our retriever:

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

// @lc-docs-hide-cell

import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0,
});

import { SelfQueryRetriever } from "langchain/retrievers/self_query";
import { FunctionalTranslator } from "@langchain/core/structured_query";

const selfQueryRetriever = SelfQueryRetriever.fromLLM({
  llm: llm,
  vectorStore: vectorStore,
  /** A short summary of what the document contents represent. */
  documentContents: "Brief summary of a movie",
  attributeInfo: attributeInfo,
  /**
   * We need to create a basic translator that translates the queries into a
   * filter format that the vector store can understand. We provide a basic translator
   * translator here, but you can create your own translator by extending BaseTranslator
   * abstract class. Note that the vector store needs to support filtering on the metadata
   * attributes you want to query on.
   */
  structuredQueryTranslator: new FunctionalTranslator(),
});

"""
## Usage

Now, ask a question that requires some knowledge of the document's metadata to answer. You can see that the retriever will generate the correct result:
"""

await selfQueryRetriever.invoke(
  "Which movies are rated higher than 8.5?"
);
# Output:
#   [

#     Document {

#       pageContent: 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea',

#       metadata: { year: 2006, director: 'Satoshi Kon', rating: 8.6 },

#       id: undefined

#     },

#     Document {

#       pageContent: 'Three men walk into the Zone, three men walk out of the Zone',

#       metadata: {

#         year: 1979,

#         director: 'Andrei Tarkovsky',

#         genre: 'science fiction',

#         rating: 9.9

#       },

#       id: undefined

#     }

#   ]


"""
## Use within a chain

Like other retrievers, HNSWLib self-query retrievers can be incorporated into LLM applications via [chains](/docs/how_to/sequence/).

Note that because their returned answers can heavily depend on document metadata, we format the retrieved documents differently to include that information.
"""

import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers";

import type { Document } from "@langchain/core/documents";

const prompt = ChatPromptTemplate.fromTemplate(`
Answer the question based only on the context provided.

Context: {context}

Question: {question}`);

const formatDocs = (docs: Document[]) => {
  return docs.map((doc) => JSON.stringify(doc)).join("\n\n");
}

// See https://js.langchain.com/docs/tutorials/rag
const ragChain = RunnableSequence.from([
  {
    context: selfQueryRetriever.pipe(formatDocs),
    question: new RunnablePassthrough(),
  },
  prompt,
  llm,
  new StringOutputParser(),
]);

await ragChain.invoke("Which movies are rated higher than 8.5?")
# Output:
#   The movies rated higher than 8.5 are:

#   

#   1. The movie directed by Satoshi Kon in 2006, which has a rating of 8.6.

#   2. The movie directed by Andrei Tarkovsky in 1979, which has a rating of 9.9.


"""
## Default search params

You can also pass a `searchParams` field into the above method that provides default filters applied in addition to any generated query. The filter syntax is a predicate function:
"""

const selfQueryRetrieverWithDefaults = SelfQueryRetriever.fromLLM({
  llm,
  vectorStore,
  documentContents: "Brief summary of a movie",
  attributeInfo,
  structuredQueryTranslator: new FunctionalTranslator(),
  searchParams: {
    filter: (doc: Document) => doc.metadata && doc.metadata.rating > 8.5,
    mergeFiltersOperator: "and",
  },
});

"""
## API reference

For detailed documentation of all HNSWLib self-query retriever features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.retrievers_self_query.SelfQueryRetriever.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/retrievers/self_query/index.mdx
================================================
---
sidebar-position: 0
---

# Self-querying retrievers

Learn about how self-querying retrievers work [here](/docs/how_to/self_query).

import DocCardList from "@theme/DocCardList";

<DocCardList />



================================================
FILE: docs/core_docs/docs/integrations/retrievers/self_query/memory.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: In-memory
---
"""

"""
# In-memory

This guide will help you getting started with such a retriever backed by an [in-memory vector store](/docs/integrations/vectorstores/memory). For detailed documentation of all features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.retrievers_self_query.SelfQueryRetriever.html).

## Overview

A [self-query retriever](/docs/how_to/self_query/) retrieves documents by dynamically generating metadata filters based on some input query. This allows the retriever to account for underlying document metadata in addition to pure semantic similarity when fetching results.

It uses a module called a `Translator` that generates a filter based on information about metadata fields and the query language that a given vector store supports.

### Integration details

| Backing vector store | Self-host | Cloud offering | Package | Py support |
| :--- | :--- | :---: | :---: | :---: |
[`MemoryVectorStore`](https://api.js.langchain.com/classes/langchain.vectorstores_memory.MemoryVectorStore.html) | ✅ | ❌ | [`langchain`](https://www.npmjs.com/package/langchain) | ❌ |

## Setup

Set up an in-memory vector store instance as documented [here](/docs/integrations/vectorstores/memory).

If you want to get automated tracing from individual queries, you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
// process.env.LANGSMITH_API_KEY = "<YOUR API KEY HERE>";
// process.env.LANGSMITH_TRACING = "true";
```

### Installation

The vector store lives in the `@langchain/community` package. You'll also need to install the `langchain` package to import the main `SelfQueryRetriever` class.

For this example, we'll also use OpenAI embeddings, so you'll need to install the `@langchain/openai` package and [obtain an API key](https://platform.openai.com):

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community langchain @langchain/openai @langchain/core
</Npm2Yarn>
```
"""

"""
## Instantiation

First, initialize your in-memory vector store with some documents that contain metadata:
"""

import { OpenAIEmbeddings } from "@langchain/openai";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { Document } from "@langchain/core/documents";
import type { AttributeInfo } from "langchain/chains/query_constructor";

/**
 * First, we create a bunch of documents. You can load your own documents here instead.
 * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.
 */
const docs = [
  new Document({
    pageContent:
      "A bunch of scientists bring back dinosaurs and mayhem breaks loose",
    metadata: { year: 1993, rating: 7.7, genre: "science fiction" },
  }),
  new Document({
    pageContent:
      "Leo DiCaprio gets lost in a dream within a dream within a dream within a ...",
    metadata: { year: 2010, director: "Christopher Nolan", rating: 8.2 },
  }),
  new Document({
    pageContent:
      "A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea",
    metadata: { year: 2006, director: "Satoshi Kon", rating: 8.6 },
  }),
  new Document({
    pageContent:
      "A bunch of normal-sized women are supremely wholesome and some men pine after them",
    metadata: { year: 2019, director: "Greta Gerwig", rating: 8.3 },
  }),
  new Document({
    pageContent: "Toys come alive and have a blast doing so",
    metadata: { year: 1995, genre: "animated" },
  }),
  new Document({
    pageContent: "Three men walk into the Zone, three men walk out of the Zone",
    metadata: {
      year: 1979,
      director: "Andrei Tarkovsky",
      genre: "science fiction",
      rating: 9.9,
    },
  }),
];

/**
 * Next, we define the attributes we want to be able to query on.
 * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.
 * We also provide a description of each attribute and the type of the attribute.
 * This is used to generate the query prompts.
 */
const attributeInfo: AttributeInfo[] = [
  {
    name: "genre",
    description: "The genre of the movie",
    type: "string or array of strings",
  },
  {
    name: "year",
    description: "The year the movie was released",
    type: "number",
  },
  {
    name: "director",
    description: "The director of the movie",
    type: "string",
  },
  {
    name: "rating",
    description: "The rating of the movie (1-10)",
    type: "number",
  },
  {
    name: "length",
    description: "The length of the movie in minutes",
    type: "number",
  },
];

/**
 * Next, we instantiate a vector store. This is where we store the embeddings of the documents.
 * We also need to provide an embeddings object. This is used to embed the documents.
 */
const embeddings = new OpenAIEmbeddings();
const vectorStore = await MemoryVectorStore.fromDocuments(docs, embeddings);

"""
Now we can instantiate our retriever:

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

// @lc-docs-hide-cell

import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0,
});

import { SelfQueryRetriever } from "langchain/retrievers/self_query";
import { FunctionalTranslator } from "@langchain/core/structured_query";

const selfQueryRetriever = SelfQueryRetriever.fromLLM({
  llm: llm,
  vectorStore: vectorStore,
  /** A short summary of what the document contents represent. */
  documentContents: "Brief summary of a movie",
  attributeInfo: attributeInfo,
  /**
   * We need to create a basic translator that translates the queries into a
   * filter format that the vector store can understand. We provide a basic translator
   * translator here, but you can create your own translator by extending BaseTranslator
   * abstract class. Note that the vector store needs to support filtering on the metadata
   * attributes you want to query on.
   */
  structuredQueryTranslator: new FunctionalTranslator(),
});

"""
## Usage

Now, ask a question that requires some knowledge of the document's metadata to answer. You can see that the retriever will generate the correct result:
"""

await selfQueryRetriever.invoke(
  "Which movies are rated higher than 8.5?"
);
# Output:
#   [

#     Document {

#       pageContent: 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea',

#       metadata: { year: 2006, director: 'Satoshi Kon', rating: 8.6 },

#       id: undefined

#     },

#     Document {

#       pageContent: 'Three men walk into the Zone, three men walk out of the Zone',

#       metadata: {

#         year: 1979,

#         director: 'Andrei Tarkovsky',

#         genre: 'science fiction',

#         rating: 9.9

#       },

#       id: undefined

#     }

#   ]


"""
## Use within a chain

Like other retrievers, in-memory self-query retrievers can be incorporated into LLM applications via [chains](/docs/how_to/sequence/).

Note that because their returned answers can heavily depend on document metadata, we format the retrieved documents differently to include that information.
"""

import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers";

import type { Document } from "@langchain/core/documents";

const prompt = ChatPromptTemplate.fromTemplate(`
Answer the question based only on the context provided.

Context: {context}

Question: {question}`);

const formatDocs = (docs: Document[]) => {
  return docs.map((doc) => JSON.stringify(doc)).join("\n\n");
}

// See https://js.langchain.com/docs/tutorials/rag
const ragChain = RunnableSequence.from([
  {
    context: selfQueryRetriever.pipe(formatDocs),
    question: new RunnablePassthrough(),
  },
  prompt,
  llm,
  new StringOutputParser(),
]);

await ragChain.invoke("Which movies are rated higher than 8.5?")
# Output:
#   The movies rated higher than 8.5 are:

#   

#   1. The movie directed by Satoshi Kon in 2006, which has a rating of 8.6.

#   2. The movie directed by Andrei Tarkovsky in 1979, which has a rating of 9.9.


"""
## Default search params

You can also pass a `searchParams` field into the above method that provides default filters applied in addition to any generated query. The filter syntax is a predicate function:
"""

const selfQueryRetrieverWithDefaultParams = SelfQueryRetriever.fromLLM({
  llm: llm,
  vectorStore: vectorStore,
  documentContents: "Brief summary of a movie",
  attributeInfo: attributeInfo,
  structuredQueryTranslator: new FunctionalTranslator(),
  searchParams: {
    filter: (doc: Document) => doc.metadata && doc.metadata.rating > 8.5,
    mergeFiltersOperator: "and",
  },
});

"""
## API reference

For detailed documentation of all in-memory self-query retriever features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.retrievers_self_query.SelfQueryRetriever.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/retrievers/self_query/pinecone.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Pinecone
---
"""

"""
# Pinecone

This guide will help you getting started with such a retriever backed by a [Pinecone vector store](/docs/integrations/vectorstores/pinecone). For detailed documentation of all features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.retrievers_self_query.SelfQueryRetriever.html).

## Overview

A [self-query retriever](/docs/how_to/self_query/) retrieves documents by dynamically generating metadata filters based on some input query. This allows the retriever to account for underlying document metadata in addition to pure semantic similarity when fetching results.

It uses a module called a `Translator` that generates a filter based on information about metadata fields and the query language that a given vector store supports.

### Integration details

| Backing vector store | Self-host | Cloud offering | Package | [Py support](https://python.langchain.com/docs/integrations/retrievers/self_query/pinecone/) |
| :--- | :--- | :---: | :---: | :---: |
[`PineconeStore`](https://api.js.langchain.com/classes/langchain_pinecone.PineconeStore.html) | ❌ | ✅ | [`@langchain/pinecone`](https://www.npmjs.com/package/@langchain/pinecone) | ✅ |

## Setup

Set up a Pinecone instance as documented [here](/docs/integrations/vectorstores/pinecone). Set the following environment variables:

```ts
process.env.PINECONE_API_KEY = "YOUR_API_KEY";
process.env.PINECONE_ENVIRONMENT = "YOUR_ENVIRONMENT";
process.env.PINECONE_INDEX = "YOUR_INDEX";
```

If you want to get automated tracing from individual queries, you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
// process.env.LANGSMITH_API_KEY = "<YOUR API KEY HERE>";
// process.env.LANGSMITH_TRACING = "true";
```

### Installation

The vector store lives in the `@langchain/pinecone` package. You'll also need to install the `langchain` package to import the main `SelfQueryRetriever` class.

You will also need to install the official Pinecone SDK (`@pinecone-database/pinecone@5`).

For this example, we'll also use OpenAI embeddings, so you'll need to install the `@langchain/openai` package and [obtain an API key](https://platform.openai.com):

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/pinecone langchain @langchain/openai @langchain/core @pinecone-database/pinecone
</Npm2Yarn>
```
"""

"""
## Instantiation

First, initialize your Pinecone vector store with some documents that contain metadata:
"""

import { Pinecone } from "@pinecone-database/pinecone";

import { OpenAIEmbeddings } from "@langchain/openai";
import { PineconeStore } from "@langchain/pinecone";
import { Document } from "@langchain/core/documents";
import type { AttributeInfo } from "langchain/chains/query_constructor";

/**
 * First, we create a bunch of documents. You can load your own documents here instead.
 * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.
 */
const docs = [
  new Document({
    pageContent:
      "A bunch of scientists bring back dinosaurs and mayhem breaks loose",
    metadata: { year: 1993, rating: 7.7, genre: "science fiction" },
  }),
  new Document({
    pageContent:
      "Leo DiCaprio gets lost in a dream within a dream within a dream within a ...",
    metadata: { year: 2010, director: "Christopher Nolan", rating: 8.2 },
  }),
  new Document({
    pageContent:
      "A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea",
    metadata: { year: 2006, director: "Satoshi Kon", rating: 8.6 },
  }),
  new Document({
    pageContent:
      "A bunch of normal-sized women are supremely wholesome and some men pine after them",
    metadata: { year: 2019, director: "Greta Gerwig", rating: 8.3 },
  }),
  new Document({
    pageContent: "Toys come alive and have a blast doing so",
    metadata: { year: 1995, genre: "animated" },
  }),
  new Document({
    pageContent: "Three men walk into the Zone, three men walk out of the Zone",
    metadata: {
      year: 1979,
      director: "Andrei Tarkovsky",
      genre: "science fiction",
      rating: 9.9,
    },
  }),
];

/**
 * Next, we define the attributes we want to be able to query on.
 * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.
 * We also provide a description of each attribute and the type of the attribute.
 * This is used to generate the query prompts.
 */
const attributeInfo: AttributeInfo[] = [
  {
    name: "genre",
    description: "The genre of the movie",
    type: "string or array of strings",
  },
  {
    name: "year",
    description: "The year the movie was released",
    type: "number",
  },
  {
    name: "director",
    description: "The director of the movie",
    type: "string",
  },
  {
    name: "rating",
    description: "The rating of the movie (1-10)",
    type: "number",
  },
  {
    name: "length",
    description: "The length of the movie in minutes",
    type: "number",
  },
];

/**
 * Next, we instantiate a vector store. This is where we store the embeddings of the documents.
 * We also need to provide an embeddings object. This is used to embed the documents.
 */

const pinecone = new Pinecone();

const pineconeIndex = pinecone.Index(process.env.PINECONE_INDEX!);

const embeddings = new OpenAIEmbeddings();
const vectorStore = await PineconeStore.fromDocuments(docs, embeddings, {
  pineconeIndex: pineconeIndex,
});

"""
Now we can instantiate our retriever:

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

// @lc-docs-hide-cell

import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0,
});

import { SelfQueryRetriever } from "langchain/retrievers/self_query";
import { PineconeTranslator } from "@langchain/pinecone";

const selfQueryRetriever = SelfQueryRetriever.fromLLM({
  llm: llm,
  vectorStore: vectorStore,
  /** A short summary of what the document contents represent. */
  documentContents: "Brief summary of a movie",
  attributeInfo: attributeInfo,
  /**
   * We need to create a basic translator that translates the queries into a
   * filter format that the vector store can understand. We provide a basic translator
   * translator here, but you can create your own translator by extending BaseTranslator
   * abstract class. Note that the vector store needs to support filtering on the metadata
   * attributes you want to query on.
   */
  structuredQueryTranslator: new PineconeTranslator(),
});

"""
## Usage

Now, ask a question that requires some knowledge of the document's metadata to answer. You can see that the retriever will generate the correct result:
"""

await selfQueryRetriever.invoke(
  "Which movies are rated higher than 8.5?"
);
# Output:
#   [

#     Document {

#       pageContent: 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea',

#       metadata: { director: 'Satoshi Kon', rating: 8.6, year: 2006 },

#       id: undefined

#     },

#     Document {

#       pageContent: 'Three men walk into the Zone, three men walk out of the Zone',

#       metadata: {

#         director: 'Andrei Tarkovsky',

#         genre: 'science fiction',

#         rating: 9.9,

#         year: 1979

#       },

#       id: undefined

#     }

#   ]


"""
## Use within a chain

Like other retrievers, Pinecone self-query retrievers can be incorporated into LLM applications via [chains](/docs/how_to/sequence/).

Note that because their returned answers can heavily depend on document metadata, we format the retrieved documents differently to include that information.
"""

import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers";

import type { Document } from "@langchain/core/documents";

const prompt = ChatPromptTemplate.fromTemplate(`
Answer the question based only on the context provided.

Context: {context}

Question: {question}`);

const formatDocs = (docs: Document[]) => {
  return docs.map((doc) => JSON.stringify(doc)).join("\n\n");
}

// See https://js.langchain.com/docs/tutorials/rag
const ragChain = RunnableSequence.from([
  {
    context: selfQueryRetriever.pipe(formatDocs),
    question: new RunnablePassthrough(),
  },
  prompt,
  llm,
  new StringOutputParser(),
]);

await ragChain.invoke("Which movies are rated higher than 8.5?")
# Output:
#   The movies rated higher than 8.5 are the ones directed by Satoshi Kon (rating: 8.6) and Andrei Tarkovsky (rating: 9.9).


"""
## Default search params

You can also pass a `searchParams` field into the above method that provides default filters applied in addition to any generated query. The filter syntax is the same as the backing Pinecone vector store:
"""

const selfQueryRetrieverWithDefaultParams = SelfQueryRetriever.fromLLM({
  llm: llm,
  vectorStore: vectorStore,
  documentContents: "Brief summary of a movie",
  attributeInfo: attributeInfo,
  structuredQueryTranslator: new PineconeTranslator(),
  searchParams: {
    filter: {
      rating: {
        $gt: 8.5,
      },
    },
    mergeFiltersOperator: "and",
  },
});

"""
## API reference

For detailed documentation of all Pinecone self-query retriever features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.retrievers_self_query.SelfQueryRetriever.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/retrievers/self_query/qdrant.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Qdrant
---
"""

"""
# Qdrant

This guide will help you getting started with such a retriever backed by a [Qdrant vector store](/docs/integrations/vectorstores/qdrant). For detailed documentation of all features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.retrievers_self_query.SelfQueryRetriever.html).

## Overview

A [self-query retriever](/docs/how_to/self_query/) retrieves documents by dynamically generating metadata filters based on some input query. This allows the retriever to account for underlying document metadata in addition to pure semantic similarity when fetching results.

It uses a module called a `Translator` that generates a filter based on information about metadata fields and the query language that a given vector store supports.

### Integration details

| Backing vector store | Self-host | Cloud offering | Package | [Py support](https://python.langchain.com/docs/integrations/retrievers/self_query/qdrant_self_query/) |
| :--- | :--- | :---: | :---: | :---: |
[`QdrantVectorStore`](https://api.js.langchain.com/classes/langchain_qdrant.QdrantVectorStore.html) | ✅ | ✅ | [`@langchain/qdrant`](https://www.npmjs.com/package/@langchain/qdrant) | ✅ |

## Setup

Set up a Qdrant instance as documented [here](/docs/integrations/vectorstores/qdrant). Set the following environment variables:

```ts
process.env.QDRANT_URL = "YOUR_QDRANT_URL_HERE" // for example, http://localhost:6333
```

If you want to get automated tracing from individual queries, you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
// process.env.LANGSMITH_API_KEY = "<YOUR API KEY HERE>";
// process.env.LANGSMITH_TRACING = "true";
```

### Installation

The vector store lives in the `@langchain/qdrant` package. You'll also need to install the `langchain` and `@langchain/community` packages to import the main `SelfQueryRetriever` classes.

For this example, we'll also use OpenAI embeddings, so you'll need to install the `@langchain/openai` package and [obtain an API key](https://platform.openai.com):

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/qdrant langchain @langchain/community @langchain/openai @langchain/core
</Npm2Yarn>
```

The official Qdrant SDK (`@qdrant/js-client-rest`) is automatically installed as a dependency of `@langchain/qdrant`, but you may wish to install it independently as well.
"""

"""
## Instantiation

First, initialize your Qdrant vector store with some documents that contain metadata:
"""

import { OpenAIEmbeddings } from "@langchain/openai";
import { QdrantVectorStore } from "@langchain/qdrant";
import { Document } from "@langchain/core/documents";
import type { AttributeInfo } from "langchain/chains/query_constructor";

import { QdrantClient } from "@qdrant/js-client-rest";

/**
 * First, we create a bunch of documents. You can load your own documents here instead.
 * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.
 */
const docs = [
  new Document({
    pageContent:
      "A bunch of scientists bring back dinosaurs and mayhem breaks loose",
    metadata: { year: 1993, rating: 7.7, genre: "science fiction" },
  }),
  new Document({
    pageContent:
      "Leo DiCaprio gets lost in a dream within a dream within a dream within a ...",
    metadata: { year: 2010, director: "Christopher Nolan", rating: 8.2 },
  }),
  new Document({
    pageContent:
      "A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea",
    metadata: { year: 2006, director: "Satoshi Kon", rating: 8.6 },
  }),
  new Document({
    pageContent:
      "A bunch of normal-sized women are supremely wholesome and some men pine after them",
    metadata: { year: 2019, director: "Greta Gerwig", rating: 8.3 },
  }),
  new Document({
    pageContent: "Toys come alive and have a blast doing so",
    metadata: { year: 1995, genre: "animated" },
  }),
  new Document({
    pageContent: "Three men walk into the Zone, three men walk out of the Zone",
    metadata: {
      year: 1979,
      director: "Andrei Tarkovsky",
      genre: "science fiction",
      rating: 9.9,
    },
  }),
];

/**
 * Next, we define the attributes we want to be able to query on.
 * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.
 * We also provide a description of each attribute and the type of the attribute.
 * This is used to generate the query prompts.
 */
const attributeInfo: AttributeInfo[] = [
  {
    name: "genre",
    description: "The genre of the movie",
    type: "string or array of strings",
  },
  {
    name: "year",
    description: "The year the movie was released",
    type: "number",
  },
  {
    name: "director",
    description: "The director of the movie",
    type: "string",
  },
  {
    name: "rating",
    description: "The rating of the movie (1-10)",
    type: "number",
  },
  {
    name: "length",
    description: "The length of the movie in minutes",
    type: "number",
  },
];

/**
 * Next, we instantiate a vector store. This is where we store the embeddings of the documents.
 * We also need to provide an embeddings object. This is used to embed the documents.
 */

const client = new QdrantClient({ url: process.env.QDRANT_URL });

const embeddings = new OpenAIEmbeddings();
const vectorStore = await QdrantVectorStore.fromDocuments(docs, embeddings, {
  client,
  collectionName: "movie-collection",
});

"""
Now we can instantiate our retriever:

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

// @lc-docs-hide-cell

import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0,
});

import { SelfQueryRetriever } from "langchain/retrievers/self_query";
import { QdrantTranslator } from "@langchain/community/structured_query/qdrant";

const selfQueryRetriever = SelfQueryRetriever.fromLLM({
  llm: llm,
  vectorStore: vectorStore,
  /** A short summary of what the document contents represent. */
  documentContents: "Brief summary of a movie",
  attributeInfo: attributeInfo,
  structuredQueryTranslator: new QdrantTranslator(),
});

"""
## Usage

Now, ask a question that requires some knowledge of the document's metadata to answer. You can see that the retriever will generate the correct result:
"""

await selfQueryRetriever.invoke(
  "Which movies are rated higher than 8.5?"
);
# Output:
#   [

#     Document {

#       pageContent: 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea',

#       metadata: { director: 'Satoshi Kon', rating: 8.6, year: 2006 },

#       id: undefined

#     },

#     Document {

#       pageContent: 'Three men walk into the Zone, three men walk out of the Zone',

#       metadata: {

#         director: 'Andrei Tarkovsky',

#         genre: 'science fiction',

#         rating: 9.9,

#         year: 1979

#       },

#       id: undefined

#     }

#   ]


"""
## Use within a chain

Like other retrievers, Qdrant self-query retrievers can be incorporated into LLM applications via [chains](/docs/how_to/sequence/).

Note that because their returned answers can heavily depend on document metadata, we format the retrieved documents differently to include that information.
"""

import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers";

import type { Document } from "@langchain/core/documents";

const prompt = ChatPromptTemplate.fromTemplate(`
Answer the question based only on the context provided.

Context: {context}

Question: {question}`);

const formatDocs = (docs: Document[]) => {
  return docs.map((doc) => JSON.stringify(doc)).join("\n\n");
}

// See https://js.langchain.com/docs/tutorials/rag
const ragChain = RunnableSequence.from([
  {
    context: selfQueryRetriever.pipe(formatDocs),
    question: new RunnablePassthrough(),
  },
  prompt,
  llm,
  new StringOutputParser(),
]);

await ragChain.invoke("Which movies are rated higher than 8.5?")
# Output:
#   The movies rated higher than 8.5 are the ones directed by Satoshi Kon (rating: 8.6) and Andrei Tarkovsky (rating: 9.9).


"""
## Default search params

You can also pass a `searchParams` field into the above method that provides default filters applied in addition to any generated query. The filter syntax is the same as the backing Qdrant vector store:
"""

const selfQueryRetrieverWithDefaultParams = SelfQueryRetriever.fromLLM({
  llm: llm,
  vectorStore: vectorStore,
  documentContents: "Brief summary of a movie",
  attributeInfo: attributeInfo,
  structuredQueryTranslator: new QdrantTranslator(),
  searchParams: {
    filter: {
      must: [
        {
          key: "metadata.rating",
          range: {
            gt: 8.5,
          },
        },
      ],
    },
    mergeFiltersOperator: "and",
  },
});

"""
## API reference

For detailed documentation of all Qdrant self-query retriever features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.retrievers_self_query.SelfQueryRetriever.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/retrievers/self_query/supabase.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Supabase
---
"""

"""
# Supabase

This guide will help you getting started with such a retriever backed by a [Supabase vector store](/docs/integrations/vectorstores/supabase). For detailed documentation of all features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.retrievers_self_query.SelfQueryRetriever.html).

## Overview

A [self-query retriever](/docs/how_to/self_query/) retrieves documents by dynamically generating metadata filters based on some input query. This allows the retriever to account for underlying document metadata in addition to pure semantic similarity when fetching results.

It uses a module called a `Translator` that generates a filter based on information about metadata fields and the query language that a given vector store supports.

### Integration details

| Backing vector store | Self-host | Cloud offering | Package | [Py support](https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/) |
| :--- | :--- | :---: | :---: | :---: |
[`SupabaseVectorStore`](https://api.js.langchain.com/classes/langchain_community_vectorstores_supabase.SupabaseVectorStore.html) | ✅ | ✅ | [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) | ✅ |

## Setup

Set up a Supabase instance as documented [here](/docs/integrations/vectorstores/supabase). Set the following environment variables:

```ts
process.env.SUPABASE_PRIVATE_KEY = "YOUR_SUPABASE_PRIVATE_KEY";
process.env.SUPABASE_URL = "YOUR_SUPABASE_URL";
```

If you want to get automated tracing from individual queries, you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
// process.env.LANGSMITH_API_KEY = "<YOUR API KEY HERE>";
// process.env.LANGSMITH_TRACING = "true";
```

### Installation

The vector store lives in the `@langchain/community` package, which requires the official Supabase SDK as a peer dependency. You'll also need to install the `langchain` package to import the main `SelfQueryRetriever` class.

For this example, we'll also use OpenAI embeddings, so you'll need to install the `@langchain/openai` package and [obtain an API key](https://platform.openai.com):

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community langchain @langchain/openai @langchain/core @supabase/supabase-js
</Npm2Yarn>
```
"""

"""
## Instantiation

First, initialize your Supabase vector store with some documents that contain metadata:
"""

import { OpenAIEmbeddings } from "@langchain/openai";
import { SupabaseVectorStore } from "@langchain/community/vectorstores/supabase";
import { Document } from "@langchain/core/documents";
import type { AttributeInfo } from "langchain/chains/query_constructor";

import { createClient } from "@supabase/supabase-js";

/**
 * First, we create a bunch of documents. You can load your own documents here instead.
 * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.
 */
const docs = [
  new Document({
    pageContent:
      "A bunch of scientists bring back dinosaurs and mayhem breaks loose",
    metadata: { year: 1993, rating: 7.7, genre: "science fiction" },
  }),
  new Document({
    pageContent:
      "Leo DiCaprio gets lost in a dream within a dream within a dream within a ...",
    metadata: { year: 2010, director: "Christopher Nolan", rating: 8.2 },
  }),
  new Document({
    pageContent:
      "A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea",
    metadata: { year: 2006, director: "Satoshi Kon", rating: 8.6 },
  }),
  new Document({
    pageContent:
      "A bunch of normal-sized women are supremely wholesome and some men pine after them",
    metadata: { year: 2019, director: "Greta Gerwig", rating: 8.3 },
  }),
  new Document({
    pageContent: "Toys come alive and have a blast doing so",
    metadata: { year: 1995, genre: "animated" },
  }),
  new Document({
    pageContent: "Three men walk into the Zone, three men walk out of the Zone",
    metadata: {
      year: 1979,
      director: "Andrei Tarkovsky",
      genre: "science fiction",
      rating: 9.9,
    },
  }),
];

/**
 * Next, we define the attributes we want to be able to query on.
 * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.
 * We also provide a description of each attribute and the type of the attribute.
 * This is used to generate the query prompts.
 */
const attributeInfo: AttributeInfo[] = [
  {
    name: "genre",
    description: "The genre of the movie",
    type: "string or array of strings",
  },
  {
    name: "year",
    description: "The year the movie was released",
    type: "number",
  },
  {
    name: "director",
    description: "The director of the movie",
    type: "string",
  },
  {
    name: "rating",
    description: "The rating of the movie (1-10)",
    type: "number",
  },
  {
    name: "length",
    description: "The length of the movie in minutes",
    type: "number",
  },
];

/**
 * Next, we instantiate a vector store. This is where we store the embeddings of the documents.
 * We also need to provide an embeddings object. This is used to embed the documents.
 */

const client = createClient(
  process.env.SUPABASE_URL,
  process.env.SUPABASE_PRIVATE_KEY
);

const embeddings = new OpenAIEmbeddings();
const vectorStore = await SupabaseVectorStore.fromDocuments(docs, embeddings, {
  client,
});

"""
Now we can instantiate our retriever:

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

// @lc-docs-hide-cell

import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0,
});

import { SelfQueryRetriever } from "langchain/retrievers/self_query";
import { SupabaseTranslator } from "@langchain/community/structured_query/supabase";

const selfQueryRetriever = SelfQueryRetriever.fromLLM({
  llm: llm,
  vectorStore: vectorStore,
  /** A short summary of what the document contents represent. */
  documentContents: "Brief summary of a movie",
  attributeInfo: attributeInfo,
  structuredQueryTranslator: new SupabaseTranslator(),
});

"""
## Usage

Now, ask a question that requires some knowledge of the document's metadata to answer. You can see that the retriever will generate the correct result:
"""

await selfQueryRetriever.invoke(
  "Which movies are rated higher than 8.5?"
);
# Output:
#   [

#     Document {

#       pageContent: 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea',

#       metadata: { year: 2006, rating: 8.6, director: 'Satoshi Kon' },

#       id: undefined

#     },

#     Document {

#       pageContent: 'Three men walk into the Zone, three men walk out of the Zone',

#       metadata: {

#         year: 1979,

#         genre: 'science fiction',

#         rating: 9.9,

#         director: 'Andrei Tarkovsky'

#       },

#       id: undefined

#     }

#   ]


"""
## Use within a chain

Like other retrievers, Supabase self-query retrievers can be incorporated into LLM applications via [chains](/docs/how_to/sequence/).

Note that because their returned answers can heavily depend on document metadata, we format the retrieved documents differently to include that information.
"""

import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers";

import type { Document } from "@langchain/core/documents";

const prompt = ChatPromptTemplate.fromTemplate(`
Answer the question based only on the context provided.

Context: {context}

Question: {question}`);

const formatDocs = (docs: Document[]) => {
  return docs.map((doc) => JSON.stringify(doc)).join("\n\n");
}

// See https://js.langchain.com/docs/tutorials/rag
const ragChain = RunnableSequence.from([
  {
    context: selfQueryRetriever.pipe(formatDocs),
    question: new RunnablePassthrough(),
  },
  prompt,
  llm,
  new StringOutputParser(),
]);

await ragChain.invoke("Which movies are rated higher than 8.5?");
# Output:
#   The movies rated higher than 8.5 are:

#   

#   1. The movie directed by Satoshi Kon in 2006, which has a rating of 8.6.

#   2. The movie directed by Andrei Tarkovsky in 1979, which has a rating of 9.9.


"""
## Default search params

You can also pass a `searchParams` field into the above method that provides default filters applied in addition to any generated query. The filter syntax is a function that returns a [Supabase filter](https://supabase.com/docs/reference/javascript/filter):
"""

import type { SupabaseFilter } from "@langchain/community/vectorstores/supabase";

const selfQueryRetrieverWithDefaultParams = SelfQueryRetriever.fromLLM({
  llm: llm,
  vectorStore: vectorStore,
  documentContents: "Brief summary of a movie",
  attributeInfo: attributeInfo,
  structuredQueryTranslator: new SupabaseTranslator(),
  searchParams: {
    filter: (rpc: SupabaseFilter) => rpc.filter("metadata->>type", "eq", "movie"),
    mergeFiltersOperator: "and",
  },
});

"""
## API reference

For detailed documentation of all Supabase self-query retriever features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.retrievers_self_query.SelfQueryRetriever.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/retrievers/self_query/vectara.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Vectara
---
"""

"""
# Vectara

This guide will help you getting started with such a retriever backed by a [Vectara vector store](/docs/integrations/vectorstores/vectara). For detailed documentation of all features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.retrievers_self_query.SelfQueryRetriever.html).

## Overview

A [self-query retriever](/docs/how_to/self_query/) retrieves documents by dynamically generating metadata filters based on some input query. This allows the retriever to account for underlying document metadata in addition to pure semantic similarity when fetching results.

It uses a module called a `Translator` that generates a filter based on information about metadata fields and the query language that a given vector store supports.

### Integration details

| Backing vector store | Self-host | Cloud offering | Package | [Py support](https://python.langchain.com/docs/integrations/retrievers/self_query/vectara_self_query/) |
| :--- | :--- | :---: | :---: | :---: |
[`VectaraStore`](https://api.js.langchain.com/classes/langchain_community_vectorstores_vectara.VectaraStore.html) | ❌ | ✅ | [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) | ✅ |

## Setup

Set up a Vectara instance as documented [here](/docs/integrations/vectorstores/vectara). Set the following environment variables:

```typescript
process.env.VECTARA_CUSTOMER_ID = "your_customer_id";
process.env.VECTARA_CORPUS_ID = "your_corpus_id";
process.env.VECTARA_API_KEY = "your-vectara-api-key";
```

If you want to get automated tracing from individual queries, you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
// process.env.LANGSMITH_API_KEY = "<YOUR API KEY HERE>";
// process.env.LANGSMITH_TRACING = "true";
```

### Installation

The vector store lives in the `@langchain/community` package. You'll also need to install the `langchain` package to import the main `SelfQueryRetriever` class.

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community langchain @langchain/core
</Npm2Yarn>
```
"""

"""
## Instantiation

First, initialize your Vectara vector store with some documents that contain metadata:
"""

import { VectaraStore } from "@langchain/community/vectorstores/vectara";
import { Document } from "@langchain/core/documents";
import type { AttributeInfo } from "langchain/chains/query_constructor";

// Vectara provides embeddings
import { FakeEmbeddings } from "@langchain/core/utils/testing";

/**
 * First, we create a bunch of documents. You can load your own documents here instead.
 * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.
 */
const docs = [
  new Document({
    pageContent:
      "A bunch of scientists bring back dinosaurs and mayhem breaks loose",
    metadata: { year: 1993, rating: 7.7, genre: "science fiction" },
  }),
  new Document({
    pageContent:
      "Leo DiCaprio gets lost in a dream within a dream within a dream within a ...",
    metadata: { year: 2010, director: "Christopher Nolan", rating: 8.2 },
  }),
  new Document({
    pageContent:
      "A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea",
    metadata: { year: 2006, director: "Satoshi Kon", rating: 8.6 },
  }),
  new Document({
    pageContent:
      "A bunch of normal-sized women are supremely wholesome and some men pine after them",
    metadata: { year: 2019, director: "Greta Gerwig", rating: 8.3 },
  }),
  new Document({
    pageContent: "Toys come alive and have a blast doing so",
    metadata: { year: 1995, genre: "animated" },
  }),
  new Document({
    pageContent: "Three men walk into the Zone, three men walk out of the Zone",
    metadata: {
      year: 1979,
      director: "Andrei Tarkovsky",
      genre: "science fiction",
      rating: 9.9,
    },
  }),
];

/**
 * Next, we define the attributes we want to be able to query on.
 * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.
 * We also provide a description of each attribute and the type of the attribute.
 * This is used to generate the query prompts.
 */
const attributeInfo: AttributeInfo[] = [
  {
    name: "genre",
    description: "The genre of the movie",
    type: "string or array of strings",
  },
  {
    name: "year",
    description: "The year the movie was released",
    type: "number",
  },
  {
    name: "director",
    description: "The director of the movie",
    type: "string",
  },
  {
    name: "rating",
    description: "The rating of the movie (1-10)",
    type: "number",
  },
  {
    name: "length",
    description: "The length of the movie in minutes",
    type: "number",
  },
];

/**
 * Next, we instantiate a vector store. This is where we store the embeddings of the documents.
 * We also need to provide an embeddings object. This is used to embed the documents.
 */
// Vectara provides embeddings
const embeddings = new FakeEmbeddings();
const vectorStore = await VectaraStore.fromDocuments(docs, embeddings, {
  customerId: Number(process.env.VECTARA_CUSTOMER_ID),
  corpusId: Number(process.env.VECTARA_CORPUS_ID),
  apiKey: String(process.env.VECTARA_API_KEY),
});

"""
Now we can instantiate our retriever:

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

// @lc-docs-hide-cell

import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0,
});

import { SelfQueryRetriever } from "langchain/retrievers/self_query";
import { VectaraTranslator } from "@langchain/community/structured_query/vectara";

const selfQueryRetriever = SelfQueryRetriever.fromLLM({
  llm: llm,
  vectorStore: vectorStore,
  /** A short summary of what the document contents represent. */
  documentContents: "Brief summary of a movie",
  attributeInfo: attributeInfo,
  structuredQueryTranslator: new VectaraTranslator(),
});

"""
## Usage

Now, ask a question that requires some knowledge of the document's metadata to answer. You can see that the retriever will generate the correct result:
"""

await selfQueryRetriever.invoke(
  "Which movies are rated higher than 8.5?"
);
# Output:
#   [

#     Document {

#       pageContent: 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea',

#       metadata: { year: 2006, rating: 8.6, director: 'Satoshi Kon' },

#       id: undefined

#     },

#     Document {

#       pageContent: 'Three men walk into the Zone, three men walk out of the Zone',

#       metadata: {

#         year: 1979,

#         genre: 'science fiction',

#         rating: 9.9,

#         director: 'Andrei Tarkovsky'

#       },

#       id: undefined

#     }

#   ]


"""
## Use within a chain

Like other retrievers, Vectara self-query retrievers can be incorporated into LLM applications via [chains](/docs/how_to/sequence/).

Note that because their returned answers can heavily depend on document metadata, we format the retrieved documents differently to include that information.
"""

import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers";

import type { Document } from "@langchain/core/documents";

const prompt = ChatPromptTemplate.fromTemplate(`
Answer the question based only on the context provided.

Context: {context}

Question: {question}`);

const formatDocs = (docs: Document[]) => {
  return docs.map((doc) => JSON.stringify(doc)).join("\n\n");
}

// See https://js.langchain.com/docs/tutorials/rag
const ragChain = RunnableSequence.from([
  {
    context: selfQueryRetriever.pipe(formatDocs),
    question: new RunnablePassthrough(),
  },
  prompt,
  llm,
  new StringOutputParser(),
]);

await ragChain.invoke("Which movies are rated higher than 8.5?");
# Output:
#   The movies rated higher than 8.5 are:

#   

#   1. The movie directed by Satoshi Kon in 2006, which has a rating of 8.6.

#   2. The movie directed by Andrei Tarkovsky in 1979, which has a rating of 9.9.


"""
## Default search params

You can also pass a `searchParams` field into the above method that provides default filters applied in addition to any generated query.

See the [official docs](https://docs.vectara.com/) for more on how to construct metadata filters.
"""

const selfQueryRetrieverWithDefaultParams = SelfQueryRetriever.fromLLM({
  llm,
  vectorStore,
  documentContents: "Brief summary of a movie",
  attributeInfo,
  /**
   * We need to use a translator that translates the queries into a
   * filter format that the vector store can understand. LangChain provides one here.
   */
  structuredQueryTranslator: new VectaraTranslator(),
  searchParams: {
    filter: {
      filter: "( doc.genre = 'science fiction' ) and ( doc.rating > 8.5 )",
    },
    mergeFiltersOperator: "and",
  },
});

"""
## API reference

For detailed documentation of all Vectara self-query retriever features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.retrievers_self_query.SelfQueryRetriever.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/retrievers/self_query/weaviate.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Weaviate
---
"""

"""
# Weaviate

This guide will help you getting started with such a retriever backed by a [Weaviate vector store](/docs/integrations/vectorstores/weaviate). For detailed documentation of all features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.retrievers_self_query.SelfQueryRetriever.html).

## Overview

A [self-query retriever](/docs/how_to/self_query/) retrieves documents by dynamically generating metadata filters based on some input query. This allows the retriever to account for underlying document metadata in addition to pure semantic similarity when fetching results.

It uses a module called a `Translator` that generates a filter based on information about metadata fields and the query language that a given vector store supports.

### Integration details

| Backing vector store | Self-host | Cloud offering | Package | [Py support](https://python.langchain.com/docs/integrations/retrievers/self_query/weaviate_self_query/) |
| :--- | :--- | :---: | :---: | :---: |
[`WeaviateVectorStore`](https://api.js.langchain.com/classes/langchain_weaviate.WeaviateStore.html) | ✅ | ✅ | [`@langchain/weaviate`](https://www.npmjs.com/package/@langchain/weaviate) | ✅ |

## Setup

Set up a Weaviate instance as documented [here](/docs/integrations/vectorstores/weaviate). Set the following environment variables if relevant:

```ts
process.env.WEAVIATE_SCHEME = "https";
// Include port if relevant, e.g. "localhost:8080"
process.env.WEAVIATE_HOST = "YOUR_WEAVIATE_HOST";
process.env.WEAVIATE_URL = "YOUR_WEAVIATE_URL";
```

If you want to get automated tracing from individual queries, you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
// process.env.LANGSMITH_API_KEY = "<YOUR API KEY HERE>";
// process.env.LANGSMITH_TRACING = "true";
```

### Installation

The vector store lives in the `@langchain/weaviate` package. You'll also need to install the `langchain` package to import the main `SelfQueryRetriever` class.

The official Weaviate SDK (`weaviate-ts-client`) is automatically installed as a dependency of `@langchain/weaviate`, but you may wish to install it independently as well.

For this example, we'll also use OpenAI embeddings, so you'll need to install the `@langchain/openai` package and [obtain an API key](https://platform.openai.com):

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/weaviate langchain @langchain/openai @langchain/core weaviate-ts-client
</Npm2Yarn>
```
"""

"""
## Instantiation

First, initialize your Weaviate vector store with some documents that contain metadata:
"""

import { OpenAIEmbeddings } from "@langchain/openai";
import { WeaviateStore } from "@langchain/weaviate";
import { Document } from "@langchain/core/documents";
import type { AttributeInfo } from "langchain/chains/query_constructor";

import weaviate from "weaviate-ts-client";

/**
 * First, we create a bunch of documents. You can load your own documents here instead.
 * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.
 */
const docs = [
  new Document({
    pageContent:
      "A bunch of scientists bring back dinosaurs and mayhem breaks loose",
    metadata: { year: 1993, rating: 7.7, genre: "science fiction" },
  }),
  new Document({
    pageContent:
      "Leo DiCaprio gets lost in a dream within a dream within a dream within a ...",
    metadata: { year: 2010, director: "Christopher Nolan", rating: 8.2 },
  }),
  new Document({
    pageContent:
      "A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea",
    metadata: { year: 2006, director: "Satoshi Kon", rating: 8.6 },
  }),
  new Document({
    pageContent:
      "A bunch of normal-sized women are supremely wholesome and some men pine after them",
    metadata: { year: 2019, director: "Greta Gerwig", rating: 8.3 },
  }),
  new Document({
    pageContent: "Toys come alive and have a blast doing so",
    metadata: { year: 1995, genre: "animated" },
  }),
  new Document({
    pageContent: "Three men walk into the Zone, three men walk out of the Zone",
    metadata: {
      year: 1979,
      director: "Andrei Tarkovsky",
      genre: "science fiction",
      rating: 9.9,
    },
  }),
];

/**
 * Next, we define the attributes we want to be able to query on.
 * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.
 * We also provide a description of each attribute and the type of the attribute.
 * This is used to generate the query prompts.
 */
const attributeInfo: AttributeInfo[] = [
  {
    name: "genre",
    description: "The genre of the movie",
    type: "string or array of strings",
  },
  {
    name: "year",
    description: "The year the movie was released",
    type: "number",
  },
  {
    name: "director",
    description: "The director of the movie",
    type: "string",
  },
  {
    name: "rating",
    description: "The rating of the movie (1-10)",
    type: "number",
  },
  {
    name: "length",
    description: "The length of the movie in minutes",
    type: "number",
  },
];

/**
 * Next, we instantiate a vector store. This is where we store the embeddings of the documents.
 * We also need to provide an embeddings object. This is used to embed the documents.
 */
const client = (weaviate as any).client({
  scheme: process.env.WEAVIATE_SCHEME || "https",
  host: process.env.WEAVIATE_HOST || "localhost",
  apiKey: process.env.WEAVIATE_API_KEY
    ? // eslint-disable-next-line @typescript-eslint/no-explicit-any
      new (weaviate as any).ApiKey(process.env.WEAVIATE_API_KEY)
    : undefined,
});

const embeddings = new OpenAIEmbeddings();
const vectorStore = await WeaviateStore.fromDocuments(docs, embeddings, {
  client,
  indexName: "Test",
  textKey: "text",
  metadataKeys: ["year", "director", "rating", "genre"],
});

"""
Now we can instantiate our retriever:

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

// @lc-docs-hide-cell

import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0,
});

import { SelfQueryRetriever } from "langchain/retrievers/self_query";
import { WeaviateTranslator } from "@langchain/weaviate";

const selfQueryRetriever = SelfQueryRetriever.fromLLM({
  llm: llm,
  vectorStore: vectorStore,
  /** A short summary of what the document contents represent. */
  documentContents: "Brief summary of a movie",
  attributeInfo: attributeInfo,
  structuredQueryTranslator: new WeaviateTranslator(),
});

"""
## Usage

Now, ask a question that requires some knowledge of the document's metadata to answer. You can see that the retriever will generate the correct result:
"""

await selfQueryRetriever.invoke(
  "Which movies are rated higher than 8.5?"
);
# Output:
#   [

#     Document {

#       pageContent: 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea',

#       metadata: { director: 'Satoshi Kon', genre: null, rating: 8.6, year: 2006 },

#       id: undefined

#     },

#     Document {

#       pageContent: 'Three men walk into the Zone, three men walk out of the Zone',

#       metadata: {

#         director: 'Andrei Tarkovsky',

#         genre: 'science fiction',

#         rating: 9.9,

#         year: 1979

#       },

#       id: undefined

#     }

#   ]


"""
## Use within a chain

Like other retrievers, Weaviate self-query retrievers can be incorporated into LLM applications via [chains](/docs/how_to/sequence/).

Note that because their returned answers can heavily depend on document metadata, we format the retrieved documents differently to include that information.
"""

import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers";

import type { Document } from "@langchain/core/documents";

const prompt = ChatPromptTemplate.fromTemplate(`
Answer the question based only on the context provided.

Context: {context}

Question: {question}`);

const formatDocs = (docs: Document[]) => {
  return docs.map((doc) => JSON.stringify(doc)).join("\n\n");
}

// See https://js.langchain.com/docs/tutorials/rag
const ragChain = RunnableSequence.from([
  {
    context: selfQueryRetriever.pipe(formatDocs),
    question: new RunnablePassthrough(),
  },
  prompt,
  llm,
  new StringOutputParser(),
]);

await ragChain.invoke("Which movies are rated higher than 8.5?");
# Output:
#   Both movies are rated higher than 8.5. The first movie directed by Satoshi Kon has a rating of 8.6, and the second movie directed by Andrei Tarkovsky has a rating of 9.9.


"""
## Default search params

You can also pass a `searchParams` field into the above method that provides default filters applied in addition to any generated query.
"""

const selfQueryRetrieverWithDefaultParams = SelfQueryRetriever.fromLLM({
  llm: llm,
  vectorStore: vectorStore,
  documentContents: "Brief summary of a movie",
  attributeInfo: attributeInfo,
  structuredQueryTranslator: new WeaviateTranslator(),
  searchParams: {
    filter: {
      where: {
        operator: "Equal",
        path: ["type"],
        valueText: "movie",
      },
    },
    mergeFiltersOperator: "or",
  },
});

"""
## API reference

For detailed documentation of all Weaviate self-query retriever features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.retrievers_self_query.SelfQueryRetriever.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/stores/cassandra_storage.mdx
================================================
---
sidebar_class_name: node-only
---

# Cassandra KV

This example demonstrates how to setup chat history storage using the `CassandraKVStore` `BaseStore` integration. Note there is a `CassandraChatMessageHistory`
integration which may be easier to use for chat history storage; the `CassandraKVStore` is useful if you want a more general-purpose key-value store with
prefixable keys.

## Setup

```bash npm2yarn
npm install @langchain/community @langchain/core cassandra-driver
```

Depending on your database providers, the specifics of how to connect to the database will vary. We will create a document `configConnection` which will be used as part of the store configuration.

### Apache Cassandra®

Storage Attached Indexes (used by `yieldKeys`) are supported in [Apache Cassandra® 5.0](https://cassandra.apache.org/_/blog/Apache-Cassandra-5.0-Features-Storage-Attached-Indexes.html) and above. You can use a standard connection document, for example:

```typescript
const configConnection = {
  contactPoints: ['h1', 'h2'],
  localDataCenter: 'datacenter1',
  credentials: {
    username: <...> as string,
    password: <...> as string,
  },
};
```

### Astra DB

Astra DB is a cloud-native Cassandra-as-a-Service platform.

1. Create an [Astra DB account](https://astra.datastax.com/register).
2. Create a [vector enabled database](https://astra.datastax.com/createDatabase).
3. Create a [token](https://docs.datastax.com/en/astra/docs/manage-application-tokens.html) for your database.

```typescript
const configConnection = {
  serviceProviderArgs: {
    astra: {
      token: <...> as string,
      endpoint: <...> as string,
    },
  },
};
```

Instead of `endpoint:`, you many provide property `datacenterID:` and optionally `regionName:`.

## Usage

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/stores/cassandra_storage.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## Related

- [Key-value store conceptual guide](/docs/concepts/key_value_stores)



================================================
FILE: docs/core_docs/docs/integrations/stores/file_system.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: File System Store
sidebar_class_name: node-only
---
"""

"""
# LocalFileStore

```{=mdx}

:::tip Compatibility

Only available on Node.js.

:::

```

This will help you get started with [LocalFileStore](/docs/concepts/key_value_stores). For detailed documentation of all LocalFileStore features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.storage_file_system.LocalFileStore.html).

## Overview

The `LocalFileStore` is a wrapper around the `fs` module for storing data as key-value pairs.
Each key value pair has its own file nested inside the directory passed to the `.fromPath` method.
The file name is the key and inside contains the value of the key.

```{=mdx}

:::info

The path passed to the `.fromPath` must be a directory, not a file.

:::

:::warning


This file store can alter any text file in the provided directory and any subfolders.
Make sure that the path you specify when initializing the store is free of other files.

:::

```

### Integration details

| Class | Package | Local | [PY support](https://python.langchain.com/docs/integrations/stores/file_system/) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: |
| [LocalFileStore](https://api.js.langchain.com/classes/langchain.storage_file_system.LocalFileStore.html) | [langchain](https://api.js.langchain.com/modules/langchain.storage_file_system.html) | ✅ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/langchain?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/langchain?style=flat-square&label=%20&) |

## Setup

### Installation

The LangChain `LocalFileStore` integration lives in the `langchain` package:

```{=mdx}

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  langchain @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our byte store:
"""

import { LocalFileStore } from "langchain/storage/file_system"

const kvStore = await LocalFileStore.fromPath("./messages");

"""
Define an encoder and decoder for converting the data to `Uint8Array` and back:
"""

const encoder = new TextEncoder();
const decoder = new TextDecoder();

"""
## Usage

You can set data under keys like this using the `mset` method:
"""

await kvStore.mset(
  [
    ["key1", encoder.encode("value1")],
    ["key2", encoder.encode("value2")],
  ]
)

const results = await kvStore.mget(
  [
    "key1",
    "key2",
  ]
)
console.log(results.map((v) => decoder.decode(v)));
# Output:
#   [ 'value1', 'value2' ]


"""
And you can delete data using the `mdelete` method:
"""

await kvStore.mdelete(
  [
    "key1",
    "key2",
  ]
)

await kvStore.mget(
  [
    "key1",
    "key2",
  ]
)
# Output:
#   [ undefined, undefined ]


"""
## Yielding values

If you want to get back all the keys you can call the `yieldKeys` method. Optionally, you can pass a key prefix to only get back keys which match that prefix.
"""

import { LocalFileStore } from "langchain/storage/file_system"

const kvStoreForYield = await LocalFileStore.fromPath("./messages");

const encoderForYield = new TextEncoder();

// Add some data to the store
await kvStoreForYield.mset(
  [
    ["message:id:key1", encoderForYield.encode("value1")],
    ["message:id:key2", encoderForYield.encode("value2")],
  ]
)

const yieldedKeys = [];
for await (const key of kvStoreForYield.yieldKeys("message:id:")) {
  yieldedKeys.push(key);
}

console.log(yieldedKeys);
# Output:
#   [ 'message:id:key1', 'message:id:key2' ]


import fs from "fs";

// Cleanup
await fs.promises.rm("./messages", { recursive: true, force: true });

"""
## API reference

For detailed documentation of all LocalFileStore features and configurations, head to the [API reference](https://api.js.langchain.com/classes/langchain_storage_file_system.LocalFileStore.html)
"""



================================================
FILE: docs/core_docs/docs/integrations/stores/in_memory.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: InMemory Store
---
"""

"""
# InMemoryStore

This will help you get started with [InMemoryStore](/docs/concepts/key_value_stores). For detailed documentation of all InMemoryStore features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_core.stores.InMemoryStore.html).

The `InMemoryStore` allows for a generic type to be assigned to the values in the store. We'll assign type `BaseMessage` as the type of our values, keeping with the theme of a chat history store.

## Overview

### Integration details

| Class | Package | Local | [PY support](https://python.langchain.com/docs/integrations/stores/in_memory/) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: |
| [InMemoryStore](https://api.js.langchain.com/classes/langchain_core.stores.InMemoryStore.html) | [@langchain/core](https://api.js.langchain.com/modules/langchain_core.stores.html) | ✅ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/core?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/core?style=flat-square&label=%20&) |

## Setup

### Installation

The LangChain InMemoryStore integration lives in the `@langchain/core` package:

```{=mdx}

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our byte store:
"""

import { InMemoryStore } from "@langchain/core/stores"
import { BaseMessage } from "@langchain/core/messages";

const kvStore = new InMemoryStore<BaseMessage>();

"""
## Usage

You can set data under keys like this using the `mset` method:
"""

import { AIMessage, HumanMessage } from "@langchain/core/messages";

await kvStore.mset(
  [
    ["key1", new HumanMessage("value1")],
    ["key2", new AIMessage("value2")],
  ]
)

await kvStore.mget(
  [
    "key1",
    "key2",
  ]
)
# Output:
#   [

#     HumanMessage {

#       "content": "value1",

#       "additional_kwargs": {},

#       "response_metadata": {}

#     },

#     AIMessage {

#       "content": "value2",

#       "additional_kwargs": {},

#       "response_metadata": {},

#       "tool_calls": [],

#       "invalid_tool_calls": []

#     }

#   ]


"""
And you can delete data using the `mdelete` method:
"""

await kvStore.mdelete(
  [
    "key1",
    "key2",
  ]
)

await kvStore.mget(
  [
    "key1",
    "key2",
  ]
)
# Output:
#   [ undefined, undefined ]


"""
## Yielding values

If you want to get back all the keys you can call the `yieldKeys` method. Optionally, you can pass a key prefix to only get back keys which match that prefix.
"""

import { InMemoryStore } from "@langchain/core/stores"
import { AIMessage, BaseMessage, HumanMessage } from "@langchain/core/messages";

const kvStoreForYield = new InMemoryStore<BaseMessage>();

// Add some data to the store
await kvStoreForYield.mset(
  [
    ["message:id:key1", new HumanMessage("value1")],
    ["message:id:key2", new AIMessage("value2")],
  ]
)

const yieldedKeys = [];
for await (const key of kvStoreForYield.yieldKeys("message:id:")) {
  yieldedKeys.push(key);
}

console.log(yieldedKeys);
# Output:
#   [ 'message:id:key1', 'message:id:key2' ]


"""
## API reference

For detailed documentation of all InMemoryStore features and configurations, head to the [API reference](https://api.js.langchain.com/classes/langchain_core.stores.InMemoryStore.html)
"""



================================================
FILE: docs/core_docs/docs/integrations/stores/index.mdx
================================================
---
sidebar_class_name: hidden
---

# Key-value stores

import { CategoryTable, IndexTable } from "@theme/FeatureTables";

[Key-value stores](/docs/concepts/key_value_stores) are used by other LangChain components to store and retrieve data.

## All key-value stores

<IndexTable />



================================================
FILE: docs/core_docs/docs/integrations/stores/ioredis_storage.mdx
================================================
# IORedis

This example demonstrates how to setup chat history storage using the `RedisByteStore` `BaseStore` integration.

## Setup

```bash npm2yarn
npm install @langchain/community @langchain/core ioredis
```

## Usage

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/stores/ioredis_storage.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## Related

- [Key-value store conceptual guide](/docs/concepts/key_value_stores)



================================================
FILE: docs/core_docs/docs/integrations/stores/upstash_redis_storage.mdx
================================================
# Upstash Redis

This example demonstrates how to setup chat history storage using the `UpstashRedisStore` `BaseStore` integration.

## Setup

```bash npm2yarn
npm install @langchain/community @langchain/core @upstash/redis
```

## Usage

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/stores/upstash_redis_storage.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## Related

- [Key-value store conceptual guide](/docs/concepts/key_value_stores)



================================================
FILE: docs/core_docs/docs/integrations/stores/vercel_kv_storage.mdx
================================================
# Vercel KV

This example demonstrates how to setup chat history storage using the `VercelKVStore` `BaseStore` integration.

## Setup

```bash npm2yarn
npm install @langchain/community @langchain/core @vercel/kv
```

## Usage

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/stores/vercel_kv_storage.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## Related

- [Key-value store conceptual guide](/docs/concepts/key_value_stores)



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/alibaba_tongyi.mdx
================================================
---
sidebar_class_name: node-only
---

# Alibaba Tongyi

The `AlibabaTongyiEmbeddings` class uses the Alibaba Tongyi API to generate embeddings for a given text.

## Setup

You'll need to sign up for an Alibaba API key and set it as an environment variable named `ALIBABA_API_KEY`.

Then, you'll need to install the [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) package:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core
```

## Usage

import CodeBlock from "@theme/CodeBlock";
import AlibabaTongyiExample from "@examples/embeddings/alibaba_tongyi.ts";

<CodeBlock language="typescript">{AlibabaTongyiExample}</CodeBlock>

## Related

- Embedding model [conceptual guide](/docs/concepts/embedding_models)
- Embedding model [how-to guides](/docs/how_to/#embedding-models)



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/azure_openai.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Azure OpenAI
---
"""

"""
# AzureOpenAIEmbeddings

[Azure OpenAI](https://azure.microsoft.com/products/ai-services/openai-service/) is a cloud service to help you quickly develop generative AI experiences with a diverse set of prebuilt and curated models from OpenAI, Meta and beyond.

LangChain.js supports integration with [Azure OpenAI](https://azure.microsoft.com/products/ai-services/openai-service/) using the new Azure integration in the [OpenAI SDK](https://github.com/openai/openai-node).

You can learn more about Azure OpenAI and its difference with the OpenAI API on [this page](https://learn.microsoft.com/azure/ai-services/openai/overview). If you don't have an Azure account, you can [create a free account](https://azure.microsoft.com/free/) to get started.

This will help you get started with AzureOpenAIEmbeddings [embedding models](/docs/concepts/embedding_models) using LangChain. For detailed documentation on `AzureOpenAIEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_openai.AzureOpenAIEmbeddings.html).


```{=mdx}

:::info

Previously, LangChain.js supported integration with Azure OpenAI using the dedicated [Azure OpenAI SDK](https://github.com/Azure/azure-sdk-for-js/tree/main/sdk/openai/openai). This SDK is now deprecated in favor of the new Azure integration in the OpenAI SDK, which allows to access the latest OpenAI models and features the same day they are released, and allows seamless transition between the OpenAI API and Azure OpenAI.

If you are using Azure OpenAI with the deprecated SDK, see the [migration guide](#migration-from-azure-openai-sdk) to update to the new API.

:::

```

## Overview
### Integration details

| Class | Package | Local | [Py support](https://python.langchain.com/docs/integrations/text_embedding/azureopenai/) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: |
| [AzureOpenAIEmbeddings](https://api.js.langchain.com/classes/langchain_openai.AzureOpenAIEmbeddings.html) | [@langchain/openai](https://api.js.langchain.com/modules/langchain_openai.html) | ❌ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/openai?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/openai?style=flat-square&label=%20&) |

## Setup

To access Azure OpenAI embedding models you'll need to create an Azure account, get an API key, and install the `@langchain/openai` integration package.

### Credentials

You'll need to have an Azure OpenAI instance deployed. You can deploy a version on Azure Portal following [this guide](https://learn.microsoft.com/azure/ai-services/openai/how-to/create-resource?pivots=web-portal).

Once you have your instance running, make sure you have the name of your instance and key. You can find the key in the Azure Portal, under the "Keys and Endpoint" section of your instance.

If you're using Node.js, you can define the following environment variables to use the service:

```bash
AZURE_OPENAI_API_INSTANCE_NAME=<YOUR_INSTANCE_NAME>
AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME=<YOUR_EMBEDDINGS_DEPLOYMENT_NAME>
AZURE_OPENAI_API_KEY=<YOUR_KEY>
AZURE_OPENAI_API_VERSION="2024-02-01"
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain AzureOpenAIEmbeddings integration lives in the `@langchain/openai` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/openai @langchain/core
</Npm2Yarn>

:::info

You can find the list of supported API versions in the [Azure OpenAI documentation](https://learn.microsoft.com/azure/ai-services/openai/reference).

:::

:::tip

If `AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME` is not defined, it will fall back to the value of `AZURE_OPENAI_API_DEPLOYMENT_NAME` for the deployment name. The same applies to the `azureOpenAIApiEmbeddingsDeploymentName` parameter in the `AzureOpenAIEmbeddings` constructor, which will fall back to the value of `azureOpenAIApiDeploymentName` if not defined.

:::

```
"""

"""
## Instantiation

Now we can instantiate our model object and embed text:
"""

import { AzureOpenAIEmbeddings } from "@langchain/openai";

const embeddings = new AzureOpenAIEmbeddings({
  azureOpenAIApiKey: "<your_key>", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY
  azureOpenAIApiInstanceName: "<your_instance_name>", // In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME
  azureOpenAIApiEmbeddingsDeploymentName: "<your_embeddings_deployment_name>", // In Node.js defaults to process.env.AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME
  azureOpenAIApiVersion: "<api_version>", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION
  maxRetries: 1,
});

"""
## Indexing and Retrieval

Embedding models are often used in retrieval-augmented generation (RAG) flows, both as part of indexing data as well as later retrieving it. For more detailed instructions, please see our RAG tutorials under the [working with external knowledge tutorials](/docs/tutorials/#working-with-external-knowledge).

Below, see how to index and retrieve data using the `embeddings` object we initialized above. In this example, we will index and retrieve a sample document using the demo [`MemoryVectorStore`](/docs/integrations/vectorstores/memory).
"""

// Create a vector store with a sample text
import { MemoryVectorStore } from "langchain/vectorstores/memory";

const text = "LangChain is the framework for building context-aware reasoning applications";

const vectorstore = await MemoryVectorStore.fromDocuments(
  [{ pageContent: text, metadata: {} }],
  embeddings,
);

// Use the vector store as a retriever that returns a single document
const retriever = vectorstore.asRetriever(1);

// Retrieve the most similar text
const retrievedDocuments = await retriever.invoke("What is LangChain?");

retrievedDocuments[0].pageContent;
# Output:
#   LangChain is the framework for building context-aware reasoning applications


"""
## Direct Usage

Under the hood, the vectorstore and retriever implementations are calling `embeddings.embedDocument(...)` and `embeddings.embedQuery(...)` to create embeddings for the text(s) used in `fromDocuments` and the retriever's `invoke` operations, respectively.

You can directly call these methods to get embeddings for your own use cases.

### Embed single texts

You can embed queries for search with `embedQuery`. This generates a vector representation specific to the query:
"""

const singleVector = await embeddings.embedQuery(text);

console.log(singleVector.slice(0, 100));
# Output:
#   [

#      -0.024253517, -0.0054218727,   0.048715446,   0.020580322,    0.03180832,

#      0.0028770117,  -0.012367731,   0.037383243,  -0.054915592,   0.032225136,

#        0.00825818,  -0.023888804,   -0.01184671,   0.012257014,   0.016294925,

#       0.009254632,  0.0051353113,  -0.008889917,   0.016855022,    0.04207243,

#     0.00082589936,  -0.011664353,    0.00818654,   0.029020859,  -0.012335167,

#      -0.019603407,  0.0013945447,    0.05538451,  -0.011625277,  -0.008153976,

#       0.038607642,   -0.03811267, -0.0074440846,   0.047647353,   -0.00927417,

#       0.024201415, -0.0069230637,  -0.008538228,   0.003910912,   0.052805457,

#      -0.023159374,  0.0014352495,  -0.038659744,   0.017141584,   0.005587948,

#       0.007971618,  -0.016920151,    0.06658646, -0.0016916894,   0.045667473,

#      -0.042202685,   -0.03983204,   -0.04160351,  -0.011729481,  -0.055905532,

#       0.012543576,  0.0038848612,   0.007919516,   0.010915386,  0.0033117384,

#      -0.007548289,  -0.030427614,  -0.041890074,   0.036002535,  -0.023771575,

#      -0.008792226,  -0.049444873,   0.016490309, -0.0060568666,   0.040196754,

#       0.014106638,  -0.014575557, -0.0017356506,  -0.011234511,  -0.012517525,

#       0.008362384,    0.01253055,   0.036158845,   0.008297256, -0.0010908874,

#      -0.014888169,  -0.020489143,   0.018965157,  -0.057937514, -0.0037122732,

#       0.004402626,   -0.00840146,   0.042984217,   -0.04936672,   -0.03714878,

#       0.004969236,    0.03707063,   0.015396165,   -0.02055427,    0.01988997,

#       0.030219207,  -0.021257648,    0.01340326,   0.003692735,   0.012595678

#   ]


"""
### Embed multiple texts

You can embed multiple texts for indexing with `embedDocuments`. The internals used for this method may (but do not have to) differ from embedding queries:
"""

const text2 = "LangGraph is a library for building stateful, multi-actor applications with LLMs";

const vectors = await embeddings.embedDocuments([text, text2]);

console.log(vectors[0].slice(0, 100));
console.log(vectors[1].slice(0, 100));
# Output:
#   [

#      -0.024253517, -0.0054218727,   0.048715446,   0.020580322,    0.03180832,

#      0.0028770117,  -0.012367731,   0.037383243,  -0.054915592,   0.032225136,

#        0.00825818,  -0.023888804,   -0.01184671,   0.012257014,   0.016294925,

#       0.009254632,  0.0051353113,  -0.008889917,   0.016855022,    0.04207243,

#     0.00082589936,  -0.011664353,    0.00818654,   0.029020859,  -0.012335167,

#      -0.019603407,  0.0013945447,    0.05538451,  -0.011625277,  -0.008153976,

#       0.038607642,   -0.03811267, -0.0074440846,   0.047647353,   -0.00927417,

#       0.024201415, -0.0069230637,  -0.008538228,   0.003910912,   0.052805457,

#      -0.023159374,  0.0014352495,  -0.038659744,   0.017141584,   0.005587948,

#       0.007971618,  -0.016920151,    0.06658646, -0.0016916894,   0.045667473,

#      -0.042202685,   -0.03983204,   -0.04160351,  -0.011729481,  -0.055905532,

#       0.012543576,  0.0038848612,   0.007919516,   0.010915386,  0.0033117384,

#      -0.007548289,  -0.030427614,  -0.041890074,   0.036002535,  -0.023771575,

#      -0.008792226,  -0.049444873,   0.016490309, -0.0060568666,   0.040196754,

#       0.014106638,  -0.014575557, -0.0017356506,  -0.011234511,  -0.012517525,

#       0.008362384,    0.01253055,   0.036158845,   0.008297256, -0.0010908874,

#      -0.014888169,  -0.020489143,   0.018965157,  -0.057937514, -0.0037122732,

#       0.004402626,   -0.00840146,   0.042984217,   -0.04936672,   -0.03714878,

#       0.004969236,    0.03707063,   0.015396165,   -0.02055427,    0.01988997,

#       0.030219207,  -0.021257648,    0.01340326,   0.003692735,   0.012595678

#   ]

#   [

#      -0.033366997,   0.010419146,  0.0118083665,  -0.040441725, 0.0020355924,

#      -0.015808804,  -0.023629595, -0.0066180876,  -0.040004376,  0.020053642,

#     -0.0010797002,   -0.03900105,  -0.009956073,  0.0027896944,  0.003305828,

#      -0.034010153,   0.009833873,  0.0061164247,   0.022536227,  0.029147884,

#       0.017789727,    0.03182342,   0.010869357,   0.031849146, -0.028093107,

#       0.008283865, -0.0145610785,    0.01645196,  -0.029430874,  -0.02508313,

#       0.046178687,   -0.01722375,  -0.010046115,   0.013101112, 0.0044538635,

#        0.02197025,    0.03985002,   0.007955855,  0.0008819293,  0.012657333,

#       0.014368132,  -0.014007963,   -0.03722594,   0.031617608, -0.011570398,

#       0.039052505,  0.0020018267,   0.023706773, -0.0046950476,  0.056083307,

#       -0.08412496,  -0.043425974,  -0.015512952,   0.015950298,  -0.03624834,

#     -0.0053317733,  -0.037251666,  0.0046339477,    0.04193385,  0.023475237,

#      -0.021378545,   0.013699248,  -0.026009277,   0.050757967,   -0.0494202,

#      0.0007874656,   -0.07208506,   0.015885983,  -0.003259199,  0.015127057,

#      0.0068946453,  -0.035373647,  -0.005875241, -0.0032238255,  -0.04185667,

#      -0.022047428,  0.0014326327, -0.0070940237, -0.0027864785, -0.016271876,

#       0.005097021,   0.034473225,   0.012361481,  -0.026498076, 0.0067274245,

#      -0.026330855,  -0.006132504,   0.008180959,  -0.049368747, -0.032337945,

#       0.011049441,    0.00186194,  -0.012097787,    0.01930758,   0.07059293,

#       0.029713862,    0.04337452, -0.0048461896,  -0.019976463,  0.011473924

#   ]


"""
## Using Azure Managed Identity

If you're using Azure Managed Identity, you can configure the credentials like this:
"""

import {
  DefaultAzureCredential,
  getBearerTokenProvider,
} from "@azure/identity";
import { AzureOpenAIEmbeddings } from "@langchain/openai";

const credentials = new DefaultAzureCredential();
const azureADTokenProvider = getBearerTokenProvider(
  credentials,
  "https://cognitiveservices.azure.com/.default"
);

const modelWithManagedIdentity = new AzureOpenAIEmbeddings({
  azureADTokenProvider,
  azureOpenAIApiInstanceName: "<your_instance_name>",
  azureOpenAIApiEmbeddingsDeploymentName: "<your_embeddings_deployment_name>",
  azureOpenAIApiVersion: "<api_version>",
});


"""
## Using a different domain

If your instance is hosted under a domain other than the default `openai.azure.com`, you'll need to use the alternate `AZURE_OPENAI_BASE_PATH` environment variable.
For example, here's how you would connect to the domain `https://westeurope.api.microsoft.com/openai/deployments/{DEPLOYMENT_NAME}`:
"""

import { AzureOpenAIEmbeddings } from "@langchain/openai";

const embeddingsDifferentDomain = new AzureOpenAIEmbeddings({
  azureOpenAIApiKey: "<your_key>", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY
  azureOpenAIApiEmbeddingsDeploymentName: "<your_embedding_deployment_name>", // In Node.js defaults to process.env.AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME
  azureOpenAIApiVersion: "<api_version>", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION
  azureOpenAIBasePath:
    "https://westeurope.api.microsoft.com/openai/deployments", // In Node.js defaults to process.env.AZURE_OPENAI_BASE_PATH
});


"""
## Custom headers

You can specify custom headers by passing in a `configuration` field:
"""

import { AzureOpenAIEmbeddings } from "@langchain/openai";

const embeddingsWithCustomHeaders = new AzureOpenAIEmbeddings({
  azureOpenAIApiKey: "<your_key>",
  azureOpenAIApiInstanceName: "<your_instance_name>",
  azureOpenAIApiEmbeddingsDeploymentName: "<your_embeddings_deployment_name>",
  azureOpenAIApiVersion: "<api_version>",
  configuration: {
    defaultHeaders: {
      "x-custom-header": `SOME_VALUE`,
    },
  },
});

"""
The `configuration` field also accepts other `ClientOptions` parameters accepted by the official SDK.

**Note:** The specific header `api-key` currently cannot be overridden in this manner and will pass through the value from `azureOpenAIApiKey`.
"""

"""
## Migration from Azure OpenAI SDK

If you are using the deprecated Azure OpenAI SDK with the `@langchain/azure-openai` package, you can update your code to use the new Azure integration following these steps:

1. Install the new `@langchain/openai` package and remove the previous `@langchain/azure-openai` package:
   ```bash npm2yarn
   npm install @langchain/openai
   npm uninstall @langchain/azure-openai
   ```
2. Update your imports to use the new `AzureOpenAIEmbeddings` classe from the `@langchain/openai` package:
   ```typescript
   import { AzureOpenAIEmbeddings } from "@langchain/openai";
   ```
3. Update your code to use the new `AzureOpenAIEmbeddings` class and pass the required parameters:

   ```typescript
   const model = new AzureOpenAIEmbeddings({
     azureOpenAIApiKey: "<your_key>",
     azureOpenAIApiInstanceName: "<your_instance_name>",
     azureOpenAIApiEmbeddingsDeploymentName:
       "<your_embeddings_deployment_name>",
     azureOpenAIApiVersion: "<api_version>",
   });
   ```

   Notice that the constructor now requires the `azureOpenAIApiInstanceName` parameter instead of the `azureOpenAIEndpoint` parameter, and adds the `azureOpenAIApiVersion` parameter to specify the API version.

   - If you were using Azure Managed Identity, you now need to use the `azureADTokenProvider` parameter to the constructor instead of `credentials`, see the [Azure Managed Identity](#using-azure-managed-identity) section for more details.

   - If you were using environment variables, you now have to set the `AZURE_OPENAI_API_INSTANCE_NAME` environment variable instead of `AZURE_OPENAI_API_ENDPOINT`, and add the `AZURE_OPENAI_API_VERSION` environment variable to specify the API version.

"""

"""
## API reference

For detailed documentation of all AzureOpenAIEmbeddings features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_openai.AzureOpenAIEmbeddings.html
"""



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/baidu_qianfan.mdx
================================================
# Baidu Qianfan

The `BaiduQianfanEmbeddings` class uses the Baidu Qianfan API to generate embeddings for a given text.

## Setup

An API key is required to use this embedding model. You can get one by registering at https://cloud.baidu.com/doc/WENXINWORKSHOP/s/alj562vvu.

Please set the acquired API key as an environment variable named BAIDU_API_KEY, and set your secret key as an environment variable named BAIDU_SECRET_KEY.

Then, you'll need to install the [`@langchain/baidu-qianfan`](https://www.npmjs.com/package/@langchain/baidu-qianfan) package:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/baidu-qianfan @langchain/core
```

## Usage

import CodeBlock from "@theme/CodeBlock";
import BaiduQianFanExample from "@examples/embeddings/baidu_qianfan.ts";

<CodeBlock language="typescript">{BaiduQianFanExample}</CodeBlock>

## Related

- Embedding model [conceptual guide](/docs/concepts/embedding_models)
- Embedding model [how-to guides](/docs/how_to/#embedding-models)



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/bedrock.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Amazon Bedrock
---
"""

"""
# BedrockEmbeddings

[Amazon Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon via a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI.

This will help you get started with Amazon Bedrock [embedding models](/docs/concepts/embedding_models) using LangChain. For detailed documentation on `Bedrock` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_aws.BedrockEmbeddings.html).

## Overview
### Integration details

| Class | Package | Local | [Py support](https://python.langchain.com/docs/integrations/text_embedding/bedrock/) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: |
| [Bedrock](https://api.js.langchain.com/classes/langchain_aws.BedrockEmbeddings.html) | [@langchain/aws](https://api.js.langchain.com/modules/langchain_aws.html) | ❌ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/aws?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/aws?style=flat-square&label=%20&) |

## Setup

To access Bedrock embedding models you'll need to create an AWS account, get an API key, and install the `@langchain/aws` integration package.

Head to the [AWS docs](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html) to sign up for AWS and setup your credentials. You'll also need to turn on model access for your account, which you can do by [following these instructions](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html).

### Credentials

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain Bedrock integration lives in the `@langchain/aws` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/aws @langchain/core
</Npm2Yarn>
```
"""

"""
## Instantiation

Now we can instantiate our model object and embed text.

There are a few different ways to authenticate with AWS - the below examples rely on an access key, secret access key and region set in your environment variables:
"""

import { BedrockEmbeddings } from "@langchain/aws";

const embeddings = new BedrockEmbeddings({
  region: process.env.BEDROCK_AWS_REGION!,
  credentials: {
    accessKeyId: process.env.BEDROCK_AWS_ACCESS_KEY_ID!,
    secretAccessKey: process.env.BEDROCK_AWS_SECRET_ACCESS_KEY!,
  },
  model: "amazon.titan-embed-text-v1",
});

"""
## Indexing and Retrieval

Embedding models are often used in retrieval-augmented generation (RAG) flows, both as part of indexing data as well as later retrieving it. For more detailed instructions, please see our RAG tutorials under the [working with external knowledge tutorials](/docs/tutorials/#working-with-external-knowledge).

Below, see how to index and retrieve data using the `embeddings` object we initialized above. In this example, we will index and retrieve a sample document using the demo [`MemoryVectorStore`](/docs/integrations/vectorstores/memory).
"""

// Create a vector store with a sample text
import { MemoryVectorStore } from "langchain/vectorstores/memory";

const text = "LangChain is the framework for building context-aware reasoning applications";

const vectorstore = await MemoryVectorStore.fromDocuments(
  [{ pageContent: text, metadata: {} }],
  embeddings,
);

// Use the vector store as a retriever that returns a single document
const retriever = vectorstore.asRetriever(1);

// Retrieve the most similar text
const retrievedDocuments = await retriever.invoke("What is LangChain?");

retrievedDocuments[0].pageContent;
# Output:
#   LangChain is the framework for building context-aware reasoning applications


"""
## Direct Usage

Under the hood, the vectorstore and retriever implementations are calling `embeddings.embedDocument(...)` and `embeddings.embedQuery(...)` to create embeddings for the text(s) used in `fromDocuments` and the retriever's `invoke` operations, respectively.

You can directly call these methods to get embeddings for your own use cases.

### Embed single texts

You can embed queries for search with `embedQuery`. This generates a vector representation specific to the query:
"""

const singleVector = await embeddings.embedQuery(text);

console.log(singleVector.slice(0, 100));
# Output:
#   [

#            0.625,  0.111328125,      0.265625,   -0.20019531,  0.40820312,

#     -0.010803223,  -0.22460938, -0.0002937317,    0.29882812, -0.14355469,

#     -0.068847656,   -0.3984375,          0.75,    -0.1953125,  -0.5546875,

#     -0.087402344,       0.5625,      1.390625,    -0.3515625,  0.39257812,

#     -0.061767578,      0.65625,   -0.36328125,   -0.06591797,    0.234375,

#      -0.36132812,   0.42382812,  -0.115234375,   -0.28710938, -0.29296875,

#        -0.765625,  -0.16894531,    0.23046875,     0.6328125, -0.08544922,

#       0.13671875, 0.0004272461,        0.3125,    0.12207031,   -0.546875,

#       0.14257812, -0.119628906,  -0.111328125,    0.61328125,      0.6875,

#        0.3671875,   -0.2578125,   -0.27734375,      0.703125,    0.203125,

#       0.17675781,  -0.26757812,   -0.76171875,    0.71484375,  0.77734375,

#       -0.1953125, -0.007232666,  -0.044921875,    0.23632812, -0.24121094,

#     -0.012207031,    0.5078125,    0.08984375,    0.56640625,  -0.3046875,

#        0.6484375,        -0.25,   -0.37890625,    -0.2421875,  0.38476562,

#      -0.18164062,  -0.05810547,     0.7578125,    0.04296875,    0.609375,

#       0.50390625,  0.023803711,   -0.23046875,   0.099121094,  0.79296875,

#        -1.296875,     0.671875,   -0.66796875,    0.43359375, 0.087890625,

#       0.14550781,  -0.37304688,  -0.068359375, 0.00012874603, -0.47265625,

#        -0.765625,   0.07861328,  -0.029663086,   0.076660156, -0.32617188,

#        -0.453125,   -0.5546875,   -0.45703125,     1.1015625, -0.29492188

#   ]


"""
### Embed multiple texts

You can embed multiple texts for indexing with `embedDocuments`. The internals used for this method may (but do not have to) differ from embedding queries:
"""

const text2 = "LangGraph is a library for building stateful, multi-actor applications with LLMs";

const vectors = await embeddings.embedDocuments([text, text2]);

console.log(vectors[0].slice(0, 100));
console.log(vectors[1].slice(0, 100));
# Output:
#   [

#            0.625,  0.111328125,      0.265625,   -0.20019531,  0.40820312,

#     -0.010803223,  -0.22460938, -0.0002937317,    0.29882812, -0.14355469,

#     -0.068847656,   -0.3984375,          0.75,    -0.1953125,  -0.5546875,

#     -0.087402344,       0.5625,      1.390625,    -0.3515625,  0.39257812,

#     -0.061767578,      0.65625,   -0.36328125,   -0.06591797,    0.234375,

#      -0.36132812,   0.42382812,  -0.115234375,   -0.28710938, -0.29296875,

#        -0.765625,  -0.16894531,    0.23046875,     0.6328125, -0.08544922,

#       0.13671875, 0.0004272461,        0.3125,    0.12207031,   -0.546875,

#       0.14257812, -0.119628906,  -0.111328125,    0.61328125,      0.6875,

#        0.3671875,   -0.2578125,   -0.27734375,      0.703125,    0.203125,

#       0.17675781,  -0.26757812,   -0.76171875,    0.71484375,  0.77734375,

#       -0.1953125, -0.007232666,  -0.044921875,    0.23632812, -0.24121094,

#     -0.012207031,    0.5078125,    0.08984375,    0.56640625,  -0.3046875,

#        0.6484375,        -0.25,   -0.37890625,    -0.2421875,  0.38476562,

#      -0.18164062,  -0.05810547,     0.7578125,    0.04296875,    0.609375,

#       0.50390625,  0.023803711,   -0.23046875,   0.099121094,  0.79296875,

#        -1.296875,     0.671875,   -0.66796875,    0.43359375, 0.087890625,

#       0.14550781,  -0.37304688,  -0.068359375, 0.00012874603, -0.47265625,

#        -0.765625,   0.07861328,  -0.029663086,   0.076660156, -0.32617188,

#        -0.453125,   -0.5546875,   -0.45703125,     1.1015625, -0.29492188

#   ]

#   [

#          0.65625,    0.48242188,    0.70703125,   -0.13378906,    0.859375,

#        0.2578125,   -0.13378906, -0.0002670288,      -0.34375,  0.25585938,

#      -0.33984375,   -0.26367188,      0.828125,   -0.23242188, -0.61328125,

#       0.12695312,    0.43359375,     1.3828125,  -0.099121094,   0.3203125,

#      -0.34765625,    0.35351562,   -0.28710938,   0.009521484, 0.083496094,

#      0.040283203,   -0.25390625,    0.17871094,   0.044189453, -0.19628906,

#       0.45898438,    0.21191406,    0.67578125,     0.8359375, -0.29101562,

#      0.021118164,    0.13671875,   0.083984375,    0.34570312,  0.30859375,

#     -0.001625061,    0.31835938,   -0.18164062, -0.0058288574,  0.22460938,

#       0.26757812,   -0.09082031,    0.17480469,     1.4921875, -0.24316406,

#       0.36523438,    0.14550781,     -0.609375,    0.33007812,  0.10595703,

#        0.3671875,    0.18359375,   -0.62109375,    0.51171875, 0.024047852,

#      0.092285156,   -0.44335938,     0.4921875,      0.609375, -0.48242188,

#         0.796875,   -0.47851562,      -0.53125,   -0.66796875,  0.68359375,

#      -0.16796875,   0.110839844,    0.84765625,      0.703125,   0.8671875,

#       0.37695312, -0.0022888184,   -0.30664062,     0.3671875,  0.16503906,

#      -0.59765625,     0.3203125,      -0.34375,    0.08251953,    0.890625,

#       0.38476562,   -0.24707031,        -0.125, 0.00013160706, -0.69921875,

#         -0.53125,   0.052490234,    0.27734375,    0.42773438, -0.38867188,

#       -0.2578125,         -0.25,      -0.46875,      0.828125, -0.94140625

#   ]


"""
## Configuring the Bedrock Runtime Client

You can pass in your own instance of the `BedrockRuntimeClient` if you want to customize options like
`credentials`, `region`, `retryPolicy`, etc.
"""

import { BedrockRuntimeClient } from "@aws-sdk/client-bedrock-runtime";
import { BedrockEmbeddings } from "@langchain/aws";

const getCredentials = () => {
  // do something to get credentials
}

// @lc-ts-ignore
const client = new BedrockRuntimeClient({
  region: "us-east-1",
  credentials: getCredentials(),
});

const embeddingsWithCustomClient = new BedrockEmbeddings({
  client,
});

"""
## API reference

For detailed documentation of all Bedrock features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_aws.BedrockEmbeddings.html
"""



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/bytedance_doubao.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: ByteDance Doubao
---
"""

"""
# ByteDanceDoubaoEmbeddings

This will help you get started with ByteDanceDoubao [embedding models](/docs/concepts/embedding_models) using LangChain.  For detailed documentation on `ByteDanceDoubaoEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/_langchain_community.embeddings_bytedance_doubao.ByteDanceDoubaoEmbeddings.html).

## Overview
### Integration details

| Class | Package | Local | Py support | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: |
| [ByteDanceDoubaoEmbeddings](https://api.js.langchain.com/classes/_langchain_community.embeddings_bytedance_doubao.ByteDanceDoubaoEmbeddings.html) | [@langchain/community](https://api.js.langchain.com/modules/_langchain_community.html) | ❌ | ❌ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/community?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |

## Setup

You'll need to sign up for an [ARK API key](https://console.volcengine.com/ark/region:ark+cn-beijing/apiKey) and set it as an environment variable named `ARK_API_KEY`. Then you should [create a entrypoint](https://console.volcengine.com/ark/region:ark+cn-beijing/endpoint) for embedding models, and use the entrypoint's name as `model`.

Then, you'll need to install the [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) package

### Credentials

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGCHAIN_TRACING_V2="true"
# export LANGCHAIN_API_KEY="your-api-key"
```

### Installation

The LangChain ByteDanceDoubaoEmbeddings integration lives in the `@langchain/community` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community
</Npm2Yarn>
```
"""

"""
## Instantiation

Now we can instantiate our model object and embed text:
"""

import { ByteDanceDoubaoEmbeddings } from "@langchain/community/embeddings/bytedance_doubao";

const embeddings = new ByteDanceDoubaoEmbeddings({
  model: 'ep-xxx-xxx' // your entrypoint's name
});

"""
## Indexing and Retrieval

Embedding models are often used in retrieval-augmented generation (RAG) flows, both as part of indexing data as well as later retrieving it. For more detailed instructions, please see our RAG tutorials under the [working with external knowledge tutorials](/docs/tutorials/#working-with-external-knowledge).

Below, see how to index and retrieve data using the `embeddings` object we initialized above. In this example, we will index and retrieve a sample document using the demo [`MemoryVectorStore`](/docs/integrations/vectorstores/memory).
"""

// Create a vector store with a sample text
import { MemoryVectorStore } from "langchain/vectorstores/memory";

const text = "LangChain is the framework for building context-aware reasoning applications";

const vectorstore = await MemoryVectorStore.fromDocuments(
  [{ pageContent: text, metadata: {} }],
  embeddings,
);

// Use the vector store as a retriever that returns a single document
const retriever = vectorstore.asRetriever(1);

// Retrieve the most similar text
const retrievedDocuments = await retriever.invoke("What is LangChain?");

retrievedDocuments[0].pageContent;
# Output:
#   LangChain is the framework for building context-aware reasoning applications


"""
## Direct Usage

Under the hood, the vectorstore and retriever implementations are calling `embeddings.embedDocument(...)` and `embeddings.embedQuery(...)` to create embeddings for the text(s) used in `fromDocuments` and the retriever's `invoke` operations, respectively.

You can directly call these methods to get embeddings for your own use cases.

### Embed single texts

You can embed queries for search with `embedQuery`. This generates a vector representation specific to the query:
"""

const singleVector = await embeddings.embedQuery(text);

console.log(singleVector.slice(0, 100));
# Output:
#   [

#      0.026051683,   0.029081265,  -0.040726297,  -0.015116953, -0.010691089,

#      0.030181013, -0.0065084146,   -0.02079503,   0.013575795,   0.03452527,

#      0.009578291,   0.007026421,  -0.030110886,   0.013489622,  -0.04294787,

#      0.011141899,  -0.043768786,   -0.00362867, -0.0081198225,  -0.03426076,

#      0.010075142,   0.027787417,   -0.09052663,   -0.06039698, -0.009462592,

#       0.06232288,   0.051121354,   0.011977532,   0.089046724,  0.059000008,

#      0.031860664,  -0.034242127,   0.020339863,   0.011483523,  -0.05429335,

#      -0.04963588,    0.03263794,   -0.05581542,   0.013908403, -0.012356067,

#     -0.007802118,  -0.010027855,    0.00281217,  -0.101886116, -0.079341754,

#      0.011269771,  0.0035983133,  -0.027667878,   0.032092705, -0.052843474,

#     -0.045283325,     0.0382421,     0.0193055,   0.011050924,  0.021132186,

#     -0.037696265,  0.0006107435,  0.0043520257,  -0.028798066,  0.049155913,

#       0.03590549, -0.0040995986,   0.019772101,  -0.076119535, 0.0031298609,

#       0.03368174,   0.039398745,  -0.011813277,  -0.019313531, -0.013108803,

#     -0.044905286,  -0.022326004,   -0.01656178,   -0.06658457,  0.016789088,

#      0.049952697,   0.006615693,   -0.01694402,  -0.018105473, 0.0049101883,

#     -0.004966945,   0.049762275,   -0.03556957,  -0.015986584,  -0.03190983,

#      -0.05336687, -0.0020468342, -0.0016106658,  -0.035291273, -0.029783724,

#     -0.010153295,   0.052100364,    0.05528949,    0.01379487, -0.024542747,

#      0.028773975,   0.010087022,   0.030448131,  -0.042391222,  0.016596776

#   ]


"""
### Embed multiple texts

You can embed multiple texts for indexing with `embedDocuments`. The internals used for this method may (but do not have to) differ from embedding queries:
"""

const text2 = "LangGraph is a library for building stateful, multi-actor applications with LLMs";

const vectors = await embeddings.embedDocuments([text, text2]);

console.log(vectors[0].slice(0, 100));
console.log(vectors[1].slice(0, 100));
# Output:
#   [

#      0.026051683,   0.029081265,  -0.040726297,  -0.015116953, -0.010691089,

#      0.030181013, -0.0065084146,   -0.02079503,   0.013575795,   0.03452527,

#      0.009578291,   0.007026421,  -0.030110886,   0.013489622,  -0.04294787,

#      0.011141899,  -0.043768786,   -0.00362867, -0.0081198225,  -0.03426076,

#      0.010075142,   0.027787417,   -0.09052663,   -0.06039698, -0.009462592,

#       0.06232288,   0.051121354,   0.011977532,   0.089046724,  0.059000008,

#      0.031860664,  -0.034242127,   0.020339863,   0.011483523,  -0.05429335,

#      -0.04963588,    0.03263794,   -0.05581542,   0.013908403, -0.012356067,

#     -0.007802118,  -0.010027855,    0.00281217,  -0.101886116, -0.079341754,

#      0.011269771,  0.0035983133,  -0.027667878,   0.032092705, -0.052843474,

#     -0.045283325,     0.0382421,     0.0193055,   0.011050924,  0.021132186,

#     -0.037696265,  0.0006107435,  0.0043520257,  -0.028798066,  0.049155913,

#       0.03590549, -0.0040995986,   0.019772101,  -0.076119535, 0.0031298609,

#       0.03368174,   0.039398745,  -0.011813277,  -0.019313531, -0.013108803,

#     -0.044905286,  -0.022326004,   -0.01656178,   -0.06658457,  0.016789088,

#      0.049952697,   0.006615693,   -0.01694402,  -0.018105473, 0.0049101883,

#     -0.004966945,   0.049762275,   -0.03556957,  -0.015986584,  -0.03190983,

#      -0.05336687, -0.0020468342, -0.0016106658,  -0.035291273, -0.029783724,

#     -0.010153295,   0.052100364,    0.05528949,    0.01379487, -0.024542747,

#      0.028773975,   0.010087022,   0.030448131,  -0.042391222,  0.016596776

#   ]

#   [

#         0.0558515,   0.028698817,  -0.037476595,  0.0048659276,  -0.019229038,

#       -0.04713716,  -0.020947812,  -0.017550547,    0.01205507,   0.027693441,

#      -0.011791304,   0.009862203,   0.019662278,  -0.037511427,  -0.022662448,

#       0.036224432,  -0.051760387,  -0.030165697,  -0.008899774,  -0.024518963,

#       0.010077767,   0.032209765,    -0.0854303,  -0.038666975,  -0.036021013,

#       0.060899545,   0.045867186,   0.003365381,    0.09387081,   0.038216405,

#       0.011449426,  -0.016495887,   0.020602569,   -0.02368503,  -0.014733645,

#      -0.065408126, -0.0065152845,  -0.027103946, 0.00038956117,   -0.08648814,

#       0.029316466,  -0.054449145,   0.034129277,  -0.055225655,  -0.043182302,

#      0.0011148591,   0.044116337,  -0.046552557,   0.032423045,   -0.03269365,

#       -0.05062933,   0.021473562,  -0.011019348,  -0.019621233, -0.0003149565,

#     -0.0046085776,  0.0052610254, -0.0029293327,  -0.035793293,   0.034469575,

#       0.037724957,   0.009572597,   0.014198464,    -0.0878237,  0.0056973165,

#       0.023563445,   0.030928325,   0.025520306,    0.01836824,  -0.016456697,

#      -0.061934732,   0.009764942,  -0.035812028,   -0.04429064,   0.031323086,

#       0.056027107, -0.0019782048,  -0.015204176,  -0.008684945, -0.0010460864,

#       0.054642987,   0.044149086,  -0.032964867,  -0.012044753,  -0.019075096,

#      -0.027932597,   0.018542245,   -0.02602878,   -0.04645578,  -0.020976603,

#       0.018999187,   0.050663687,   0.016725155,  0.0076955976,   0.011448177,

#       0.053931057,   -0.03234989,   0.024429373,  -0.023123834,    0.02197912

#   ]


"""
## Related

- Embedding model [conceptual guide](/docs/concepts/embedding_models)
- Embedding model [how-to guides](/docs/how_to/#embedding-models)
"""

"""
## API reference

For detailed documentation of all ByteDanceDoubaoEmbeddings features and configurations head to the API reference: https://api.js.langchain.com/classes/_langchain_community.embeddings_bytedance_doubao.ByteDanceDoubaoEmbeddings.html
"""



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/cloudflare_ai.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Cloudflare Workers AI
lc_docs_skip_validation: true
---
"""

"""
# CloudflareWorkersAIEmbeddings

This will help you get started with Cloudflare Workers AI [embedding models](/docs/concepts/embedding_models) using LangChain. For detailed documentation on `CloudflareWorkersAIEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_cloudflare.CloudflareWorkersAIEmbeddings.html).

## Overview
### Integration details

| Class | Package | Local | Py support | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: |
| [`CloudflareWorkersAIEmbeddings`](https://api.js.langchain.com/classes/langchain_cloudflare.CloudflareWorkersAIEmbeddings.html) | [`@langchain/cloudflare`](https://npmjs.com/@langchain/cloudflare) | ❌ | ❌ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/cloudflare?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/cloudflare?style=flat-square&label=%20&) |

## Setup

To access Cloudflare embedding models you'll need to create a Cloudflare account and install the `@langchain/cloudflare` integration package. This integration is made to run in a Cloudflare worker and accept a binding.

Follow [the official docs](https://developers.cloudflare.com/workers-ai/get-started/workers-wrangler/) to set up your worker.

Your `wrangler.toml` file should look similar to this:

```toml
name = "langchain-test"
main = "worker.js"
compatibility_date = "2024-01-10"

[[vectorize]]
binding = "VECTORIZE_INDEX"
index_name = "langchain-test"

[ai]
binding = "AI"
```

### Credentials

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain CloudflareWorkersAIEmbeddings integration lives in the `@langchain/cloudflare` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/cloudflare @langchain/core
</Npm2Yarn>
```
"""

"""
## Usage

Below is an example worker that uses Workers AI embeddings with a [Cloudflare Vectorize vectorstore](/docs/integrations/vectorstores/cloudflare_vectorize/).
"""

// @ts-nocheck

import type {
  VectorizeIndex,
  Fetcher,
  Request,
} from "@cloudflare/workers-types";

import {
  CloudflareVectorizeStore,
  CloudflareWorkersAIEmbeddings,
} from "@langchain/cloudflare";

export interface Env {
  VECTORIZE_INDEX: VectorizeIndex;
  AI: Fetcher;
}

export default {
  async fetch(request: Request, env: Env) {
    const { pathname } = new URL(request.url);
    const embeddings = new CloudflareWorkersAIEmbeddings({
      binding: env.AI,
      model: "@cf/baai/bge-small-en-v1.5",
    });
    const store = new CloudflareVectorizeStore(embeddings, {
      index: env.VECTORIZE_INDEX,
    });
    if (pathname === "/") {
      const results = await store.similaritySearch("hello", 5);
      return Response.json(results);
    } else if (pathname === "/load") {
      // Upsertion by id is supported
      await store.addDocuments(
        [
          {
            pageContent: "hello",
            metadata: {},
          },
          {
            pageContent: "world",
            metadata: {},
          },
          {
            pageContent: "hi",
            metadata: {},
          },
        ],
        { ids: ["id1", "id2", "id3"] }
      );

      return Response.json({ success: true });
    } else if (pathname === "/clear") {
      await store.delete({ ids: ["id1", "id2", "id3"] });
      return Response.json({ success: true });
    }

    return Response.json({ error: "Not Found" }, { status: 404 });
  },
};

"""
## API reference

For detailed documentation of all `CloudflareWorkersAIEmbeddings` features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_cloudflare.CloudflareWorkersAIEmbeddings.html
"""



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/cohere.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Cohere
lc_docs_skip_validation: true
---
"""

"""
# CohereEmbeddings

This will help you get started with CohereEmbeddings [embedding models](/docs/concepts/embedding_models) using LangChain. For detailed documentation on `CohereEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_cohere.CohereEmbeddings.html).

## Overview
### Integration details

| Class | Package | Local | [Py support](https://python.langchain.com/docs/integrations/text_embedding/cohere/) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: |
| [CohereEmbeddings](https://api.js.langchain.com/classes/langchain_cohere.CohereEmbeddings.html) | [@langchain/cohere](https://api.js.langchain.com/modules/langchain_cohere.html) | ❌ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/cohere?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/cohere?style=flat-square&label=%20&) |

## Setup

To access Cohere embedding models you'll need to create a Cohere account, get an API key, and install the `@langchain/cohere` integration package.

### Credentials

Head to [cohere.com](https://cohere.com) to sign up to `Cohere` and generate an API key. Once you've done this set the `COHERE_API_KEY` environment variable:

```bash
export COHERE_API_KEY="your-api-key"
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain CohereEmbeddings integration lives in the `@langchain/cohere` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/cohere @langchain/core
</Npm2Yarn>
```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

import { CohereEmbeddings } from "@langchain/cohere";

const embeddings = new CohereEmbeddings({
  apiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.COHERE_API_KEY
  batchSize: 48, // Default value if omitted is 48. Max value is 96
  model: "embed-english-v3.0",
});

"""
### Custom client for Cohere on Azure, Cohere on AWS Bedrock, and Standalone Cohere Instance.

We can instantiate a custom `CohereClient` and pass it to the ChatCohere constructor.

**Note:** If a custom client is provided both `COHERE_API_KEY` environment variable and apiKey parameter in the constructor will be ignored
"""

import { CohereEmbeddings } from "@langchain/cohere";
import { CohereClient } from "cohere-ai";

const client = new CohereClient({
  token: "<your-api-key>",
  environment: "<your-cohere-deployment-url>", //optional
  // other params
});

const embeddingsWithCustomClient = new CohereEmbeddings({
  client,
  // other params...
});

"""
## Indexing and Retrieval

Embedding models are often used in retrieval-augmented generation (RAG) flows, both as part of indexing data as well as later retrieving it. For more detailed instructions, please see our RAG tutorials under the [working with external knowledge tutorials](/docs/tutorials/#working-with-external-knowledge).

Below, see how to index and retrieve data using the `embeddings` object we initialized above. In this example, we will index and retrieve a sample document using the demo [`MemoryVectorStore`](/docs/integrations/vectorstores/memory).
"""

// Create a vector store with a sample text
import { MemoryVectorStore } from "langchain/vectorstores/memory";

const text = "LangChain is the framework for building context-aware reasoning applications";

const vectorstore = await MemoryVectorStore.fromDocuments(
  [{ pageContent: text, metadata: {} }],
  embeddings,
);

// Use the vector store as a retriever that returns a single document
const retriever = vectorstore.asRetriever(1);

// Retrieve the most similar text
const retrievedDocuments = await retriever.invoke("What is LangChain?");

retrievedDocuments[0].pageContent;
# Output:
#   LangChain is the framework for building context-aware reasoning applications


"""
## Direct Usage

Under the hood, the vectorstore and retriever implementations are calling `embeddings.embedDocument(...)` and `embeddings.embedQuery(...)` to create embeddings for the text(s) used in `fromDocuments` and the retriever's `invoke` operations, respectively.

You can directly call these methods to get embeddings for your own use cases.

### Embed single texts

You can embed queries for search with `embedQuery`. This generates a vector representation specific to the query:
"""

const singleVector = await embeddings.embedQuery(text);

console.log(singleVector.slice(0, 100));
# Output:
#   [

#      -0.022979736,  -0.030212402,   -0.08886719,  -0.08569336,   0.007030487,

#     -0.0010671616,  -0.033813477,    0.08843994, 0.0119018555,   0.049926758,

#       -0.03616333,   0.007408142, 0.00034809113, -0.005744934,  -0.016021729,

#      -0.015296936, -0.0011606216,   -0.02458191, -0.044006348,    -0.0335083,

#       0.024658203,  -0.051086426,  0.0020427704,   0.06298828,   0.020507812,

#       0.037475586,    0.05117798,  0.0059814453,  0.025360107,  0.0060577393,

#        0.02255249,  -0.070129395,   0.024017334,  0.022766113,  -0.042755127,

#      -0.024673462,    -0.0236969, -0.0073623657,  0.002161026,   0.011329651,

#       0.038330078,   -0.03050232,  0.0022201538, -0.007911682, -0.0023536682,

#       0.029937744,  -0.027297974,  -0.064086914,  0.027267456,   0.016738892,

#      0.0028972626,   0.015510559,   -0.01725769,  0.011497498,  -0.012954712,

#       0.002380371,   -0.03366089,   -0.02746582,  0.014022827,    0.04196167,

#       0.007698059,  -0.027069092,   0.025405884, -0.029815674,   0.013298035,

#        0.01737976,    0.07269287,   0.017822266, 0.0012550354,  -0.009597778,

#       -0.02961731,  0.0049057007,    0.01965332, -0.009994507,  -0.019561768,

#      -0.004764557,   0.019317627, -0.0045433044,  0.031143188,  -0.018188477,

#     -0.0026893616,  0.0050964355,  -0.044189453,   0.02029419,  -0.019088745,

#        0.02166748,  -0.011657715,  -0.025405884, -0.028030396, -0.0051460266,

#      -0.010818481,  -0.000364542,  -0.028686523,  0.015029907,  0.0013790131,

#     -0.0069770813,  -0.030639648,  -0.051208496,  0.005279541, -0.0109939575

#   ]


"""
### Embed multiple texts

You can embed multiple texts for indexing with `embedDocuments`. The internals used for this method may (but do not have to) differ from embedding queries:
"""

const text2 = "LangGraph is a library for building stateful, multi-actor applications with LLMs";

const vectors = await embeddings.embedDocuments([text, text2]);

console.log(vectors[0].slice(0, 100));
console.log(vectors[1].slice(0, 100));
# Output:
#   [

#      -0.028869629,  -0.030410767,   -0.099121094,    -0.07116699,  -0.012748718,

#     -0.0059432983,   -0.04360962,     0.07965088,   -0.027114868,   0.057403564,

#      -0.013549805,   0.014480591,    0.021697998,   -0.026870728,  0.0071983337,

#     -0.0099105835, -0.0034332275,   -0.026031494,    -0.05206299,  -0.045288086,

#       0.027450562,  -0.060333252,   -0.019210815,    0.039794922,  0.0055351257,

#       0.046325684,   0.017837524,   -0.012619019,    0.023147583,  -0.008201599,

#       0.022155762,  -0.035888672,    0.016921997,    0.027679443,  -0.023605347,

#     -0.0022029877,  -0.025253296,    0.013076782,   0.0049705505, -0.0024280548,

#       0.021957397,  -0.008644104, -0.00004029274,   -0.003501892,  -0.012641907,

#        0.01600647,  -0.014312744,   -0.037841797,    0.011764526,  -0.019622803,

#       -0.01928711,  -0.017044067,   -0.017547607,    0.028533936,  -0.019073486,

#     -0.0061073303,  -0.024520874,     0.01638794,    0.017852783, -0.0013303757,

#      -0.023040771,   -0.01713562,    0.027786255,    -0.02583313,    0.03060913,

#     0.00013923645,    0.01977539,    0.025283813, -0.00068569183,   0.032806396,

#      -0.021392822,  -0.016174316,    0.016464233,    0.006023407, -0.0025043488,

#      -0.033813477,   0.023269653,    0.012329102,    0.030334473,   0.014419556,

#      -0.026245117,  -0.018356323,   -0.016433716,    0.022628784,  -0.024108887,

#        0.02897644,  -0.017105103,   -0.009208679,   -0.015541077,  -0.020004272,

#      -0.005153656,    0.03741455,   -0.050750732,    0.012176514,  -0.017501831,

#      -0.014503479,  0.0052223206,    -0.03250122,    0.008666992,  -0.015823364

#   ]

#   [

#      -0.047332764, -0.049957275,   -0.07458496,  -0.034332275,   -0.057922363,

#     -0.0112838745,  -0.06994629,    0.06347656,   -0.03326416,    0.019897461,

#         0.0103302,   0.04660034,  -0.059753418,  -0.027511597,    0.012245178,

#       -0.03164673, -0.010215759,   -0.00687027,   -0.03314209,   -0.019866943,

#       0.008399963, -0.042144775,   -0.03781128,   0.025970459,    0.007335663,

#        0.04107666, -0.015991211,     0.0158844,  -0.008483887,   -0.008399963,

#        0.01777649,  -0.01109314,    0.01864624,   0.014328003,   -0.005264282,

#       0.077697754,  0.017684937,  0.0020427704,   0.032470703,  -0.0029354095,

#       0.003063202, 0.0008301735,   0.016281128,  -0.005897522,   -0.023254395,

#       0.004043579, -0.021987915,  -0.015419006,  0.0009803772,    0.044677734,

#     -0.0045814514, 0.0039901733,  -0.019058228,   0.063964844,   -0.012496948,

#      -0.027755737,   0.01574707,   -0.03781128,  0.0038909912, -0.00002193451,

#     0.00013685226,  0.027832031,   0.015182495,  -0.008590698,     0.03933716,

#     -0.0020141602, -0.050567627,    0.02017212,   0.020523071,     0.07287598,

#      0.0031375885,  -0.05227661,   -0.01838684, -0.0019626617,  -0.0039482117,

#        0.02494812, 0.0009508133,   0.008583069,    0.02923584,    0.028198242,

#      -0.030334473, -0.014076233,  -0.017990112,  0.0026245117,   -0.017150879,

#       0.004497528,  -0.00365448, -0.0012168884,   0.011741638,    0.012886047,

#     0.00084400177,  0.060638428,  -0.024002075,   0.022415161,   -0.015823364,

#     -0.0026760101,  0.028625488,   0.041015625,   0.006893158,    -0.01902771

#   ]


"""
## API reference

For detailed documentation of all CohereEmbeddings features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_cohere.CohereEmbeddings.html
"""



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/deepinfra.mdx
================================================
---
sidebar_label: DeepInfra
---

# DeepInfra Embeddings

The `DeepInfraEmbeddings` class utilizes the DeepInfra API to generate embeddings for given text inputs. This guide will walk you through the setup and usage of the `DeepInfraEmbeddings` class, helping you integrate it into your project seamlessly.

## Installation

Install the `@langchain/community` package as shown below:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm i @langchain/community @langchain/core
```

## Initialization

With this integration, you can use the DeepInfra embeddings model to get embeddings for your text data. Here is the [link](https://deepinfra.com/models/embeddings) to the embeddings models.

First, you need to sign up on the DeepInfra website and get the API token from [here](https://deepinfra.com/dash/api_keys). You can copy names from the model cards and start using them in your code.

To use the `DeepInfraEmbeddings` class, you need an API token from DeepInfra. You can pass this token directly to the constructor or set it as an environment variable (`DEEPINFRA_API_TOKEN`).

### Basic Usage

Here’s how to create an instance of `DeepInfraEmbeddings`:

```typescript
import { DeepInfraEmbeddings } from "@langchain/community/embeddings/deepinfra";

const embeddings = new DeepInfraEmbeddings({
  apiToken: "YOUR_API_TOKEN",
  modelName: "sentence-transformers/clip-ViT-B-32", // Optional, defaults to "sentence-transformers/clip-ViT-B-32"
  batchSize: 1024, // Optional, defaults to 1024
});
```

If the `apiToken` is not provided, it will be read from the `DEEPINFRA_API_TOKEN` environment variable.

## Generating Embeddings

### Embedding a Single Query

To generate embeddings for a single text query, use the `embedQuery` method:

```typescript
const embedding = await embeddings.embedQuery(
  "What would be a good company name for a company that makes colorful socks?"
);
console.log(embedding);
```

### Embedding Multiple Documents

To generate embeddings for multiple documents, use the `embedDocuments` method. This method will handle batching automatically based on the `batchSize` parameter:

```typescript
const documents = [
  "Document 1 text...",
  "Document 2 text...",
  "Document 3 text...",
];

const embeddingsArray = await embeddings.embedDocuments(documents);
console.log(embeddingsArray);
```

## Customizing Requests

You can customize the base URL the SDK sends requests to by passing a `configuration` parameter:

```typescript
const customEmbeddings = new DeepInfraEmbeddings({
  apiToken: "YOUR_API_TOKEN",
  configuration: {
    baseURL: "https://your_custom_url.com",
  },
});
```

This allows you to route requests through a custom endpoint if needed.

## Error Handling

If the API token is not provided and cannot be found in the environment variables, an error will be thrown:

```typescript
try {
  const embeddings = new DeepInfraEmbeddings();
} catch (error) {
  console.error("DeepInfra API token not found");
}
```

## Example

Here’s a complete example of how to set up and use the `DeepInfraEmbeddings` class:

```typescript
import { DeepInfraEmbeddings } from "@langchain/community/embeddings/deepinfra";

const embeddings = new DeepInfraEmbeddings({
  apiToken: "YOUR_API_TOKEN",
  modelName: "sentence-transformers/clip-ViT-B-32",
  batchSize: 512,
});

async function runExample() {
  const queryEmbedding = await embeddings.embedQuery("Example query text.");
  console.log("Query Embedding:", queryEmbedding);

  const documents = ["Text 1", "Text 2", "Text 3"];
  const documentEmbeddings = await embeddings.embedDocuments(documents);
  console.log("Document Embeddings:", documentEmbeddings);
}

runExample();
```

## Feedback and Support

For feedback or questions, please contact [feedback@deepinfra.com](mailto:feedback@deepinfra.com).

## Related

- Embedding model [conceptual guide](/docs/concepts/embedding_models)
- Embedding model [how-to guides](/docs/how_to/#embedding-models)



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/fireworks.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Fireworks
---
"""

"""
# FireworksEmbeddings

This will help you get started with FireworksEmbeddings [embedding models](/docs/concepts/embedding_models) using LangChain. For detailed documentation on `FireworksEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_community_embeddings_fireworks.FireworksEmbeddings.html).

## Overview
### Integration details

| Class | Package | Local | [Py support](https://python.langchain.com/docs/integrations/text_embedding/fireworks/) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: |
| [FireworksEmbeddings](https://api.js.langchain.com/classes/langchain_community_embeddings_fireworks.FireworksEmbeddings.html) | [@langchain/community](https://api.js.langchain.com/modules/langchain_community_embeddings_fireworks.html) | ❌ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/community?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |

## Setup

To access Fireworks embedding models you'll need to create a Fireworks account, get an API key, and install the `@langchain/community` integration package.

### Credentials

Head to [fireworks.ai](https://fireworks.ai/) to sign up to `Fireworks` and generate an API key. Once you've done this set the `FIREWORKS_API_KEY` environment variable:

```bash
export FIREWORKS_API_KEY="your-api-key"
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain `FireworksEmbeddings` integration lives in the `@langchain/community` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core
</Npm2Yarn>
```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

import { FireworksEmbeddings } from "@langchain/community/embeddings/fireworks";

const embeddings = new FireworksEmbeddings({
  modelName: "nomic-ai/nomic-embed-text-v1.5",
});

"""
## Indexing and Retrieval

Embedding models are often used in retrieval-augmented generation (RAG) flows, both as part of indexing data as well as later retrieving it. For more detailed instructions, please see our RAG tutorials under the [working with external knowledge tutorials](/docs/tutorials/#working-with-external-knowledge).

Below, see how to index and retrieve data using the `embeddings` object we initialized above. In this example, we will index and retrieve a sample document using the demo [`MemoryVectorStore`](/docs/integrations/vectorstores/memory).
"""

// Create a vector store with a sample text
import { MemoryVectorStore } from "langchain/vectorstores/memory";

const text = "LangChain is the framework for building context-aware reasoning applications";

const vectorstore = await MemoryVectorStore.fromDocuments(
  [{ pageContent: text, metadata: {} }],
  embeddings,
);

// Use the vector store as a retriever that returns a single document
const retriever = vectorstore.asRetriever(1);

// Retrieve the most similar text
const retrievedDocuments = await retriever.invoke("What is LangChain?");

retrievedDocuments[0].pageContent;
# Output:
#   [32m"LangChain is the framework for building context-aware reasoning applications"[39m

"""
## Direct Usage

Under the hood, the vectorstore and retriever implementations are calling `embeddings.embedDocument(...)` and `embeddings.embedQuery(...)` to create embeddings for the text(s) used in `fromDocuments` and the retriever's `invoke` operations, respectively.

You can directly call these methods to get embeddings for your own use cases.

### Embed single texts

You can embed queries for search with `embedQuery`. This generates a vector representation specific to the query:
"""

const singleVector = await embeddings.embedQuery(text);

console.log(singleVector.slice(0, 100));
# Output:
#   [

#           0.01666259765625,      0.011688232421875,          -0.1181640625,

#             -0.10205078125,       0.05438232421875,      -0.08905029296875,

#         -0.018096923828125,    0.00952911376953125,         -0.08056640625,

#        -0.0283050537109375,   -0.01512908935546875,     0.0312042236328125,

#           0.08197021484375,      0.022552490234375,  0.0012683868408203125,

#            0.0133056640625,      -0.04327392578125,  -0.004322052001953125,

#          -0.02410888671875, -0.0012350082397460938,      -0.04632568359375,

#           0.02996826171875,    -0.0134124755859375,     -0.037811279296875,

#           0.07672119140625,      0.021759033203125,     0.0179290771484375,

#     -0.0002741813659667969,       -0.0582275390625,    -0.0224456787109375,

#      0.0027675628662109375,     -0.017425537109375,   -0.01520538330078125,

#       -0.01146697998046875,     -0.055267333984375,           -0.083984375,

#          0.056793212890625,  -0.003383636474609375,     -0.034271240234375,

#           0.05108642578125,   -0.01018524169921875,        0.0462646484375,

#      0.0012178421020507812,   0.005779266357421875,        0.0684814453125,

#        0.00797271728515625,    -0.0176544189453125,    0.00257110595703125,

#          0.059539794921875,      -0.06573486328125,        -0.075439453125,

#        -0.0247344970703125,    -0.0276947021484375,   0.003940582275390625,

#           0.02630615234375,        0.0660400390625,        0.0157470703125,

#          0.033050537109375,          -0.0478515625,      -0.03338623046875,

#          0.050384521484375,       0.07757568359375,        -0.045166015625,

#           0.07586669921875,  0.0021915435791015625,     0.0237579345703125,

#         -0.052703857421875,       0.05023193359375,    -0.0274810791015625,

#     -0.0025081634521484375,         0.019287109375,      -0.03802490234375,

#         0.0216217041015625,      0.025360107421875,         -0.04443359375,

#         -0.029205322265625,  -0.002414703369140625,      0.027130126953125,

#          0.028961181640625,         0.078857421875, -0.0009660720825195312,

#          0.017608642578125,       0.05755615234375,    -0.0285797119140625,

#         0.0039215087890625,  -0.006908416748046875,      -0.05364990234375,

#       -0.01342010498046875,       -0.0247802734375,       0.08331298828125,

#          0.032928466796875,    0.00543975830078125,    -0.0168304443359375,

#         -0.050018310546875,         -0.05908203125,      0.031951904296875,

#        -0.0200347900390625,      0.019134521484375,     -0.018035888671875,

#       -0.01178741455078125

#   ]


"""
### Embed multiple texts

You can embed multiple texts for indexing with `embedDocuments`. The internals used for this method may (but do not have to) differ from embedding queries:
"""

const text2 = "LangGraph is a library for building stateful, multi-actor applications with LLMs";

const vectors = await embeddings.embedDocuments([text, text2]);

console.log(vectors[0].slice(0, 100));
console.log(vectors[1].slice(0, 100));
# Output:
#   [

#          0.016632080078125,    0.01165008544921875,          -0.1181640625,

#          -0.10186767578125,       0.05438232421875,      -0.08905029296875,

#        -0.0180511474609375,    0.00957489013671875,         -0.08056640625,

#              -0.0283203125,    -0.0151214599609375,        0.0311279296875,

#           0.08184814453125,     0.0225982666015625,  0.0012750625610351562,

#           0.01336669921875,     -0.043365478515625,  -0.004322052001953125,

#          -0.02410888671875, -0.0012559890747070312,     -0.046356201171875,

#         0.0298919677734375,     -0.013458251953125,      -0.03765869140625,

#           0.07672119140625,     0.0217132568359375,     0.0179290771484375,

#     -0.0002269744873046875,       -0.0582275390625,          -0.0224609375,

#       0.002834320068359375,    -0.0174407958984375,   -0.01512908935546875,

#       -0.01146697998046875,     -0.055206298828125,      -0.08404541015625,

#            0.0567626953125, -0.0033092498779296875,     -0.034271240234375,

#           0.05108642578125,     -0.010101318359375,      0.046173095703125,

#      0.0011806488037109375,      0.005706787109375,       0.06854248046875,

#         0.0079193115234375,    -0.0176239013671875,   0.002552032470703125,

#          0.059539794921875,      -0.06573486328125,      -0.07537841796875,

#          -0.02484130859375,     -0.027740478515625,   0.003925323486328125,

#               0.0263671875,        0.0660400390625,     0.0156402587890625,

#          0.033050537109375,     -0.047821044921875,       -0.0333251953125,

#          0.050445556640625,       0.07757568359375,     -0.045257568359375,

#           0.07586669921875,  0.0021724700927734375,     0.0237274169921875,

#         -0.052703857421875,      0.050323486328125,       -0.0274658203125,

#     -0.0024662017822265625,     0.0194244384765625,      -0.03802490234375,

#           0.02166748046875,      0.025360107421875,     -0.044464111328125,

#        -0.0292816162109375, -0.0025119781494140625,     0.0271148681640625,

#          0.028961181640625,         0.078857421875, -0.0008907318115234375,

#          0.017669677734375,           0.0576171875,    -0.0285797119140625,

#         0.0039825439453125,   -0.00687408447265625,       -0.0535888671875,

#        -0.0134735107421875,    -0.0247650146484375,        0.0831298828125,

#          0.032989501953125,   0.005443572998046875,    -0.0167999267578125,

#         -0.050018310546875,     -0.059051513671875,        0.0318603515625,

#        -0.0200958251953125,     0.0191192626953125,    -0.0180206298828125,

#       -0.01175689697265625

#   ]

#   [

#          -0.02667236328125,      0.036651611328125,         -0.1630859375,

#           -0.0904541015625,     -0.022430419921875,       -0.095458984375,

#         -0.037628173828125,    0.00473785400390625,    -0.046051025390625,

#         0.0109710693359375,        0.0113525390625,    0.0254364013671875,

#           0.09368896484375,     0.0144195556640625, -0.007564544677734375,

#     -0.0014705657958984375, -0.0007691383361816406,    -0.015716552734375,

#        -0.0242156982421875,     -0.024871826171875,      0.00885009765625,

#      0.0012922286987304688,      0.023712158203125,    -0.054595947265625,

#           0.06329345703125,        0.0289306640625,    0.0233612060546875,

#           -0.0374755859375,       -0.0489501953125,    -0.029510498046875,

#         0.0173492431640625,     0.0171356201171875,     -0.01629638671875,

#           -0.0352783203125,     -0.039398193359375,     -0.11138916015625,

#         0.0296783447265625,   -0.01467132568359375, 0.0009188652038574219,

#          0.048187255859375,     -0.010650634765625,               0.03125,

#       0.005214691162109375,        -0.015869140625,      0.06939697265625,

#           -0.0428466796875,     0.0266571044921875,        0.044189453125,

#          0.049957275390625,     -0.054290771484375,    0.0107574462890625,

#          -0.03265380859375,    -0.0109100341796875,   -0.0144805908203125,

#           0.03936767578125,       0.07989501953125,    -0.056976318359375,

#         0.0308380126953125,     -0.035125732421875,    -0.038848876953125,

#           0.10748291015625,       0.01129150390625,      -0.0665283203125,

#           0.09710693359375,       0.03143310546875,   -0.0104522705078125,

#         -0.062469482421875,      0.021148681640625,     -0.00970458984375,

#          -0.06756591796875,       0.01019287109375,      0.00433349609375,

#          0.032928466796875,      0.020233154296875,     -0.01336669921875,

#         -0.015472412109375,    -0.0175933837890625,   -0.0142364501953125,

#      -0.007450103759765625,          0.03759765625,  0.003551483154296875,

#            0.0069580078125,      0.042266845703125, -0.007488250732421875,

#           0.01922607421875,         0.007080078125,   -0.0255889892578125,

#      -0.007686614990234375,       -0.0848388671875,     0.058563232421875,

#          0.021148681640625,      0.034393310546875,   0.01087188720703125,

#        -0.0196380615234375,      -0.09515380859375,       0.0054931640625,

#         -0.012481689453125,   0.003322601318359375,    -0.019683837890625,

#        -0.0307159423828125

#   ]


"""
## API reference

For detailed documentation of all FireworksEmbeddings features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_community_embeddings_fireworks.FireworksEmbeddings.html
"""



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/google_generativeai.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Google Generative AI
---
"""

"""
# GoogleGenerativeAIEmbeddings

This will help you get started with Google Generative AI [embedding models](/docs/concepts/embedding_models) using LangChain. For detailed documentation on `GoogleGenerativeAIEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_google_genai.GoogleGenerativeAIEmbeddings.html).

## Overview
### Integration details

| Class | Package | Local | [Py support](https://python.langchain.com/docs/integrations/text_embedding/google_generative_ai/) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: |
| [`GoogleGenerativeAIEmbeddings`](https://api.js.langchain.com/classes/langchain_google_genai.GoogleGenerativeAIEmbeddings.html) | [`@langchain/google-genai`](https://npmjs.com/@langchain/google-genai) | ❌ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/google-genai?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/google-genai?style=flat-square&label=%20&) |

## Setup

To access Google Generative AI embedding models you'll need to sign up for a Google AI account, get an API key, and install the `@langchain/google-genai` integration package.

### Credentials

Get an API key here: https://ai.google.dev/tutorials/setup.

Next, set your key as an environment variable named `GOOGLE_API_KEY`:

```bash
export GOOGLE_API_KEY="your-api-key"
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain `GoogleGenerativeAIEmbeddings` integration lives in the `@langchain/google-genai` package. You may also wish to install the official SDK:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/google-genai @langchain/core @google/generative-ai
</Npm2Yarn>
```
"""

"""
## Instantiation

Now we can instantiate our model object and embed text:
"""

import { GoogleGenerativeAIEmbeddings } from "@langchain/google-genai";
import { TaskType } from "@google/generative-ai";

const embeddings = new GoogleGenerativeAIEmbeddings({
  model: "text-embedding-004", // 768 dimensions
  taskType: TaskType.RETRIEVAL_DOCUMENT,
  title: "Document title",
});

"""
## Indexing and Retrieval

Embedding models are often used in retrieval-augmented generation (RAG) flows, both as part of indexing data as well as later retrieving it. For more detailed instructions, please see our RAG tutorials under the [working with external knowledge tutorials](/docs/tutorials/#working-with-external-knowledge).

Below, see how to index and retrieve data using the `embeddings` object we initialized above. In this example, we will index and retrieve a sample document using the demo [`MemoryVectorStore`](/docs/integrations/vectorstores/memory).
"""

// Create a vector store with a sample text
import { MemoryVectorStore } from "langchain/vectorstores/memory";

const text = "LangChain is the framework for building context-aware reasoning applications";

const vectorstore = await MemoryVectorStore.fromDocuments(
  [{ pageContent: text, metadata: {} }],
  embeddings,
);

// Use the vector store as a retriever that returns a single document
const retriever = vectorstore.asRetriever(1);

// Retrieve the most similar text
const retrievedDocuments = await retriever.invoke("What is LangChain?");

retrievedDocuments[0].pageContent;
# Output:
#   LangChain is the framework for building context-aware reasoning applications


"""
## Direct Usage

Under the hood, the vectorstore and retriever implementations are calling `embeddings.embedDocument(...)` and `embeddings.embedQuery(...)` to create embeddings for the text(s) used in `fromDocuments` and the retriever's `invoke` operations, respectively.

You can directly call these methods to get embeddings for your own use cases.

### Embed single texts

You can embed queries for search with `embedQuery`. This generates a vector representation specific to the query:
"""

const singleVector = await embeddings.embedQuery(text);

console.log(singleVector.slice(0, 100));
# Output:
#   [

#     -0.018286658,   0.020051053,  -0.057487167,   0.0059406986, -0.0036901247,

#     -0.010400916,    0.03396853,  -0.010867519,    0.015650319,   0.026443942,

#      0.012251757,   -0.01581729,    0.02031182, -0.00062176475,  0.0065521155,

#      -0.07107355,   0.033614952,    0.07109807,   -0.021078493,   0.048039366,

#      0.022973344,    -0.0361746,   -0.04550704,   -0.048807852,    0.03414146,

#      0.042450827,    0.02930612,   0.027274853,   -0.027707053,   -0.04167595,

#       0.01708843,   0.028532283, -0.0018593844,      -0.096786,  -0.034648854,

#     0.0013152987,   0.024425535,    0.04937838,    0.036890924,  -0.074619934,

#     -0.028723065,   0.029158255,  -0.023993572,     0.03163398,   -0.02036324,

#      -0.02333609,  -0.017407075, -0.0059643993,    -0.05564625,   0.051022638,

#       0.03264913,  -0.008254581,  -0.030552095,    0.072952054,   -0.05448913,

#      0.012030814,   -0.07978849,  -0.030417662,   0.0038343794,    0.03237516,

#     -0.054259773,    -0.0524064,   -0.02145499,    0.006439614,    0.04988943,

#      -0.03232189,    0.00990776,   -0.03863326,    -0.04979561,   0.009874035,

#      -0.02617946,    0.02135152,  -0.070599854,     0.08655627,   -0.02080979,

#     -0.014944934,  0.0034440767,  -0.035236854,    0.027093545,   0.032249685,

#      -0.03559674,   0.046849757,    0.06965356,    0.028780492,    0.02865287,

#      -0.07999455, -0.0058599655,  -0.050316703,   -0.018346578,  -0.038311094,

#       0.08026719,   0.049136136,   -0.05372233,  -0.0062247813,    0.01791339,

#      -0.03635157,  -0.031860247,  -0.031322744,    0.044055287,   0.034934316

#   ]


"""
### Embed multiple texts

You can embed multiple texts for indexing with `embedDocuments`. The internals used for this method may (but do not have to) differ from embedding queries:
"""

const text2 = "LangGraph is a library for building stateful, multi-actor applications with LLMs";

const vectors = await embeddings.embedDocuments([text, text2]);

console.log(vectors[0].slice(0, 100));
console.log(vectors[1].slice(0, 100));
# Output:
#   [

#     -0.018286658,   0.020051053,  -0.057487167,   0.0059406986, -0.0036901247,

#     -0.010400916,    0.03396853,  -0.010867519,    0.015650319,   0.026443942,

#      0.012251757,   -0.01581729,    0.02031182, -0.00062176475,  0.0065521155,

#      -0.07107355,   0.033614952,    0.07109807,   -0.021078493,   0.048039366,

#      0.022973344,    -0.0361746,   -0.04550704,   -0.048807852,    0.03414146,

#      0.042450827,    0.02930612,   0.027274853,   -0.027707053,   -0.04167595,

#       0.01708843,   0.028532283, -0.0018593844,      -0.096786,  -0.034648854,

#     0.0013152987,   0.024425535,    0.04937838,    0.036890924,  -0.074619934,

#     -0.028723065,   0.029158255,  -0.023993572,     0.03163398,   -0.02036324,

#      -0.02333609,  -0.017407075, -0.0059643993,    -0.05564625,   0.051022638,

#       0.03264913,  -0.008254581,  -0.030552095,    0.072952054,   -0.05448913,

#      0.012030814,   -0.07978849,  -0.030417662,   0.0038343794,    0.03237516,

#     -0.054259773,    -0.0524064,   -0.02145499,    0.006439614,    0.04988943,

#      -0.03232189,    0.00990776,   -0.03863326,    -0.04979561,   0.009874035,

#      -0.02617946,    0.02135152,  -0.070599854,     0.08655627,   -0.02080979,

#     -0.014944934,  0.0034440767,  -0.035236854,    0.027093545,   0.032249685,

#      -0.03559674,   0.046849757,    0.06965356,    0.028780492,    0.02865287,

#      -0.07999455, -0.0058599655,  -0.050316703,   -0.018346578,  -0.038311094,

#       0.08026719,   0.049136136,   -0.05372233,  -0.0062247813,    0.01791339,

#      -0.03635157,  -0.031860247,  -0.031322744,    0.044055287,   0.034934316

#   ]

#   [

#       0.011669316,    0.02170385,   -0.07519182,     0.003981285,

#      0.0053525288,   0.008397044,   0.036672726,     0.016549919,

#       0.061946314,    0.06280753,  -0.009199135,     0.014644887,

#       0.046459496,  0.0122919325,  -0.013300706,    -0.051746193,

#        -0.0490098,   0.045586824,   -0.05053146,     0.044294067,

#      -0.012607168, -0.0071777054,  -0.048455723,    -0.075109236,

#       0.013327612,  -0.025612017,   0.050875787,     0.030091539,

#      -0.027163379,   -0.05760821,   0.014368641,    0.0044602253,

#       0.035219245,  -0.033304706,  -0.045474708,    -0.038022216,

#       0.012366698,   0.028978042,   0.038591366,     -0.10646444,

#      -0.036803752,   0.018911313,   0.005681761,     0.025365992,

#      -0.017165288, -0.0048005017,  -0.011460135,    0.0027811683,

#       -0.04971402, -0.0019232291,    0.02141983,   -0.0013272346,

#       -0.03337951,   0.030568397,   -0.05704511,     -0.01187748,

#      -0.025354648,   0.016188234,  -0.022018699,    0.0096449675,

#      -0.027020318,  -0.038059015,  -0.024455398,     0.021858294,

#       0.010713859,   -0.07203855,   -0.05562406, 0.0000034690818,

#      -0.054289237, -0.0027928432, -0.0010051605,     0.008493095,

#      -0.064746305,   0.024419345,  -0.016629996,     -0.02686531,

#       -0.02300653,   -0.03263113,   0.019998727,     0.029680967,

#       -0.04365641,   0.013594972,   0.056486532,     0.025913332,

#       0.025457978,  -0.048536208,   0.020046104,     -0.05857287,

#      -0.032664414,  -0.032940287,    0.10053288,    -0.021389635,

#     -0.0044220444,   0.037026003,    0.03142132,    -0.048912503,

#       -0.07961264,  -0.051056523,   0.048032805,      0.04831778

#   ]


"""
## API reference

For detailed documentation of all `GoogleGenerativeAIEmbeddings` features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_google_genai.GoogleGenerativeAIEmbeddings.html
"""



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/google_vertex_ai.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Google Vertex AI
---
"""

"""
# VertexAIEmbeddings

[Google Vertex](https://cloud.google.com/vertex-ai) is a service that exposes all foundation models available in Google Cloud.

This will help you get started with Google Vertex AI [embedding models](/docs/concepts/embedding_models) using LangChain. For detailed documentation on `VertexAIEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_google_vertexai.GoogleVertexAIEmbeddings.html).

## Overview
### Integration details

| Class | Package | Local | [Py support](https://python.langchain.com/docs/integrations/text_embedding/google_vertex_ai_palm/) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: |
| [`VertexAIEmbeddings`](https://api.js.langchain.com/classes/langchain_google_vertexai.GoogleVertexAIEmbeddings.html) | [`@langchain/google-vertexai`](https://npmjs.com/@langchain/google-vertexai) | ❌ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/google-vertexai?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/google-vertexai?style=flat-square&label=%20&) |

## Setup

LangChain.js supports two different authentication methods based on whether
you're running in a Node.js environment or a web environment.

To access `ChatVertexAI` models you'll need to setup Google VertexAI in your Google Cloud Platform (GCP) account, save the credentials file, and install the `@langchain/google-vertexai` integration package.

### Credentials

Head to your [GCP account](https://console.cloud.google.com/) and generate a credentials file. Once you've done this set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable:

```bash
export GOOGLE_APPLICATION_CREDENTIALS="path/to/your/credentials.json"
```

If running in a web environment, you should set the `GOOGLE_VERTEX_AI_WEB_CREDENTIALS` environment variable as a JSON stringified object, and install the `@langchain/google-vertexai-web` package:

```bash
GOOGLE_VERTEX_AI_WEB_CREDENTIALS={"type":"service_account","project_id":"YOUR_PROJECT-12345",...}
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain `VertexAIEmbeddings` integration lives in the `@langchain/google-vertexai` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/google-vertexai @langchain/core
</Npm2Yarn>
```
"""

"""
## Instantiation

Now we can instantiate our model object and embed text:
"""

import { VertexAIEmbeddings } from "@langchain/google-vertexai";
// Uncomment the following line if you're running in a web environment:
// import { VertexAIEmbeddings } from "@langchain/google-vertexai-web"

const embeddings = new VertexAIEmbeddings({
  model: "text-embedding-004",
  // ...
});

"""
## Indexing and Retrieval

Embedding models are often used in retrieval-augmented generation (RAG) flows, both as part of indexing data as well as later retrieving it. For more detailed instructions, please see our RAG tutorials under the [working with external knowledge tutorials](/docs/tutorials/#working-with-external-knowledge).

Below, see how to index and retrieve data using the `embeddings` object we initialized above. In this example, we will index and retrieve a sample document using the demo [`MemoryVectorStore`](/docs/integrations/vectorstores/memory).
"""

// Create a vector store with a sample text
import { MemoryVectorStore } from "langchain/vectorstores/memory";

const text = "LangChain is the framework for building context-aware reasoning applications";

const vectorstore = await MemoryVectorStore.fromDocuments(
  [{ pageContent: text, metadata: {} }],
  embeddings,
);

// Use the vector store as a retriever that returns a single document
const retriever = vectorstore.asRetriever(1);

// Retrieve the most similar text
const retrievedDocuments = await retriever.invoke("What is LangChain?");

retrievedDocuments[0].pageContent;
# Output:
#   LangChain is the framework for building context-aware reasoning applications


"""
## Direct Usage

Under the hood, the vectorstore and retriever implementations are calling `embeddings.embedDocument(...)` and `embeddings.embedQuery(...)` to create embeddings for the text(s) used in `fromDocuments` and the retriever's `invoke` operations, respectively.

You can directly call these methods to get embeddings for your own use cases.

### Embed single texts

You can embed queries for search with `embedQuery`. This generates a vector representation specific to the query:
"""

const singleVector = await embeddings.embedQuery(text);

console.log(singleVector.slice(0, 100));
# Output:
#   [

#       -0.02831101417541504,   0.022063178941607475,  -0.07454229146242142,

#       0.006448323838412762,   0.001955120824277401, -0.017617391422390938,

#          0.018649872392416,   -0.05262855067849159, 0.0006953597767278552,

#     -0.0018249079585075378,   0.022437218576669693, 0.0036489504855126143,

#      0.0018086736090481281,   0.016940006986260414, -0.007894322276115417,

#       -0.04187627509236336,   0.039501357823610306,   0.06918870657682419,

#      -0.006931832991540432,   0.049655742943286896,  0.021211417391896248,

#      -0.029322246089577675,   -0.04546992480754852,  -0.01769082061946392,

#       0.046703994274139404,    0.03127637133002281,  0.006355373188853264,

#       0.014901148155331612,  -0.006893016863614321,  -0.05992589890956879,

#      -0.009733330458402634,   0.015709295868873596, -0.017982766032218933,

#        -0.0852997675538063,  -0.032453566789627075, 0.0014507169835269451,

#        0.03345133736729622,   0.048862338066101074,  0.006664620712399483,

#       -0.06287197023630142,   -0.02109423652291298,  0.018176473677158356,

#      -0.022175665944814682,    0.03340170532464981, -0.008905526250600815,

#       -0.03492079675197601,   -0.03819998353719711,  -0.05230168625712395,

#       -0.05247239023447037,   0.048254698514938354,  0.046494755893945694,

#      -0.029708227142691612,  -0.002180763054639101,  0.051957979798316956,

#       -0.05483679473400116,    0.00700812041759491,  -0.08181990683078766,

#       -0.02295914851129055,   0.026530204340815544,   0.04028692841529846,

#       -0.05230272561311722,  -0.057705819606781006, -0.015022763051092625,

#          0.002143724123016,    0.06361843645572662, -0.027828887104988098,

#       0.006870461627840996,  -0.016140831634402275, -0.034440942108631134,

#      -0.004059414379298687,  -0.042537953704595566,  -0.00984653178602457,

#       -0.07701274752616882,    0.09815558046102524, -0.025801729410886765,

#      -0.008693721145391464, -0.0010926402173936367, -0.027235493063926697,

#        0.06945550441741943,   0.023456251248717308,  -0.02160717360675335,

#        0.03252667561173439,    0.05874639376997948, -0.001329384627752006,

#        0.03664775192737579,   -0.07353461533784866, -0.028453022241592407,

#       -0.05666429176926613,  -0.012955721467733383, -0.041723109781742096,

#        0.07209191471338272,     0.0326194241642952,   -0.0496046207845211,

#      -0.025037819519639015,   0.004625750705599785,  -0.03622527793049812,

#      -0.022546149790287018,  0.0053468807600438595,   0.03879072889685631,

#        0.03238753229379654

#   ]


"""
### Embed multiple texts

You can embed multiple texts for indexing with `embedDocuments`. The internals used for this method may (but do not have to) differ from embedding queries:
"""

const text2 = "LangGraph is a library for building stateful, multi-actor applications with LLMs";

const vectors = await embeddings.embedDocuments([text, text2]);

console.log(vectors[0].slice(0, 100));
console.log(vectors[1].slice(0, 100));
# Output:
#   [

#       -0.02831101417541504,   0.022063178941607475,  -0.07454229146242142,

#       0.006448323838412762,   0.001955120824277401, -0.017617391422390938,

#          0.018649872392416,   -0.05262855067849159, 0.0006953597767278552,

#     -0.0018249079585075378,   0.022437218576669693, 0.0036489504855126143,

#      0.0018086736090481281,   0.016940006986260414, -0.007894322276115417,

#       -0.04187627509236336,   0.039501357823610306,   0.06918870657682419,

#      -0.006931832991540432,   0.049655742943286896,  0.021211417391896248,

#      -0.029322246089577675,   -0.04546992480754852,  -0.01769082061946392,

#       0.046703994274139404,    0.03127637133002281,  0.006355373188853264,

#       0.014901148155331612,  -0.006893016863614321,  -0.05992589890956879,

#      -0.009733330458402634,   0.015709295868873596, -0.017982766032218933,

#        -0.0852997675538063,  -0.032453566789627075, 0.0014507169835269451,

#        0.03345133736729622,   0.048862338066101074,  0.006664620712399483,

#       -0.06287197023630142,   -0.02109423652291298,  0.018176473677158356,

#      -0.022175665944814682,    0.03340170532464981, -0.008905526250600815,

#       -0.03492079675197601,   -0.03819998353719711,  -0.05230168625712395,

#       -0.05247239023447037,   0.048254698514938354,  0.046494755893945694,

#      -0.029708227142691612,  -0.002180763054639101,  0.051957979798316956,

#       -0.05483679473400116,    0.00700812041759491,  -0.08181990683078766,

#       -0.02295914851129055,   0.026530204340815544,   0.04028692841529846,

#       -0.05230272561311722,  -0.057705819606781006, -0.015022763051092625,

#          0.002143724123016,    0.06361843645572662, -0.027828887104988098,

#       0.006870461627840996,  -0.016140831634402275, -0.034440942108631134,

#      -0.004059414379298687,  -0.042537953704595566,  -0.00984653178602457,

#       -0.07701274752616882,    0.09815558046102524, -0.025801729410886765,

#      -0.008693721145391464, -0.0010926402173936367, -0.027235493063926697,

#        0.06945550441741943,   0.023456251248717308,  -0.02160717360675335,

#        0.03252667561173439,    0.05874639376997948, -0.001329384627752006,

#        0.03664775192737579,   -0.07353461533784866, -0.028453022241592407,

#       -0.05666429176926613,  -0.012955721467733383, -0.041723109781742096,

#        0.07209191471338272,     0.0326194241642952,   -0.0496046207845211,

#      -0.025037819519639015,   0.004625750705599785,  -0.03622527793049812,

#      -0.022546149790287018,  0.0053468807600438595,   0.03879072889685631,

#        0.03238753229379654

#   ]

#   [

#     -0.00007261172140715644,    0.03209814056754112,  -0.10099327564239502,

#      -0.0017932605696842074, -0.0016863049240782857,  0.009428824298083782,

#        0.023065969347953796,  -0.018305035308003426,   0.03765229508280754,

#         0.03357342258095741,  0.0018431750359013677,   0.03230319544672966,

#        0.024983661249279976,    0.02752346731722355, -0.027390114963054657,

#        -0.01945030689239502,   -0.05770668387413025,  0.046621184796094894,

#        -0.03308689966797829,    0.03985097259283066, -0.021250328049063683,

#       -0.001940526650287211,   -0.06034174561500549,  -0.05026412755250931,

#         0.02385033667087555,   -0.03279203176498413,   0.02966252714395523,

#         0.01294293999671936,  -0.009747475385665894,  -0.07896383106708527,

#       -0.013269499875605106,  -0.011228476651012897,  0.022224457934498787,

#       -0.018957728520035744,   -0.05092151463031769, -0.043285638093948364,

#        0.016826728358864784,   0.010665969923138618,  0.021219193935394287,

#        -0.08588971197605133,  -0.038367897272109985,  0.012244532816112041,

#        0.009497410617768764,   0.017629485577344894, 0.0013116559712216258,

#       -0.016468070447444916,   -0.04423798993229866,  -0.04043079912662506,

#        -0.05485917255282402,  -0.007577189709991217,  0.028067218139767647,

#       -0.022974666208028793,  0.0006999042234383523,  0.009812192991375923,

#        -0.05387532711029053,  -0.016531387344002724, -0.015153753571212292,

#         0.03397523611783981, -0.0018232968868687749,   0.01200891938060522,

#       -0.013123664073646069,  -0.043459296226501465,  -0.01856262981891632,

#        0.018269911408424377,   0.016155652701854706,  -0.05597233399748802,

#        -0.05852395296096802,   0.020076945424079895, -0.033808667212724686,

#       -0.008225022815167904,  -0.014589417725801468,  -0.01408824510872364,

#        -0.06293410807847977,   0.026668129488825798,  -0.01397104375064373,

#       -0.017627086490392685,   -0.03409220278263092, -0.018559949472546577,

#         0.07163946330547333,   0.015611495822668076, -0.034166790544986725,

#       -0.005098687019199133,    0.04163505882024765, -0.010681619867682457,

#        0.027817489579319954,  -0.031076539307832718, -0.006825212389230728,

#        -0.06810358166694641,   -0.03793689236044884,  -0.03981738165020943,

#         0.09524374455213547,   -0.03607913851737976,  0.003638653317466378,

#         0.02828306518495083,   0.018808560445904732, -0.047244682908058167,

#        -0.06114668399095535,   -0.02395530976355076,  0.036157332360744476,

#          0.0422002375125885

#   ]


"""
## API reference

For detailed documentation of all `VertexAIEmbeddings` features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_google_vertexai.GoogleVertexAIEmbeddings.html
"""



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/gradient_ai.mdx
================================================
---
sidebar_class_name: node-only
---

# Gradient AI

The `GradientEmbeddings` class uses the Gradient AI API to generate embeddings for a given text.

## Setup

You'll need to install the official Gradient Node SDK as a peer dependency:

```bash npm2yarn
npm i @langchain/community @langchain/core @gradientai/nodejs-sdk
```

You will need to set the following environment variables for using the Gradient AI API.

```
export GRADIENT_ACCESS_TOKEN=<YOUR_ACCESS_TOKEN>
export GRADIENT_WORKSPACE_ID=<YOUR_WORKSPACE_ID>
```

Alternatively, these can be set during the GradientAI Class instantiation as `gradientAccessKey` and `workspaceId` respectively.
For example:

```typescript
const model = new GradientEmbeddings({
  gradientAccessKey: "My secret Access Token"
  workspaceId: "My secret workspace id"
});
```

## Usage

import CodeBlock from "@theme/CodeBlock";
import GradientEmbeddingsExample from "@examples/embeddings/gradient_ai.ts";

<CodeBlock language="typescript">{GradientEmbeddingsExample}</CodeBlock>

## Related

- Embedding model [conceptual guide](/docs/concepts/embedding_models)
- Embedding model [how-to guides](/docs/how_to/#embedding-models)



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/hugging_face_inference.mdx
================================================
# HuggingFace Inference

This Embeddings integration uses the HuggingFace Inference API to generate embeddings for a given text using by default the `sentence-transformers/distilbert-base-nli-mean-tokens` model. You can pass a different model name to the constructor to use a different model.

## Setup

You'll first need to install the [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) package and the required peer dep:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core @huggingface/inference@2
```

## Usage

```typescript
import { HuggingFaceInferenceEmbeddings } from "@langchain/community/embeddings/hf";

const embeddings = new HuggingFaceInferenceEmbeddings({
  apiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.HUGGINGFACEHUB_API_KEY
});
```

## Related

- Embedding model [conceptual guide](/docs/concepts/embedding_models)
- Embedding model [how-to guides](/docs/how_to/#embedding-models)



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/ibm.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: IBM watsonx.ai
---
"""

"""
# IBM watsonx.ai


This will help you get started with IBM watsonx.ai [embedding models](/docs/concepts/embedding_models) using LangChain. For detailed documentation on `IBM watsonx.ai` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/modules/_langchain_community.embeddings_ibm.html).

## Overview
### Integration details


| Class | Package | Local | [Py support](https://python.langchain.com/docs/integrations/text_embedding/ibm_watsonx/) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: |
| [`WatsonxEmbeddings`](https://api.js.langchain.com/classes/_langchain_community.embeddings_ibm.WatsonxEmbeddings.html) | [@langchain/community](https://www.npmjs.com/package/@langchain/community)| ❌ | ✅  | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/community?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |

## Setup

To access IBM WatsonxAI embeddings you'll need to create an IBM watsonx.ai account, get an API key or any other type of credentials, and install the `@langchain/community` integration package.

### Credentials


Head to [IBM Cloud](https://cloud.ibm.com/login) to sign up to IBM watsonx.ai and generate an API key or provide any other authentication form as presented below.

#### IAM authentication

```bash
export WATSONX_AI_AUTH_TYPE=iam
export WATSONX_AI_APIKEY=<YOUR-APIKEY>
```

#### Bearer token authentication

```bash
export WATSONX_AI_AUTH_TYPE=bearertoken
export WATSONX_AI_BEARER_TOKEN=<YOUR-BEARER-TOKEN>
```

#### IBM watsonx.ai software authentication

```bash
export WATSONX_AI_AUTH_TYPE=cp4d
export WATSONX_AI_USERNAME=<YOUR_USERNAME>
export WATSONX_AI_PASSWORD=<YOUR_PASSWORD>
export WATSONX_AI_URL=<URL>
```

Once these are placed in your environment variables and object is initialized authentication will proceed automatically.

Authentication can also be accomplished by passing these values as parameters to a new instance.

## IAM authentication

```typescript
import { WatsonxEmbeddings } from "@langchain/community/embeddings/ibm";

const props = {
  version: "YYYY-MM-DD",
  serviceUrl: "<SERVICE_URL>",
  projectId: "<PROJECT_ID>",
  watsonxAIAuthType: "iam",
  watsonxAIApikey: "<YOUR-APIKEY>",
};
const instance = new WatsonxEmbeddings(props);
```

## Bearer token authentication

```typescript
import { WatsonxEmbeddings } from "@langchain/community/embeddings/ibm";

const props = {
  version: "YYYY-MM-DD",
  serviceUrl: "<SERVICE_URL>",
  projectId: "<PROJECT_ID>",
  watsonxAIAuthType: "bearertoken",
  watsonxAIBearerToken: "<YOUR-BEARERTOKEN>",
};
const instance = new WatsonxEmbeddings(props);
```

### IBM watsonx.ai software authentication

```typescript
import { WatsonxEmbeddings } from "@langchain/community/embeddings/ibm";

const props = {
  version: "YYYY-MM-DD",
  serviceUrl: "<SERVICE_URL>",
  projectId: "<PROJECT_ID>",
  watsonxAIAuthType: "cp4d",
  watsonxAIUsername: "<YOUR-USERNAME>",
  watsonxAIPassword: "<YOUR-PASSWORD>",
  watsonxAIUrl: "<url>",
};
const instance = new WatsonxEmbeddings(props);
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain IBM watsonx.ai integration lives in the `@langchain/community` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core
</Npm2Yarn>

```
"""

"""
## Instantiation

Now we can instantiate our model object and embed text:

"""

import { WatsonxEmbeddings } from "@langchain/community/embeddings/ibm";

const embeddings = new WatsonxEmbeddings({
  version: "YYYY-MM-DD",
  serviceUrl: process.env.API_URL,
  projectId: "<PROJECT_ID>",
  spaceId: "<SPACE_ID>",
  model: "<MODEL_ID>",
});

"""
Note:

- You must provide `spaceId` or `projectId` in order to proceed.
- Depending on the region of your provisioned service instance, use correct serviceUrl.
"""

"""
## Indexing and Retrieval

Embedding models are often used in retrieval-augmented generation (RAG) flows, both as part of indexing data as well as later retrieving it. For more detailed instructions, please see our RAG tutorials under the [working with external knowledge tutorials](/docs/tutorials/#working-with-external-knowledge).

Below, see how to index and retrieve data using the `embeddings` object we initialized above. In this example, we will index and retrieve a sample document using the demo [`MemoryVectorStore`](/docs/integrations/vectorstores/memory).
"""

// Create a vector store with a sample text
import { MemoryVectorStore } from "langchain/vectorstores/memory";

const text = "LangChain is the framework for building context-aware reasoning applications";

const vectorstore = await MemoryVectorStore.fromDocuments(
  [{ pageContent: text, metadata: {} }],
  embeddings,
);

// Use the vector store as a retriever that returns a single document
const retriever = vectorstore.asRetriever(1);

// Retrieve the most similar text
const retrievedDocuments = await retriever.invoke("What is LangChain?");

retrievedDocuments[0].pageContent;
# Output:
#   LangChain is the framework for building context-aware reasoning applications


"""
## Direct Usage

Under the hood, the vectorstore and retriever implementations are calling `embeddings.embedDocument(...)` and `embeddings.embedQuery(...)` to create embeddings for the text(s) used in `fromDocuments` and the retriever's `invoke` operations, respectively.

You can directly call these methods to get embeddings for your own use cases.

### Embed single texts

You can embed queries for search with `embedQuery`. This generates a vector representation specific to the query:
"""

    const singleVector = await embeddings.embedQuery(text);
    singleVector.slice(0, 10);
# Output:
#   [

#      -0.017436018,  -0.01469498,

#      -0.015685871, -0.013543149,

#     -0.0011519607, -0.008123747,

#       0.015286108, -0.023845721,

#       -0.02454774,   0.07235078

#   ]


"""
### Embed multiple texts

You can embed multiple texts for indexing with `embedDocuments`. The internals used for this method may (but do not have to) differ from embedding queries:
"""



    const text2 = "LangGraph is a library for building stateful, multi-actor applications with LLMs";

    const vectors = await embeddings.embedDocuments([text, text2]);
   
    console.log(vectors[0].slice(0, 10));
    console.log(vectors[1].slice(0, 10));

# Output:
#   [

#     -0.017436024, -0.014695002,

#      -0.01568589, -0.013543164,

#     -0.001151976, -0.008123703,

#      0.015286064, -0.023845702,

#     -0.024547677,   0.07235076

#   ]

#   [

#        0.03278884, -0.017893745,

#     -0.0027520044,  0.016506646,

#       0.028271576,  -0.01284331,

#       0.014344065, -0.007968607,

#       -0.03899479,  0.039327156

#   ]


"""
## API reference

For detailed documentation of all __module_name__ features and configurations head to the API reference: __api_ref_module__
"""



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/index.mdx
================================================
---
sidebar_position: 0
sidebar_class_name: hidden
---

# Embeddings

[Embedding models](/docs/concepts/embedding_models) create a vector representation of a piece of text.

This page documents integrations with various model providers that allow you to use embeddings in LangChain.

import EmbeddingTabs from "@theme/EmbeddingTabs";

<EmbeddingTabs />

```javascript
await embeddings.embedQuery("Hello, world!");
```

import { CategoryTable, IndexTable } from "@theme/FeatureTables";

<IndexTable />



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/jina.mdx
================================================
---
sidebar_label: Jina
---

# Jina Embeddings

The `JinaEmbeddings` class utilizes the Jina API to generate embeddings for given text inputs. This guide will walk you through the setup and usage of the `JinaEmbeddings` class, helping you integrate it into your project seamlessly.

## Installation

Install the `@langchain/community` package as shown below:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm i @langchain/community @langchain/core
```

## Initialization

With this integration, you can use the Jina embeddings model to get embeddings for your text data. Here is the [link](https://jina.ai/embeddings) to the embeddings models.

First, you need to sign up on the Jina website and get the API token from [here](https://jina.ai/embeddings). You can copy model names from the dropdown in the api playground.

To use the `JinaEmbeddings` class, you need an API token from Jina. You can pass this token directly to the constructor or set it as an environment variable (`JINA_API_KEY`).

### Basic Usage

Here’s how to create an instance of `JinaEmbeddings`:

```typescript
import { JinaEmbeddings } from "@langchain/community/embeddings/jina";

const embeddings = new JinaEmbeddings({
  apiKey: "YOUR_API_TOKEN",
  model: "jina-clip-v2", // Optional, defaults to "jina-clip-v2"
});
```

If the `apiKey` is not provided, it will be read from the `JINA_API_KEY` environment variable.

## Generating Embeddings

### Embedding a Single Query

To generate embeddings for a single text query, use the `embedQuery` method:

```typescript
const embedding = await embeddings.embedQuery(
  "What would be a good company name for a company that makes colorful socks?"
);
console.log(embedding);
```

### Embedding Multiple Documents

To generate embeddings for multiple documents, use the `embedDocuments` method.

```typescript
import { localImageToBase64 } from "@langchain/community/utils/local_image_to_base64";
const documents = [
  "hello",
  {
    text: "hello",
  },
  {
    image: "https://i.ibb.co/nQNGqL0/beach1.jpg",
  },
  {
    image: await localImageToBase64("beach1.jpg"),
  },
];

const embeddingsArray = await embeddings.embedDocuments(documents);
console.log(embeddingsArray);
```

## Error Handling

If the API token is not provided and cannot be found in the environment variables, an error will be thrown:

```typescript
try {
  const embeddings = new JinaEmbeddings();
} catch (error) {
  console.error("Jina API token not found");
}
```

## Example

Here’s a complete example of how to set up and use the `JinaEmbeddings` class:

```typescript
import { JinaEmbeddings } from "@langchain/community/embeddings/jina";
import { localImageToBase64 } from "@langchain/community/embeddings/jina/util";

const embeddings = new JinaEmbeddings({
  apiKey: "YOUR_API_TOKEN",
  model: "jina-embeddings-v2-base-en",
});

async function runExample() {
  const queryEmbedding = await embeddings.embedQuery("Example query text.");
  console.log("Query Embedding:", queryEmbedding);

  const documents = [
    "hello",
    {
      text: "hello",
    },
    {
      image: "https://i.ibb.co/nQNGqL0/beach1.jpg",
    },
    {
      image: await localImageToBase64("beach1.jpg"),
    },
  ];
  const documentEmbeddings = await embeddings.embedDocuments(documents);
  console.log("Document Embeddings:", documentEmbeddings);
}

runExample();
```

## Feedback and Support

For feedback or questions, please contact [support@jina.ai](mailto:support@jina.ai).

## Related

- Embedding model [conceptual guide](/docs/concepts/embedding_models)
- Embedding model [how-to guides](/docs/how_to/#embedding-models)



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/llama_cpp.mdx
================================================
---
sidebar_class_name: node-only
---

# Llama CPP

:::tip Compatibility
Only available on Node.js.
:::

This module is based on the [node-llama-cpp](https://github.com/withcatai/node-llama-cpp) Node.js bindings for [llama.cpp](https://github.com/ggerganov/llama.cpp), allowing you to work with a locally running LLM. This allows you to work with a much smaller quantized model capable of running on a laptop environment, ideal for testing and scratch padding ideas without running up a bill!

## Setup

You'll need to install major version `3` of the [node-llama-cpp](https://github.com/withcatai/node-llama-cpp) module to communicate with your local model.

```bash npm2yarn
npm install -S node-llama-cpp@3
```

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core
```

You will also need a local Llama 3 model (or a model supported by [node-llama-cpp](https://github.com/withcatai/node-llama-cpp)). You will need to pass the path to this model to the LlamaCpp module as a part of the parameters (see example).

Out-of-the-box `node-llama-cpp` is tuned for running on a MacOS platform with support for the Metal GPU of Apple M-series of processors. If you need to turn this off or need support for the CUDA architecture then refer to the documentation at [node-llama-cpp](https://withcatai.github.io/node-llama-cpp/).

For advice on getting and preparing `llama3` see the documentation for the LLM version of this module.

A note to LangChain.js contributors: if you want to run the tests associated with this module you will need to put the path to your local model in the environment variable `LLAMA_PATH`.

## Usage

### Basic use

We need to provide a path to our local Llama3 model, also the `embeddings` property is always set to `true` in this module.

import CodeBlock from "@theme/CodeBlock";
import BasicExample from "@examples/embeddings/llama_cpp_basic.ts";

<CodeBlock language="typescript">{BasicExample}</CodeBlock>

### Document embedding

import DocsExample from "@examples/embeddings/llama_cpp_docs.ts";

<CodeBlock language="typescript">{DocsExample}</CodeBlock>

## Related

- Embedding model [conceptual guide](/docs/concepts/embedding_models)
- Embedding model [how-to guides](/docs/how_to/#embedding-models)



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/minimax.mdx
================================================
# Minimax

The `MinimaxEmbeddings` class uses the Minimax API to generate embeddings for a given text.

# Setup

To use Minimax model, you'll need a [Minimax account](https://api.minimax.chat), an [API key](https://api.minimax.chat/user-center/basic-information/interface-key), and a [Group ID](https://api.minimax.chat/user-center/basic-information)

# Usage

```typescript
import { MinimaxEmbeddings } from "langchain/embeddings/minimax";

export const run = async () => {
  /* Embed queries */
  const embeddings = new MinimaxEmbeddings();
  const res = await embeddings.embedQuery("Hello world");
  console.log(res);
  /* Embed documents */
  const documentRes = await embeddings.embedDocuments([
    "Hello world",
    "Bye bye",
  ]);
  console.log({ documentRes });
};
```

## Related

- Embedding model [conceptual guide](/docs/concepts/embedding_models)
- Embedding model [how-to guides](/docs/how_to/#embedding-models)



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/mistralai.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: MistralAI
---
"""

"""
# MistralAIEmbeddings

This will help you get started with MistralAIEmbeddings [embedding models](/docs/concepts/embedding_models) using LangChain. For detailed documentation on `MistralAIEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_mistralai.MistralAIEmbeddings.html).

## Overview
### Integration details

| Class | Package | Local | [Py support](https://python.langchain.com/docs/integrations/text_embedding/mistralai/) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: |
| [MistralAIEmbeddings](https://api.js.langchain.com/classes/langchain_mistralai.MistralAIEmbeddings.html) | [@langchain/mistralai](https://api.js.langchain.com/modules/langchain_mistralai.html) | ❌ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/mistralai?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/mistralai?style=flat-square&label=%20&) |

## Setup

To access MistralAI embedding models you'll need to create a MistralAI account, get an API key, and install the `@langchain/mistralai` integration package.

### Credentials

Head to [console.mistral.ai](https://console.mistral.ai/) to sign up to `MistralAI` and generate an API key. Once you've done this set the `MISTRAL_API_KEY` environment variable:

```bash
export MISTRAL_API_KEY="your-api-key"
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain MistralAIEmbeddings integration lives in the `@langchain/mistralai` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/mistralai @langchain/core
</Npm2Yarn>
```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

import { MistralAIEmbeddings } from "@langchain/mistralai";

const embeddings = new MistralAIEmbeddings({
  model: "mistral-embed", // Default value
});

"""
## Indexing and Retrieval

Embedding models are often used in retrieval-augmented generation (RAG) flows, both as part of indexing data as well as later retrieving it. For more detailed instructions, please see our RAG tutorials under the [working with external knowledge tutorials](/docs/tutorials/#working-with-external-knowledge).

Below, see how to index and retrieve data using the `embeddings` object we initialized above. In this example, we will index and retrieve a sample document using the demo [`MemoryVectorStore`](/docs/integrations/vectorstores/memory).
"""

// Create a vector store with a sample text
import { MemoryVectorStore } from "langchain/vectorstores/memory";

const text = "LangChain is the framework for building context-aware reasoning applications";

const vectorstore = await MemoryVectorStore.fromDocuments(
  [{ pageContent: text, metadata: {} }],
  embeddings,
);

// Use the vector store as a retriever that returns a single document
const retriever = vectorstore.asRetriever(1);

// Retrieve the most similar text
const retrievedDocuments = await retriever.invoke("What is LangChain?");

retrievedDocuments[0].pageContent;
# Output:
#   LangChain is the framework for building context-aware reasoning applications


"""
## Direct Usage

Under the hood, the vectorstore and retriever implementations are calling `embeddings.embedDocument(...)` and `embeddings.embedQuery(...)` to create embeddings for the text(s) used in `fromDocuments` and the retriever's `invoke` operations, respectively.

You can directly call these methods to get embeddings for your own use cases.

### Embed single texts

You can embed queries for search with `embedQuery`. This generates a vector representation specific to the query:
"""

const singleVector = await embeddings.embedQuery(text);

console.log(singleVector.slice(0, 100));
# Output:
#   [

#             -0.04443359375,         0.01885986328125,      0.018035888671875,

#       -0.00864410400390625,        0.049652099609375,     -0.001190185546875,

#          0.028900146484375,       -0.035675048828125,   -0.00702667236328125,

#     0.00016105175018310547,          -0.027587890625,      0.029388427734375,

#         -0.053253173828125,   -0.0003020763397216797,     -0.046112060546875,

#         0.0258026123046875,   -0.0010776519775390625,       0.02703857421875,

#          0.040985107421875,       -0.004547119140625,     -0.020172119140625,

#          -0.02606201171875,     -0.01457977294921875,          0.01220703125,

#        -0.0078582763671875,         -0.0084228515625,      -0.02056884765625,

#            -0.071044921875,         -0.0404052734375,    0.00923919677734375,

#        0.01407623291015625,      -0.0210113525390625,  0.0006284713745117188,

#       -0.01465606689453125,       0.0186309814453125,     -0.015838623046875,

#      0.0007920265197753906,        -0.04437255859375,      0.008758544921875,

#           -0.0172119140625,         0.01312255859375,   -0.01358795166015625,

#        -0.0212860107421875, -0.000035822391510009766,    -0.0226898193359375,

#       -0.01390838623046875,       -0.007659912109375,     -0.016021728515625,

#          0.025909423828125,       -0.034515380859375,       -0.0372314453125,

#          0.020355224609375,        -0.02606201171875,    -0.0158843994140625,

#         -0.037994384765625,      0.00450897216796875,        0.0142822265625,

#         -0.012725830078125,         -0.0770263671875,       0.02630615234375,

#         -0.048614501953125,        0.006072998046875,    0.00417327880859375,

#      -0.005138397216796875,         0.02557373046875,        0.0311279296875,

#          0.026519775390625,      -0.0103607177734375,    -0.0108489990234375,

#         -0.029510498046875,        0.022186279296875,     0.0256500244140625,

#        -0.0186309814453125,          0.0443115234375,    -0.0304107666015625,

#          -0.03131103515625,     0.007427215576171875,     0.0234527587890625,

#         0.0224761962890625,      0.00463104248046875, -0.0037021636962890625,

#         0.0302581787109375,          0.0733642578125,    -0.0121612548828125,

#        -0.0172576904296875,        0.019317626953125,         0.029052734375,

#        -0.0024871826171875,       0.0174713134765625,      0.026092529296875,

#           0.04425048828125,   -0.0004563331604003906,     0.0146026611328125,

#       -0.00748443603515625,         0.06146240234375,          0.02294921875,

#            -0.016845703125,   -0.0014057159423828125,   -0.01435089111328125,

#           0.06097412109375

#   ]


"""
### Embed multiple texts

You can embed multiple texts for indexing with `embedDocuments`. The internals used for this method may (but do not have to) differ from embedding queries:
"""

const text2 = "LangGraph is a library for building stateful, multi-actor applications with LLMs";

const vectors = await embeddings.embedDocuments([text, text2]);

console.log(vectors[0].slice(0, 100));
console.log(vectors[1].slice(0, 100));
# Output:
#   [

#             -0.04443359375,         0.01885986328125,    0.0180511474609375,

#        -0.0086517333984375,        0.049652099609375,  -0.00121307373046875,

#         0.0289154052734375,        -0.03570556640625, -0.007015228271484375,

#      0.0001499652862548828,      -0.0276641845703125,    0.0294036865234375,

#             -0.05322265625,   -0.0002808570861816406,     -0.04608154296875,

#           0.02581787109375,   -0.0011072158813476562,        0.027099609375,

#          0.040985107421875,       -0.004547119140625,   -0.0201873779296875,

#        -0.0260772705078125,      -0.0146026611328125,    0.0121917724609375,

#         -0.007843017578125,      -0.0084381103515625,   -0.0205535888671875,

#          -0.07110595703125,        -0.04046630859375,   0.00931549072265625,

#           0.01409912109375,           -0.02099609375, 0.0006232261657714844,

#         -0.014678955078125,       0.0186614990234375,   -0.0158233642578125,

#       0.000812530517578125,        -0.04437255859375,   0.00873565673828125,

#           -0.0172119140625,        0.013092041015625,      -0.0135498046875,

#        -0.0212860107421875, -0.000006735324859619141,   -0.0226898193359375,

#       -0.01389312744140625,      -0.0076751708984375,   -0.0160064697265625,

#         0.0259246826171875,         -0.0345458984375,    -0.037200927734375,

#          0.020355224609375,         -0.0260009765625,   -0.0159149169921875,

#          -0.03802490234375,     0.004489898681640625,    0.0143280029296875,

#       -0.01274871826171875,        -0.07708740234375,    0.0263214111328125,

#          -0.04864501953125,      0.00608062744140625,  0.004192352294921875,

#      -0.005115509033203125,       0.0255889892578125,       0.0311279296875,

#         0.0265045166015625,      -0.0103607177734375,  -0.01084136962890625,

#        -0.0294952392578125,        0.022186279296875,    0.0256500244140625,

#           -0.0186767578125,        0.044342041015625,    -0.030426025390625,

#          -0.03131103515625,     0.007396697998046875,    0.0234527587890625,

#               0.0224609375,     0.004634857177734375, -0.003643035888671875,

#         0.0302886962890625,         0.07342529296875,  -0.01221466064453125,

#         -0.017303466796875,       0.0193023681640625,        0.029052734375,

#     -0.0024890899658203125,       0.0174407958984375,        0.026123046875,

#          0.044219970703125,   -0.0004944801330566406,   0.01462554931640625,

#      -0.007450103759765625,         0.06146240234375,     0.022979736328125,

#            -0.016845703125,    -0.001445770263671875,   -0.0143890380859375,

#           0.06097412109375

#   ]

#   [

#          -0.02032470703125,       0.02606201171875,      0.051605224609375,

#           -0.0281982421875,      0.055755615234375,   0.001987457275390625,

#             0.031982421875,    -0.0131378173828125,       -0.0252685546875,

#       0.001010894775390625,     -0.024017333984375,      0.053375244140625,

#         -0.042816162109375,      0.005584716796875,      -0.04132080078125,

#           0.03021240234375,       0.01324462890625,      0.016876220703125,

#          0.041961669921875,  -0.004299163818359375,    -0.0273895263671875,

#         -0.039642333984375,     -0.021575927734375,     0.0309295654296875,

#        -0.0099945068359375,    -0.0163726806640625,   -0.00968170166015625,

#          -0.07733154296875,     -0.030364990234375,  -0.003864288330078125,

#          0.016387939453125,       -0.0389404296875,    -0.0026702880859375,

#        -0.0176544189453125,     0.0264434814453125,      -0.01226806640625,

#     -0.0022220611572265625,     -0.039703369140625,   -0.00907135009765625,

#        -0.0260467529296875,       0.03155517578125, -0.0004324913024902344,

#         -0.019500732421875,    -0.0120697021484375,        -0.008544921875,

#          -0.01654052734375,       0.00067138671875,    -0.0134735107421875,

#           0.01080322265625,     -0.034759521484375,         -0.06201171875,

#          0.012359619140625,  -0.006237030029296875,    -0.0168914794921875,

#        -0.0183563232421875,     0.0236053466796875, -0.0021419525146484375,

#        -0.0164947509765625,     -0.052581787109375,      0.022125244140625,

#         -0.045745849609375, -0.0009088516235351562,     0.0097808837890625,

#     -0.0009326934814453125,      0.041656494140625,        0.0269775390625,

#             0.016845703125, -0.0022335052490234375,    -0.0182342529296875,

#        -0.0245208740234375,  0.0036602020263671875,    -0.0188751220703125,

#        -0.0023956298828125,     0.0238800048828125,     -0.034942626953125,

#         -0.033782958984375,     0.0046234130859375,        0.0318603515625,

#         0.0251007080078125, -0.0023288726806640625,    -0.0225677490234375,

#      0.0004394054412841797,         0.064208984375,    -0.0254669189453125,

#        -0.0234222412109375,  0.0009264945983886719,    0.01464080810546875,

#       0.006626129150390625,  -0.007450103759765625,       0.02642822265625,

#            0.0260009765625,    0.00536346435546875,    0.01479339599609375,

#     -0.0032253265380859375,           0.0498046875,      0.048248291015625,

#       -0.01519012451171875,    0.00605010986328125,      0.019744873046875,

#         0.0296478271484375

#   ]


"""
## Hooks

Mistral AI supports custom hooks for three events: beforeRequest, requestError, and reponse. Examples of the function signature for each hook type can be seen below:
"""

const beforeRequestHook = (req: Request): Request | void | Promise<Request | void> => {
    // Code to run before a request is processed by Mistral
};

const requestErrorHook = (err: unknown, req: Request): void | Promise<void> => {
    // Code to run when an error occurs as Mistral is processing a request
};

const responseHook = (res: Response, req: Request): void | Promise<void> => {
    // Code to run before Mistral sends a successful response
};

"""
To add these hooks to the chat model, either pass them as arguments and they are automatically added:
"""

import { ChatMistralAI } from "@langchain/mistralai" 

const modelWithHooks = new ChatMistralAI({
    model: "mistral-large-latest",
    temperature: 0,
    maxRetries: 2,
    beforeRequestHooks: [ beforeRequestHook ],
    requestErrorHooks: [ requestErrorHook ],
    responseHooks: [ responseHook ],
    // other params...
});

"""
Or assign and add them manually after instantiation:
"""

import { ChatMistralAI } from "@langchain/mistralai" 

const model = new ChatMistralAI({
    model: "mistral-large-latest",
    temperature: 0,
    maxRetries: 2,
    // other params...
});

model.beforeRequestHooks = [ ...model.beforeRequestHooks, beforeRequestHook ];
model.requestErrorHooks = [ ...model.requestErrorHooks, requestErrorHook ];
model.responseHooks = [ ...model.responseHooks, responseHook ];

model.addAllHooksToHttpClient();

"""
The method addAllHooksToHttpClient clears all currently added hooks before assigning the entire updated hook lists to avoid hook duplication.

Hooks can be removed one at a time, or all hooks can be cleared from the model at once.
"""

model.removeHookFromHttpClient(beforeRequestHook);

model.removeAllHooksFromHttpClient();

"""
## API reference

For detailed documentation of all MistralAIEmbeddings features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_mistralai.MistralAIEmbeddings.html
"""



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/mixedbread_ai.mdx
================================================
# Mixedbread AI

The `MixedbreadAIEmbeddings` class uses the [Mixedbread AI](https://mixedbread.ai/) API to generate text embeddings. This guide will walk you through setting up and using the `MixedbreadAIEmbeddings` class, helping you integrate it into your project effectively.

## Installation

To install the `@langchain/mixedbread-ai` package, use the following command:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/mixedbread-ai @langchain/core
```

## Initialization

First, sign up on the Mixedbread AI website and get your API key from [here](https://mixedbread.ai/). You can then use this key to initialize the `MixedbreadAIEmbeddings` class.

You can pass the API key directly to the constructor or set it as an environment variable (`MXBAI_API_KEY`).

### Basic Usage

Here’s how to create an instance of `MixedbreadAIEmbeddings`:

```typescript
import { MixedbreadAIEmbeddings } from "@langchain/mixedbread-ai";

const embeddings = new MixedbreadAIEmbeddings({
  apiKey: "YOUR_API_KEY",
  // Optionally specify model
  // model: "mixedbread-ai/mxbai-embed-large-v1",
});
```

If the `apiKey` is not provided, it will be read from the `MXBAI_API_KEY` environment variable.

## Generating Embeddings

### Embedding a Single Query

To generate embeddings for a single text query, use the `embedQuery` method:

```typescript
const embedding = await embeddings.embedQuery(
  "Represent this sentence for searching relevant passages: Is baking fun?"
);
console.log(embedding);
```

### Embedding Multiple Documents

To generate embeddings for multiple documents, use the `embedDocuments` method. This method handles batching automatically based on the `batchSize` parameter:

```typescript
const documents = ["Baking bread is fun", "I love baking"];

const embeddingsArray = await embeddings.embedDocuments(documents);
console.log(embeddingsArray);
```

## Customizing Requests

You can customize the SDK by passing additional parameters.

```typescript
const customEmbeddings = new MixedbreadAIEmbeddings({
  apiKey: "YOUR_API_KEY",
  baseUrl: "...",
  maxRetries: 6,
});
```

## Error Handling

If the API key is not provided and cannot be found in the environment variables, an error will be thrown:

```typescript
try {
  const embeddings = new MixedbreadAIEmbeddings();
} catch (error) {
  console.error(error);
}
```

## Related

- Embedding model [conceptual guide](/docs/concepts/embedding_models)
- Embedding model [how-to guides](/docs/how_to/#embedding-models)



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/nomic.mdx
================================================
---
sidebar_label: Nomic
---

# Nomic

The `NomicEmbeddings` class uses the Nomic AI API to generate embeddings for a given text.

## Setup

In order to use the Nomic API you'll need an API key.
You can sign up for a Nomic account and create an API key [here](https://atlas.nomic.ai/).

You'll first need to install the [`@langchain/nomic`](https://www.npmjs.com/package/@langchain/nomic) package:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/nomic @langchain/core
```

## Usage

import CodeBlock from "@theme/CodeBlock";
import NomicExample from "@examples/models/embeddings/nomic.ts";

<CodeBlock language="typescript">{NomicExample}</CodeBlock>

## Related

- Embedding model [conceptual guide](/docs/concepts/embedding_models)
- Embedding model [how-to guides](/docs/how_to/#embedding-models)



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/ollama.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Ollama
---
"""

"""
# OllamaEmbeddings

This will help you get started with Ollama [embedding models](/docs/concepts/embedding_models) using LangChain. For detailed documentation on `OllamaEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_ollama.OllamaEmbeddings.html).

## Overview
### Integration details

| Class | Package | Local | [Py support](https://python.langchain.com/docs/integrations/text_embedding/ollama/) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: |
| [`OllamaEmbeddings`](https://api.js.langchain.com/classes/langchain_ollama.OllamaEmbeddings.html) | [`@langchain/ollama`](https://npmjs.com/@langchain/ollama) | ✅ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/ollama?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/ollama?style=flat-square&label=%20&) |

## Setup

To access Ollama embedding models you'll need to follow [these instructions](https://github.com/jmorganca/ollama) to install Ollama, and install the `@langchain/ollama` integration package.

### Credentials

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain OllamaEmbeddings integration lives in the `@langchain/ollama` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/ollama @langchain/core
</Npm2Yarn>
```
"""

"""
## Instantiation

Now we can instantiate our model object and embed text:
"""

import { OllamaEmbeddings } from "@langchain/ollama";

const embeddings = new OllamaEmbeddings({
  model: "mxbai-embed-large", // Default value
  baseUrl: "http://localhost:11434", // Default value
});

"""
## Indexing and Retrieval

Embedding models are often used in retrieval-augmented generation (RAG) flows, both as part of indexing data as well as later retrieving it. For more detailed instructions, please see our RAG tutorials under the [working with external knowledge tutorials](/docs/tutorials/#working-with-external-knowledge).

Below, see how to index and retrieve data using the `embeddings` object we initialized above. In this example, we will index and retrieve a sample document using the demo [`MemoryVectorStore`](/docs/integrations/vectorstores/memory).
"""

// Create a vector store with a sample text
import { MemoryVectorStore } from "langchain/vectorstores/memory";

const text = "LangChain is the framework for building context-aware reasoning applications";

const vectorstore = await MemoryVectorStore.fromDocuments(
  [{ pageContent: text, metadata: {} }],
  embeddings,
);

// Use the vector store as a retriever that returns a single document
const retriever = vectorstore.asRetriever(1);

// Retrieve the most similar text
const retrievedDocuments = await retriever.invoke("What is LangChain?");

retrievedDocuments[0].pageContent;
# Output:
#   LangChain is the framework for building context-aware reasoning applications


"""
## Direct Usage

Under the hood, the vectorstore and retriever implementations are calling `embeddings.embedDocument(...)` and `embeddings.embedQuery(...)` to create embeddings for the text(s) used in `fromDocuments` and the retriever's `invoke` operations, respectively.

You can directly call these methods to get embeddings for your own use cases.

### Embed single texts

You can embed queries for search with `embedQuery`. This generates a vector representation specific to the query:
"""

const singleVector = await embeddings.embedQuery(text);

console.log(singleVector.slice(0, 100));
# Output:
#   [

#      0.026051683,   0.029081265,  -0.040726297,  -0.015116953, -0.010691089,

#      0.030181013, -0.0065084146,   -0.02079503,   0.013575795,   0.03452527,

#      0.009578291,   0.007026421,  -0.030110886,   0.013489622,  -0.04294787,

#      0.011141899,  -0.043768786,   -0.00362867, -0.0081198225,  -0.03426076,

#      0.010075142,   0.027787417,   -0.09052663,   -0.06039698, -0.009462592,

#       0.06232288,   0.051121354,   0.011977532,   0.089046724,  0.059000008,

#      0.031860664,  -0.034242127,   0.020339863,   0.011483523,  -0.05429335,

#      -0.04963588,    0.03263794,   -0.05581542,   0.013908403, -0.012356067,

#     -0.007802118,  -0.010027855,    0.00281217,  -0.101886116, -0.079341754,

#      0.011269771,  0.0035983133,  -0.027667878,   0.032092705, -0.052843474,

#     -0.045283325,     0.0382421,     0.0193055,   0.011050924,  0.021132186,

#     -0.037696265,  0.0006107435,  0.0043520257,  -0.028798066,  0.049155913,

#       0.03590549, -0.0040995986,   0.019772101,  -0.076119535, 0.0031298609,

#       0.03368174,   0.039398745,  -0.011813277,  -0.019313531, -0.013108803,

#     -0.044905286,  -0.022326004,   -0.01656178,   -0.06658457,  0.016789088,

#      0.049952697,   0.006615693,   -0.01694402,  -0.018105473, 0.0049101883,

#     -0.004966945,   0.049762275,   -0.03556957,  -0.015986584,  -0.03190983,

#      -0.05336687, -0.0020468342, -0.0016106658,  -0.035291273, -0.029783724,

#     -0.010153295,   0.052100364,    0.05528949,    0.01379487, -0.024542747,

#      0.028773975,   0.010087022,   0.030448131,  -0.042391222,  0.016596776

#   ]


"""
### Embed multiple texts

You can embed multiple texts for indexing with `embedDocuments`. The internals used for this method may (but do not have to) differ from embedding queries:
"""

const text2 = "LangGraph is a library for building stateful, multi-actor applications with LLMs";

const vectors = await embeddings.embedDocuments([text, text2]);

console.log(vectors[0].slice(0, 100));
console.log(vectors[1].slice(0, 100));
# Output:
#   [

#      0.026051683,   0.029081265,  -0.040726297,  -0.015116953, -0.010691089,

#      0.030181013, -0.0065084146,   -0.02079503,   0.013575795,   0.03452527,

#      0.009578291,   0.007026421,  -0.030110886,   0.013489622,  -0.04294787,

#      0.011141899,  -0.043768786,   -0.00362867, -0.0081198225,  -0.03426076,

#      0.010075142,   0.027787417,   -0.09052663,   -0.06039698, -0.009462592,

#       0.06232288,   0.051121354,   0.011977532,   0.089046724,  0.059000008,

#      0.031860664,  -0.034242127,   0.020339863,   0.011483523,  -0.05429335,

#      -0.04963588,    0.03263794,   -0.05581542,   0.013908403, -0.012356067,

#     -0.007802118,  -0.010027855,    0.00281217,  -0.101886116, -0.079341754,

#      0.011269771,  0.0035983133,  -0.027667878,   0.032092705, -0.052843474,

#     -0.045283325,     0.0382421,     0.0193055,   0.011050924,  0.021132186,

#     -0.037696265,  0.0006107435,  0.0043520257,  -0.028798066,  0.049155913,

#       0.03590549, -0.0040995986,   0.019772101,  -0.076119535, 0.0031298609,

#       0.03368174,   0.039398745,  -0.011813277,  -0.019313531, -0.013108803,

#     -0.044905286,  -0.022326004,   -0.01656178,   -0.06658457,  0.016789088,

#      0.049952697,   0.006615693,   -0.01694402,  -0.018105473, 0.0049101883,

#     -0.004966945,   0.049762275,   -0.03556957,  -0.015986584,  -0.03190983,

#      -0.05336687, -0.0020468342, -0.0016106658,  -0.035291273, -0.029783724,

#     -0.010153295,   0.052100364,    0.05528949,    0.01379487, -0.024542747,

#      0.028773975,   0.010087022,   0.030448131,  -0.042391222,  0.016596776

#   ]

#   [

#         0.0558515,   0.028698817,  -0.037476595,  0.0048659276,  -0.019229038,

#       -0.04713716,  -0.020947812,  -0.017550547,    0.01205507,   0.027693441,

#      -0.011791304,   0.009862203,   0.019662278,  -0.037511427,  -0.022662448,

#       0.036224432,  -0.051760387,  -0.030165697,  -0.008899774,  -0.024518963,

#       0.010077767,   0.032209765,    -0.0854303,  -0.038666975,  -0.036021013,

#       0.060899545,   0.045867186,   0.003365381,    0.09387081,   0.038216405,

#       0.011449426,  -0.016495887,   0.020602569,   -0.02368503,  -0.014733645,

#      -0.065408126, -0.0065152845,  -0.027103946, 0.00038956117,   -0.08648814,

#       0.029316466,  -0.054449145,   0.034129277,  -0.055225655,  -0.043182302,

#      0.0011148591,   0.044116337,  -0.046552557,   0.032423045,   -0.03269365,

#       -0.05062933,   0.021473562,  -0.011019348,  -0.019621233, -0.0003149565,

#     -0.0046085776,  0.0052610254, -0.0029293327,  -0.035793293,   0.034469575,

#       0.037724957,   0.009572597,   0.014198464,    -0.0878237,  0.0056973165,

#       0.023563445,   0.030928325,   0.025520306,    0.01836824,  -0.016456697,

#      -0.061934732,   0.009764942,  -0.035812028,   -0.04429064,   0.031323086,

#       0.056027107, -0.0019782048,  -0.015204176,  -0.008684945, -0.0010460864,

#       0.054642987,   0.044149086,  -0.032964867,  -0.012044753,  -0.019075096,

#      -0.027932597,   0.018542245,   -0.02602878,   -0.04645578,  -0.020976603,

#       0.018999187,   0.050663687,   0.016725155,  0.0076955976,   0.011448177,

#       0.053931057,   -0.03234989,   0.024429373,  -0.023123834,    0.02197912

#   ]


"""
Ollama [model parameters](https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values) are also supported:
"""

import { OllamaEmbeddings } from "@langchain/ollama";

const embeddingsCustomParams = new OllamaEmbeddings({
  requestOptions: {
    useMmap: true, // use_mmap 1
    numThread: 6, // num_thread 6
    numGpu: 1, // num_gpu 1
  },
});

"""
## Related

- Embedding model [conceptual guide](/docs/concepts/embedding_models)
- Embedding model [how-to guides](/docs/how_to/#embedding-models)
"""

"""
## API reference

For detailed documentation of all `OllamaEmbeddings` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_ollama.OllamaEmbeddings.html)
"""



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/openai.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: OpenAI
---
"""

"""
# OpenAI

This will help you get started with OpenAIEmbeddings [embedding models](/docs/concepts/embedding_models) using LangChain. For detailed documentation on `OpenAIEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_openai.OpenAIEmbeddings.html).

## Overview
### Integration details

| Class | Package | Local | [Py support](https://python.langchain.com/docs/integrations/text_embedding/openai/) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: |
| [OpenAIEmbeddings](https://api.js.langchain.com/classes/langchain_openai.OpenAIEmbeddings.html) | [@langchain/openai](https://api.js.langchain.com/modules/langchain_openai.html) | ❌ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/openai?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/openai?style=flat-square&label=%20&) |

## Setup

To access OpenAIEmbeddings embedding models you'll need to create an OpenAI account, get an API key, and install the `@langchain/openai` integration package.

### Credentials

Head to [platform.openai.com](https://platform.openai.com) to sign up to OpenAI and generate an API key. Once you've done this set the `OPENAI_API_KEY` environment variable:

```bash
export OPENAI_API_KEY="your-api-key"
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain OpenAIEmbeddings integration lives in the `@langchain/openai` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/openai @langchain/core
</Npm2Yarn>
```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

import { OpenAIEmbeddings } from "@langchain/openai";

const embeddings = new OpenAIEmbeddings({
  apiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.OPENAI_API_KEY
  batchSize: 512, // Default value if omitted is 512. Max is 2048
  model: "text-embedding-3-large",
});

"""
If you're part of an organization, you can set `process.env.OPENAI_ORGANIZATION` to your OpenAI organization id, or pass it in as `organization` when
initializing the model.
"""

"""
## Indexing and Retrieval

Embedding models are often used in retrieval-augmented generation (RAG) flows, both as part of indexing data as well as later retrieving it. For more detailed instructions, please see our RAG tutorials under the [working with external knowledge tutorials](/docs/tutorials/#working-with-external-knowledge).

Below, see how to index and retrieve data using the `embeddings` object we initialized above. In this example, we will index and retrieve a sample document using the demo [`MemoryVectorStore`](/docs/integrations/vectorstores/memory).
"""

// Create a vector store with a sample text
import { MemoryVectorStore } from "langchain/vectorstores/memory";

const text = "LangChain is the framework for building context-aware reasoning applications";

const vectorstore = await MemoryVectorStore.fromDocuments(
  [{ pageContent: text, metadata: {} }],
  embeddings,
);

// Use the vector store as a retriever that returns a single document
const retriever = vectorstore.asRetriever(1);

// Retrieve the most similar text
const retrievedDocuments = await retriever.invoke("What is LangChain?");

retrievedDocuments[0].pageContent;
# Output:
#   LangChain is the framework for building context-aware reasoning applications


"""
## Direct Usage

Under the hood, the vectorstore and retriever implementations are calling `embeddings.embedDocument(...)` and `embeddings.embedQuery(...)` to create embeddings for the text(s) used in `fromDocuments` and the retriever's `invoke` operations, respectively.

You can directly call these methods to get embeddings for your own use cases.

### Embed single texts

You can embed queries for search with `embedQuery`. This generates a vector representation specific to the query:
"""

const singleVector = await embeddings.embedQuery(text);

console.log(singleVector.slice(0, 100));
# Output:
#   [

#       -0.01927683,  0.0037708976,  -0.032942563,  0.0037671267,  0.008175306,

#      -0.012511838,  -0.009713832,   0.021403614,  -0.015377721, 0.0018684798,

#       0.020574018,   0.022399133,   -0.02322873,   -0.01524951,  -0.00504169,

#      -0.007375876,   -0.03448109, 0.00015130726,   0.021388533, -0.012564631,

#      -0.020031009,   0.027406884,  -0.039217334,    0.03036327,  0.030393435,

#      -0.021750538,   0.032610722,  -0.021162277,  -0.025898525,  0.018869571,

#       0.034179416,  -0.013371604,  0.0037652412,   -0.02146395, 0.0012641934,

#      -0.055688616,    0.05104287,  0.0024982197,  -0.019095825, 0.0037369595,

#     0.00088757504,   0.025189597,  -0.018779071,   0.024978427,  0.016833287,

#     -0.0025868358,  -0.011727491, -0.0021154736,  -0.017738303, 0.0013839195,

#     -0.0131151825,   -0.05405959,   0.029729757,  -0.003393808,  0.019774588,

#       0.028885076,   0.004355387,   0.026094612,    0.06479911,  0.038040817,

#       -0.03478276,  -0.012594799,  -0.024767255, -0.0031430433,  0.017874055,

#      -0.015294761,   0.005709139,   0.025355516,   0.044798266,   0.02549127,

#       -0.02524993, 0.00014553308,  -0.019427665,  -0.023545485,  0.008748483,

#       0.019850006,  -0.028417485,  -0.001860938,   -0.02318348, -0.010799851,

#        0.04793565, -0.0048983963,    0.02193154,  -0.026411368,  0.026426451,

#      -0.012149832,   0.035355937,  -0.047814984,  -0.027165547, -0.008228099,

#      -0.007737882,   0.023726488,  -0.046487626,  -0.007783133, -0.019638835,

#        0.01793439,  -0.018024892,  0.0030336871,  -0.019578502, 0.0042837397

#   ]


"""
### Embed multiple texts

You can embed multiple texts for indexing with `embedDocuments`. The internals used for this method may (but do not have to) differ from embedding queries:
"""

const text2 = "LangGraph is a library for building stateful, multi-actor applications with LLMs";

const vectors = await embeddings.embedDocuments([text, text2]);

console.log(vectors[0].slice(0, 100));
console.log(vectors[1].slice(0, 100));
# Output:
#   [

#       -0.01927683,  0.0037708976,  -0.032942563,  0.0037671267,  0.008175306,

#      -0.012511838,  -0.009713832,   0.021403614,  -0.015377721, 0.0018684798,

#       0.020574018,   0.022399133,   -0.02322873,   -0.01524951,  -0.00504169,

#      -0.007375876,   -0.03448109, 0.00015130726,   0.021388533, -0.012564631,

#      -0.020031009,   0.027406884,  -0.039217334,    0.03036327,  0.030393435,

#      -0.021750538,   0.032610722,  -0.021162277,  -0.025898525,  0.018869571,

#       0.034179416,  -0.013371604,  0.0037652412,   -0.02146395, 0.0012641934,

#      -0.055688616,    0.05104287,  0.0024982197,  -0.019095825, 0.0037369595,

#     0.00088757504,   0.025189597,  -0.018779071,   0.024978427,  0.016833287,

#     -0.0025868358,  -0.011727491, -0.0021154736,  -0.017738303, 0.0013839195,

#     -0.0131151825,   -0.05405959,   0.029729757,  -0.003393808,  0.019774588,

#       0.028885076,   0.004355387,   0.026094612,    0.06479911,  0.038040817,

#       -0.03478276,  -0.012594799,  -0.024767255, -0.0031430433,  0.017874055,

#      -0.015294761,   0.005709139,   0.025355516,   0.044798266,   0.02549127,

#       -0.02524993, 0.00014553308,  -0.019427665,  -0.023545485,  0.008748483,

#       0.019850006,  -0.028417485,  -0.001860938,   -0.02318348, -0.010799851,

#        0.04793565, -0.0048983963,    0.02193154,  -0.026411368,  0.026426451,

#      -0.012149832,   0.035355937,  -0.047814984,  -0.027165547, -0.008228099,

#      -0.007737882,   0.023726488,  -0.046487626,  -0.007783133, -0.019638835,

#        0.01793439,  -0.018024892,  0.0030336871,  -0.019578502, 0.0042837397

#   ]

#   [

#      -0.010181213,   0.023419594,   -0.04215527, -0.0015320902,  -0.023573855,

#     -0.0091644935,  -0.014893179,   0.019016149,  -0.023475688,  0.0010219777,

#       0.009255648,    0.03996757,   -0.04366983,   -0.01640774,  -0.020194141,

#       0.019408813,  -0.027977299,  -0.022017224,   0.013539891,  -0.007769135,

#       0.032647192,  -0.015089511,  -0.022900717,   0.023798235,   0.026084099,

#      -0.024625633,   0.035003178,  -0.017978394,  -0.049615882,   0.013364594,

#       0.031132633,   0.019142363,   0.023195215,  -0.038396914,   0.005584942,

#      -0.031946007,   0.053682756, -0.0036356465,   0.011240003,  0.0056690844,

#     -0.0062791156,   0.044146635,  -0.037387207,    0.01300699,   0.018946031,

#      0.0050415234,   0.029618073,  -0.021750772,  -0.000649473, 0.00026951815,

#      -0.014710871,  -0.029814405,    0.04204308,  -0.014710871,  0.0039616977,

#      -0.021512369,   0.054608323,   0.021484323,    0.02790718,  -0.010573876,

#      -0.023952495,  -0.035143413,  -0.048802506, -0.0075798146,   0.023279356,

#      -0.022690361,  -0.016590048,  0.0060477243,   0.014100839,   0.005476258,

#      -0.017221114, -0.0100059165,  -0.017922299,  -0.021989176,    0.01830094,

#        0.05516927,   0.001033372,  0.0017310516,   -0.00960624,  -0.037864015,

#       0.013063084,   0.006591143,  -0.010160177,  0.0011394264,    0.04953174,

#       0.004806626,   0.029421741,  -0.037751824,   0.003618117,   0.007162609,

#       0.027696826, -0.0021070621,  -0.024485396, -0.0042141243,   -0.02801937,

#      -0.019605145,   0.016281527,  -0.035143413,    0.01640774,   0.042323552

#   ]


"""
## Specifying dimensions

With the `text-embedding-3` class of models, you can specify the size of the embeddings you want returned. For example by default `text-embedding-3-large` returns embeddings of dimension 3072:

"""

import { OpenAIEmbeddings } from "@langchain/openai";

const embeddingsDefaultDimensions = new OpenAIEmbeddings({
  model: "text-embedding-3-large",
});

const vectorsDefaultDimensions = await embeddingsDefaultDimensions.embedDocuments(["some text"]);
console.log(vectorsDefaultDimensions[0].length);
# Output:
#   3072


"""
But by passing in `dimensions: 1024` we can reduce the size of our embeddings to 1024:
"""

import { OpenAIEmbeddings } from "@langchain/openai";

const embeddings1024 = new OpenAIEmbeddings({
  model: "text-embedding-3-large",
  dimensions: 1024,
});

const vectors1024 = await embeddings1024.embedDocuments(["some text"]);
console.log(vectors1024[0].length);
# Output:
#   1024


"""
## Custom URLs

You can customize the base URL the SDK sends requests to by passing a `configuration` parameter like this:
"""

import { OpenAIEmbeddings } from "@langchain/openai";

const model = new OpenAIEmbeddings({
  configuration: {
    baseURL: "https://your_custom_url.com",
  },
});

"""
You can also pass other `ClientOptions` parameters accepted by the official SDK.

If you are hosting on Azure OpenAI, see the [dedicated page instead](/docs/integrations/text_embedding/azure_openai).
"""

"""
## API reference

For detailed documentation of all OpenAIEmbeddings features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_openai.OpenAIEmbeddings.html
"""



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/pinecone.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Pinecone
---
"""

"""
# PineconeEmbeddings

This will help you get started with PineconeEmbeddings [embedding models](/docs/concepts/embedding_models) using LangChain. For detailed documentation on `PineconeEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/_langchain_pinecone.PineconeEmbeddings.html).

## Overview
### Integration details

| Class | Package | Local | [Py support](https://python.langchain.com/docs/integrations/text_embedding/pinecone/) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: |
| [PineconeEmbeddings](https://api.js.langchain.com/classes/_langchain_pinecone.PineconeEmbeddings.html) | [@langchain/pinecone](https://api.js.langchain.com/classes/_langchain_pinecone.PineconeEmbeddings.html) | ❌ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/pinecone?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/pinecone?style=flat-square&label=%20&) |

## Setup

To access Pinecone embedding models you'll need to create a Pinecone account, get an API key, and install the `@langchain/pinecone` integration package.

### Credentials

Sign up for a [Pinecone](https://www.pinecone.io/) account, retrieve your API key, and set it as an environment variable named `PINECONE_API_KEY`:

```typescript
process.env.PINECONE_API_KEY = "your-pinecone-api-key";
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain PineconeEmbeddings integration lives in the `@langchain/pinecone` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/pinecone @langchain/core @pinecone-database/pinecone@5
</Npm2Yarn>
```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

import { PineconeEmbeddings } from "@langchain/pinecone";

const embeddings = new PineconeEmbeddings({
  model: "multilingual-e5-large",
});

"""
## Indexing and Retrieval

Embedding models are often used in retrieval-augmented generation (RAG) flows, both as part of indexing data as well as later retrieving it. For more detailed instructions, please see our RAG tutorials under the [working with external knowledge tutorials](/docs/tutorials/#working-with-external-knowledge).

Below, see how to index and retrieve data using the `embeddings` object we initialized above. In this example, we will index and retrieve a sample document using the demo [`MemoryVectorStore`](/docs/integrations/vectorstores/memory).
"""

// Create a vector store with a sample text
import { MemoryVectorStore } from "langchain/vectorstores/memory";

const text = "LangChain is the framework for building context-aware reasoning applications";

const vectorstore = await MemoryVectorStore.fromDocuments(
  [{ pageContent: text, metadata: {} }],
  embeddings,
);

// Use the vector store as a retriever that returns a single document
const retriever = vectorstore.asRetriever(1);

// Retrieve the most similar text
const retrievedDocuments = await retriever.invoke("What is LangChain?");

retrievedDocuments[0].pageContent;
# Output:
#   LangChain is the framework for building context-aware reasoning applications


"""
## Direct Usage

Under the hood, the vectorstore and retriever implementations are calling `embeddings.embedDocument(...)` and `embeddings.embedQuery(...)` to create embeddings for the text(s) used in `fromDocuments` and the retriever's `invoke` operations, respectively.

You can directly call these methods to get embeddings for your own use cases.

### Embed single texts

You can embed queries for search with `embedQuery`. This generates a vector representation specific to the query:
"""

const singleVector = await embeddings.embedQuery(text);

console.log(singleVector.slice(0, 100));
# Output:
#   [

#            0.0191650390625,  0.004924774169921875,     -0.015838623046875,

#             -0.04248046875,     0.040191650390625,      -0.02679443359375,

#        -0.0240936279296875,     0.058624267578125,      0.027069091796875,

#           -0.0435791015625,      0.01934814453125,      0.040191650390625,

#        -0.0194244384765625,   0.01386260986328125,    -0.0216827392578125,

#       -0.01073455810546875,   -0.0166168212890625,    0.01073455810546875,

#           -0.0228271484375,       0.0062255859375,      0.035064697265625,

#        -0.0114593505859375,   -0.0257110595703125,    -0.0285797119140625,

#           0.01190185546875,    -0.022186279296875,   -0.01500701904296875,

#          -0.03240966796875, 0.0019063949584960938,     -0.039337158203125,

#        -0.0047454833984375,              -0.03125,       -0.0123291015625,

#       -0.00899505615234375,        -0.02880859375,      0.014678955078125,

#            0.0452880859375,      0.05035400390625,     -0.053436279296875,

#         0.0265960693359375,   -0.0206756591796875,       0.06658935546875,

#         -0.032989501953125,  -0.00724029541015625,  0.0024967193603515625,

#         0.0282135009765625,     0.047088623046875,       -0.0255126953125,

#         -0.008453369140625,   -0.0039215087890625,     0.0282135009765625,

#         0.0270843505859375,      -0.0133056640625,    -0.0296173095703125,

#           -0.0455322265625,    0.0225982666015625,      -0.04803466796875,

#       -0.00891876220703125,     -0.04669189453125,      0.022064208984375,

#        -0.0266571044921875,  -0.01480865478515625,     0.0295257568359375,

#       -0.01561737060546875,      -0.0411376953125,    0.01345062255859375,

#         0.0219879150390625,    -0.012786865234375,     -0.051727294921875,

#     -0.0002830028533935547,   0.00690460205078125,   -0.01303863525390625,

#           -0.0457763671875,    -0.026763916015625,    -0.0181121826171875,

#        0.00946807861328125,       0.0250244140625,      -0.01458740234375,

#            0.0394287109375,   -0.0162200927734375,       0.05169677734375,

#        0.01126861572265625,   0.01265716552734375,     -0.009307861328125,

#             0.052490234375,    0.0135345458984375,    0.01332855224609375,

#          0.040130615234375,       0.0638427734375,     0.0181121826171875,

#       0.004207611083984375,          0.0771484375,      0.024078369140625,

#          0.012420654296875,       -0.030517578125, -0.0019245147705078125,

#         0.0243682861328125,    0.0254974365234375,  0.0036334991455078125,

#      -0.004550933837890625

#   ]


"""
### Embed multiple texts

You can embed multiple texts for indexing with `embedDocuments`. The internals used for this method may (but do not have to) differ from embedding queries:
"""

const text2 = "LangGraph is a library for building stateful, multi-actor applications with LLMs";

const vectors = await embeddings.embedDocuments([text, text2]);

console.log(vectors[0].slice(0, 100));
console.log(vectors[1].slice(0, 100));
# Output:
#   [

#         0.0190887451171875,  0.00482940673828125,   -0.0158233642578125,

#          -0.04254150390625,    0.040130615234375,   -0.0268096923828125,

#             -0.02392578125,    0.058624267578125,    0.0269927978515625,

#             -0.04345703125,   0.0193328857421875,     0.040374755859375,

#        -0.0196075439453125,  0.01384735107421875,    -0.021881103515625,

#       -0.01068878173828125,   -0.016510009765625,   0.01079559326171875,

#        -0.0227813720703125,        0.00634765625,     0.035064697265625,

#        -0.0113983154296875,  -0.0257720947265625,   -0.0285491943359375,

#          0.011749267578125,  -0.0222625732421875,   -0.0148468017578125,

#           -0.0325927734375,  0.00203704833984375,      -0.0394287109375,

#      -0.004878997802734375,  -0.0311126708984375,  -0.01248931884765625,

#       -0.00897979736328125,  -0.0286407470703125,    0.0146331787109375,

#           0.04522705078125,    0.050201416015625,    -0.053314208984375,

#         0.0265960693359375,  -0.0207366943359375,      0.06658935546875,

#          -0.03302001953125,  -0.0073699951171875,    0.0024261474609375,

#          0.028228759765625,     0.04705810546875,   -0.0255279541015625,

#        -0.0084075927734375,   -0.003814697265625,    0.0281524658203125,

#         0.0272064208984375, -0.01322174072265625,   -0.0295257568359375,

#         -0.045623779296875,    0.022735595703125,         -0.0478515625,

#       -0.00885772705078125,   -0.046844482421875,     0.022003173828125,

#         -0.026458740234375,  -0.0148468017578125,    0.0295562744140625,

#       -0.01555633544921875,   -0.041229248046875,      0.01336669921875,

#          0.022125244140625, -0.01276397705078125,    -0.051666259765625,

#     -0.0002474784851074219, 0.006740570068359375,  -0.01306915283203125,

#          -0.04583740234375,      -0.026611328125,   -0.0182342529296875,

#           0.00946044921875,   0.0250701904296875,   -0.0146942138671875,

#          0.039459228515625,   -0.016265869140625,     0.051788330078125,

#        0.01110076904296875,         0.0126953125,  -0.00925445556640625,

#          0.052581787109375,  0.01363372802734375,   0.01332855224609375,

#           0.04010009765625,      0.0638427734375,     0.018157958984375,

#         0.0040740966796875,     0.07720947265625,    0.0240325927734375,

#         0.0123443603515625,  -0.0302886962890625, -0.001865386962890625,

#          0.024383544921875,    0.025604248046875,   0.00353240966796875,

#      -0.004474639892578125

#   ]

#   [

#        0.0053253173828125,    0.01305389404296875,    -0.0253448486328125,

#         -0.04241943359375,      0.034942626953125,     -0.017425537109375,

#            -0.02783203125,         0.064208984375,     0.0244903564453125,

#          -0.0467529296875,      0.021209716796875,       0.02191162109375,

#         -0.03131103515625,     -0.019073486328125,   -0.01413726806640625,

#        -0.008636474609375,     -0.011627197265625,     0.0229339599609375,

#         -0.00762939453125,    0.00594329833984375,     0.0201263427734375,

#          -0.0247802734375,      -0.05047607421875,      -0.03765869140625,

#        0.0034332275390625,     -0.014617919921875,     -0.043548583984375,

#         -0.03594970703125,  0.0002884864807128906,      -0.03656005859375,

#       -0.0102691650390625,     0.0121307373046875,    -0.0284271240234375,

#          -0.0113525390625,   -0.01195526123046875,    0.01143646240234375,

#         0.051727294921875,        0.0230712890625,     -0.046417236328125,

#        0.0198211669921875,      -0.02337646484375,      0.040985107421875,

#         -0.03314208984375,     -0.025909423828125,   -0.00809478759765625,

#        0.0291595458984375,             0.04296875,     -0.016143798828125,

#         0.005706787109375,       0.00860595703125, -0.0035343170166015625,

#        0.0118560791015625,    -0.0135650634765625,    -0.0294036865234375,

#        -0.029876708984375,             0.03515625,       -0.0545654296875,

#      0.006862640380859375,     -0.041839599609375,      0.021148681640625,

#       -0.0279998779296875,   -0.00949859619140625,       0.03314208984375,

#     -0.002727508544921875,          -0.0400390625,    0.01311492919921875,

#       0.01177215576171875, -0.0010013580322265625,        -0.052001953125,

#       0.00112152099609375,   -0.00815582275390625,        0.0321044921875,

#          -0.0496826171875,    -0.0151519775390625,    -0.0262908935546875,

#     -0.005207061767578125,     0.0207977294921875,        -0.022705078125,

#         0.009735107421875,   0.000682830810546875,       0.05792236328125,

#          -0.0145263671875,       0.03643798828125,  0.0018339157104492188,

#         0.047210693359375,  0.0017986297607421875,     0.0300140380859375,

#         0.027923583984375,      0.044708251953125,      0.027618408203125,

#       0.00104522705078125,       0.05987548828125,       0.06304931640625,

#        -0.039703369140625,       -0.0386962890625,    0.00797271728515625,

#        0.0254974365234375,     0.0245819091796875,      0.010467529296875,

#       -0.0080413818359375

#   ]


"""
## API reference

For detailed documentation of all PineconeEmbeddings features and configurations head to the API reference: https://api.js.langchain.com/classes/_langchain_pinecone.PineconeEmbeddings.html
"""



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/premai.mdx
================================================
---
sidebar_label: Prem AI
---

# Prem AI

The `PremEmbeddings` class uses the Prem AI API to generate embeddings for a given text.

## Setup

In order to use the Prem API you'll need an API key. You can sign up for a Prem account and create an API key [here](https://app.premai.io/accounts/signup/).

You'll first need to install the [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) package:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core
```

## Usage

import CodeBlock from "@theme/CodeBlock";
import PremExample from "@examples/embeddings/premai.ts";

<CodeBlock language="typescript">{PremExample}</CodeBlock>

## Related

- Embedding model [conceptual guide](/docs/concepts/embedding_models)
- Embedding model [how-to guides](/docs/how_to/#embedding-models)



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/tencent_hunyuan.mdx
================================================
---
sidebar_label: Tencent Hunyuan
---

# TencentHunyuan

The `TencentHunyuanEmbeddings` class uses the Tencent Hunyuan API to generate embeddings for a given text.

## Setup

1. Sign up for a Tencent Cloud account [here](https://cloud.tencent.com/register).
2. Create SecretID & SecretKey [here](https://console.cloud.tencent.com/cam/capi).
3. Set SecretID and SecretKey as environment variables named `TENCENT_SECRET_ID` and `TENCENT_SECRET_KEY`, respectively.

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core
```

If you are using LangChain.js in a browser environment, you'll also need to install the following dependencies:

```bash npm2yarn
npm install crypto-js
```

And then make sure that you import from the `web` as shown below.

## Usage

Here's an example:

import CodeBlock from "@theme/CodeBlock";
import TencentHunyuan from "@examples/models/embeddings/tencent_hunyuan.ts";

<CodeBlock language="typescript">{TencentHunyuan}</CodeBlock>

## Related

- Embedding model [conceptual guide](/docs/concepts/embedding_models)
- Embedding model [how-to guides](/docs/how_to/#embedding-models)



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/tensorflow.mdx
================================================
# TensorFlow

This Embeddings integration runs the embeddings entirely in your browser or Node.js environment, using [TensorFlow.js](https://www.tensorflow.org/js). This means that your data isn't sent to any third party, and you don't need to sign up for any API keys. However, it does require more memory and processing power than the other integrations.

```bash npm2yarn
npm install @langchain/community @langchain/core @tensorflow/tfjs-core@3.6.0 @tensorflow/tfjs-converter@3.6.0 @tensorflow-models/universal-sentence-encoder@1.3.3 @tensorflow/tfjs-backend-cpu
```

```typescript
import "@tensorflow/tfjs-backend-cpu";
import { TensorFlowEmbeddings } from "@langchain/community/embeddings/tensorflow";

const embeddings = new TensorFlowEmbeddings();
```

This example uses the CPU backend, which works in any JS environment. However, you can use any of the backends supported by TensorFlow.js, including GPU and WebAssembly, which will be a lot faster. For Node.js you can use the `@tensorflow/tfjs-node` package, and for the browser you can use the `@tensorflow/tfjs-backend-webgl` package. See the [TensorFlow.js documentation](https://www.tensorflow.org/js/guide/platform_environment) for more information.

## Related

- Embedding model [conceptual guide](/docs/concepts/embedding_models)
- Embedding model [how-to guides](/docs/how_to/#embedding-models)



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/togetherai.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: TogetherAI
---
"""

"""
# TogetherAIEmbeddings

This will help you get started with TogetherAIEmbeddings [embedding models](/docs/concepts/embedding_models) using LangChain. For detailed documentation on `TogetherAIEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_community_embeddings_togetherai.TogetherAIEmbeddings.html).

## Overview
### Integration details

| Class | Package | Local | [Py support](https://python.langchain.com/docs/integrations/text_embedding/together/) | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |  :---: | :---: |
| [TogetherAIEmbeddings](https://api.js.langchain.com/classes/langchain_community_embeddings_togetherai.TogetherAIEmbeddings.html) | [@langchain/community](https://api.js.langchain.com/modules/langchain_community_embeddings_togetherai.html) | ❌ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/community?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |

## Setup

To access TogetherAI embedding models you'll need to create a TogetherAI account, get an API key, and install the `@langchain/community` integration package.

### Credentials

You can sign up for a Together account and create an API key [here](https://api.together.xyz/). Once you've done this set the `TOGETHER_AI_API_KEY` environment variable:

```bash
export TOGETHER_AI_API_KEY="your-api-key"
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain TogetherAIEmbeddings integration lives in the `@langchain/community` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core
</Npm2Yarn>
```
"""

"""
## Instantiation

Now we can instantiate our model object and generate chat completions:
"""

import { TogetherAIEmbeddings } from "@langchain/community/embeddings/togetherai";

const embeddings = new TogetherAIEmbeddings({
  model: "togethercomputer/m2-bert-80M-8k-retrieval", // Default value
});

"""
## Indexing and Retrieval

Embedding models are often used in retrieval-augmented generation (RAG) flows, both as part of indexing data as well as later retrieving it. For more detailed instructions, please see our RAG tutorials under the [working with external knowledge tutorials](/docs/tutorials/#working-with-external-knowledge).

Below, see how to index and retrieve data using the `embeddings` object we initialized above. In this example, we will index and retrieve a sample document using the demo [`MemoryVectorStore`](/docs/integrations/vectorstores/memory).
"""

// Create a vector store with a sample text
import { MemoryVectorStore } from "langchain/vectorstores/memory";

const text = "LangChain is the framework for building context-aware reasoning applications";

const vectorstore = await MemoryVectorStore.fromDocuments(
  [{ pageContent: text, metadata: {} }],
  embeddings,
);

// Use the vector store as a retriever that returns a single document
const retriever = vectorstore.asRetriever(1);

// Retrieve the most similar text
const retrievedDocuments = await retriever.invoke("What is LangChain?");

retrievedDocuments[0].pageContent;
# Output:
#   LangChain is the framework for building context-aware reasoning applications


"""
## Direct Usage

Under the hood, the vectorstore and retriever implementations are calling `embeddings.embedDocument(...)` and `embeddings.embedQuery(...)` to create embeddings for the text(s) used in `fromDocuments` and the retriever's `invoke` operations, respectively.

You can directly call these methods to get embeddings for your own use cases.

### Embed single texts

You can embed queries for search with `embedQuery`. This generates a vector representation specific to the query:
"""

const singleVector = await embeddings.embedQuery(text);

console.log(singleVector.slice(0, 100));
# Output:
#   [

#        0.3812227, -0.052848946,  -0.10564975,   0.03480297,    0.2878488,

#     0.0084609175,   0.11605915,   0.05303011,   0.14711718,  -0.14407106,

#      -0.29865336,  -0.15807179, -0.068397366,   -0.2708063,  0.056596708,

#      -0.07656515,  0.052995138,  -0.11275427,  0.028096694,  0.123501234,

#     -0.039519835,   0.12148692,  -0.12820457,   0.15691335,  0.033519063,

#      -0.27026987,  -0.08460162,  -0.23792154,    -0.234982,  -0.05786798,

#      0.016467346,  -0.17168592, -0.060787182,  0.038752213,  -0.08169927,

#       0.09327062,   0.29490772,    0.0167866,  -0.32224452,   -0.2037822,

#      -0.10284172, -0.124050565,   0.25344968,  -0.06275548,  -0.14180769,

#     0.0046709594,  0.073105976,   0.12004031,   0.19224276, -0.022589967,

#      0.102790825,    0.1138286, -0.057701062, -0.050010648,   -0.1632584,

#      -0.18942119,  -0.12018798,   0.15288158,   0.07941474,   0.10440051,

#      -0.13257962,  -0.19282033,  0.044656333,   0.13560675, -0.068929024,

#      0.028590716,  0.055663664,   0.04652713,  0.014936657,  0.120679885,

#      0.053866718,  -0.16296014,  0.119450666,  -0.29559663,  0.008097747,

#       0.07380408,  -0.09010084,   -0.0687739,  -0.08575685,  -0.07202606,

#       0.18868081,  -0.08392917,  0.014016109,   0.15435852, -0.030115498,

#      -0.16927013,   0.02836557, -0.050763763,    0.0840437,  -0.22718845,

#      0.111397505,  0.033395614, -0.123287566,   -0.2111604,   -0.1580479,

#       0.05520573,   -0.1422921,   0.08828953,  0.051058788,  -0.13312188

#   ]


"""
### Embed multiple texts

You can embed multiple texts for indexing with `embedDocuments`. The internals used for this method may (but do not have to) differ from embedding queries:
"""

const text2 = "LangGraph is a library for building stateful, multi-actor applications with LLMs";

const vectors = await embeddings.embedDocuments([text, text2]);

console.log(vectors[0].slice(0, 100));
console.log(vectors[1].slice(0, 100));
# Output:
#   [

#        0.3812227, -0.052848946,  -0.10564975,   0.03480297,    0.2878488,

#     0.0084609175,   0.11605915,   0.05303011,   0.14711718,  -0.14407106,

#      -0.29865336,  -0.15807179, -0.068397366,   -0.2708063,  0.056596708,

#      -0.07656515,  0.052995138,  -0.11275427,  0.028096694,  0.123501234,

#     -0.039519835,   0.12148692,  -0.12820457,   0.15691335,  0.033519063,

#      -0.27026987,  -0.08460162,  -0.23792154,    -0.234982,  -0.05786798,

#      0.016467346,  -0.17168592, -0.060787182,  0.038752213,  -0.08169927,

#       0.09327062,   0.29490772,    0.0167866,  -0.32224452,   -0.2037822,

#      -0.10284172, -0.124050565,   0.25344968,  -0.06275548,  -0.14180769,

#     0.0046709594,  0.073105976,   0.12004031,   0.19224276, -0.022589967,

#      0.102790825,    0.1138286, -0.057701062, -0.050010648,   -0.1632584,

#      -0.18942119,  -0.12018798,   0.15288158,   0.07941474,   0.10440051,

#      -0.13257962,  -0.19282033,  0.044656333,   0.13560675, -0.068929024,

#      0.028590716,  0.055663664,   0.04652713,  0.014936657,  0.120679885,

#      0.053866718,  -0.16296014,  0.119450666,  -0.29559663,  0.008097747,

#       0.07380408,  -0.09010084,   -0.0687739,  -0.08575685,  -0.07202606,

#       0.18868081,  -0.08392917,  0.014016109,   0.15435852, -0.030115498,

#      -0.16927013,   0.02836557, -0.050763763,    0.0840437,  -0.22718845,

#      0.111397505,  0.033395614, -0.123287566,   -0.2111604,   -0.1580479,

#       0.05520573,   -0.1422921,   0.08828953,  0.051058788,  -0.13312188

#   ]

#   [

#      0.066308185, -0.032866564,  0.115751594,   0.19082588,      0.14017,

#      -0.26976448, -0.056340694,  -0.26923394,    0.2548541,  -0.27271318,

#       -0.2244126,   0.07949589,  -0.27710953,  -0.17993368,   0.09681616,

#      -0.08692256,   0.22127126,  -0.14512022,  -0.18016525,   0.14892976,

#       -0.0526347, -0.008140617,   -0.2916987,   0.23706906,  -0.38488507,

#      -0.35881752,   0.09276949,  -0.07051063,  -0.07778231,   0.12552947,

#       0.06256748,  -0.25832427,  0.025054429,   -0.1451448,   -0.2662871,

#       0.13676351,  -0.07413256,   0.14966589,  -0.39968985,   0.15542287,

#      -0.13107607,   0.02761394,  0.108077586,  -0.12076956,     0.128296,

#      -0.05625126,   0.15723586, -0.056932643,   0.23720805,   0.23993455,

#     -0.035553705, -0.053907514,  -0.11852807,   0.07005695,  -0.06317475,

#      0.070009425,     0.284697,    0.2212059,  0.018890115,   0.16924675,

#       0.21651487,   0.07259682,    0.1328156,    0.3261852,    0.1914124,

#      -0.10120423,   0.03450111,  -0.22588971,  -0.04458192,   0.24116798,

#     -0.021830376,  -0.30731413,   0.08586451, -0.058835756, 0.0010347435,

#     0.0031927782,  -0.09403646,  -0.22608931,   0.15865424,   0.15738021,

#       0.23582733,    0.1714161,    0.1585189,  -0.18085755,  0.019376995,

#     -0.026587496, -0.017079154,  -0.04588549, -0.047336094, -0.082413346,

#       -0.1114185,  -0.05403556,   0.12438637,  -0.20476522,     0.073182,

#      -0.12210378, -0.010543863,  -0.09767598,    0.1057683, -0.050204434

#   ]


"""
## API reference

For detailed documentation of all TogetherAIEmbeddings features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_community_embeddings_togetherai.TogetherAIEmbeddings.html
"""



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/transformers.mdx
================================================
import CodeBlock from "@theme/CodeBlock";

# HuggingFace Transformers

The `TransformerEmbeddings` class uses the [Transformers.js](https://huggingface.co/docs/transformers.js/index) package to generate embeddings for a given text.

It runs locally and even works directly in the browser, allowing you to create web apps with built-in embeddings.

## Setup

You'll need to install the [@huggingface/transformers](https://www.npmjs.com/package/@huggingface/transformers) package as a peer dependency:

:::tip Compatibility
If you are using a version of community older than 0.3.21, install the older `@xenova/transformers` package and
import the embeddings from `"@langchain/community/embeddings/hf_transformers"` below.
:::

```bash npm2yarn
npm install @huggingface/transformers
```

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core
```

## Example

Note that if you're using in a browser context, you'll likely want to put all inference-related code in a web worker to avoid
blocking the main thread.

See [this guide](https://huggingface.co/docs/transformers.js/tutorials/next) and the other resources in the Transformers.js docs for an idea of how to
set up your project.

import HFTransformersExample from "@examples/models/embeddings/hf_transformers.ts";

<CodeBlock language="typescript">{HFTransformersExample}</CodeBlock>

## Related

- Embedding model [conceptual guide](/docs/concepts/embedding_models)
- Embedding model [how-to guides](/docs/how_to/#embedding-models)



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/voyageai.mdx
================================================
# Voyage AI

The `VoyageEmbeddings` class uses the Voyage AI REST API to generate embeddings for a given text.

The `inputType` parameter allows you to specify the type of input text for better embedding results. You can set it to `query`, `document`, or leave it undefined (which is equivalent to `None`).

- `query`: Use this for search or retrieval queries. Voyage AI will prepend a prompt to optimize the embeddings for query use cases.
- `document`: Use this for documents or content that you want to be retrievable. Voyage AI will prepend a prompt to optimize the embeddings for document use cases.
- `None` (default): The input text will be directly encoded without any additional prompt.

Additionally, the class supports new parameters for further customization of the embedding process:

- **truncation**: Whether to truncate the input texts to the maximum length allowed by the model.
- **outputDimension**: The desired dimension of the output embeddings.
- **outputDtype**: The data type of the output embeddings. Can be `"float"` or `"int8"`.
- **encodingFormat**: The format of the output embeddings. Can be `"float"`, `"base64"`, or `"ubinary"`.

```typescript
import { VoyageEmbeddings } from "@langchain/community/embeddings/voyage";

const embeddings = new VoyageEmbeddings({
  apiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.VOYAGEAI_API_KEY
  inputType: "document", // Optional: specify input type as 'query', 'document', or omit for None / Undefined / Null
  truncation: true, // Optional: enable truncation of input texts
  outputDimension: 768, // Optional: set desired output embedding dimension
  outputDtype: "float", // Optional: set output data type ("float" or "int8")
  encodingFormat: "float", // Optional: set output encoding format ("float", "base64", or "ubinary")
});
```

## Related

- Embedding model [conceptual guide](/docs/concepts/embedding_models)
- Embedding model [how-to guides](/docs/how_to/#embedding-models)



================================================
FILE: docs/core_docs/docs/integrations/text_embedding/zhipuai.mdx
================================================
---
sidebar_class_name: node-only
---

# ZhipuAI

The `ZhipuAIEmbeddings` class uses the ZhipuAI API to generate embeddings for a given text.

## Setup

You'll need to sign up for an ZhipuAI API key and set it as an environment variable named `ZHIPUAI_API_KEY`.

https://open.bigmodel.cn

Then, you'll need to install the [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) package:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core jsonwebtoken
```

## Usage

import CodeBlock from "@theme/CodeBlock";
import ZhipuAIExample from "@examples/embeddings/zhipuai.ts";

<CodeBlock language="typescript">{ZhipuAIExample}</CodeBlock>

## Related

- Embedding model [conceptual guide](/docs/concepts/embedding_models)
- Embedding model [how-to guides](/docs/how_to/#embedding-models)



================================================
FILE: docs/core_docs/docs/integrations/toolkits/connery.mdx
================================================
import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/agents/connery_mrkl.ts";
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

# Connery Toolkit

Using this toolkit, you can integrate Connery Actions into your LangChain agent.

:::note
If you want to use only one particular Connery Action in your agent,
check out the [Connery Action Tool](/docs/integrations/tools/connery) documentation.
:::

## What is Connery?

Connery is an open-source plugin infrastructure for AI.

With Connery, you can easily create a custom plugin with a set of actions and seamlessly integrate them into your LangChain agent.
Connery will take care of critical aspects such as runtime, authorization, secret management, access management, audit logs, and other vital features.

Furthermore, Connery, supported by our community, provides a diverse collection of ready-to-use open-source plugins for added convenience.

Learn more about Connery:

- GitHub: https://github.com/connery-io/connery
- Documentation: https://docs.connery.io

## Prerequisites

To use Connery Actions in your LangChain agent, you need to do some preparation:

1. Set up the Connery runner using the [Quickstart](https://docs.connery.io/docs/runner/quick-start/) guide.
2. Install all the plugins with the actions you want to use in your agent.
3. Set environment variables `CONNERY_RUNNER_URL` and `CONNERY_RUNNER_API_KEY` so the toolkit can communicate with the Connery Runner.

## Example of using Connery Toolkit

### Setup

To use the Connery Toolkit you need to install the following official peer dependency:

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

<IntegrationInstallTooltip></IntegrationInstallTooltip>

### Usage

In the example below, we create an agent that uses two Connery Actions to summarize a public webpage and send the summary by email:

1. **Summarize public webpage** action from the [Summarization](https://github.com/connery-io/summarization-plugin) plugin.
2. **Send email** action from the [Gmail](https://github.com/connery-io/gmail) plugin.

:::info
You can see a LangSmith trace of this example [here](https://smith.langchain.com/public/5485cb37-b73d-458f-8162-43639f2b49e1/r).
:::

<CodeBlock language="typescript">{Example}</CodeBlock>

:::note
Connery Action is a structured tool, so you can only use it in the agents supporting structured tools.
:::



================================================
FILE: docs/core_docs/docs/integrations/toolkits/ibm.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: WatsonxToolkit
---
"""

"""
# WatsonxToolkit

This will help you getting started with the [WatsonxToolkit](/docs/concepts/#toolkits). For detailed documentation of all WatsonxToolkit features and configurations head to the [API reference](https://v03.api.js.langchain.com/classes/_langchain_community.agents_toolkits_ibm.html).

The toolkit contains following tools:

| Name | Description |
| ---- | ----------- |
| `GoogleSearch` | Search for online trends, news, current events, real-time information, or research topics. |
| `WebCrawler` | Useful for when you need to summarize a webpage. Do not use for Web search. |
| `PythonInterpreter` | Run Python code generated by the agent model. |
| `SDXLTurbo` | Generate an image from text using Stability.ai |
| `Weather` | Find the weather for a city. |
| `RAGQuery` | Search the documents in a vector index. |


### Integration details

| Class | Package | [PY support](https://python.langchain.com/docs/integrations/tools/ibm_watsonx/) | Package latest |
| :--- | :--- | :---: | :---: |
| [WatsonxToolkit](https://api.js.langchain.com/classes/_langchain_community.agents_toolkits_ibm.WatsonxToolkit.html) | [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) | ✅ |  ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |

## Setup

If you want to get automated tracing from runs of individual tools, you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
process.env.LANGSMITH_TRACING="true"
process.env.LANGSMITH_API_KEY="your-api-key"
```

### Installation

This toolkit lives in the `@langchain/community` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core
</Npm2Yarn>
```
"""

"""
## Instantiation

Now we can instantiate our toolkit:
"""

import { WatsonxToolkit } from "@langchain/community/agents/toolkits/ibm";
import "dotenv/config"

const toolkit = await WatsonxToolkit.init({
  version: '2024-05-31',
  serviceUrl: process.env.WATSONX_AI_SERVICE_URL
});
# Output:
#   [Module: null prototype] { default: {} }

"""
## Tools

View available tools:
"""

const tools = toolkit.getTools();

console.log(tools.map((tool) => ({
  name: tool.name,
  description: tool.description,
})))
# Output:
#   [

#     {

#       name: "GoogleSearch",

#       description: "Search for online trends, news, current events, real-time information, or research topics."

#     },

#     {

#       name: "WebCrawler",

#       description: "Useful for when you need to summarize a webpage. Do not use for Web search."

#     },

#     {

#       name: "PythonInterpreter",

#       description: "Run Python code generated by the agent model."

#     },

#     {

#       name: "SDXLTurbo",

#       description: "Generate an image from text using Stability.ai"

#     },

#     { name: "Weather", description: "Find the weather for a city." },

#     {

#       name: "RAGQuery",

#       description: "Search the documents in a vector index."

#     }

#   ]


"""
For detailed info about tools please visit [watsonx.ai API docs](https://cloud.ibm.com/apidocs/watsonx-ai#get-utility-agent-tools)
"""

"""
## Use within an agent

First, ensure you have LangGraph installed:

```{=mdx}
<Npm2Yarn>
  @langchain/langgraph
</Npm2Yarn>
```
"""

"""
Then, instanciate your LLM to be used in the React agent:

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

// @lc-docs-hide-cell

import { ChatWatsonx } from "@langchain/community/chat_models/ibm";

const llm = new ChatWatsonx({
  version: '2024-05-31',
  serviceUrl: process.env.WATSONX_AI_SERVICE_URL,
  model: 'ibm/granite-3-8b-instruct',
  projectId: process.env.WATSONX_AI_PROJECT_ID
});

import { createReactAgent } from "@langchain/langgraph/prebuilt"

const agent = createReactAgent({ llm, tools });

const exampleQuery = "Who won F1 championship in 2022?"

const events = await agent.stream(
  { messages: [{ role: "user", content: exampleQuery }]},
  { streamMode: "values", }
)

for await (const event of events) {
  const lastMsg = event.messages[event.messages.length - 1];
  if (lastMsg.tool_calls?.length) {
    console.dir(lastMsg.tool_calls, { depth: null });
  } else if (lastMsg.content) {
    console.log(lastMsg.content);
  }
}
# Output:
#   Who won F1 championship in 2022?

#   [

#     {

#       name: "GoogleSearch",

#       args: { input: "F1 championship 2022" },

#       type: "tool_call",

#       id: "chatcmpl-tool-9da3456b9bbc475fb822296fdb8353a8"

#     }

#   ]

#   [{"title":"2022 DRIVER STANDINGS","description":"Official F1® Race Programme · Modern Slavery Statement; Do Not Sell or Share My Personal Information. Formula 1. © 2003-2025 Formula One World Championship ...","url":"https://www.formula1.com/en/results/2022/drivers"},{"title":"2022 Formula One World Championship - Wikipedia","description":"2022 Formula One World Championship · Max Verstappen won his second consecutive World Drivers' Championship driving for Red Bull Racing. · Charles Leclerc ...","url":"https://en.wikipedia.org/wiki/2022_Formula_One_World_Championship"},{"title":"2022 Formula One World Championship - Simple English Wikipedia ...","description":"Max Verstappen, who was the reigning Drivers' Champion, claimed his second title at the Japanese Grand Prix, while his team, Red Bull Racing, achieved their ...","url":"https://simple.wikipedia.org/wiki/2022_Formula_One_World_Championship"},{"title":"Max Verstappen wins the 2022 F1 Drivers World Championship : r ...","description":"Oct 9, 2022 ... One day this guy will win a championship just by finishing the race and celebrating with a world championship lap on the day he won.","url":"https://www.reddit.com/r/sports/comments/xzg8pf/max_verstappen_wins_the_2022_f1_drivers_world/"},{"title":"Red Bull Simulator Championship Edition | F1 Authentics","description":"Based on the livery of the 2022 Oracle Red Bull Racing Championship-winning RB18, this F1 simulator has been expertly engineered and manufactured by Memento ...","url":"https://www.f1authentics.com/products/red-bull-simulator-championship-edition"},{"title":"F1 2022 Drivers Championship without Max Verstappen : r/formula1","description":"Nov 26, 2022 ... 1.1K votes, 65 comments. 5.2M subscribers in the formula1 community. Welcome to r/Formula1, the best independent online Formula 1 community!","url":"https://www.reddit.com/r/formula1/comments/z5cwl7/f1_2022_drivers_championship_without_max/"},{"title":"F1® Sim Racing World Championship 2022","description":"World Championship 2022 Bahrain International Circuit Bahrain International Circuit Race Distance: 29 laps Track Length: 5.412 km","url":"https://f1esports.com/world/results/2022"},{"title":"Our personal F1 2022 Predictions Championship : r/formula1","description":"May 5, 2022 ... F1 2025 Driver Predictions from mathematical model. Leclerc and Sainz predicted to beat Hamilton and Albon. r/formula1 - F1 2025 Driver ...","url":"https://www.reddit.com/r/formula1/comments/uitbbu/our_personal_f1_2022_predictions_championship/"},{"title":"Haas F1 Team Esports announces roster for 2022 F1 Esports Series ...","description":"Sep 13, 2022 ... Haas F1 Team is ready to commence battle in the 2022 F1 Esports Series Pro Championship featuring an updated line-up for this season.","url":"https://www.haasf1team.com/news/haas-f1-team-esports-announces-roster-2022-f1-esports-series-pro-championship"},{"title":"2022 F1 Deconstructors Championship : r/formula1","description":"Nov 22, 2022 ... Congratulations to our 2022 champion Carlos Sainz. Alonso put in a late surge but had to settle for second place. Alfa put in the best effort ...","url":"https://www.reddit.com/r/formula1/comments/z1pkkx/2022_f1_deconstructors_championship/"}]

#   Max Verstappen won the 2022 F1 Championship, driving for Red Bull Racing. He claimed his second title at the Japanese Grand Prix.


"""
## API reference

For detailed documentation of all `WatsonxToolkit` features and configurations head to the [API
 reference](https://api.js.langchain.com/classes/_langchain_community.agents_toolkits_ibm.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/toolkits/index.mdx
================================================
---
sidebar_position: 0
sidebar_class_name: hidden
---

# Toolkits

import { CategoryTable, IndexTable } from "@theme/FeatureTables";

Also see [Tools page](/docs/integrations/tools/).

## All Toolkits

<IndexTable />



================================================
FILE: docs/core_docs/docs/integrations/toolkits/json.mdx
================================================
# JSON Agent Toolkit

This example shows how to load and use an agent with a JSON toolkit.

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core
```

```typescript
import * as fs from "fs";
import * as yaml from "js-yaml";
import { OpenAI } from "@langchain/openai";
import { JsonSpec, JsonObject } from "langchain/tools";
import { JsonToolkit, createJsonAgent } from "langchain/agents";

export const run = async () => {
  let data: JsonObject;
  try {
    const yamlFile = fs.readFileSync("openai_openapi.yaml", "utf8");
    data = yaml.load(yamlFile) as JsonObject;
    if (!data) {
      throw new Error("Failed to load OpenAPI spec");
    }
  } catch (e) {
    console.error(e);
    return;
  }

  const toolkit = new JsonToolkit(new JsonSpec(data));
  const model = new OpenAI({ temperature: 0 });
  const executor = createJsonAgent(model, toolkit);

  const input = `What are the required parameters in the request body to the /completions endpoint?`;

  console.log(`Executing with input "${input}"...`);

  const result = await executor.invoke({ input });

  console.log(`Got output ${result.output}`);

  console.log(
    `Got intermediate steps ${JSON.stringify(
      result.intermediateSteps,
      null,
      2
    )}`
  );
};
```



================================================
FILE: docs/core_docs/docs/integrations/toolkits/openapi.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: OpenApi Toolkit
---
"""

"""
# OpenApiToolkit

```{=mdx}

:::caution Disclaimer ⚠️

This agent can make requests to external APIs. Use with caution, especially when granting access to users.

Be aware that this agent could theoretically send requests with provided credentials or other sensitive data to unverified or potentially malicious URLs --although it should never in theory.

Consider adding limitations to what actions can be performed via the agent, what APIs it can access, what headers can be passed, and more.

In addition, consider implementing measures to validate URLs before sending requests, and to securely handle and protect sensitive data such as credentials.

:::

```

This will help you getting started with the [OpenApiToolkit](/docs/concepts/tools/#toolkits). For detailed documentation of all OpenApiToolkit features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.agents.OpenApiToolkit.html).

The `OpenAPIToolkit` has access to the following tools:

| Name            | Description |
|-----------------|-------------|
| `requests_get`  | A portal to the internet. Use this when you need to get specific content from a website. Input should be a url string (i.e. "https://www.google.com"). The output will be the text response of the GET request. |
| `requests_post` | Use this when you want to POST to a website. Input should be a json string with two keys: "url" and "data". The value of "url" should be a string, and the value of "data" should be a dictionary of key-value pairs you want to POST to the url as a JSON body. Be careful to always use double quotes for strings in the json string. The output will be the text response of the POST request. |
| `json_explorer` | Can be used to answer questions about the openapi spec for the API. Always use this tool before trying to make a request. Example inputs to this tool: 'What are the required query parameters for a GET request to the /bar endpoint?' 'What are the required parameters in the request body for a POST request to the /foo endpoint?' Always give this tool a specific question. |

## Setup

This toolkit requires an OpenAPI spec file. The LangChain.js repository has a [sample OpenAPI spec file in the `examples` directory](https://github.com/langchain-ai/langchainjs/blob/cc21aa29102571204f4443a40b53d28581a12e30/examples/openai_openapi.yaml). You can use this file to test the toolkit.

If you want to get automated tracing from runs of individual tools, you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
process.env.LANGSMITH_TRACING="true"
process.env.LANGSMITH_API_KEY="your-api-key"
```

### Installation

This toolkit lives in the `langchain` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  langchain @langchain/core
</Npm2Yarn>
```
"""

"""
## Instantiation

Now we can instantiate our toolkit. First, we need to define the LLM we would like to use in the toolkit.

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

// @lc-docs-hide-cell

import { ChatOpenAI } from "@langchain/openai";
const llm = new ChatOpenAI({
  model: "gpt-4o-mini",
  temperature: 0,
})

import { OpenApiToolkit } from "langchain/agents/toolkits"
import * as fs from "fs";
import * as yaml from "js-yaml";
import { JsonSpec, JsonObject } from "langchain/tools";

// Load & convert the OpenAPI spec from YAML to JSON.
const yamlFile = fs.readFileSync("../../../../../examples/openai_openapi.yaml", "utf8");
const data = yaml.load(yamlFile) as JsonObject;
if (!data) {
  throw new Error("Failed to load OpenAPI spec");
}

// Define headers for the API requests.
const headers = {
  "Content-Type": "application/json",
  Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,
};

const toolkit = new OpenApiToolkit(new JsonSpec(data), llm, headers);

"""
## Tools

View available tools:
"""

const tools = toolkit.getTools();

console.log(tools.map((tool) => ({
  name: tool.name,
  description: tool.description,
})))
# Output:
#   [

#     {

#       name: 'requests_get',

#       description: 'A portal to the internet. Use this when you need to get specific content from a website.\n' +

#         '  Input should be a url string (i.e. "https://www.google.com"). The output will be the text response of the GET request.'

#     },

#     {

#       name: 'requests_post',

#       description: 'Use this when you want to POST to a website.\n' +

#         '  Input should be a json string with two keys: "url" and "data".\n' +

#         '  The value of "url" should be a string, and the value of "data" should be a dictionary of\n' +

#         '  key-value pairs you want to POST to the url as a JSON body.\n' +

#         '  Be careful to always use double quotes for strings in the json string\n' +

#         '  The output will be the text response of the POST request.'

#     },

#     {

#       name: 'json_explorer',

#       description: '\n' +

#         'Can be used to answer questions about the openapi spec for the API. Always use this tool before trying to make a request. \n' +

#         'Example inputs to this tool: \n' +

#         "    'What are the required query parameters for a GET request to the /bar endpoint?'\n" +

#         "    'What are the required parameters in the request body for a POST request to the /foo endpoint?'\n" +

#         'Always give this tool a specific question.'

#     }

#   ]


"""
## Use within an agent

First, ensure you have LangGraph installed:

```{=mdx}
<Npm2Yarn>
  @langchain/langgraph
</Npm2Yarn>
```
"""

import { createReactAgent } from "@langchain/langgraph/prebuilt"

const agentExecutor = createReactAgent({ llm, tools });

const exampleQuery = "Make a POST request to openai /chat/completions. The prompt should be 'tell me a joke.'. Ensure you use the model 'gpt-4o-mini'."

const events = await agentExecutor.stream(
  { messages: [["user", exampleQuery]]},
  { streamMode: "values", }
)

for await (const event of events) {
  const lastMsg = event.messages[event.messages.length - 1];
  if (lastMsg.tool_calls?.length) {
    console.dir(lastMsg.tool_calls, { depth: null });
  } else if (lastMsg.content) {
    console.log(lastMsg.content);
  }
}
# Output:
#   [

#     {

#       name: 'requests_post',

#       args: {

#         input: '{"url":"https://api.openai.com/v1/chat/completions","data":{"model":"gpt-4o-mini","messages":[{"role":"user","content":"tell me a joke."}]}}'

#       },

#       type: 'tool_call',

#       id: 'call_1HqyZrbYgKFwQRfAtsZA2uL5'

#     }

#   ]

#   {

#     "id": "chatcmpl-9t36IIuRCs0WGMEy69HUqPcKvOc1w",

#     "object": "chat.completion",

#     "created": 1722906986,

#     "model": "gpt-4o-mini-2024-07-18",

#     "choices": [

#       {

#         "index": 0,

#         "message": {

#           "role": "assistant",

#           "content": "Why don't skeletons fight each other? \n\nThey don't have the guts!"

#         },

#         "logprobs": null,

#         "finish_reason": "stop"

#       }

#     ],

#     "usage": {

#       "prompt_tokens": 12,

#       "completion_tokens": 15,

#       "total_tokens": 27

#     },

#     "system_fingerprint": "fp_48196bc67a"

#   }

#   

#   Here's a joke for you:

#   

#   **Why don't skeletons fight each other?**  

#   They don't have the guts!


"""
## API reference

For detailed documentation of all OpenApiToolkit features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.agents.OpenApiToolkit.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/toolkits/sfn_agent.mdx
================================================
---
sidebar_label: AWS Step Functions Toolkit
hide_table_of_contents: true
---

# AWS Step Functions Toolkit

**AWS Step Functions** are a visual workflow service that helps developers use AWS services to build distributed applications, automate processes, orchestrate microservices, and create data and machine learning (ML) pipelines.

By including a `AWSSfn` tool in the list of tools provided to an Agent, you can grant your Agent the ability to invoke async workflows running in your AWS Cloud.

When an Agent uses the `AWSSfn` tool, it will provide an argument of type `string` which will in turn be passed into one of the supported actions this tool supports. The supported actions are: `StartExecution`, `DescribeExecution`, and `SendTaskSuccess`.

## Setup

You'll need to install the Node AWS Step Functions SDK:

```bash npm2yarn
npm install @aws-sdk/client-sfn
```

## Usage

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

### Note about credentials:

- If you have not run [`aws configure`](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) via the AWS CLI, the `region`, `accessKeyId`, and `secretAccessKey` must be provided to the AWSSfn constructor.
- The IAM role corresponding to those credentials must have permission to invoke the Step Function.

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/agents/aws_sfn.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>



================================================
FILE: docs/core_docs/docs/integrations/toolkits/sql.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Sql Toolkit
---
"""

"""
# SqlToolkit

This will help you getting started with the [SqlToolkit](/docs/concepts/tools/#toolkits). For detailed documentation of all SqlToolkit features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.agents_toolkits_sql.SqlToolkit.html). You can also find the documentation for the Python equivalent [here](https://python.langchain.com/docs/integrations/toolkits/sql_database/).

This toolkit contains a the following tools:

| Name              | Description                                                                                                                                                                                                                               |
|-------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `query-sql`       | Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. |
| `info-sql`        | Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling list-tables-sql first! Example Input: "table1, table2, table3".          |
| `list-tables-sql` | Input is an empty string, output is a comma-separated list of tables in the database.                                                                                                                                                     |
| `query-checker`   | Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with query-sql!                                                                                                 |

This toolkit is useful for asking questions, performing queries, validating queries and more on a SQL database.

## Setup

This example uses Chinook database, which is a sample database available for SQL Server, Oracle, MySQL, etc. To set it up, follow [these instructions](https://database.guide/2-sample-databases-sqlite/), placing the `.db` file in the directory where your code lives.

If you want to get automated tracing from runs of individual tools, you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
process.env.LANGSMITH_TRACING="true"
process.env.LANGSMITH_API_KEY="your-api-key"
```

### Installation

This toolkit lives in the `langchain` package. You'll also need to install the `typeorm` peer dependency.

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  langchain @langchain/core typeorm
</Npm2Yarn>
```
"""

"""
## Instantiation

First, we need to define our LLM to be used in the toolkit.

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

// @lc-docs-hide-cell

import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({
  model: "gpt-4o-mini",
  temperature: 0,
})

import { SqlToolkit } from "langchain/agents/toolkits/sql"
import { DataSource } from "typeorm";
import { SqlDatabase } from "langchain/sql_db";

const datasource = new DataSource({
  type: "sqlite",
  database: "../../../../../../Chinook.db", // Replace with the link to your database
});
const db = await SqlDatabase.fromDataSourceParams({
  appDataSource: datasource,
});

const toolkit = new SqlToolkit(db, llm);

"""
## Tools

View available tools:
"""

const tools = toolkit.getTools();

console.log(tools.map((tool) => ({
  name: tool.name,
  description: tool.description,
})))
# Output:
#   [

#     {

#       name: 'query-sql',

#       description: 'Input to this tool is a detailed and correct SQL query, output is a result from the database.\n' +

#         '  If the query is not correct, an error message will be returned.\n' +

#         '  If an error is returned, rewrite the query, check the query, and try again.'

#     },

#     {

#       name: 'info-sql',

#       description: 'Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables.\n' +

#         '    Be sure that the tables actually exist by calling list-tables-sql first!\n' +

#         '\n' +

#         '    Example Input: "table1, table2, table3.'

#     },

#     {

#       name: 'list-tables-sql',

#       description: 'Input is an empty string, output is a comma-separated list of tables in the database.'

#     },

#     {

#       name: 'query-checker',

#       description: 'Use this tool to double check if your query is correct before executing it.\n' +

#         '    Always use this tool before executing a query with query-sql!'

#     }

#   ]


"""
## Use within an agent

First, ensure you have LangGraph installed:

```{=mdx}
<Npm2Yarn>
  @langchain/langgraph
</Npm2Yarn>
```
"""

import { createReactAgent } from "@langchain/langgraph/prebuilt"

const agentExecutor = createReactAgent({ llm, tools });

const exampleQuery = "Can you list 10 artists from my database?"

const events = await agentExecutor.stream(
  { messages: [["user", exampleQuery]]},
  { streamMode: "values", }
)

for await (const event of events) {
  const lastMsg = event.messages[event.messages.length - 1];
  if (lastMsg.tool_calls?.length) {
    console.dir(lastMsg.tool_calls, { depth: null });
  } else if (lastMsg.content) {
    console.log(lastMsg.content);
  }
}
# Output:
#   [

#     {

#       name: 'list-tables-sql',

#       args: {},

#       type: 'tool_call',

#       id: 'call_LqsRA86SsKmzhRfSRekIQtff'

#     }

#   ]

#   Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track

#   [

#     {

#       name: 'query-checker',

#       args: { input: 'SELECT * FROM Artist LIMIT 10;' },

#       type: 'tool_call',

#       id: 'call_MKBCjt4gKhl5UpnjsMHmDrBH'

#     }

#   ]

#   The SQL query you provided is:

#   

#   ```sql

#   SELECT * FROM Artist LIMIT 10;

#   ```

#   

#   This query is straightforward and does not contain any of the common mistakes listed. It simply selects all columns from the `Artist` table and limits the result to 10 rows. 

#   

#   Therefore, there are no mistakes to correct, and the original query can be reproduced as is:

#   

#   ```sql

#   SELECT * FROM Artist LIMIT 10;

#   ```

#   [

#     {

#       name: 'query-sql',

#       args: { input: 'SELECT * FROM Artist LIMIT 10;' },

#       type: 'tool_call',

#       id: 'call_a8MPiqXPMaN6yjN9i7rJctJo'

#     }

#   ]

#   [{"ArtistId":1,"Name":"AC/DC"},{"ArtistId":2,"Name":"Accept"},{"ArtistId":3,"Name":"Aerosmith"},{"ArtistId":4,"Name":"Alanis Morissette"},{"ArtistId":5,"Name":"Alice In Chains"},{"ArtistId":6,"Name":"Antônio Carlos Jobim"},{"ArtistId":7,"Name":"Apocalyptica"},{"ArtistId":8,"Name":"Audioslave"},{"ArtistId":9,"Name":"BackBeat"},{"ArtistId":10,"Name":"Billy Cobham"}]

#   Here are 10 artists from your database:

#   

#   1. AC/DC

#   2. Accept

#   3. Aerosmith

#   4. Alanis Morissette

#   5. Alice In Chains

#   6. Antônio Carlos Jobim

#   7. Apocalyptica

#   8. Audioslave

#   9. BackBeat

#   10. Billy Cobham


"""
## API reference

For detailed documentation of all SqlToolkit features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.agents_toolkits_sql.SqlToolkit.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/toolkits/vectorstore.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: VectorStore Toolkit
---
"""

"""
# VectorStoreToolkit

This will help you getting started with the [VectorStoreToolkit](/docs/concepts/tools/#toolkits). For detailed documentation of all VectorStoreToolkit features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.agents.VectorStoreToolkit.html).

The `VectorStoreToolkit` is a toolkit which takes in a vector store, and converts it to a tool which can then be invoked, passed to LLMs, agents and more.

## Setup

If you want to get automated tracing from runs of individual tools, you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
process.env.LANGSMITH_TRACING="true"
process.env.LANGSMITH_API_KEY="your-api-key"
```

### Installation

This toolkit lives in the `langchain` package:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  langchain @langchain/core
</Npm2Yarn>
```
"""

"""
## Instantiation

Now we can instantiate our toolkit. First, we need to define the LLM we'll use in the toolkit.

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

// @lc-docs-hide-cell

import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({
  model: "gpt-4o-mini",
  temperature: 0,
})

import { VectorStoreToolkit, VectorStoreInfo } from "langchain/agents/toolkits"
import { OpenAIEmbeddings } from "@langchain/openai"
import { MemoryVectorStore } from "langchain/vectorstores/memory"
import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";
import fs from "fs";

// Load a text file to use as our data source.
const text = fs.readFileSync("../../../../../examples/state_of_the_union.txt", "utf8");

// Split the text into chunks before inserting to our store
const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
const docs = await textSplitter.createDocuments([text]);

const vectorStore = await MemoryVectorStore.fromDocuments(docs, new OpenAIEmbeddings());

const vectorStoreInfo: VectorStoreInfo = {
  name: "state_of_union_address",
  description: "the most recent state of the Union address",
  vectorStore,
};

const toolkit = new VectorStoreToolkit(vectorStoreInfo, llm);

"""
## Tools

Here, we can see it converts our vector store into a tool:
"""

const tools = toolkit.getTools();

console.log(tools.map((tool) => ({
  name: tool.name,
  description: tool.description,
})))
# Output:
#   [

#     {

#       name: 'state_of_union_address',

#       description: 'Useful for when you need to answer questions about state_of_union_address. Whenever you need information about the most recent state of the Union address you should ALWAYS use this. Input should be a fully formed question.'

#     }

#   ]


"""
## Use within an agent

First, ensure you have LangGraph installed:

```{=mdx}
<Npm2Yarn>
  @langchain/langgraph
</Npm2Yarn>
```

Then, instantiate the agent:
"""

import { createReactAgent } from "@langchain/langgraph/prebuilt"

const agentExecutor = createReactAgent({ llm, tools });

const exampleQuery = "What did biden say about Ketanji Brown Jackson is the state of the union address?"

const events = await agentExecutor.stream(
  { messages: [["user", exampleQuery]]},
  { streamMode: "values", }
)

for await (const event of events) {
  const lastMsg = event.messages[event.messages.length - 1];
  if (lastMsg.tool_calls?.length) {
    console.dir(lastMsg.tool_calls, { depth: null });
  } else if (lastMsg.content) {
    console.log(lastMsg.content);
  }
}
# Output:
#   [

#     {

#       name: 'state_of_union_address',

#       args: {

#         input: 'What did Biden say about Ketanji Brown Jackson in the State of the Union address?'

#       },

#       type: 'tool_call',

#       id: 'call_glJSWLNrftKHa92A6j8x4jhd'

#     }

#   ]

#   In the State of the Union address, Biden mentioned that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson, describing her as one of the nation’s top legal minds who will continue Justice Breyer’s legacy of excellence. He highlighted her background as a former top litigator in private practice, a former federal public defender, and noted that she comes from a family of public school educators and police officers. He also pointed out that she has received a broad range of support since her nomination.

#   In the State of the Union address, President Biden spoke about Ketanji Brown Jackson, stating that he nominated her as one of the nation’s top legal minds who will continue Justice Breyer’s legacy of excellence. He highlighted her experience as a former top litigator in private practice and a federal public defender, as well as her background coming from a family of public school educators and police officers. Biden also noted that she has received a broad range of support since her nomination.


"""
## API reference

For detailed documentation of all VectorStoreToolkit features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.agents.VectorStoreToolkit.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/tools/aiplugin-tool.mdx
================================================
---
hide_table_of_contents: true
sidebar_class_name: hidden
---

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/agents/aiplugin-tool.ts";

# ChatGPT Plugins

:::warning
OpenAI has [deprecated plugins](https://openai.com/index/chatgpt-plugins/).
:::

This example shows how to use ChatGPT Plugins within LangChain abstractions.

Note 1: This currently only works for plugins with no auth.

Note 2: There are almost certainly other ways to do this, this is just a first pass. If you have better ideas, please open a PR!

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core
```

<CodeBlock language="typescript">{Example}</CodeBlock>

````
Entering new agent_executor chain...
Thought: Klarna is a payment provider, not a store. I need to check if there is a Klarna Shopping API that I can use to search for t-shirts.
Action:
```

{
"action": "KlarnaProducts",
"action_input": ""
}

```

Usage Guide: Use the Klarna plugin to get relevant product suggestions for any shopping or researching purpose. The query to be sent should not include stopwords like articles, prepositions and determinants. The api works best when searching for words that are related to products, like their name, brand, model or category. Links will always be returned and should be shown to the user.

OpenAPI Spec: {"openapi":"3.0.1","info":{"version":"v0","title":"Open AI Klarna product Api"},"servers":[{"url":"https://www.klarna.com/us/shopping"}],"tags":[{"name":"open-ai-product-endpoint","description":"Open AI Product Endpoint. Query for products."}],"paths":{"/public/openai/v0/products":{"get":{"tags":["open-ai-product-endpoint"],"summary":"API for fetching Klarna product information","operationId":"productsUsingGET","parameters":[{"name":"q","in":"query","description":"query, must be between 2 and 100 characters","required":true,"schema":{"type":"string"}},{"name":"size","in":"query","description":"number of products returned","required":false,"schema":{"type":"integer"}},{"name":"budget","in":"query","description":"maximum price of the matching product in local currency, filters results","required":false,"schema":{"type":"integer"}}],"responses":{"200":{"description":"Products found","content":{"application/json":{"schema":{"$ref":"#/components/schemas/ProductResponse"}}}},"503":{"description":"one or more services are unavailable"}},"deprecated":false}}},"components":{"schemas":{"Product":{"type":"object","properties":{"attributes":{"type":"array","items":{"type":"string"}},"name":{"type":"string"},"price":{"type":"string"},"url":{"type":"string"}},"title":"Product"},"ProductResponse":{"type":"object","properties":{"products":{"type":"array","items":{"$ref":"#/components/schemas/Product"}}},"title":"ProductResponse"}}}}
Now that I know there is a Klarna Shopping API, I can use it to search for t-shirts. I will make a GET request to the API with the query parameter "t-shirt".
Action:
```

{
"action": "requests_get",
"action_input": "https://www.klarna.com/us/shopping/public/openai/v0/products?q=t-shirt"
}

```


{"products":[{"name":"Psycho Bunny Mens Copa Gradient Logo Graphic Tee","url":"https://www.klarna.com/us/shopping/pl/cl10001/3203663222/Clothing/Psycho-Bunny-Mens-Copa-Gradient-Logo-Graphic-Tee/?source=openai","price":"$35.00","attributes":["Material:Cotton","Target Group:Man","Color:White,Blue,Black,Orange"]},{"name":"T-shirt","url":"https://www.klarna.com/us/shopping/pl/cl10001/3203506327/Clothing/T-shirt/?source=openai","price":"$20.45","attributes":["Material:Cotton","Target Group:Man","Color:Gray,White,Blue,Black,Orange"]},{"name":"Palm Angels Bear T-shirt - Black","url":"https://www.klarna.com/us/shopping/pl/cl10001/3201090513/Clothing/Palm-Angels-Bear-T-shirt-Black/?source=openai","price":"$168.36","attributes":["Material:Cotton","Target Group:Man","Color:Black"]},{"name":"Tommy Hilfiger Essential Flag Logo T-shirt","url":"https://www.klarna.com/us/shopping/pl/cl10001/3201840629/Clothing/Tommy-Hilfiger-Essential-Flag-Logo-T-shirt/?source=openai","price":"$22.52","attributes":["Material:Cotton","Target Group:Man","Color:Red,Gray,White,Blue,Black","Pattern:Solid Color","Environmental Attributes :Organic"]},{"name":"Coach Outlet Signature T Shirt","url":"https://www.klarna.com/us/shopping/pl/cl10001/3203005573/Clothing/Coach-Outlet-Signature-T-Shirt/?source=openai","price":"$75.00","attributes":["Material:Cotton","Target Group:Man","Color:Gray"]}]}
Finished chain.
{
  result: {
    output: 'The available t-shirts in Klarna are Psycho Bunny Mens Copa Gradient Logo Graphic Tee, T-shirt, Palm Angels Bear T-shirt - Black, Tommy Hilfiger Essential Flag Logo T-shirt, and Coach Outlet Signature T Shirt.',
    intermediateSteps: [ [Object], [Object] ]
  }
}
````

## Related

- Tool [conceptual guide](/docs/concepts/tools)
- Tool [how-to guides](/docs/how_to/#tools)



================================================
FILE: docs/core_docs/docs/integrations/tools/azure_dynamic_sessions.mdx
================================================
# Azure Container Apps Dynamic Sessions

> [Azure Container Apps dynamic sessions](https://learn.microsoft.com/azure/container-apps/sessions) provide fast access to secure sandboxed environments that are ideal for running code or applications that require strong isolation from other workloads.

You can learn more about Azure Container Apps dynamic sessions and its code interpretation capabilities on [this page](https://learn.microsoft.com/azure/container-apps/sessions). If you don't have an Azure account, you can [create a free account](https://azure.microsoft.com/free/) to get started.

## Setup

You'll first need to install the [`@langchain/azure-dynamic-sessions`](https://www.npmjs.com/package/@langchain/azure-dynamic-sessions) package:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/azure-dynamic-sessions @langchain/core
```

You'll also need to have a code interpreter session pool instance running. You can deploy a version using [Azure CLI](https://learn.microsoft.com/cli/azure/install-azure-cli) following [this guide](https://learn.microsoft.com/azure/container-apps/sessions-code-interpreter).

Once you have your instance running, you need to make sure you have properly set up the Azure Entra authentication for it. You can find the instructions on how to do that [here](https://learn.microsoft.com/azure/container-apps/sessions?tabs=azure-cli#authentication).

After you've added the role for your identity, you need to retrieve the **session pool management endpoint**. You can find it in the Azure Portal, under the "Overview" section of your instance. Then you need to set the following environment variable:

import CodeBlock from "@theme/CodeBlock";
import EnvVars from "@examples/tools/azure_dynamic_sessions/.env.example";

<CodeBlock language="text">{EnvVars}</CodeBlock>

## Usage example

Below is a simple example that creates a new Python code interpreter session, invoke the tool and prints the result.

import Example from "@examples/tools/azure_dynamic_sessions/azure_dynamic_sessions.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

Here is a complete example where we use an Azure OpenAI chat model to call the Python code interpreter session tool to execute the code and get the result:

import AgentExample from "@examples/tools/azure_dynamic_sessions/azure_dynamic_sessions-agent.ts";

<CodeBlock language="typescript">{AgentExample}</CodeBlock>

## Related

- Tool [conceptual guide](/docs/concepts/tools)
- Tool [how-to guides](/docs/how_to/#tools)



================================================
FILE: docs/core_docs/docs/integrations/tools/connery.mdx
================================================
import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/tools/connery.ts";
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

# Connery Action Tool

Using this tool, you can integrate individual Connery Action into your LangChain agent.

:::note
If you want to use more than one Connery Action in your agent,
check out the [Connery Toolkit](/docs/integrations/toolkits/connery) documentation.
:::

## What is Connery?

Connery is an open-source plugin infrastructure for AI.

With Connery, you can easily create a custom plugin with a set of actions and seamlessly integrate them into your LangChain agent.
Connery will take care of critical aspects such as runtime, authorization, secret management, access management, audit logs, and other vital features.

Furthermore, Connery, supported by our community, provides a diverse collection of ready-to-use open-source plugins for added convenience.

Learn more about Connery:

- GitHub: https://github.com/connery-io/connery
- Documentation: https://docs.connery.io

## Prerequisites

To use Connery Actions in your LangChain agent, you need to do some preparation:

1. Set up the Connery runner using the [Quickstart](https://docs.connery.io/docs/runner/quick-start/) guide.
2. Install all the plugins with the actions you want to use in your agent.
3. Set environment variables `CONNERY_RUNNER_URL` and `CONNERY_RUNNER_API_KEY` so the toolkit can communicate with the Connery Runner.

## Example of using Connery Action Tool

### Setup

To use the Connery Action Tool you need to install the following official peer dependency:

```bash npm2yarn
npm install @langchain/community @langchain/core
```

<IntegrationInstallTooltip></IntegrationInstallTooltip>

### Usage

In the example below, we fetch action by its ID from the Connery Runner and then call it with the specified parameters.

Here, we use the ID of the **Send email** action from the [Gmail](https://github.com/connery-io/gmail) plugin.

:::info
You can see a LangSmith trace of this example [here](https://smith.langchain.com/public/c4b6723d-f91c-440c-8682-16ec8297a602/r).
:::

<CodeBlock language="typescript">{Example}</CodeBlock>

:::note
Connery Action is a structured tool, so you can only use it in the agents supporting structured tools.
:::

## Related

- Tool [conceptual guide](/docs/concepts/tools)
- Tool [how-to guides](/docs/how_to/#tools)



================================================
FILE: docs/core_docs/docs/integrations/tools/dalle.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";
import ToolExample from "@examples/tools/dalle_image_generation.ts";

# Dall-E Tool

The Dall-E tool allows your agent to create images using OpenAI's Dall-E image generation tool.

## Setup

You will need an OpenAI API Key which you can get from the [OpenAI web site](https://openai.com)
and then set the OPENAI_API_KEY environment variable to the key you just created.

To use the Dall-E Tool you need to install the LangChain OpenAI integration package:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core
```

<CodeBlock language="typescript">{ToolExample}</CodeBlock>

## Related

- Tool [conceptual guide](/docs/concepts/tools)
- Tool [how-to guides](/docs/how_to/#tools)



================================================
FILE: docs/core_docs/docs/integrations/tools/discord.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# Discord Tool

The Discord Tool gives your agent the ability to search, read, and write messages to discord channels.
It is useful for when you need to interact with a discord channel.

## Setup

To use the Discord Tool you need to install the following official peer depencency:

```bash npm2yarn
npm install discord.js
```

## Usage, standalone

import ToolExample from "@examples/tools/discord.ts";

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core
```

<CodeBlock language="typescript">{ToolExample}</CodeBlock>

## Usage, in an Agent

import AgentExample from "@examples/agents/discord.ts";

<CodeBlock language="typescript">{AgentExample}</CodeBlock>

## Related

- Tool [conceptual guide](/docs/concepts/tools)
- Tool [how-to guides](/docs/how_to/#tools)



================================================
FILE: docs/core_docs/docs/integrations/tools/duckduckgo_search.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: DuckDuckGoSearch
---
"""

"""
# DuckDuckGoSearch

This notebook provides a quick overview for getting started with [DuckDuckGoSearch](/docs/integrations/tools/). For detailed documentation of all DuckDuckGoSearch features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_tools_duckduckgo_search.DuckDuckGoSearch.html).

DuckDuckGoSearch offers a privacy-focused search API designed for LLM Agents. It provides seamless integration with a wide range of data sources, prioritizing user privacy and relevant search results.

## Overview

### Integration details

| Class | Package | [PY support](https://python.langchain.com/docs/integrations/tools/ddg/) | Package latest |
| :--- | :--- | :---: | :---: |
| [DuckDuckGoSearch](https://api.js.langchain.com/classes/langchain_community_tools_duckduckgo_search.DuckDuckGoSearch.html) | [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) | ✅ |  ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |

## Setup

The integration lives in the `@langchain/community` package, along with the `duck-duck-scrape` dependency:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core duck-duck-scrape
</Npm2Yarn>
```

### Credentials

It's also helpful (but not needed) to set up [LangSmith](https://smith.langchain.com/) for best-in-class observability:

```typescript
process.env.LANGSMITH_TRACING="true"
process.env.LANGSMITH_API_KEY="your-api-key"
```
"""

"""
## Instantiation

You can instantiate an instance of the `DuckDuckGoSearch` tool like this:
"""

import { DuckDuckGoSearch } from "@langchain/community/tools/duckduckgo_search"

const tool = new DuckDuckGoSearch({ maxResults: 1 })

"""
## Invocation

### [Invoke directly with args](/docs/concepts/tools)
"""

await tool.invoke("what is the current weather in sf?")
# Output:
#   [{"title":"San Francisco, CA Current Weather | AccuWeather","link":"https://www.accuweather.com/en/us/san-francisco/94103/current-weather/347629","snippet":"<b>Current</b> <b>weather</b> <b>in</b> San Francisco, CA. Check <b>current</b> conditions in San Francisco, CA with radar, hourly, and more."}]


"""
### [Invoke with ToolCall](/docs/concepts/tools)

We can also invoke the tool with a model-generated `ToolCall`, in which case a `ToolMessage` will be returned:
"""

// This is usually generated by a model, but we'll create a tool call directly for demo purposes.
const modelGeneratedToolCall = {
  args: {
    input: "what is the current weather in sf?"
  },
  id: "tool_call_id",
  name: tool.name,
  type: "tool_call",
}
await tool.invoke(modelGeneratedToolCall)
# Output:
#   ToolMessage {

#     "content": "[{\"title\":\"San Francisco, CA Weather Conditions | Weather Underground\",\"link\":\"https://www.wunderground.com/weather/us/ca/san-francisco\",\"snippet\":\"San Francisco <b>Weather</b> Forecasts. <b>Weather</b> Underground provides local & long-range <b>weather</b> forecasts, weatherreports, maps & tropical <b>weather</b> conditions for the San Francisco area.\"}]",

#     "name": "duckduckgo-search",

#     "additional_kwargs": {},

#     "response_metadata": {},

#     "tool_call_id": "tool_call_id"

#   }


"""
## Chaining

We can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```

"""

// @lc-docs-hide-cell

import { ChatOpenAI } from "@langchain/openai"

const llm = new ChatOpenAI({
  model: "gpt-4o-mini",
})

import { HumanMessage } from "@langchain/core/messages";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnableLambda } from "@langchain/core/runnables";

const prompt = ChatPromptTemplate.fromMessages(
  [
    ["system", "You are a helpful assistant."],
    ["placeholder", "{messages}"],
  ]
)

const llmWithTools = llm.bindTools([tool]);

const chain = prompt.pipe(llmWithTools);

const toolChain = RunnableLambda.from(
  async (userInput: string, config) => {
    const humanMessage = new HumanMessage(userInput,);
    const aiMsg = await chain.invoke({
      messages: [new HumanMessage(userInput)],
    }, config);
    const toolMsgs = await tool.batch(aiMsg.tool_calls, config);
    return chain.invoke({
      messages: [humanMessage, aiMsg, ...toolMsgs],
    }, config);
  }
);

const toolChainResult = await toolChain.invoke("how many people have climbed mount everest?");

const { tool_calls, content } = toolChainResult;

console.log("AIMessage", JSON.stringify({
  tool_calls,
  content,
}, null, 2));
# Output:
#   AIMessage {

#     "tool_calls": [],

#     "content": "As of December 2023, a total of 6,664 different people have reached the summit of Mount Everest."

#   }


"""
## Agents

For guides on how to use LangChain tools in agents, see the [LangGraph.js](https://langchain-ai.github.io/langgraphjs/) docs.
"""

"""
## API reference

For detailed documentation of all DuckDuckGoSearch features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_tools_duckduckgo_search.DuckDuckGoSearch.html)
"""



================================================
FILE: docs/core_docs/docs/integrations/tools/exa_search.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# ExaSearchResults

Exa (formerly Metaphor Search) is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.

Unlike keyword-based search (Google), Exa's neural search capabilities allow it to semantically understand queries and return relevant documents. For example, we could search `"fascinating article about cats"` and compare the search results from Google and Exa. Google gives us SEO-optimized listicles based on the keyword “fascinating”. Exa just works.

This page goes over how to use `ExaSearchResults` with LangChain.

## Overview

### Integration details

| Class | Package | Serializable | [PY support](https://python.langchain.com/docs/integrations/tools/exa_search/) |  Package latest |
| :--- | :--- | :---: | :---: | :---: |
| [ExaSearchResults](https://api.js.langchain.com/classes/langchain_exa.ExaSearchResults.html) | [@langchain/exa](https://npmjs.com/package/@langchain/exa) | ❌ | ✅ |  ![NPM - Version](https://img.shields.io/npm/v/@langchain/exa?style=flat-square&label=%20&) |

## Setup

The integration lives in the `@langchain/exa` package.

```{=mdx}

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/exa @langchain/core
</Npm2Yarn>

```

### Credentials

First, get an Exa API key and add it as an environment variable. Get 1000 free searches/month by signing up [here](https://dashboard.exa.ai/login).

```typescript
process.env.EXASEARCH_API_KEY="your-api-key"
```

It's also helpful (but not needed) to set up LangSmith for best-in-class observability:

```typescript
process.env.LANGSMITH_TRACING="true"
process.env.LANGSMITH_API_KEY="your-api-key"
```
"""

"""
## Instantiation

Here we show how to insatiate an instance of the `ExaSearchResults` tool:
"""

import { ExaSearchResults } from "@langchain/exa"
import Exa from "exa-js";

// @lc-ts-ignore
const client = new Exa(process.env.EXASEARCH_API_KEY)

const tool = new ExaSearchResults({
  // @lc-ts-ignore
  client,
  searchArgs: {
    numResults: 2,
  }
})

"""
## Invocation

### [Invoke directly with args](/docs/concepts/tools)
"""

await tool.invoke("what is the weather in wailea?")
# Output:
#   {"results":[{"score":0.16085544228553772,"title":"Hawaii Weather Forecast","id":"https://www.willyweather.com/hi/hawaii.html","url":"https://www.willyweather.com/hi/hawaii.html","publishedDate":"2023-01-01","author":"","text":"Get an account to remove ads    View More Real-Time Extremes   Nation State County      Hottest 78.8 °FFaleolo Intl / Apia, Samoa, HI    Coldest 51.6 °FBradshaw Army Air Field / Hawaii, HI    Windiest 12.7mphBradshaw Army Air Field / Hawaii, HI    Most Humid 100%Hilo, Hilo International Airport, HI    Least Humid 73.32%Kailua / Kona, Keahole Airport, HI    Highest Pressure 1030.5 hPaBradshaw Army Air Field / Hawaii, HI    Lowest Pressure 1008 hPaFaleolo Intl / Apia, Samoa, HI"},{"score":0.1591680943965912,"title":"The Hawaii Climate To Prepare For Your Maui Wedding","id":"https://mymauiwedding.weebly.com/blog6/the-hawaii-climate-to-prepare-for-your-maui-wedding","url":"https://mymauiwedding.weebly.com/blog6/the-hawaii-climate-to-prepare-for-your-maui-wedding","publishedDate":"2012-04-26","author":"","text":"Since the The hawaiian islands environment is very constant throughout the season with only slight heat range changes, you can travel there any season. While the moisture is very high, the continuous exotic sea breezes keep the circumstances very relaxed throughout the season. During the day you will be relaxed in a T-shirt or an Aloha clothing and a couple of shoes. Once the sun places you will probably want to wear a light coat since the circumstances can fall around ten levels. The protecting impact of the hills and the variations in climate at various levels make a variety of environment areas. The unique micro-climates are specific for the internal valleys, hill hills and seashores in The hawaiian islands. Located at the side of the exotic location and due to year-round heated sea exterior circumstances, which keep the overlying environment heated, The hawaiian islands has only two circumstances, both of them heated and one with a little bit more rain. Hawaii Climate During Summer  Between the several weeks of Apr and Nov the environment is more dry and hotter with the conditions including 75-88. In the summer time the northern eastern business gusts of wind carry most of the rain to the destinations leeward part, which delivers a welcome comfort from the hot and dry climate.The conditions you will encounter will be proportional to where you are on the destinations. If you are on the edges that are protected from the gusts of wind, the the southeast part of and European factors, you will encounters hot and dry circumstances. If you are on the windward factors, northern or eastern, you will obtain the complete power of the gusts of wind and encounter moister and shade circumstances. Go windward for exotic circumstances and leeward for an dry environment. Hawaii Climate During Winter  From Dec to Apr it is just a little bit chilly, with conditions between 68-80 F. Winter season is regarded rain. The biggest down pours come between Oct and Apr (the hoo'ilo season). Though stormy weather may be common, they usually complete through the destinations quickly and without event. There are more dark times to mess up your laying in the sun, but it hardly ever down pours more than 3 times in a row in one identify. Winter is search period, so if you're a search participant, come to the Northern Coast in Explore to get the ideal trend. Also, whale viewing period is at the end of winter, during Jan to Apr, so make sure you are here if you want to see these spectacular creatures! Hawaii Climate is Greatly Influenced by the Mountains  The hills around the destinations are accountable for the large variety of circumstances. As an example, Kauai's Mt. Waialele is one of the rainiest destinations on the world. Mt. Waialele gets over 420 inches large of rainfall each season, but just a few kilometers down the line, Waimea Canyn is absolutely dry and has been nicknamed the \"Grand Canyn of the Pacific\". On Big Isle The hawaiian destinations, Hilo is one of the rainiest places in the nation, with 180 inches large of rainfall a season. But Puako, only 60 kilometers away, gets less than 6 inches large of rainfall. If you choose to discover the organic charm discovered at greater levels such as Mauna Kea, use long jeans and several levels of awesome climate outfits. The heat variety in the greater destinations falls 3.5 levels for every 1,000 toes above sea level.Watching the dawn from Mt Haleakala's peak is a incredible concept, but be sure to package up with neckties and work gloves that will keep you comfortable. The circumstances at the peak can fall to 30 F!. Also know that there is less security from the sun at greater levels so be sure to utilize the sun display liberally and use eyewear and a hat. The environment can modify greatly in just a few time when you are in the hills. The exclusive The hawaiian destinations environment makes it possible to sun shower on the Kona Shore and ski on Mauna Kea in the same day."}],"requestId":"2145d8de65373c70250400c2c9e8eb13"}


"""
### [Invoke with ToolCall](/docs/concepts/tools)

We can also invoke the tool with a model-generated `ToolCall`, in which case a `ToolMessage` will be returned:
"""

// This is usually generated by a model, but we'll create a tool call directly for demo purposes.
const modelGeneratedToolCall = {
  args: {
    input: "what is the weather in wailea"
  },
  id: "1",
  name: tool.name,
  type: "tool_call",
}
await tool.invoke(modelGeneratedToolCall)
# Output:
#   ToolMessage {

#     "content": "{\"results\":[{\"score\":0.12955062091350555,\"title\":\"Urban Dictionary: Waianae\",\"id\":\"https://www.urbandictionary.com/define.php?term=Waianae\",\"url\":\"https://www.urbandictionary.com/define.php?term=Waianae\",\"publishedDate\":\"2006-04-19\",\"author\":\"\",\"text\":\"Hot but good time for go beach ,in this part of Hawaii you HAVE to have respect ,with people and their stuff, but Some people like act dumb and stupid so that’s the only thing that make Waianae look bad , but foreal kine in this part of Hawaii we have respect and if we don’t get that respect you gon expect no respect back .   Get the Waianae mug.    Advertise here for $5/day    Located on the west end of Oahu. Waianae gets a bad reputation for being poor, dirty, scary, etc. Its hot and dry out west and the beaches are super nice. Makaha, Yokes, and Pray for Sex are some great beaches to name a few. Mostly locals and the majority of the homeless live out here. Even though its a little rough, the people have alot of aloha who live out here. Most important thing here is to have respect for other people and their stuff.   Get the WAIANAE mug.    Advertise here for $5/day    When going too the island of Honolulu if you go to an amazing part for the island called Waianae, say ho sole u know where can find 1 top banggahh like get 1 Waianae special. Then say shoots boto  by   August 1, 2021   Get the Waianae special mug.\"},{\"score\":0.12563708424568176,\"title\":\"Mount Waialeale: One of the Wettest Spots on Earth | Hawaii.com\",\"id\":\"https://www.hawaii.com/trip-ideas/mount-waialeale-one-of-the-wettest-spots-on-earth/\",\"url\":\"https://www.hawaii.com/trip-ideas/mount-waialeale-one-of-the-wettest-spots-on-earth/\",\"publishedDate\":\"2022-01-18\",\"author\":\"Matthew Jones\",\"text\":\"Wai’ale’ale, Kauai without much cloud cover. Photo:  WiseTim .   \\nMount Wai‘ale‘ale on the gorgeous island of Kaua‘i is often referred to as the wettest spot on earth. While the more than 5,000-foot tall mountain that’s often enshrouded in clouds does receive a tremendous amount of rainfall each year, it’s more accurately “one of” the wettest spots on earth. The average annual rainfall is around 500 inches but some spots on the planet, such as “Big Bog” on Maui, typically acquire even more moisture.\\nLegend Has It\\n    Road to Waialeale Basin, Kauai. Photo:  Bryce Edwards .  \\nMany legends surround this mystical peak that includes native inhabitants climbing to the top to make offerings to the Hawaiian god, Kane. Remains of a heiau (place of worship constructed from rocks) at the summit confirm that some kind of ancient activity took place here, even though getting to the water-logged location seems nearly impossible.\\nWai‘ale‘ale, which is actually a dormant shield volcano, means “rippling or overflowing water” in Hawaiian. Consider yourself lucky if you capture a glimpse of the top of the sky-high summit during your vacation. The best opportunity is during crisp, early mornings before clouds form. But you also need to be in the proper location – Līhu‘e, Kapa‘a, and Wailua offer some of the best vantage points for Wai‘ale‘ale.\\nAs Seen From Kuilau Ridge\\n    Views of Mount Waialeale from Kuilau Ridge, Kauai. Photo:  Martin Bravenboer .  \\nTo get even closer to the second-highest peak on the island you can traverse the Kuilau Ridge Trail in Wailua, located near the end of Kuamo‘o Road. About midway through the easy 2-mile roundtrip hike is a great spot for viewing the mountain.\\nWeeping Wall\\n    Mount Waialeale “Wall of Tears” from the air. Photo:  FH .  \\nFurther down the road and well beyond the paved portion is another hike that takes daring souls to the basin of Wai‘ale‘ale called the “Weeping Wall” where numerous ribbons of waterfalls cascade from the summit. But don’t even consider this adventure unless you’re accompanied by an experienced local guide, as you can easily get lost since there is no maintained trail and there is always a high risk for flash flooding that creates dangerous encounters with rushing water.\\nViews from the Alakai Swamp Trail\\n    Kilohana Overlook of Hanalei Bay. Photo:  Hawaii Savvy .  \\nThat said, there is another safer way to get close to this magical mountain – via the Alaka‘i Swamp Trail located in Koke‘e State Park. The difficult hike is about 8 miles roundtrip and you must start out extremely early to get to the midway point in time to see the vista before fog settles in. But those who see Wai‘ale‘ale uncovered at this prime vantage point, along with Hanalei Bay below, are in for a tremendous treat.\"}],\"requestId\":\"37fb09f547148c664026aa61f19c27ed\"}",

#     "name": "exa_search_results_json",

#     "additional_kwargs": {},

#     "response_metadata": {},

#     "tool_call_id": "1"

#   }


"""
## Chaining

We can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling) and then calling it:

```{=mdx}

import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />

```
"""

// @lc-docs-hide-cell

import { ChatOpenAI } from "@langchain/openai"

const llm = new ChatOpenAI({
  model: "gpt-4o-mini",
})

import { ChatPromptTemplate } from "@langchain/core/prompts"
import { RunnableConfig } from "@langchain/core/runnables"
import { AIMessage } from "@langchain/core/messages"

const prompt = ChatPromptTemplate.fromMessages(
  [
    ["system", "You are a helpful assistant."],
    ["human", "{user_input}"],
    ["placeholder", "{messages}"],
  ]
)

// specifying tool_choice will force the model to call this tool.
const llmWithTools = llm.bindTools([tool], {
  tool_choice: tool.name
})

const llmChain = prompt.pipe(llmWithTools);

const toolChain = async (userInput: string, config?: RunnableConfig): Promise<AIMessage> => {
  const input_ = { user_input: userInput };
  const aiMsg = await llmChain.invoke(input_, config);
  const toolMsgs = await tool.batch(aiMsg.tool_calls, config);
  return llmChain.invoke({ ...input_, messages: [aiMsg, ...toolMsgs] }, config);
};

const toolChainResult = await toolChain("What is Anthropic's estimated revenue for 2024?");

const { tool_calls, content } = toolChainResult;

console.log("AIMessage", JSON.stringify({
  tool_calls,
  content
}, null, 2))
# Output:
#   AIMessage {

#     "tool_calls": [

#       {

#         "name": "exa_search_results_json",

#         "args": {

#           "input": "Anthropic revenue 2024 projections"

#         },

#         "type": "tool_call",

#         "id": "call_cgC1G9vjXIjHub0TkVfxiDcr"

#       }

#     ],

#     "content": ""

#   }


"""
## With an Agent

We can create LangChain tools which use the `ExaRetriever` and the `createRetrieverTool` Using these tools we can construct a simple search agent that can answer questions about any topic.

We'll use LangGraph to create the agent. Make sure you have `@langchain/langgraph` installed:

```{=mdx}
<Npm2Yarn>
  @langchain/langgraph
</Npm2Yarn>

Then, define the LLM to use with the agent

<ChatModelTabs customVarName="llmForAgent" />
```
"""

// @lc-docs-hide-cell
import { ChatOpenAI } from "@langchain/openai";

const llmForAgent = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0
})

import Exa from "exa-js";
import { createRetrieverTool } from "langchain/tools/retriever";
import { ExaRetriever } from "@langchain/exa";
import { createReactAgent } from "@langchain/langgraph/prebuilt";

// @lc-ts-ignore
const agentClient = new Exa(process.env.EXASEARCH_API_KEY);

const exaRetrieverForAgent = new ExaRetriever({
  // @lc-ts-ignore
  client: agentClient,
  searchArgs: {
    numResults: 2,
  },
});

// Convert the ExaRetriever into a tool
const searchToolForAgent = createRetrieverTool(exaRetrieverForAgent, {
  name: "search",
  description: "Get the contents of a webpage given a string search query.",
});

const toolsForAgent = [searchToolForAgent];

const agentExecutor = createReactAgent({
  llm: llmForAgent,
  tools: toolsForAgent,
})

const exampleQuery = "Summarize for me a fascinating article about cats."

const events = await agentExecutor.stream(
  { messages: [
    [
      "system",
      `You are a web researcher who answers user questions by looking up information on the internet and retrieving contents of helpful documents. Cite your sources.`,
    ],
    ["human", exampleQuery],
  ] },
  { streamMode: "values", }
)

for await (const event of events) {
  const lastMsg = event.messages[event.messages.length - 1];
  if (lastMsg.tool_calls?.length) {
    console.dir(lastMsg.tool_calls, { depth: null });
  } else if (lastMsg.content) {
    console.log(lastMsg.content);
  }
}
# Output:
#   [

#     {

#       name: 'search',

#       args: { query: 'fascinating article about cats' },

#       type: 'tool_call',

#       id: 'call_EcA0tmWsyNktO7HAsGQnqLVt'

#     }

#   ]

#   No one seems to think brushing kitty's teeth is worth the hassle.      Tyler Comrie / The Atlantic; Getty      On the list of perfect pet parents, Mikel Delgado, a professional feline-behavior consultant, probably ranks high. The Ph.D. expert in animal cognition spends half an hour each evening playing with her three torbie cats, Ruby, Coriander, and Professor Scribbles. She’s trained them to take pills in gelatin capsules, just in case they eventually need meds. She even commissioned a screened-in backyard catio so that the girls can safely venture outside. Delgado would do anything for her cats—well, almost anything. “Guilty as charged,” Delgado told me. “I do not brush my cats’ teeth.” To be fair, most cat owners don’t—probably because they’re well aware that it’s weird, if not downright terrifying, to stick one’s fingers inside an ornery cat’s mouth. Reliable stats are scarce, but informal surveys suggest that less than 5 percent of owners give their cats the dental scrub-a-dub-dub—an estimate that the vets I spoke with endorse. “I’m always very shocked if someone says they brush their cat’s teeth,” says Anson Tsugawa, a veterinary dentist in California. When Steve Valeika, a vet in North Carolina, suggests the practice to his clients, many of them “look at me like I’ve totally lost it,” he told me. (This is where I out myself as one of the loons: My cats, Calvin and Hobbes, get their teeth brushed thrice weekly.) There certainly is an element of absurdity to all of this. Lions, after all, aren’t skulking the savannas for Oral-Bs. But our pets don’t share the diets and lifestyles of their wild counterparts, and their teeth are quite susceptible to the buildup of bacteria that can eventually invade the gums to trigger prolonged, painful disease. Studies suggest that most domestic cats older than four end up developing some sort of gum affliction; several experts told me that the rates of periodontal disease in household felines can exceed 80 percent. Left untreated, these ailments can cost a cat one or more teeth, or even spread their effects throughout the body, potentially compromising organs such as the kidneys, liver, and heart. To stave off kitty gum disease, veterinary guidelines and professionals generally recommend that owners clean their cat’s chompers daily, ideally for at least a minute, hitting every tooth. “That’s the gold standard,” says Santiago Peralta, a veterinary dentist at Cornell University. Even a gap of two or three days can leave enough time for tartar to cement, Jeanne Perrone, a veterinary-dentistry trainer in Florida, told me. But brushing feline teeth is also really, really,  really  hard. Most cats aren’t keen on having things shoved into their mouth, especially not bristly, sludge-covered sticks. (Dogs don’t always love cleanings either, but they’re at least used to engaging their owners with their mouths.) My old cat, Luna, was once so desperate to escape a brushing that she shrieked in my face, then peed all over the floor.  Read: Why we think cats are psychopaths  A niche industry has sprouted to ease the ordeal for hygiene-conscious humans: poultry-flavored toothpastes, cat-size toothbrushes, silicone scrubbers that fit on fingers. Sometimes the gear helps; when Chin-Sun Lee, a New Orleans–based writer, purchased malt-flavored toothpaste for her cat, Tuesday, he went bonkers for the stuff. Every morning, he comes trotting over just so he can lick the brush. Krissy Lyon, a neuroscientist at the Salk Institute, told me that one of her cats, Cocchi, is so crazy for his toothpaste that she and her partner have to “restrain him or lock him in a different room” while they’re brushing the teeth of their other cat, Noma.   Tuesday (left) and Calvin (right) getting their teeth brushed. (Courtesy of Chin-Sun Lee and Katherine J. Wu)   But tasty toothpaste isn’t a sufficient lure for all. Valeika, who extols the virtues of feline oral health, admitted that even his own cat, Boocat, doesn’t reap the benefits of his brushing expertise. He “tried hard-core for a couple weeks” when he adopted her seven years ago. But Boocat was too feisty to stand for such a thing. “She can be a real terror,” Valeika told me. “We once saw her chase a bear out of our yard.” Maybe Boocat is picking up on how odd the whole toothbrushing ritual can be. Even most American people weren’t regularly scrubbing their dentition until around the time of World War II. Vet dentistry, which borrowed principles from its human analogue, “is a relatively new discipline,” Peralta told me. “Thirty years ago, nobody was even thinking about dog or cat teeth.” Nor was it all that long ago that people across the country routinely let their pets sleep outside, eat only table scraps, and run hog wild through the streets. Now pets have become overly pampered, their accessories Gooped. Experts told me that they’ve seen all kinds of snake-oil hacks that purport to functionally replace feline toothbrushing—sprays, gels, toys, water additives, even calls to rub cat teeth with coconut oil. A lot of these products end up just cosmetically whitening teeth, temporarily freshening breath, or accomplishing nothing at all. If a super-simple, once-a-month magic bullet for dental hygiene existed, Tsugawa told me, “we’d be doing it for our own teeth.” There are probably a lot of un-toothbrushed cats out there who could be s-l-o-w-l-y taught to accept the process and maybe even enjoy it. Mary Berg, the president of Beyond the Crown Veterinary Education, told me that one of her colleagues trained her pet to relish the process so much that “she could just say ‘Brusha brusha brusha’ and the cat would come running.” But getting to that point can require weeks or months of conditioning. Berg recommends taking it day by day, introducing cats first to the toothpaste, then to getting one or two teeth touched, and on and on until they’re comfy with the whole set—always “with lots of praise and reward afterward,” she said. And that’s all before “you introduce that scary plastic thing.”  Read: An offbeat approach to bonding with cats  That’s a big ask for many owners, especially those who went the cat route because of the creatures’ rep for being low-maintenance. The consequences of skipping toothbrushing are also subtle because they don’t directly affect humans, Delgado told me. Miss a nail trimming, and the couch might pay the price. But cat teeth aren’t often glimpsed.   Boocat, defender of the realm (Courtesy of Steve Valeika)   The potential downsides of brushing, meanwhile, can be screamingly clear. On cat forums and Twitter, the cat-toothbrushing-phobic joke about losing their fingers. But what a lot of people are really afraid of sacrificing is their cat’s love. Broken trust can mar the relationship between owner and pet, Perrone said; people simply can’t communicate to skittish animals that this act of apparent torture is for their own good. Some cats never learn to deal. Even among veterinary experts, toothbrushing rituals are rare. Peralta and his wife just try to clear the bar of “at least once a week” with their own cat, Kit Kat; Berg and Perrone don’t brush their felines’ teeth at all. (Tsugawa does not currently own a cat, but he wasn’t a brusher when he did.) I’m no pro, but I feel a bit torn too. I never took the time to teach Calvin and Hobbes to see toothbrushing as a treat, and they can get pretty grumpy during the ritual itself. Valeika, the North Carolina vet, told me that seeing Boocat’s horrified reactions was the main thing that prompted him to quit the brush. “She would hate it if we were always doing that to her,” he said. “She really would just not be our pet anymore.”   Feline-health experts know they’re losing this fight. “A lot of us are not even talking about toothbrushing anymore, because nobody’s doing it,” Berg said. Luckily, a few well-vetted alternatives to toothbrushing do exist. Berg and Delgado use specialty kibble that can cut down on plaque, and Perrone’s cat, Noriko, is into Greenies dental treats—both options that many pets may be more receptive to. Scientifically, nothing beats bona fide brushing. But realistically, this young art may already be obsolete. The best interventions, Delgado told me, will be the ones that people actually use. “If someone in my profession doesn’t brush their pet’s teeth,” Berg said, “I can’t blame anybody else.”

#   

#   Last night I watched the Netflix documentary,  Inside the Mind of a Cat . It was a good show that demonstrated cats are thoughtful creatures and amazing predators and that they may have intellectual capacities on par with dogs.

#   In addition to learning about the research, I watched the show from my perspective as a mental behaviorist. A mental behavioral position is one that bridges and resolves the old divide in psychology between the behaviorists (who say that the mind is not a scientific construct and thus they just study behavior) and the mentalists (who say they study overt behaviors and then infer mental processes that are presumed to cause the behaviors).

#   The mental behavioral view says that animals like cats are “minded” creatures, and that they exhibit mental behaviors. To see this, imagine three cats in a tree; one is dead, one is anesthetized, and one is alive and well. Now drop the cats. The mental behaviorist says all three cats behave, but they exhibit different kinds of behaviors as they fall. The first cat falls through the air and lands on the ground. Its behavior is “physical” in that it is caused by the laws and forces of the material world as mapped by the physical sciences. The second cat also falls much like the first. However, if we were to peer inside the cat, we would see that its physiology is very active in maintaining its complex organization. The behaviors of the cat’s cells and organ systems that keep it alive are living behaviors studied by the biological sciences.

#   The third cat rotates, lands on its feet, and takes off. This is a different kind of behavior that cannot be well-described as either physical or biological. Rather, the proper description is mental. Mental behavior is a particular kind of functional awareness and responsivity that animals exhibit. Such behaviors are actions mediated by the brain and nervous system and the complex active body of the cat. More specifically, mental behaviors are a pattern of activity that emerges as function of a complex adaptive, sensory-motor looping system. 

#   Just as we consider entities like cells that exhibit living behaviors to be alive, we should consider creatures like cats that exhibit mental behaviors to be “minded.” The mental behaviorist argues that mindedness is one of the most important concepts that both science and most modern people are blind to. I say “modern people” because, historically, most cultures have seen clearly that animals behave very differently when compared to plants or bacteria, and most cultures have had some kind of category for specifying this difference. For example, Aristotle divided the "soul" into the vegetative, animal, and human layers. In addition, the Great Chain of Being differentiated animals from the rest of the living world. However, our modern scientific system does not have a word for the way animals are in the world that makes them so different. We just call it "animal behavior." And this gap is a major blind spot in our grammar for understanding the world around us.

#   Returning to the documentary, if we did not really look inside "the mind" of cats, what did the documentary actually show? It showed the mental behavioral investment patterns of cats. That is, it showed how cats demonstrate functional awareness and responsivity to various kinds of situations and stimuli. For example, it showed they clearly recognize and respond to their names, it showed they prefer their owners to strangers, and it showed they really do have a unique skill set in their capacity to land on their feet. In other words, it showed cats are minded creatures that exhibit complex adaptive patterns of mental behavior.

#   Although it becomes obvious when you know how to see the world this way (i.e., when I go for a walk in the woods, the mindedness of the squirrels, birds, and bees is as blatantly apparent to me as the living behaviors of the trees and mushrooms), it nevertheless takes practice to learn how to see mindedness in the world. However, we need to make the effort because failing to see mindedness in the world results in much blindness.

#   I found a fascinating article from The Atlantic that delves into the often-overlooked aspect of feline care: brushing cats' teeth. Despite being a crucial part of maintaining a cat's health, very few cat owners actually brush their pets' teeth. The article highlights the challenges and absurdities of this task, noting that less than 5% of cat owners engage in this practice. 

#   

#   Veterinary experts emphasize the importance of dental hygiene for cats, as most domestic cats over the age of four develop some form of gum disease, which can lead to severe health issues if left untreated. The article discusses various tools and techniques, such as poultry-flavored toothpaste and cat-sized toothbrushes, that can make the process easier. However, it also acknowledges the difficulties and resistance many cats show towards toothbrushing.

#   

#   Interestingly, the article also touches on the broader context of pet care, noting how the expectations and practices around pet maintenance have evolved over time. It suggests that while brushing a cat's teeth is ideal, alternative methods like dental treats and specialized kibble can also be effective.

#   

#   For more details, you can read the full article [here](https://www.theatlantic.com/health/archive/2023/10/cat-dental-care-toothbrushing/675123/).


"""
## Related

- Tool [conceptual guide](/docs/concepts/tools)
- Tool [how-to guides](/docs/how_to/#tools)
"""

"""
## API reference

For detailed documentation of all `ExaSearchResults` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_exa.ExaSearchResults.html)
"""



================================================
FILE: docs/core_docs/docs/integrations/tools/gmail.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# Gmail Tool

The Gmail Tool allows your agent to create and view messages from a linked email account.

## Setup

You can authenticate via two methods:

1. Provide an access token, obtained via OAuth2 token exchange, to the credentials object.
   This can be a string or a function so that token expiry and validation can be handled.
   This can be done using an Identity Provider that supports getting access tokens from federated connections.
   This is the most secure method as the access and scope will be limited to the specific end user.
   This method will be more appropriate when using the tool in an application that is meant to be used by end
   users with their own Gmail account.
2. You will need to get an API key from [Google here](https://developers.google.com/gmail/api/guides)
   and [enable the new Gmail API](https://console.cloud.google.com/apis/library/gmail.googleapis.com).
   Then, set the environment variables for `GMAIL_CLIENT_EMAIL`, and either `GMAIL_PRIVATE_KEY`, or `GMAIL_KEYFILE`.

To use the Gmail Tool you need to install the following official peer dependency:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core googleapis
```

## Usage

import ToolExample from "@examples/tools/gmail.ts";

<CodeBlock language="typescript">{ToolExample}</CodeBlock>

## Related

- Tool [conceptual guide](/docs/concepts/tools)
- Tool [how-to guides](/docs/how_to/#tools)



================================================
FILE: docs/core_docs/docs/integrations/tools/google_calendar.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# Google Calendar Tool

The Google Calendar Tools allow your agent to create and view Google Calendar events from a linked calendar.

## Setup

To use the Google Calendar Tools you need to install the following official peer dependency:

```bash npm2yarn
npm install googleapis
```

## Usage

import ToolExample from "@examples/tools/google_calendar.ts";

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core @langchain/community @langchain/langgraph
```

<CodeBlock language="typescript">{ToolExample}</CodeBlock>

## Related

- Tool [conceptual guide](/docs/concepts/tools)
- Tool [how-to guides](/docs/how_to/#tools)



================================================
FILE: docs/core_docs/docs/integrations/tools/google_places.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# Google Places Tool

The Google Places Tool allows your agent to utilize the Google Places API in order to find addresses,
phone numbers, website, etc. from text about a location listed on Google Places.

## Setup

You will need to get an API key from [Google here](https://developers.google.com/maps/documentation/places/web-service/overview)
and [enable the new Places API](https://console.cloud.google.com/apis/library/places.googleapis.com). Then, set your API key
as `process.env.GOOGLE_PLACES_API_KEY` or pass it in as an `apiKey` constructor argument.

## Usage

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

import ToolExample from "@examples/tools/google_places.ts";

<CodeBlock language="typescript">{ToolExample}</CodeBlock>

## Related

- Tool [conceptual guide](/docs/concepts/tools)
- Tool [how-to guides](/docs/how_to/#tools)



================================================
FILE: docs/core_docs/docs/integrations/tools/google_routes.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# Google Routes Tool

The Google Routes Tool allows your agent to utilize the Google Routes API in order to find a route between
two or more destinations. You can get a route by walk, transit, car, motorcycle and bicycle.

## Setup

You will need to get an API key from [Google here](https://developers.google.com/maps/documentation/places/web-service/overview)
and [enable the Routes API](https://console.cloud.google.com/apis/library/routes.googleapis.com). Then, set your API key
as `process.env.GOOGLE_ROUTES_API_KEY` or pass it in as an `apiKey` constructor argument.

## Usage

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

import ToolExample from "@examples/tools/google_routes.ts";

<CodeBlock language="typescript">{ToolExample}</CodeBlock>

## Related

- Tool [conceptual guide](/docs/concepts/tools)
- Tool [how-to guides](/docs/how_to/#tools)



================================================
FILE: docs/core_docs/docs/integrations/tools/google_scholar.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Google Scholar
---
"""

"""
# Google Scholar Tool

This notebook provides a quick overview for getting started with [`SERPGoogleScholarTool`](https://api.js.langchain.com/classes/_langchain_community.tools_google_scholar.SERPGoogleScholarAPITool.html). For detailed documentation of all `SERPGoogleScholarAPITool` features and configurations, head to the [API reference](https://api.js.langchain.com/classes/_langchain_community.tools_google_scholar.SERPGoogleScholarAPITool.html).

## Overview

### Integration details

| Class | Package | [PY support](https://python.langchain.com/docs/integrations/tools/google_scholar/) | Package latest |
| :--- | :--- | :---: | :---: |
| [GoogleScholarTool](https://api.js.langchain.com/classes/_langchain_community.tools_google_scholar.SERPGoogleScholarAPITool.html) | [@langchain/community](https://www.npmjs.com/package/@langchain/community) |  ✅  |  ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |

### Tool features

- Retrieve academic publications by topic, author, or query.
- Fetch metadata such as title, author, and publication year.
- Advanced search filters, including citation count and journal name.

## Setup

The integration lives in the `@langchain/community` package.

```bash
npm install @langchain/community
```

### Credentials

Ensure you have the appropriate API key to access Google Scholar. Set it in your environment variables:

```typescript
process.env.GOOGLE_SCHOLAR_API_KEY="your-serp-api-key"
```

It's also helpful to set up [LangSmith](https://smith.langchain.com/) for best-in-class observability:

```typescript
process.env.LANGSMITH_TRACING="true"
process.env.LANGSMITH_API_KEY="your-langchain-api-key"
```
"""

"""
## Instantiation

You can import and instantiate an instance of the `SERPGoogleScholarAPITool` tool like this:
"""

import { SERPGoogleScholarAPITool } from "@langchain/community/tools/google_scholar";

const tool = new SERPGoogleScholarAPITool({
  apiKey: process.env.SERPAPI_API_KEY,
});

"""
## Invocation

### Invoke directly with args

You can invoke the tool directly with query arguments:
"""

const results = await tool.invoke({
  query: "neural networks",
  maxResults: 5,
});

console.log(results);

"""
### Invoke with ToolCall

We can also invoke the tool with a model-generated `ToolCall`:
"""

const modelGeneratedToolCall = {
  args: { query: "machine learning" },
  id: "1",
  name: tool.name,
  type: "tool_call",
};
await tool.invoke(modelGeneratedToolCall);

"""
## API reference

For detailed documentation of all `SERPGoogleScholarAPITool` features and configurations, head to the [API reference](https://api.js.langchain.com/classes/_langchain_community.tools_google_scholar.SERPGoogleScholarAPITool.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/tools/google_trends.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# Google Trends Tool

The Google Trends Tool allows your agent to utilize the Google Trends API from SerpApi to retrieve and analyze search interest data.
This can be useful for understanding trending topics, regional search interest, and historical popularity of search terms.

For API details see [here](https://serpapi.com/google-trends-api)

SerpApi caches queries, so the first query will be slower while subsequent identical queries will be fast.
Occasionally, related queries will not work while interest over time will be fine. You can check your query [here](https://serpapi.com/playground?engine=google_trends&q=monster&data_type=RELATED_QUERIES).

## Setup

To use this tool, you will need to configure access to the Google Trends API from SerpApi.

Get an API key from [SerpApi](https://serpapi.com/users/sign_in)

Then, set your API key as `process.env.SERPAPI_API_KEY` or pass it in as an `apiKey` constructor argument.

## Usage

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

import ToolExample from "@examples/tools/google_trends.ts";

<CodeBlock language="typescript">{ToolExample}</CodeBlock>

## Related

- Tool [conceptual guide](/docs/concepts/tools)
- Tool [how-to guides](/docs/how_to/#tools)



================================================
FILE: docs/core_docs/docs/integrations/tools/index.mdx
================================================
---
sidebar_position: 0
sidebar_class_name: hidden
---

# Tools and Toolkits

import { CategoryTable, IndexTable } from "@theme/FeatureTables";

[Tools](/docs/concepts/tools) are utilities designed to be called by a model: their inputs are designed to be generated by models, and their outputs are designed to be passed back to models.

A [toolkit](/docs/concepts/tools/#toolkits) is a collection of tools meant to be used together. For a list of toolkit integrations, see [this page](/docs/integrations/toolkits/).

:::info
If you'd like to write your own tool, see [this how-to](/docs/how_to/custom_tools/). If you'd like to contribute an integration, see [Contributing integrations](/docs/contributing).
:::

## All Tools

<IndexTable />



================================================
FILE: docs/core_docs/docs/integrations/tools/jigsawstack.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# JigsawStack Tool

The JigsawStack Tool provides your agent with the following capabilities:

- JigsawStackAIScrape: Scrape web content using advanced AI.

- JigsawStackAISearch: Perform AI-powered web searches and retrieve high-quality results.

- JigsawStackSpeechToText - Transcribe video and audio files using the Whisper large V3 AI model.

- JigsawStackVOCR - Recognize, describe, and extract data from images using a prompt.

- JigsawStackTextToSQL - Generate semantically correct SQL queries from text.

## Setup

- Set up an [account](https://jigsawstack.com/dashboard) (Get started for free)
- Create and retrieve your [API key](https://jigsawstack.com/dashboard)

## Credentials

```bash
export JIGSAWSTACK_API_KEY="your-api-key"
```

## Usage, standalone

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai
```

```js
import {
  JigsawStackAIScrape,
  JigsawStackAISearch,
  JigsawStackSpeechToText,
  JigsawStackVOCR,
  JigsawStackTextToSQL,
} from "@langchain/jigsawstack";

export const run = async () => {
  // AI Scrape Tool
  const aiScrapeTool = new JigsawStackAIScrape({
    params: {
      element_prompts: ["Pro plan"],
    },
  });
  const result = await aiScrapeTool.invoke("https://jigsawstack.com/pricing");

  console.log({ result });

  // AI Search Tool

  const aiSearchTool = new JigsawStackAISearch();
  const doc = await aiSearchTool.invoke("The leaning tower of pisa");
  console.log({ doc });

  // VOCR Tool

  const vocrTool = new JigsawStackVOCR({
    params: {
      prompt: "Describe the image in detail",
    },
  });
  const data = await vocrTool.invoke(
    "https://rogilvkqloanxtvjfrkm.supabase.co/storage/v1/object/public/demo/Collabo%201080x842.jpg?t=2024-03-22T09%3A22%3A48.442Z"
  );

  console.log({ data });

  // Speech-to-Text Tool
  const sttTool = new JigsawStackSpeechToText();
  await sttTool.invoke(
    "https://rogilvkqloanxtvjfrkm.supabase.co/storage/v1/object/public/demo/Video%201737458382653833217.mp4?t=2024-03-22T09%3A50%3A49.894"
  );

  // Text-to-SQL Tool
  const sqlTool = new JigsawStackTextToSQL({
    params: {
      sql_schema:
        "CREATE TABLE Transactions (transaction_id INT PRIMARY KEY, user_id INT NOT NULL,total_amount DECIMAL(10, 2 NOT NULL, transaction_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,status VARCHAR(20) DEFAULT 'pending',FOREIGN KEY(user_id) REFERENCES Users(user_id))",
    },
  });

  await sqlTool.invoke(
    "Generate a query to get transactions that amount exceed 10000 and sort by when created"
  );
};
```

## Usage, in an Agent

```js
import { ChatOpenAI } from "@langchain/openai";
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import {
  JigsawStackAIScrape,
  JigsawStackAISearch,
  JigsawStackVOCR,
  JigsawStackSpeechToText,
  JigsawStackTextToSQL,
} from "@langchain/jigsawstack";

const model = new ChatOpenAI({
  temperature: 0,
});

//  add the tools that you need
const tools = [
  new JigsawStackAIScrape(),
  new JigsawStackAISearch(),
  new JigsawStackVOCR(),
  new JigsawStackSpeechToText(),
  new JigsawStackTextToSQL(),
];

const executor = await initializeAgentExecutorWithOptions(tools, model, {
  agentType: "zero-shot-react-description",
  verbose: true,
});

const res = await executor.invoke({
  input: `Kokkalo Restaurant Santorini`,
});

console.log(res.output);

/*
{
  "query": "Kokkalo Restaurant Santorini",
  "ai_overview": "Kokkalo Restaurant, located in Fira, Santorini, offers a unique dining experience that blends traditional Greek cuisine with modern culinary trends. Here are some key details about the restaurant:\n\n- **Location**: Situated on the main road of Firostefani, Kokkalo is surrounded by the picturesque Cycladic architecture and provides stunning views of the Aegean Sea.\n- **Cuisine**: The restaurant specializes in authentic Greek dishes, crafted from high-quality, locally sourced ingredients. The menu is designed to engage all senses and features a variety of Mediterranean flavors.\n- **Ambiance**: Kokkalo boasts a chic and modern décor, creating a welcoming atmosphere for guests. The staff is known for their professionalism and attentiveness, enhancing the overall dining experience.\n- **Culinary Experience**: The name \"Kokkalo,\" meaning \"bone\" in Greek, symbolizes the strong foundation of the restaurant's culinary philosophy. Guests can expect a bold and unforgettable culinary journey.\n- **Cooking Classes**: Kokkalo also offers cooking lessons, allowing visitors to learn how to prepare traditional Greek dishes, providing a unique souvenir of their time in Santorini.\n- **Contact Information**: \n  - Address: 25 Martiou str, Fira, Santorini 84 700, Cyclades, Greece\n  - Phone: +30 22860 25407\n  - Email: reservation@kokkalosantorini.com\n\nFor more information, you can visit their [official website](https://www.santorini-view.com/restaurants/kokkalo-restaurant/) or their [Facebook page](https://www.facebook.com/kokkalorestaurant/).",
  "is_safe": true,
  "results": [
    {
      "title": "Kokkalo restaurant, Restaurants in Firostefani Santorini Greece",
      "url": "http://www.travel-to-santorini.com/restaurants/firostefani/thebonerestaurant/",
      "description": "Details Contact : George Grafakos Address : Firostefani, Opposite of Fira Primary School Zipcode : 84700 City : Santorni Phone : +30 22860 25407 Send an email",
      "content": null,
      "site_name": "Travel-to-santorini",
      "site_long_name": "travel-to-santorini.com",
      "language": "en",
      "is_safe": true,
      "favicon": "https://t1.gstatic.com/faviconV2?client=SOCIAL&type=FAVICON&fallback_opts=TYPE,SIZE,URL&url=http://travel-to-santorini.com&size=96"
    }
  ]
}
*/
```

## Related

- Tool [conceptual guide](/docs/concepts/tools)
- Tool [how-to guides](/docs/how_to/#tools)



================================================
FILE: docs/core_docs/docs/integrations/tools/lambda_agent.mdx
================================================
---
sidebar_label: Agent with AWS Lambda
hide_table_of_contents: true
---

# Agent with AWS Lambda Integration

Full docs here: https://docs.aws.amazon.com/lambda/index.html

**AWS Lambda** is a serverless computing service provided by Amazon Web Services (AWS), designed to allow developers to build and run applications and services without the need for provisioning or managing servers. This serverless architecture enables you to focus on writing and deploying code, while AWS automatically takes care of scaling, patching, and managing the infrastructure required to run your applications.

By including a AWSLambda in the list of tools provided to an Agent, you can grant your Agent the ability to invoke code running in your AWS Cloud for whatever purposes you need.

When an Agent uses the AWSLambda tool, it will provide an argument of type `string` which will in turn be passed into the Lambda function via the `event` parameter.

This quick start will demonstrate how an Agent could use a Lambda function to send an email via [Amazon Simple Email Service](https://aws.amazon.com/ses/). The lambda code which sends the email is not provided, but if you'd like to learn how this could be done, see [here](https://repost.aws/knowledge-center/lambda-send-email-ses). Keep in mind this is an intentionally simple example; Lambda can used to execute code for a near infinite number of other purposes (including executing more Langchains)!

### Note about credentials:

- If you have not run [`aws configure`](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) via the AWS CLI, the `region`, `accessKeyId`, and `secretAccessKey` must be provided to the AWSLambda constructor.
- The IAM role corresponding to those credentials must have permission to invoke the lambda function.

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core
```

```typescript
import { OpenAI } from "@langchain/openai";
import { SerpAPI } from "langchain/tools";
import { AWSLambda } from "langchain/tools/aws_lambda";
import { initializeAgentExecutorWithOptions } from "langchain/agents";

const model = new OpenAI({ temperature: 0 });
const emailSenderTool = new AWSLambda({
  name: "email-sender",
  // tell the Agent precisely what the tool does
  description:
    "Sends an email with the specified content to testing123@gmail.com",
  region: "us-east-1", // optional: AWS region in which the function is deployed
  accessKeyId: "abc123", // optional: access key id for a IAM user with invoke permissions
  secretAccessKey: "xyz456", // optional: secret access key for that IAM user
  functionName: "SendEmailViaSES", // the function name as seen in AWS Console
});
const tools = [emailSenderTool, new SerpAPI("api_key_goes_here")];
const executor = await initializeAgentExecutorWithOptions(tools, model, {
  agentType: "zero-shot-react-description",
});

const input = `Find out the capital of Croatia. Once you have it, email the answer to testing123@gmail.com.`;
const result = await executor.invoke({ input });
console.log(result);
```

## Related

- Tool [conceptual guide](/docs/concepts/tools)
- Tool [how-to guides](/docs/how_to/#tools)



================================================
FILE: docs/core_docs/docs/integrations/tools/pyinterpreter.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# Python interpreter tool

:::warning
This tool executes code and can potentially perform destructive actions. Be careful that you trust any code passed to it!
:::

LangChain offers an experimental tool for executing arbitrary Python code.
This can be useful in combination with an LLM that can generate code to perform more powerful computations.

## Usage

import ToolExample from "@examples/tools/pyinterpreter.ts";

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core
```

<CodeBlock language="typescript">{ToolExample}</CodeBlock>

## Related

- Tool [conceptual guide](/docs/concepts/tools)
- Tool [how-to guides](/docs/how_to/#tools)



================================================
FILE: docs/core_docs/docs/integrations/tools/searchapi.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# SearchApi tool

The `SearchApi` tool connects your agents and chains to the internet.

A wrapper around the Search API. This tool is handy when you need to answer questions about current events.

## Usage

Input should be a search query.

import ToolExample from "@examples/tools/searchapi_google_news.ts";

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core
```

<CodeBlock language="typescript">{ToolExample}</CodeBlock>

## Related

- Tool [conceptual guide](/docs/concepts/tools)
- Tool [how-to guides](/docs/how_to/#tools)



================================================
FILE: docs/core_docs/docs/integrations/tools/searxng.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# Searxng Search tool

The `SearxngSearch` tool connects your agents and chains to the internet.

A wrapper around the SearxNG API, this tool is useful for performing meta-search engine queries using the SearxNG API. It is particularly helpful in answering questions about current events.

## Usage

import ToolExample from "@examples/tools/searxng_search.ts";

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core
```

<CodeBlock language="typescript">{ToolExample}</CodeBlock>

## Related

- Tool [conceptual guide](/docs/concepts/tools)
- Tool [how-to guides](/docs/how_to/#tools)



================================================
FILE: docs/core_docs/docs/integrations/tools/serpapi.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: SerpAPI
---
"""

"""
# SerpAPI

[SerpAPI](https://serpapi.com/) allows you to integrate search engine results into your LLM apps

This guide provides a quick overview for getting started with the SerpAPI [tool](/docs/integrations/tools/). For detailed documentation of all `SerpAPI` features and configurations head to the [API reference](https://api.js.langchain.com/classes/_langchain_community.tools_serpapi.SerpAPI.html).

## Overview

### Integration details

| Class | Package | [PY support](https://python.langchain.com/docs/integrations/tools/serpapi/) | Package latest |
| :--- | :--- | :---: | :---: |
| [SerpAPI](https://api.js.langchain.com/classes/_langchain_community.tools_serpapi.SerpAPI.html) | [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) | ✅ |  ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |

## Setup

The integration lives in the `@langchain/community` package, which you can install as shown below:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core
</Npm2Yarn>
```

### Credentials

Set up an API key [here](https://serpapi.com/) and set it as an environment variable named `SERPAPI_API_KEY`.

```typescript
process.env.SERPAPI_API_KEY = "YOUR_API_KEY"
```

It's also helpful (but not needed) to set up [LangSmith](https://smith.langchain.com/) for best-in-class observability:

```typescript
process.env.LANGSMITH_TRACING="true"
process.env.LANGSMITH_API_KEY="your-api-key"
```
"""

"""
## Instantiation

You can import and instantiate an instance of the `SerpAPI` tool like this:
"""

import { SerpAPI } from "@langchain/community/tools/serpapi";

const tool = new SerpAPI();

"""
## Invocation

### [Invoke directly with args](/docs/concepts/#invoke-with-just-the-arguments)

You can invoke the tool directly like this:
"""

await tool.invoke({
  input: "what is the current weather in SF?"
});
# Output:
#   {"type":"weather_result","temperature":"63","unit":"Fahrenheit","precipitation":"3%","humidity":"91%","wind":"5 mph","location":"San Francisco, CA","date":"Sunday 9:00 AM","weather":"Mostly cloudy"}


"""
### [Invoke with ToolCall](/docs/concepts/#invoke-with-toolcall)

We can also invoke the tool with a model-generated `ToolCall`, in which case a `ToolMessage` will be returned:
"""

// This is usually generated by a model, but we'll create a tool call directly for demo purposes.
const modelGeneratedToolCall = {
  args: {
    input: "what is the current weather in SF?"
  },
  id: "1",
  name: tool.name,
  type: "tool_call",
}

await tool.invoke(modelGeneratedToolCall)
# Output:
#   ToolMessage {

#     "content": "{\"type\":\"weather_result\",\"temperature\":\"63\",\"unit\":\"Fahrenheit\",\"precipitation\":\"3%\",\"humidity\":\"91%\",\"wind\":\"5 mph\",\"location\":\"San Francisco, CA\",\"date\":\"Sunday 9:00 AM\",\"weather\":\"Mostly cloudy\"}",

#     "name": "search",

#     "additional_kwargs": {},

#     "response_metadata": {},

#     "tool_call_id": "1"

#   }


"""
## Chaining

We can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```

"""

// @lc-docs-hide-cell

import { ChatOpenAI } from "@langchain/openai"

const llm = new ChatOpenAI({
  model: "gpt-4o-mini",
  temperature: 0,
})

import { HumanMessage } from "@langchain/core/messages";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnableLambda } from "@langchain/core/runnables";

const prompt = ChatPromptTemplate.fromMessages(
  [
    ["system", "You are a helpful assistant."],
    ["placeholder", "{messages}"],
  ]
)

const llmWithTools = llm.bindTools([tool]);

const chain = prompt.pipe(llmWithTools);

const toolChain = RunnableLambda.from(
  async (userInput: string, config) => {
    const humanMessage = new HumanMessage(userInput,);
    const aiMsg = await chain.invoke({
      messages: [new HumanMessage(userInput)],
    }, config);
    const toolMsgs = await tool.batch(aiMsg.tool_calls, config);
    return chain.invoke({
      messages: [humanMessage, aiMsg, ...toolMsgs],
    }, config);
  }
);

const toolChainResult = await toolChain.invoke("what is the current weather in sf?");

const { tool_calls, content } = toolChainResult;

console.log("AIMessage", JSON.stringify({
  tool_calls,
  content,
}, null, 2));
# Output:
#   AIMessage {

#     "tool_calls": [],

#     "content": "The current weather in San Francisco is mostly cloudy, with a temperature of 64°F. The humidity is at 90%, there is a 3% chance of precipitation, and the wind is blowing at 5 mph."

#   }


"""
## Agents

For guides on how to use LangChain tools in agents, see the [LangGraph.js](https://langchain-ai.github.io/langgraphjs/) docs.
"""

"""
## API reference

For detailed documentation of all `SerpAPI` features and configurations head to the API reference: https://api.js.langchain.com/classes/_langchain_community.tools_serpapi.SerpAPI.html
"""



================================================
FILE: docs/core_docs/docs/integrations/tools/stackexchange.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# StackExchange Tool

The StackExchange tool connects your agents and chains to StackExchange's API.

## Usage

import ToolExample from "@examples/tools/stackexchange.ts";

<CodeBlock language="typescript">{ToolExample}</CodeBlock>

## Related

- Tool [conceptual guide](/docs/concepts/tools)
- Tool [how-to guides](/docs/how_to/#tools)



================================================
FILE: docs/core_docs/docs/integrations/tools/stagehand.mdx
================================================
---
sidebar_label: Stagehand AI Web Automation Toolkit
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/agents/stagehand_ai_web_browser.ts";
import { StagehandToolkit } from "@langchain/community/agents/toolkits/stagehand";

# Stagehand Toolkit

The Stagehand Toolkit equips your AI agent with the following capabilities:

- **navigate()**: Navigate to a specific URL.
- **act()**: Perform browser automation actions like clicking, typing, and navigation.
- **extract()**: Extract structured data from web pages using Zod schemas.
- **observe()**: Get a list of possible actions and elements on the current page.

## Setup

1. Install the required packages:

```bash
npm install @langchain/langgraph @langchain/community @langchain/core
```

2. Create a Stagehand Instance
   If you plan to run the browser locally, you'll also need to install Playwright's browser dependencies.

```bash
npx playwright install
```

3. Set up your model provider credentials:

For OpenAI:

```bash
export OPENAI_API_KEY="your-openai-api-key"
```

For Anthropic:

```bash
export ANTHROPIC_API_KEY="your-anthropic-api-key"
```

## Usage, Standalone, Local Browser

```typescript
import { StagehandToolkit } from "langchain/community/agents/toolkits/stagehand";
import { ChatOpenAI } from "@langchain/openai";
import { Stagehand } from "@browserbasehq/stagehand";

// Specify your Browserbase credentials.
process.env.BROWSERBASE_API_KEY = "";
process.env.BROWSERBASE_PROJECT_ID = "";

// Specify OpenAI API key.
process.env.OPENAI_API_KEY = "";

const stagehand = new Stagehand({
  env: "LOCAL",
  headless: false,
  verbose: 2,
  debugDom: true,
  enableCaching: false,
});

// Create a Stagehand Toolkit with all the available actions from the Stagehand.
const stagehandToolkit = await StagehandToolkit.fromStagehand(stagehand);

const navigateTool = stagehandToolkit.tools.find(
  (t) => t.name === "stagehand_navigate"
);
if (!navigateTool) {
  throw new Error("Navigate tool not found");
}
await navigateTool.invoke("https://www.google.com");

const actionTool = stagehandToolkit.tools.find(
  (t) => t.name === "stagehand_act"
);
if (!actionTool) {
  throw new Error("Action tool not found");
}
await actionTool.invoke('Search for "OpenAI"');

const observeTool = stagehandToolkit.tools.find(
  (t) => t.name === "stagehand_observe"
);
if (!observeTool) {
  throw new Error("Observe tool not found");
}
const result = await observeTool.invoke(
  "What actions can be performed on the current page?"
);
const observations = JSON.parse(result);

// Handle observations as needed
console.log(observations);

const currentUrl = stagehand.page.url();
expect(currentUrl).toContain("google.com/search?q=OpenAI");
```

## Usage with LangGraph Agents

<CodeBlock language="typescript">{Example}</CodeBlock>

## Usage on Browserbase - remote headless browser

If you want to run the browser remotely, you can use the Browserbase platform.

You need to set the `BROWSERBASE_API_KEY` environment variable to your Browserbase API key.

```bash
export BROWSERBASE_API_KEY="your-browserbase-api-key"
```

You also need to set `BROWSERBASE_PROJECT_ID` to your Browserbase project ID.

```bash
export BROWSERBASE_PROJECT_ID="your-browserbase-project-id"
```

Then initialize the Stagehand instance with the `BROWSERBASE` environment.

```typescript
const stagehand = new Stagehand({
  env: "BROWSERBASE",
});
```

## Related

- Tool [conceptual guide](/docs/concepts/tools)
- Tool [how-to guides](/docs/how_to/#tools)
- [Stagehand Documentation](https://github.com/browserbase/stagehand#readme)



================================================
FILE: docs/core_docs/docs/integrations/tools/tavily_search.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Tavily Search
---
"""

"""
# TavilySearchResults

[Tavily](https://tavily.com/) Search is a robust search API tailored specifically for LLM Agents. It seamlessly integrates with diverse data sources to ensure a superior, relevant search experience.

This guide provides a quick overview for getting started with the Tavily search results [tool](/docs/integrations/tools/). For detailed documentation of all `TavilySearchResults` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_tools_tavily_search.TavilySearchResults.html).

## Overview

### Integration details

| Class | Package | [PY support](https://python.langchain.com/docs/integrations/tools/tavily_search/) | Package latest |
| :--- | :--- | :---: | :---: |
| [TavilySearchResults](https://api.js.langchain.com/classes/langchain_community_tools_tavily_search.TavilySearchResults.html) | [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) | ✅ |  ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |

## Setup

The integration lives in the `@langchain/community` package, which you can install as shown below:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core
</Npm2Yarn>
```

### Credentials

Set up an API key [here](https://app.tavily.com) and set it as an environment variable named `TAVILY_API_KEY`.

```typescript
process.env.TAVILY_API_KEY = "YOUR_API_KEY"
```

It's also helpful (but not needed) to set up [LangSmith](https://smith.langchain.com/) for best-in-class observability:

```typescript
process.env.LANGSMITH_TRACING="true"
process.env.LANGSMITH_API_KEY="your-api-key"
```
"""

"""
## Instantiation

You can import and instantiate an instance of the `TavilySearchResults` tool like this:
"""

import { TavilySearchResults } from "@langchain/community/tools/tavily_search";

const tool = new TavilySearchResults({
  maxResults: 2,
  // ...
});

"""
## Invocation

### [Invoke directly with args](/docs/concepts/tools)

You can invoke the tool directly like this:
"""

await tool.invoke({
  input: "what is the current weather in SF?"
});
# Output:
#   [{"title":"San Francisco, CA Current Weather | AccuWeather","url":"https://www.accuweather.com/en/us/san-francisco/94103/current-weather/347629","content":"Current weather in San Francisco, CA. Check current conditions in San Francisco, CA with radar, hourly, and more.","score":0.9428234,"raw_content":null},{"title":"National Weather Service","url":"https://forecast.weather.gov/zipcity.php?inputstring=San+Francisco,CA","content":"NOAA National Weather Service. Current conditions at SAN FRANCISCO DOWNTOWN (SFOC1) Lat: 37.77056°NLon: 122.42694°WElev: 150.0ft.","score":0.94261247,"raw_content":null}]


"""
### [Invoke with ToolCall](/docs/concepts/tools)

We can also invoke the tool with a model-generated `ToolCall`, in which case a `ToolMessage` will be returned:
"""

// This is usually generated by a model, but we'll create a tool call directly for demo purposes.
const modelGeneratedToolCall = {
  args: {
    input: "what is the current weather in SF?"
  },
  id: "1",
  name: tool.name,
  type: "tool_call",
}

await tool.invoke(modelGeneratedToolCall)
# Output:
#   ToolMessage {

#     "content": "[{\"title\":\"Weather in San Francisco\",\"url\":\"https://www.weatherapi.com/\",\"content\":\"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.78, 'lon': -122.42, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1722967498, 'localtime': '2024-08-06 11:04'}, 'current': {'last_updated_epoch': 1722967200, 'last_updated': '2024-08-06 11:00', 'temp_c': 18.4, 'temp_f': 65.2, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 2.9, 'wind_kph': 4.7, 'wind_degree': 275, 'wind_dir': 'W', 'pressure_mb': 1015.0, 'pressure_in': 29.97, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 64, 'cloud': 2, 'feelslike_c': 18.5, 'feelslike_f': 65.2, 'windchill_c': 18.5, 'windchill_f': 65.2, 'heatindex_c': 18.4, 'heatindex_f': 65.2, 'dewpoint_c': 11.7, 'dewpoint_f': 53.1, 'vis_km': 10.0, 'vis_miles': 6.0, 'uv': 5.0, 'gust_mph': 4.3, 'gust_kph': 7.0}}\",\"score\":0.9983156,\"raw_content\":null},{\"title\":\"Weather in San Francisco in June 2024 - Detailed Forecast\",\"url\":\"https://www.easeweather.com/north-america/united-states/california/city-and-county-of-san-francisco/san-francisco/june\",\"content\":\"Until now, June 2024 in San Francisco is slightly cooler than the historical average by -0.6 ° C.. The forecast for June 2024 in San Francisco predicts the temperature to closely align with the historical average at 17.7 ° C. 17.7 ° C.\",\"score\":0.9905143,\"raw_content\":null}]",

#     "name": "tavily_search_results_json",

#     "additional_kwargs": {},

#     "response_metadata": {},

#     "tool_call_id": "1"

#   }


"""
## Chaining

We can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```

"""

// @lc-docs-hide-cell

import { ChatOpenAI } from "@langchain/openai"

const llm = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0,
})

import { HumanMessage } from "@langchain/core/messages";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnableLambda } from "@langchain/core/runnables";

const prompt = ChatPromptTemplate.fromMessages(
  [
    ["system", "You are a helpful assistant."],
    ["placeholder", "{messages}"],
  ]
)

const llmWithTools = llm.bindTools([tool]);

const chain = prompt.pipe(llmWithTools);

const toolChain = RunnableLambda.from(
  async (userInput: string, config) => {
    const humanMessage = new HumanMessage(userInput,);
    const aiMsg = await chain.invoke({
      messages: [new HumanMessage(userInput)],
    }, config);
    const toolMsgs = await tool.batch(aiMsg.tool_calls, config);
    return chain.invoke({
      messages: [humanMessage, aiMsg, ...toolMsgs],
    }, config);
  }
);

const toolChainResult = await toolChain.invoke("what is the current weather in sf?");

const { tool_calls, content } = toolChainResult;

console.log("AIMessage", JSON.stringify({
  tool_calls,
  content,
}, null, 2));
# Output:
#   AIMessage {

#     "tool_calls": [],

#     "content": "The current weather in San Francisco is as follows:\n\n- **Condition:** Sunny\n- **Temperature:** 18.4°C (65.2°F)\n- **Wind:** 2.9 mph (4.7 kph) from the west\n- **Humidity:** 64%\n- **Visibility:** 10 km (6 miles)\n- **UV Index:** 5\n\n![Sunny](//cdn.weatherapi.com/weather/64x64/day/113.png)\n\nFor more detailed information, you can visit [WeatherAPI](https://www.weatherapi.com/)."

#   }


"""
## Agents

For guides on how to use LangChain tools in agents, see the [LangGraph.js](https://langchain-ai.github.io/langgraphjs/) docs.
"""

"""
## API reference

For detailed documentation of all `TavilySearchResults` features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_community_tools_tavily_search.TavilySearchResults.html
"""



================================================
FILE: docs/core_docs/docs/integrations/tools/webbrowser.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# Web Browser Tool

The Webbrowser Tool gives your agent the ability to visit a website and extract information. It is described to the agent as

```
useful for when you need to find something on or summarize a webpage. input should be a comma separated list of "valid URL including protocol","what you want to find on the page or empty string for a summary".
```

It exposes two modes of operation:

- when called by the Agent with only a URL it produces a summary of the website contents
- when called by the Agent with a URL and a description of what to find it will instead use an in-memory Vector Store to find the most relevant snippets and summarise those

## Setup

To use the Webbrowser Tool you need to install the dependencies:

```bash npm2yarn
npm install cheerio axios
```

## Usage, standalone

import ToolExample from "@examples/tools/webbrowser.ts";

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core
```

<CodeBlock language="typescript">{ToolExample}</CodeBlock>

## Usage, in an Agent

import AgentExample from "@examples/agents/mrkl_browser.ts";

<CodeBlock language="typescript">{AgentExample}</CodeBlock>

## Related

- Tool [conceptual guide](/docs/concepts/tools)
- Tool [how-to guides](/docs/how_to/#tools)



================================================
FILE: docs/core_docs/docs/integrations/tools/wikipedia.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# Wikipedia tool

The `WikipediaQueryRun` tool connects your agents and chains to Wikipedia.

## Usage

import ToolExample from "@examples/tools/wikipedia.ts";

<CodeBlock language="typescript">{ToolExample}</CodeBlock>

## Related

- Tool [conceptual guide](/docs/concepts/tools)
- Tool [how-to guides](/docs/how_to/#tools)



================================================
FILE: docs/core_docs/docs/integrations/tools/wolframalpha.mdx
================================================
---
hide_table_of_contents: true
---

import CodeBlock from "@theme/CodeBlock";

# WolframAlpha Tool

The WolframAlpha tool connects your agents and chains to WolframAlpha's state-of-the-art computational intelligence engine.

## Setup

You'll need to create an app from the [WolframAlpha portal](https://developer.wolframalpha.com/) and obtain an `appid`.

## Usage

import ToolExample from "@examples/tools/wolframalpha.ts";

<CodeBlock language="typescript">{ToolExample}</CodeBlock>

## Related

- Tool [conceptual guide](/docs/concepts/tools)
- Tool [how-to guides](/docs/how_to/#tools)



================================================
FILE: docs/core_docs/docs/integrations/tools/zapier_agent.mdx
================================================
---
sidebar_class_name: hidden
---

# Agent with Zapier NLA Integration

:::warning
This module has been deprecated and is no longer supported. The documentation below will not work in versions 0.2.0 or later.
:::

Full docs here: https://nla.zapier.com/start/

**Zapier Natural Language Actions** gives you access to the 5k+ apps and 20k+ actions on Zapier's platform through a natural language API interface.

NLA supports apps like Gmail, Salesforce, Trello, Slack, Asana, HubSpot, Google Sheets, Microsoft Teams, and thousands more apps: https://zapier.com/apps

Zapier NLA handles ALL the underlying API auth and translation from natural language --> underlying API call --> return simplified output for LLMs. The key idea is you, or your users, expose a set of actions via an oauth-like setup window, which you can then query and execute via a REST API.

NLA offers both API Key and OAuth for signing NLA API requests.

Server-side (API Key): for quickly getting started, testing, and production scenarios where LangChain will only use actions exposed in the developer's Zapier account (and will use the developer's connected accounts on Zapier.com)

User-facing (Oauth): for production scenarios where you are deploying an end-user facing application and LangChain needs access to end-user's exposed actions and connected accounts on Zapier.com

Attach NLA credentials via either an environment variable (`ZAPIER_NLA_OAUTH_ACCESS_TOKEN` or `ZAPIER_NLA_API_KEY`) or refer to the params argument in the API reference for `ZapierNLAWrapper`.

Review [auth docs](https://nla.zapier.com/docs/authentication/) for more details.

The example below demonstrates how to use the Zapier integration as an Agent:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core
```

```typescript
import { OpenAI } from "@langchain/openai";
import { ZapierNLAWrapper } from "langchain/tools";
import {
  initializeAgentExecutorWithOptions,
  ZapierToolKit,
} from "langchain/agents";

const model = new OpenAI({ temperature: 0 });
const zapier = new ZapierNLAWrapper();
const toolkit = await ZapierToolKit.fromZapierNLAWrapper(zapier);

const executor = await initializeAgentExecutorWithOptions(
  toolkit.tools,
  model,
  {
    agentType: "zero-shot-react-description",
    verbose: true,
  }
);
console.log("Loaded agent.");

const input = `Summarize the last email I received regarding Silicon Valley Bank. Send the summary to the #test-zapier Slack channel.`;

console.log(`Executing with input "${input}"...`);

const result = await executor.invoke({ input });

console.log(`Got output ${result.output}`);
```

## Related

- Tool [conceptual guide](/docs/concepts/tools)
- Tool [how-to guides](/docs/how_to/#tools)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/analyticdb.mdx
================================================
---
sidebar_class_name: node-only
---

import CodeBlock from "@theme/CodeBlock";

# AnalyticDB

[AnalyticDB for PostgreSQL](https://www.alibabacloud.com/help/en/analyticdb-for-postgresql/latest/product-introduction-overview) is a massively parallel processing (MPP) data warehousing service that is designed to analyze large volumes of data online.

`AnalyticDB for PostgreSQL` is developed based on the open source `Greenplum Database` project and is enhanced with in-depth extensions by `Alibaba Cloud`. AnalyticDB for PostgreSQL is compatible with the ANSI SQL 2003 syntax and the PostgreSQL and Oracle database ecosystems. AnalyticDB for PostgreSQL also supports row store and column store. AnalyticDB for PostgreSQL processes petabytes of data offline at a high performance level and supports highly concurrent online queries.

This notebook shows how to use functionality related to the `AnalyticDB` vector database.

To run, you should have an [AnalyticDB](https://www.alibabacloud.com/help/en/analyticdb-for-postgresql/latest/product-introduction-overview) instance up and running:

- Using [AnalyticDB Cloud Vector Database](https://www.alibabacloud.com/product/hybriddb-postgresql).

:::tip Compatibility
Only available on Node.js.
:::

## Setup

LangChain.js accepts [node-postgres](https://node-postgres.com/) as the connections pool for AnalyticDB vectorstore.

```bash npm2yarn
npm install -S pg
```

And we need [pg-copy-streams](https://github.com/brianc/node-pg-copy-streams) to add batch vectors quickly.

```bash npm2yarn
npm install -S pg-copy-streams
```

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

## Usage

::::danger Security
User-generated data such as usernames should not be used as input for the collection name.  
**This may lead to SQL Injection!**
::::

import UsageExample from "@examples/indexes/vector_stores/analyticdb.ts";

<CodeBlock language="typescript">{UsageExample}</CodeBlock>

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/astradb.mdx
================================================
---
sidebar_class_name: node-only
---

import CodeBlock from "@theme/CodeBlock";

# Astra DB

:::tip Compatibility
Only available on Node.js.
:::

DataStax [Astra DB](https://astra.datastax.com/register) is a serverless vector-capable database built on [Apache Cassandra](https://cassandra.apache.org/_/index.html) and made conveniently available through an easy-to-use JSON API.

## Setup

1. Create an [Astra DB account](https://astra.datastax.com/register).
2. Create a [vector enabled database](https://astra.datastax.com/createDatabase).
3. Grab your `API Endpoint` and `Token` from the Database Details.
4. Set up the following env vars:

```bash
export ASTRA_DB_APPLICATION_TOKEN=YOUR_ASTRA_DB_APPLICATION_TOKEN_HERE
export ASTRA_DB_ENDPOINT=YOUR_ASTRA_DB_ENDPOINT_HERE
export ASTRA_DB_COLLECTION=YOUR_ASTRA_DB_COLLECTION_HERE
export OPENAI_API_KEY=YOUR_OPENAI_API_KEY_HERE
```

Where `ASTRA_DB_COLLECTION` is the desired name of your collection

6. Install the Astra TS Client & the LangChain community package

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @datastax/astra-db-ts @langchain/community @langchain/core
```

## Indexing docs

import Example from "@examples/indexes/vector_stores/astra.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## Vector Types

Astra DB supports `cosine` (the default), `dot_product`, and `euclidean` similarity search; this is defined when the
vector store is first created as part of the `CreateCollectionOptions`:

```typescript
  vector: {
      dimension: number;
      metric?: "cosine" | "euclidean" | "dot_product";
  };
```

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/azion-edgesql.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Azion EdgeSQL
---
"""

"""
# AzionVectorStore

The `AzionVectorStore` is used to manage and search through a collection of documents using vector embeddings, directly on Azion's Edge Plataform using Edge SQL. 

This guide provides a quick overview for getting started with Azion EdgeSQL [vector stores](/docs/concepts/#vectorstores). For detailed documentation of all `AzionVectorStore` features and configurations head to the [API reference](https://api.js.langchain.com/classes/_langchain_community.vectorstores_azion_edgesql.AzionVectorStore.html).
"""

"""
## Overview

### Integration details

| Class | Package | [PY support] |  Package latest |
| :--- | :--- | :---: | :---: |
| [`AzionVectorStore`](https://api.js.langchain.com/classes/langchain_community_vectorstores_azion_edgesql.AzionVectorStore.html) | [`@langchain/community`](https://npmjs.com/@langchain/community) | ❌ |  ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |
"""

"""
## Setup

To use the `AzionVectorStore` vector store, you will need to install the `@langchain/community` package. Besides that, you will need an [Azion account](https://www.azion.com/en/documentation/products/accounts/creating-account/) and a [Token](https://www.azion.com/en/documentation/products/guides/personal-tokens/) to use the Azion API, configuring it as environment variable `AZION_TOKEN`. Further information about this can be found in the [Documentation](https://www.azion.com/en/documentation/).

This guide will also use [OpenAI embeddings](/docs/integrations/text_embedding/openai), which require you to install the `@langchain/openai` integration package. You can also use [other supported embeddings models](/docs/integrations/text_embedding) if you wish.

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  azion @langchain/openai @langchain/community
</Npm2Yarn>
```

### Credentials

Once you've done this set the AZION_TOKEN environment variable:

```typescript
process.env.AZION_TOKEN = "your-api-key"
```

If you are using OpenAI embeddings for this guide, you'll need to set your OpenAI key as well:

```typescript
process.env.OPENAI_API_KEY = "YOUR_API_KEY";
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
// process.env.LANGCHAIN_TRACING_V2="true"
// process.env.LANGCHAIN_API_KEY="your-api-key"
```
"""

"""
## Instantiation
"""

import { AzionVectorStore } from "@langchain/community/vectorstores/azion_edgesql";
import { OpenAIEmbeddings } from "@langchain/openai";

const embeddings = new OpenAIEmbeddings({
  model: "text-embedding-3-small",
});

// Instantiate with the constructor if the database and table have already been created
const vectorStore = new AzionVectorStore(embeddings, { dbName: "langchain", tableName: "documents" });

// If you have not created the database and table yet, you can do so with the setupDatabase method
// await vectorStore.setupDatabase({ columns:["topic","language"], mode: "hybrid" })

// OR instantiate with the static method if the database and table have not been created yet
// const vectorStore = await AzionVectorStore.initialize(embeddingModel, { dbName: "langchain", tableName: "documents" }, { columns:[], mode: "hybrid" })

"""
## Manage vector store

### Add items to vector store
"""

import type { Document } from "@langchain/core/documents";

const document1: Document = {
  pageContent: "The powerhouse of the cell is the mitochondria",
  metadata: { language: "en", topic: "biology" }
};

const document2: Document = {
  pageContent: "Buildings are made out of brick",
  metadata: { language: "en", topic: "history" }
};

const document3: Document = {
  pageContent: "Mitochondria are made out of lipids",
  metadata: { language: "en", topic: "biology" }
};

const document4: Document = {
  pageContent: "The 2024 Olympics are in Paris",
  metadata: { language: "en", topic: "history" }
}

const documents = [document1, document2, document3, document4];

await vectorStore.addDocuments(documents);
# Output:
#   Inserting chunks

#   Inserting chunk 0

#   Chunks inserted!


"""
### Delete items from vector store
"""

await vectorStore.delete(["4"]);
# Output:
#   Deleted 1 items from documents


"""
## Query vector store

Once your vector store has been created and the relevant documents have been added you will most likely wish to query it during the running of your chain or agent. 

### Query directly

Performing a simple similarity search can be done as follows:
"""

const filter = [{ operator: "=", column: "language", value: "en" }]

const hybridSearchResults = await vectorStore.azionHybridSearch("biology", {kfts:2, kvector:1, 
                                      filter:[{ operator: "=", column: "language", value: "en" }]});

console.log("Hybrid Search Results")
for (const doc of hybridSearchResults) {
  console.log(`${JSON.stringify(doc)}`);
}
# Output:
#   Hybrid Search Results

#   [{"pageContent":"The Australian dingo is a unique species that plays a key role in the ecosystem","metadata":{"searchtype":"fulltextsearch"},"id":"6"},-0.25748711028997995]

#   [{"pageContent":"The powerhouse of the cell is the mitochondria","metadata":{"searchtype":"fulltextsearch"},"id":"16"},-0.31697985337654005]

#   [{"pageContent":"Australia s indigenous people have inhabited the continent for over 65,000 years","metadata":{"searchtype":"similarity"},"id":"3"},0.14822345972061157]


const similaritySearchResults = await vectorStore.azionSimilaritySearch("australia", {kvector:3, filter:[{ operator: "=", column: "topic", value: "history" }]});

console.log("Similarity Search Results")
for (const doc of similaritySearchResults) {
  console.log(`${JSON.stringify(doc)}`);
}
# Output:
#   Similarity Search Results

#   [{"pageContent":"Australia s indigenous people have inhabited the continent for over 65,000 years","metadata":{"searchtype":"similarity"},"id":"3"},0.4486490488052368]


"""
### Query by turning into retriever

You can also transform the vector store into a [retriever](/docs/concepts/#retrievers) for easier usage in your chains. 
"""

const retriever = vectorStore.asRetriever({
  // Optional filter
  filter: filter,
  k: 2,
});
await retriever.invoke("biology");
# Output:
#   [

#     Document {

#       pageContent: 'Australia s indigenous people have inhabited the continent for over 65,000 years',

#       metadata: { searchtype: 'similarity' },

#       id: '3'

#     },

#     Document {

#       pageContent: 'Mitochondria are made out of lipids',

#       metadata: { searchtype: 'similarity' },

#       id: '18'

#     }

#   ]


"""
### Usage for retrieval-augmented generation

For guides on how to use this vector store for retrieval-augmented generation (RAG), see the following sections:

- [Tutorials: working with external knowledge](/docs/tutorials/#working-with-external-knowledge).
- [How-to: Question and answer with RAG](/docs/how_to/#qa-with-rag)
- [Retrieval conceptual docs](/docs/concepts/retrieval)
"""

"""
## API reference

For detailed documentation of all AzionVectorStore features and configurations head to the [API reference](https://api.js.langchain.com/classes/_langchain_community.vectorstores_azion_edgesql.AzionVectorStore.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/azure_aisearch.mdx
================================================
# Azure AI Search

[Azure AI Search](https://azure.microsoft.com/products/ai-services/ai-search) (formerly known as Azure Search and Azure Cognitive Search) is a distributed, RESTful search engine optimized for speed and relevance on production-scale workloads on Azure. It supports also vector search using the [k-nearest neighbor](https://en.wikipedia.org/wiki/Nearest_neighbor_search) (kNN) algorithm and also [semantic search](https://learn.microsoft.com/azure/search/semantic-search-overview).

This vector store integration supports full text search, vector search and [hybrid search for best ranking performance](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-cognitive-search-outperforming-vector-search-with-hybrid/ba-p/3929167).

Learn how to leverage the vector search capabilities of Azure AI Search from [this page](https://learn.microsoft.com/azure/search/vector-search-overview). If you don't have an Azure account, you can [create a free account](https://azure.microsoft.com/free/) to get started.

## Setup

You'll first need to install the `@azure/search-documents` SDK and the [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) package:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install -S @langchain/community @langchain/core @azure/search-documents
```

You'll also need to have an Azure AI Search instance running. You can deploy a free version on Azure Portal without any cost, following [this guide](https://learn.microsoft.com/azure/search/search-create-service-portal).

Once you have your instance running, make sure you have the endpoint and the admin key (query keys can be used only to search document, not to index, update or delete). The endpoint is the URL of your instance which you can find in the Azure Portal, under the "Overview" section of your instance. The admin key can be found under the "Keys" section of your instance. Then you need to set the following environment variables:

import CodeBlock from "@theme/CodeBlock";
import EnvVars from "@examples/indexes/vector_stores/azure_aisearch/.env.example";

<CodeBlock language="text">{EnvVars}</CodeBlock>

## About hybrid search

Hybrid search is a feature that combines the strengths of full text search and vector search to provide the best ranking performance. It's enabled by default in Azure AI Search vector stores, but you can select a different search query type by setting the `search.type` property when creating the vector store.

You can read more about hybrid search and how it may improve your search results in the [official documentation](https://learn.microsoft.com/azure/search/hybrid-search-overview).

In some scenarios like retrieval-augmented generation (RAG), you may want to enable **semantic ranking** in addition to hybrid search to improve the relevance of the search results. You can enable semantic ranking by setting the `search.type` property to `AzureAISearchQueryType.SemanticHybrid` when creating the vector store.
Note that semantic ranking capabilities are only available in the Basic and higher pricing tiers, and subject to [regional availability](https://azure.microsoft.com/en-us/explore/global-infrastructure/products-by-region/?products=search).

You can read more about the performance of using semantic ranking with hybrid search in [this blog post](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-cognitive-search-outperforming-vector-search-with-hybrid/ba-p/3929167).

## Example: index docs, vector search and LLM integration

Below is an example that indexes documents from a file in Azure AI Search, runs a hybrid search query, and finally uses a chain to answer a question in natural language based on the retrieved documents.

import Example from "@examples/indexes/vector_stores/azure_aisearch/azure_aisearch.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/azure_cosmosdb_mongodb.mdx
================================================
# Azure Cosmos DB for MongoDB vCore

> [Azure Cosmos DB for MongoDB vCore](https://learn.microsoft.com/azure/cosmos-db/mongodb/vcore/) makes it easy to create a database with full native MongoDB support. You can apply your MongoDB experience and continue to use your favorite MongoDB drivers, SDKs, and tools by pointing your application to the API for MongoDB vCore account’s connection string. Use vector search in Azure Cosmos DB for MongoDB vCore to seamlessly integrate your AI-based applications with your data that’s stored in Azure Cosmos DB.

Azure Cosmos DB for MongoDB vCore provides developers with a fully managed MongoDB-compatible database service for building modern applications with a familiar architecture.

Learn how to leverage the vector search capabilities of Azure Cosmos DB for MongoDB vCore from [this page](https://learn.microsoft.com/azure/cosmos-db/mongodb/vcore/vector-search). If you don't have an Azure account, you can [create a free account](https://azure.microsoft.com/free/) to get started.

## Setup

You'll first need to install the [`@langchain/azure-cosmosdb`](https://www.npmjs.com/package/@langchain/azure-cosmosdb) package:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/azure-cosmosdb @langchain/core
```

You'll also need to have an Azure Cosmos DB for MongoDB vCore instance running. You can deploy a free version on Azure Portal without any cost, following [this guide](https://learn.microsoft.com/azure/cosmos-db/mongodb/vcore/quickstart-portal).

Once you have your instance running, make sure you have the connection string and the admin key. You can find them in the Azure Portal, under the "Connection strings" section of your instance. Then you need to set the following environment variables:

import CodeBlock from "@theme/CodeBlock";
import EnvVars from "@examples/indexes/vector_stores/azure_cosmosdb_mongodb/.env.example";

<CodeBlock language="text">{EnvVars}</CodeBlock>

## Example

Below is an example that indexes documents from a file in Azure Cosmos DB for MongoDB vCore, runs a vector search query, and finally uses a chain to answer a question in natural language
based on the retrieved documents.

import Example from "@examples/indexes/vector_stores/azure_cosmosdb_mongodb/azure_cosmosdb_mongodb.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/azure_cosmosdb_nosql.mdx
================================================
# Azure Cosmos DB for NoSQL

> [Azure Cosmos DB for NoSQL](https://learn.microsoft.com/azure/cosmos-db/nosql/) provides support for querying items with flexible schemas and native support for JSON. It now offers vector indexing and search. This feature is designed to handle high-dimensional vectors, enabling efficient and accurate vector search at any scale. You can now store vectors directly in the documents alongside your data. Each document in your database can contain not only traditional schema-free data, but also high-dimensional vectors as other properties of the documents.

Learn how to leverage the vector search capabilities of Azure Cosmos DB for NoSQL from [this page](https://learn.microsoft.com/azure/cosmos-db/nosql/vector-search). If you don't have an Azure account, you can [create a free account](https://azure.microsoft.com/free/) to get started.

## Setup

You'll first need to install the [`@langchain/azure-cosmosdb`](https://www.npmjs.com/package/@langchain/azure-cosmosdb) package:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/azure-cosmosdb @langchain/core
```

You'll also need to have an Azure Cosmos DB for NoSQL instance running. You can deploy a free version on Azure Portal without any cost, following [this guide](https://learn.microsoft.com/azure/cosmos-db/nosql/quickstart-portal).

Once you have your instance running, make sure you have the connection string. You can find them in the Azure Portal, under the "Settings / Keys" section of your instance. Then you need to set the following environment variables:

import CodeBlock from "@theme/CodeBlock";
import EnvVars from "@examples/indexes/vector_stores/azure_cosmosdb_nosql/.env.example";

<CodeBlock language="text">{EnvVars}</CodeBlock>

### Using Azure Managed Identity

If you're using Azure Managed Identity, you can configure the credentials like this:

import ManagedIdentityExample from "@examples/indexes/vector_stores/azure_cosmosdb_nosql/azure_cosmosdb_nosql-managed_identity.ts";

<CodeBlock language="typescript">{ManagedIdentityExample}</CodeBlock>

:::info

When using Azure Managed Identity and role-based access control, you must ensure that the database and container have been created beforehand. RBAC does not provide permissions to create databases and containers. You can get more information about the permission model in the [Azure Cosmos DB documentation](https://learn.microsoft.com/azure/cosmos-db/how-to-setup-rbac#permission-model).

:::

## Usage example

Below is an example that indexes documents from a file in Azure Cosmos DB for NoSQL, runs a vector search query, and finally uses a chain to answer a question in natural language
based on the retrieved documents.

import Example from "@examples/indexes/vector_stores/azure_cosmosdb_nosql/azure_cosmosdb_nosql.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/cassandra.mdx
================================================
---
sidebar_class_name: node-only
---

# Cassandra

:::tip Compatibility
Only available on Node.js.
:::

[Apache Cassandra®](https://cassandra.apache.org/_/index.html) is a NoSQL, row-oriented, highly scalable and highly available database.

The [latest version](<https://cwiki.apache.org/confluence/display/CASSANDRA/CEP-30%3A+Approximate+Nearest+Neighbor(ANN)+Vector+Search+via+Storage-Attached+Indexes>) of Apache Cassandra natively supports Vector Similarity Search.

## Setup

First, install the Cassandra Node.js driver:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install cassandra-driver @langchain/community @langchain/openai @langchain/core
```

Depending on your database providers, the specifics of how to connect to the database will vary. We will create a document `configConnection` which will be used as part of the vector store configuration.

### Apache Cassandra®

Vector search is supported in [Apache Cassandra® 5.0](https://cassandra.apache.org/_/Apache-Cassandra-5.0-Moving-Toward-an-AI-Driven-Future.html) and above. You can use a standard connection document, for example:

```typescript
const configConnection = {
  contactPoints: ['h1', 'h2'],
  localDataCenter: 'datacenter1',
  credentials: {
    username: <...> as string,
    password: <...> as string,
  },
};
```

### Astra DB

Astra DB is a cloud-native Cassandra-as-a-Service platform.

1. Create an [Astra DB account](https://astra.datastax.com/register).
2. Create a [vector enabled database](https://astra.datastax.com/createDatabase).
3. Create a [token](https://docs.datastax.com/en/astra/docs/manage-application-tokens.html) for your database.

```typescript
const configConnection = {
  serviceProviderArgs: {
    astra: {
      token: <...> as string,
      endpoint: <...> as string,
    },
  },
};
```

Instead of `endpoint:`, you many provide property `datacenterID:` and optionally `regionName:`.

## Indexing docs

```typescript
import { CassandraStore } from "langchain/vectorstores/cassandra";
import { OpenAIEmbeddings } from "@langchain/openai";

// The configConnection document is defined above
const config = {
  ...configConnection,
  keyspace: "test",
  dimensions: 1536,
  table: "test",
  indices: [{ name: "name", value: "(name)" }],
  primaryKey: {
    name: "id",
    type: "int",
  },
  metadataColumns: [
    {
      name: "name",
      type: "text",
    },
  ],
};

const vectorStore = await CassandraStore.fromTexts(
  ["I am blue", "Green yellow purple", "Hello there hello"],
  [
    { id: 2, name: "2" },
    { id: 1, name: "1" },
    { id: 3, name: "3" },
  ],
  new OpenAIEmbeddings(),
  cassandraConfig
);
```

## Querying docs

```typescript
const results = await vectorStore.similaritySearch("Green yellow purple", 1);
```

or filtered query:

```typescript
const results = await vectorStore.similaritySearch("B", 1, { name: "Bubba" });
```

## Vector Types

Cassandra supports `cosine` (the default), `dot_product`, and `euclidean` similarity search; this is defined when the
vector store is first created, and specifed in the constructor parameter `vectorType`, for example:

```typescript
  ...,
  vectorType: "dot_product",
  ...
```

## Indices

With Version 5, Cassandra introduced Storage Attached Indexes, or SAIs. These allow `WHERE` filtering without specifying
the partition key, and allow for additional operator types such as non-equalities. You can define these with the `indices`
parameter, which accepts zero or more dictionaries each containing `name` and `value` entries.

Indices are optional, though required if using filtered queries on non-partition columns.

- The `name` entry is part of the object name; on a table named `test_table` an index with `name: "some_column"`
  would be `idx_test_table_some_column`.
- The `value` entry is the column on which the index is created, surrounded by `(` and `)`. With the above column
  `some_column` it would be specified as `value: "(some_column)"`.
- An optional `options` entry is a map passed to the `WITH OPTIONS =` clause of the `CREATE CUSTOM INDEX` statement.
  The specific entries on this map are index type specific.

```typescript
  indices: [{ name: "some_column", value: "(some_column)" }],
```

## Advanced Filtering

By default, filters are applied with an equality `=`. For those fields that have an `indices` entry, you may
provide an `operator` with a string of a value supported by the index; in this case, you specify one or
more filters, as either a singleton or in a list (which will be `AND`-ed together). For example:

```typescript
   { name: "create_datetime", operator: ">", value: some_datetime_variable }
```

or

```typescript
[
  { userid: userid_variable },
  { name: "create_datetime", operator: ">", value: some_date_variable },
];
```

`value` can be a single value or an array. If it is not an array, or there is only one element in `value`,
the resulting query will be along the lines of `${name} ${operator} ?` with `value` bound to the `?`.

If there is more than one element in the `value` array, the number of unquoted `?` in `name` are counted
and subtracted from the length of `value`, and this number of `?` is put on the right side of the operator;
if there are more than one `?` then they will be encapsulated in `(` and `)`, e.g. `(?, ?, ?)`.

This faciliates bind values on the left of the operator, which is useful for some functions; for example
a geo-distance filter:

```typescript
{
  name: "GEO_DISTANCE(coord, ?)",
  operator: "<",
  value: [new Float32Array([53.3730617,-6.3000515]), 10000],
},
```

## Data Partitioning and Composite Keys

In some systems, you may wish to partition the data for various reasons, perhaps by user or by session. Data in Cassandra
is always partitioned; by default this library will partition by the first primary key field. You may specify multiple
columns which comprise the primary (unique) key of a record, and optionally indicate those fields which should be
part of the partition key. For example, the vector store could be partitioned by both `userid` and `collectionid`, with
additional fields `docid` and `docpart` making an individual entry unique:

```typescript
  ...,
  primaryKey: [
    {name: "userid", type: "text", partition: true},
    {name: "collectionid", type: "text", partition: true},
    {name: "docid", type: "text"},
    {name: "docpart", type: "int"},
  ],
  ...
```

When searching, you may include partition keys on the filter without defining `indices` for these columns; you do
not need to specify all partition keys, but must specify those in the key first. In the above example, you could
specify a filter of `{userid: userid_variable}` and `{userid: userid_variable, collectionid: collectionid_variable}`,
but if you wanted to specify a filter of only `{collectionid: collectionid_variable}` you would have to include
`collectionid` on the `indices` list.

## Additional Configuration Options

In the configuration document, further optional parameters are provided; their defaults are:

```typescript
  ...,
  maxConcurrency: 25,
  batchSize: 1,
  withClause: "",
  ...
```

| Parameter        | Usage                                                                                                                                                                                                                        |
| ---------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `maxConcurrency` | How many concurrent requests will be sent to Cassandra at a given time.                                                                                                                                                      |
| `batchSize`      | How many documents will be sent on a single request to Cassandra. When using a value > 1, you should ensure your batch size will not exceed the Cassandra parameter `batch_size_fail_threshold_in_kb`. Batches are unlogged. |
| `withClause`     | Cassandra tables may be created with an optional `WITH` clause; this is generally not needed but provided for completeness.                                                                                                  |

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/chroma.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Chroma
---
"""

"""
# Chroma

[Chroma](https://docs.trychroma.com/getting-started) is a AI-native open-source vector database focused on developer productivity and happiness. Chroma is licensed under Apache 2.0.

This guide provides a quick overview for getting started with Chroma [`vector stores`](/docs/concepts/#vectorstores). For detailed documentation of all `Chroma` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_vectorstores_chroma.Chroma.html).
"""

"""
## Overview

### Integration details

| Class | Package | [PY support](https://python.langchain.com/docs/integrations/vectorstores/chroma/) | Package latest |
| :--- | :--- | :---: | :---: |
| [`Chroma`](https://api.js.langchain.com/classes/langchain_community_vectorstores_chroma.Chroma.html) | [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) | ✅ |  ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |
"""

"""
## Setup

To use Chroma vector stores, you'll need to install the `@langchain/community` integration package along with the [Chroma JS SDK](https://www.npmjs.com/package/chromadb) as a peer dependency.

This guide will also use [OpenAI embeddings](/docs/integrations/text_embedding/openai), which require you to install the `@langchain/openai` integration package. You can also use [other supported embeddings models](/docs/integrations/text_embedding) if you wish.

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/openai @langchain/core chromadb
</Npm2Yarn>
```

Next, follow the following instructions to run Chroma with Docker on your computer:

```
docker pull chromadb/chroma 
docker run -p 8000:8000 chromadb/chroma
```

You can see alternative setup instructions [in this guide](https://docs.trychroma.com/getting-started).

### Credentials

If you are running Chroma through Docker, you do not need to provide any credentials.

If you are using OpenAI embeddings for this guide, you'll need to set your OpenAI key as well:

```typescript
process.env.OPENAI_API_KEY = "YOUR_API_KEY";
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
// process.env.LANGSMITH_TRACING="true"
// process.env.LANGSMITH_API_KEY="your-api-key"
```
"""

"""
## Instantiation
"""

import { Chroma } from "@langchain/community/vectorstores/chroma";
import { OpenAIEmbeddings } from "@langchain/openai";

const embeddings = new OpenAIEmbeddings({
  model: "text-embedding-3-small",
});

const vectorStore = new Chroma(embeddings, {
  collectionName: "a-test-collection",
  url: "http://localhost:8000", // Optional, will default to this value
  collectionMetadata: {
    "hnsw:space": "cosine",
  }, // Optional, can be used to specify the distance method of the embedding space https://docs.trychroma.com/usage-guide#changing-the-distance-function
});

"""
## Manage vector store

### Add items to vector store
"""

import type { Document } from "@langchain/core/documents";

const document1: Document = {
  pageContent: "The powerhouse of the cell is the mitochondria",
  metadata: { source: "https://example.com" }
};

const document2: Document = {
  pageContent: "Buildings are made out of brick",
  metadata: { source: "https://example.com" }
};

const document3: Document = {
  pageContent: "Mitochondria are made out of lipids",
  metadata: { source: "https://example.com" }
};

const document4: Document = {
  pageContent: "The 2024 Olympics are in Paris",
  metadata: { source: "https://example.com" }
}

const documents = [document1, document2, document3, document4];

await vectorStore.addDocuments(documents, { ids: ["1", "2", "3", "4"] });
# Output:
#   [ '1', '2', '3', '4' ]


"""
### Delete items from vector store

You can delete documents from Chroma by id as follows:
"""

await vectorStore.delete({ ids: ["4"] });

"""
## Query vector store

Once your vector store has been created and the relevant documents have been added you will most likely wish to query it during the running of your chain or agent. 

### Query directly

Performing a simple similarity search can be done as follows:
"""

const filter = { source: "https://example.com" };

const similaritySearchResults = await vectorStore.similaritySearch("biology", 2, filter);

for (const doc of similaritySearchResults) {
  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);
}
# Output:
#   * The powerhouse of the cell is the mitochondria [{"source":"https://example.com"}]

#   * Mitochondria are made out of lipids [{"source":"https://example.com"}]


"""
See [this page](https://docs.trychroma.com/guides#filtering-by-metadata) for more on Chroma filter syntax.

If you want to execute a similarity search and receive the corresponding scores you can run:
"""

const similaritySearchWithScoreResults = await vectorStore.similaritySearchWithScore("biology", 2, filter)

for (const [doc, score] of similaritySearchWithScoreResults) {
  console.log(`* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`);
}
# Output:
#   * [SIM=0.835] The powerhouse of the cell is the mitochondria [{"source":"https://example.com"}]

#   * [SIM=0.852] Mitochondria are made out of lipids [{"source":"https://example.com"}]


"""
### Query by turning into retriever

You can also transform the vector store into a [retriever](/docs/concepts/retrievers) for easier usage in your chains. 
"""

const retriever = vectorStore.asRetriever({
  // Optional filter
  filter: filter,
  k: 2,
});
await retriever.invoke("biology");
# Output:
#   [

#     Document {

#       pageContent: 'The powerhouse of the cell is the mitochondria',

#       metadata: { source: 'https://example.com' },

#       id: undefined

#     },

#     Document {

#       pageContent: 'Mitochondria are made out of lipids',

#       metadata: { source: 'https://example.com' },

#       id: undefined

#     }

#   ]


"""
### Usage for retrieval-augmented generation

For guides on how to use this vector store for retrieval-augmented generation (RAG), see the following sections:

- [Tutorials: working with external knowledge](/docs/tutorials/#working-with-external-knowledge).
- [How-to: Question and answer with RAG](/docs/how_to/#qa-with-rag)
- [Retrieval conceptual docs](/docs/concepts/retrieval)
"""

"""
## API reference

For detailed documentation of all `Chroma` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_vectorstores_chroma.Chroma.html)
"""



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/clickhouse.mdx
================================================
---
sidebar_class_name: node-only
---

import CodeBlock from "@theme/CodeBlock";

# ClickHouse

:::tip Compatibility
Only available on Node.js.
:::

[ClickHouse](https://clickhouse.com/) is a robust and open-source columnar database that is used for handling analytical queries and efficient storage, ClickHouse is designed to provide a powerful combination of vector search and analytics.

## Setup

1. Launch a ClickHouse cluster. Refer to the [ClickHouse Installation Guide](https://clickhouse.com/docs/en/getting-started/install/) for details.
2. After launching a ClickHouse cluster, retrieve the `Connection Details` from the cluster's `Actions` menu. You will need the host, port, username, and password.
3. Install the required Node.js peer dependency for ClickHouse in your workspace.

You will need to install the following peer dependencies:

```bash npm2yarn
npm install -S @clickhouse/client mysql2
```

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

## Index and Query Docs

import InsertExample from "@examples/indexes/vector_stores/clickhouse_fromTexts.ts";

<CodeBlock language="typescript">{InsertExample}</CodeBlock>

## Query Docs From an Existing Collection

import SearchExample from "@examples/indexes/vector_stores/clickhouse_search.ts";

<CodeBlock language="typescript">{SearchExample}</CodeBlock>

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/closevector.mdx
================================================
import CodeBlock from "@theme/CodeBlock";

# CloseVector

:::tip Compatibility
available on both browser and Node.js
:::

[CloseVector](https://closevector.getmegaportal.com/) is a cross-platform vector database that can run in both the browser and Node.js. For example, you can create your index on Node.js and then load/query it on browser. For more information, please visit [CloseVector Docs](https://closevector-docs.getmegaportal.com/).

## Setup

### CloseVector Web

```bash npm2yarn
npm install -S closevector-web
```

### CloseVector Node

```bash npm2yarn
npm install -S closevector-node
```

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

## Usage

### Create a new index from texts

import ExampleTexts from "@examples/indexes/vector_stores/closevector.ts";

<CodeBlock language="typescript">{ExampleTexts}</CodeBlock>

### Create a new index from a loader

import ExampleLoader from "@examples/indexes/vector_stores/closevector_fromdocs.ts";

<CodeBlock language="typescript">{ExampleLoader}</CodeBlock>

### Save an index to CloseVector CDN and load it again

CloseVector supports saving/loading indexes to/from cloud. To use this feature, you need to create an account on [CloseVector](https://closevector.getmegaportal.com/). Please read [CloseVector Docs](https://closevector-docs.getmegaportal.com/) and generate your API key first by [loging in](https://closevector.getmegaportal.com/).

import ExampleCloud from "@examples/indexes/vector_stores/closevector_saveload_fromcloud.ts";

<CodeBlock language="typescript">{ExampleCloud}</CodeBlock>

### Save an index to file and load it again

import ExampleSave from "@examples/indexes/vector_stores/closevector_saveload.ts";

<CodeBlock language="typescript">{ExampleSave}</CodeBlock>

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/cloudflare_vectorize.mdx
================================================
---
hide_table_of_contents: true
---

# Cloudflare Vectorize

If you're deploying your project in a Cloudflare worker, you can use [Cloudflare Vectorize](https://developers.cloudflare.com/vectorize/) with LangChain.js.
It's a powerful and convenient option that's built directly into Cloudflare.

## Setup

:::tip Compatibility
Cloudflare Vectorize is currently in open beta, and requires a Cloudflare account on a paid plan to use.
:::

After [setting up your project](https://developers.cloudflare.com/vectorize/get-started/intro/#prerequisites),
create an index by running the following Wrangler command:

```bash
$ npx wrangler vectorize create <index_name> --preset @cf/baai/bge-small-en-v1.5
```

You can see a full list of options for the `vectorize` command [in the official documentation](https://developers.cloudflare.com/workers/wrangler/commands/#vectorize).

You'll then need to update your `wrangler.toml` file to include an entry for `[[vectorize]]`:

```toml
[[vectorize]]
binding = "VECTORIZE_INDEX"
index_name = "<index_name>"
```

Finally, you'll need to install the LangChain Cloudflare integration package:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/cloudflare @langchain/core
```

## Usage

Below is an example worker that adds documents to a vectorstore, queries it, or clears it depending on the path used. It also uses [Cloudflare Workers AI Embeddings](/docs/integrations/text_embedding/cloudflare_ai).

:::note
If running locally, be sure to run wrangler as `npx wrangler dev --remote`!
:::

```toml
name = "langchain-test"
main = "worker.ts"
compatibility_date = "2024-01-10"

[[vectorize]]
binding = "VECTORIZE_INDEX"
index_name = "langchain-test"

[ai]
binding = "AI"
```

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/indexes/vector_stores/cloudflare_vectorize/example.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

You can also pass a `filter` parameter to filter by previously loaded metadata.
See [the official documentation](https://developers.cloudflare.com/vectorize/learning/metadata-filtering/)
for information on the required format.

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/convex.mdx
================================================
---
sidebar_class_name: node-only
---

# Convex

LangChain.js supports [Convex](https://convex.dev/) as a [vector store](https://docs.convex.dev/vector-search), and supports the standard similarity search.

## Setup

### Create project

Get a working [Convex](https://docs.convex.dev/) project set up, for example by using:

```bash
npm create convex@latest
```

### Add database accessors

Add query and mutation helpers to `convex/langchain/db.ts`:

```ts title="convex/langchain/db.ts"
export * from "@langchain/community/utils/convex";
```

### Configure your schema

Set up your schema (for vector indexing):

```ts title="convex/schema.ts"
import { defineSchema, defineTable } from "convex/server";
import { v } from "convex/values";

export default defineSchema({
  documents: defineTable({
    embedding: v.array(v.number()),
    text: v.string(),
    metadata: v.any(),
  }).vectorIndex("byEmbedding", {
    vectorField: "embedding",
    dimensions: 1536,
  }),
});
```

## Usage

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

### Ingestion

import CodeBlock from "@theme/CodeBlock";
import Ingestion from "@examples/indexes/vector_stores/convex/fromTexts.ts";

<CodeBlock language="typescript" title="convex/myActions.ts">
  {Ingestion}
</CodeBlock>

### Search

import Search from "@examples/indexes/vector_stores/convex/search.ts";

<CodeBlock language="typescript" title="convex/myActions.ts">
  {Search}
</CodeBlock>

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/couchbase.mdx
================================================
---
hide_table_of_contents: true
sidebar_class_name: node-only
---

import CodeBlock from "@theme/CodeBlock";

# Couchbase

[Couchbase](http://couchbase.com/) is an award-winning distributed NoSQL cloud database that delivers unmatched versatility, performance, scalability, and financial value for all of your cloud, mobile,
AI, and edge computing applications. Couchbase embraces AI with coding assistance for developers and vector search for their applications.

Vector Search is a part of the [Full Text Search Service](https://docs.couchbase.com/server/current/learn/services-and-indexes/services/search-service.html) (Search Service) in Couchbase.

This tutorial explains how to use Vector Search in Couchbase. You can work with both [Couchbase Capella](https://www.couchbase.com/products/capella/) and your self-managed Couchbase Server.

## Installation

You will need couchbase and langchain community to use couchbase vector store. For this tutorial, we will use OpenAI embeddings

```bash npm2yarn
npm install couchbase @langchain/openai @langchain/community @langchain/core
```

## Create Couchbase Connection Object

We create a connection to the Couchbase cluster initially and then pass the cluster object to the Vector Store. Here, we are connecting using the username and password.
You can also connect using any other supported way to your cluster.

For more information on connecting to the Couchbase cluster, please check the [Node SDK documentation](https://docs.couchbase.com/nodejs-sdk/current/hello-world/start-using-sdk.html#connect).

```typescript
import { Cluster } from "couchbase";

const connectionString = "couchbase://localhost"; // or couchbases://localhost if you are using TLS
const dbUsername = "Administrator"; // valid database user with read access to the bucket being queried
const dbPassword = "Password"; // password for the database user

const couchbaseClient = await Cluster.connect(connectionString, {
  username: dbUsername,
  password: dbPassword,
  configProfile: "wanDevelopment",
});
```

## Create the Search Index

Currently, the Search index needs to be created from the Couchbase Capella or Server UI or using the REST interface.

For this example, let us use the Import Index feature on the Search Service on the UI.

Let us define a Search index with the name `vector-index` on the testing bucket.
We are defining an index on the `testing` bucket's `_default` scope on the `_default` collection with the vector field set to `embedding` with 1536 dimensions and the text field set to `text`.
We are also indexing and storing all the fields under `metadata` in the document as a dynamic mapping to account for varying document structures. The similarity metric is set to `dot_product`.

### How to Import an Index to the Full Text Search service?

- [Couchbase Server](https://docs.couchbase.com/server/current/search/import-search-index.html)
  - Click on Search -> Add Index -> Import
  - Copy the following Index definition in the Import screen
  - Click on Create Index to create the index.
- [Couchbase Capella](https://docs.couchbase.com/cloud/search/import-search-index.html)
  - Copy the following index definition to a new file `index.json`
  - Import the file in Capella using the instructions in the documentation.
  - Click on Create Index to create the index.

### Index Definition

```json
{
  "name": "vector-index",
  "type": "fulltext-index",
  "params": {
    "doc_config": {
      "docid_prefix_delim": "",
      "docid_regexp": "",
      "mode": "type_field",
      "type_field": "type"
    },
    "mapping": {
      "default_analyzer": "standard",
      "default_datetime_parser": "dateTimeOptional",
      "default_field": "_all",
      "default_mapping": {
        "dynamic": true,
        "enabled": true,
        "properties": {
          "metadata": {
            "dynamic": true,
            "enabled": true
          },
          "embedding": {
            "enabled": true,
            "dynamic": false,
            "fields": [
              {
                "dims": 1536,
                "index": true,
                "name": "embedding",
                "similarity": "dot_product",
                "type": "vector",
                "vector_index_optimized_for": "recall"
              }
            ]
          },
          "text": {
            "enabled": true,
            "dynamic": false,
            "fields": [
              {
                "index": true,
                "name": "text",
                "store": true,
                "type": "text"
              }
            ]
          }
        }
      },
      "default_type": "_default",
      "docvalues_dynamic": false,
      "index_dynamic": true,
      "store_dynamic": true,
      "type_field": "_type"
    },
    "store": {
      "indexType": "scorch",
      "segmentVersion": 16
    }
  },
  "sourceType": "gocbcore",
  "sourceName": "testing",
  "sourceParams": {},
  "planParams": {
    "maxPartitionsPerPIndex": 103,
    "indexPartitions": 10,
    "numReplicas": 0
  }
}
```

For more details on how to create a search index with support for Vector fields, please refer to the documentation:

- [Couchbase Capella](https://docs.couchbase.com/cloud/search/create-search-indexes.html)
- [Couchbase Server](https://docs.couchbase.com/server/current/search/create-search-indexes.html)

For using this vector store, CouchbaseVectorStoreArgs needs to be configured.
textKey and embeddingKey are optional fields, required if you want to use specific keys

```typescript
const couchbaseConfig: CouchbaseVectorStoreArgs = {
  cluster: couchbaseClient,
  bucketName: "testing",
  scopeName: "_default",
  collectionName: "_default",
  indexName: "vector-index",
  textKey: "text",
  embeddingKey: "embedding",
};
```

## Create Vector Store

We create the vector store object with the cluster information and the search index name.

```typescript
const store = await CouchbaseVectorStore.initialize(
  embeddings, // embeddings object to create embeddings from text
  couchbaseConfig
);
```

## Basic Vector Search Example

The following example showcases how to use couchbase vector search and perform similarity search.
For this example, we are going to load the "state_of_the_union.txt" file via the TextLoader,
chunk the text into 500 character chunks with no overlaps and index all these chunks into Couchbase.

After the data is indexed, we perform a simple query to find the top 4 chunks that are similar to the
query "What did president say about Ketanji Brown Jackson".
At the end, it also shows how to get similarity score

```ts
import { OpenAIEmbeddings } from "@langchain/openai";
import {
  CouchbaseVectorStoreArgs,
  CouchbaseVectorStore,
} from "@langchain/community/vectorstores/couchbase";
import { Cluster } from "couchbase";
import { TextLoader } from "langchain/document_loaders/fs/text";
import { CharacterTextSplitter } from "@langchain/textsplitters";

const connectionString =
  process.env.COUCHBASE_DB_CONN_STR ?? "couchbase://localhost";
const databaseUsername = process.env.COUCHBASE_DB_USERNAME ?? "Administrator";
const databasePassword = process.env.COUCHBASE_DB_PASSWORD ?? "Password";

// Load documents from file
const loader = new TextLoader("./state_of_the_union.txt");
const rawDocuments = await loader.load();
const splitter = new CharacterTextSplitter({
  chunkSize: 500,
  chunkOverlap: 0,
});
const docs = await splitter.splitDocuments(rawDocuments);

const couchbaseClient = await Cluster.connect(connectionString, {
  username: databaseUsername,
  password: databasePassword,
  configProfile: "wanDevelopment",
});

// Open AI API Key is required to use OpenAIEmbeddings, some other embeddings may also be used
const embeddings = new OpenAIEmbeddings({
  apiKey: process.env.OPENAI_API_KEY,
});

const couchbaseConfig: CouchbaseVectorStoreArgs = {
  cluster: couchbaseClient,
  bucketName: "testing",
  scopeName: "_default",
  collectionName: "_default",
  indexName: "vector-index",
  textKey: "text",
  embeddingKey: "embedding",
};

const store = await CouchbaseVectorStore.fromDocuments(
  docs,
  embeddings,
  couchbaseConfig
);

const query = "What did president say about Ketanji Brown Jackson";

const resultsSimilaritySearch = await store.similaritySearch(query);
console.log("resulting documents: ", resultsSimilaritySearch[0]);

// Similarity Search With Score
const resultsSimilaritySearchWithScore = await store.similaritySearchWithScore(
  query,
  1
);
console.log("resulting documents: ", resultsSimilaritySearchWithScore[0][0]);
console.log("resulting scores: ", resultsSimilaritySearchWithScore[0][1]);

const result = await store.similaritySearch(query, 1, {
  fields: ["metadata.source"],
});
console.log(result[0]);
```

## Specifying Fields to Return

You can specify the fields to return from the document using `fields` parameter in the filter during searches.
These fields are returned as part of the `metadata` object. You can fetch any field that is stored in the index.
The `textKey` of the document is returned as part of the document's `pageContent`.

If you do not specify any fields to be fetched, all the fields stored in the index are returned.

If you want to fetch one of the fields in the metadata, you need to specify it using `.`
For example, to fetch the `source` field in the metadata, you need to use `metadata.source`.

```typescript
const result = await store.similaritySearch(query, 1, {
  fields: ["metadata.source"],
});
console.log(result[0]);
```

## Hybrid Search

Couchbase allows you to do hybrid searches by combining vector search results with searches on non-vector fields of the document like the `metadata` object.

The results will be based on the combination of the results from both vector search and the searches supported by full text search service.
The scores of each of the component searches are added up to get the total score of the result.

To perform hybrid searches, there is an optional key, `searchOptions` in `fields` parameter that can be passed to all the similarity searches.  
The different search/query possibilities for the `searchOptions` can be found [here](https://docs.couchbase.com/server/current/search/search-request-params.html#query-object).

### Create Diverse Metadata for Hybrid Search

In order to simulate hybrid search, let us create some random metadata from the existing documents.
We uniformly add three fields to the metadata, `date` between 2010 & 2020, `rating` between 1 & 5 and `author` set to either John Doe or Jane Doe.
We will also declare few sample queries.

```typescript
for (let i = 0; i < docs.length; i += 1) {
  docs[i].metadata.date = `${2010 + (i % 10)}-01-01`;
  docs[i].metadata.rating = 1 + (i % 5);
  docs[i].metadata.author = ["John Doe", "Jane Doe"][i % 2];
}

const store = await CouchbaseVectorStore.fromDocuments(
  docs,
  embeddings,
  couchbaseConfig
);

const query = "What did the president say about Ketanji Brown Jackson";
const independenceQuery = "Any mention about independence?";
```

### Example: Search by Exact Value

We can search for exact matches on a textual field like the author in the `metadata` object.

```typescript
const exactValueResult = await store.similaritySearch(query, 4, {
  fields: ["metadata.author"],
  searchOptions: {
    query: { field: "metadata.author", match: "John Doe" },
  },
});
console.log(exactValueResult[0]);
```

### Example: Search by Partial Match

We can search for partial matches by specifying a fuzziness for the search. This is useful when you want to search for slight variations or misspellings of a search query.

Here, "Johny" is close (fuzziness of 1) to "John Doe".

```typescript
const partialMatchResult = await store.similaritySearch(query, 4, {
  fields: ["metadata.author"],
  searchOptions: {
    query: { field: "metadata.author", match: "Johny", fuzziness: 1 },
  },
});
console.log(partialMatchResult[0]);
```

### Example: Search by Date Range Query

We can search for documents that are within a date range query on a date field like `metadata.date`.

```typescript
const dateRangeResult = await store.similaritySearch(independenceQuery, 4, {
  fields: ["metadata.date", "metadata.author"],
  searchOptions: {
    query: {
      start: "2016-12-31",
      end: "2017-01-02",
      inclusiveStart: true,
      inclusiveEnd: false,
      field: "metadata.date",
    },
  },
});
console.log(dateRangeResult[0]);
```

### Example: Search by Numeric Range Query

We can search for documents that are within a range for a numeric field like `metadata.rating`.

```typescript
const ratingRangeResult = await store.similaritySearch(independenceQuery, 4, {
  fields: ["metadata.rating"],
  searchOptions: {
    query: {
      min: 3,
      max: 5,
      inclusiveMin: false,
      inclusiveMax: true,
      field: "metadata.rating",
    },
  },
});
console.log(ratingRangeResult[0]);
```

### Example: Combining Multiple Search Conditions

Different queries can by combined using AND (conjuncts) or OR (disjuncts) operators.

In this example, we are checking for documents with a rating between 3 & 4 and dated between 2015 & 2018.

```typescript
const multipleConditionsResult = await store.similaritySearch(texts[0], 4, {
  fields: ["metadata.rating", "metadata.date"],
  searchOptions: {
    query: {
      conjuncts: [
        { min: 3, max: 4, inclusive_max: true, field: "metadata.rating" },
        { start: "2016-12-31", end: "2017-01-02", field: "metadata.date" },
      ],
    },
  },
});
console.log(multipleConditionsResult[0]);
```

### Other Queries

Similarly, you can use any of the supported Query methods like Geo Distance, Polygon Search, Wildcard, Regular Expressions, etc in the `searchOptions` Key of `filter` parameter.
Please refer to the documentation for more details on the available query methods and their syntax.

- [Couchbase Capella](https://docs.couchbase.com/cloud/search/search-request-params.html#query-object)
- [Couchbase Server](https://docs.couchbase.com/server/current/search/search-request-params.html#query-object)

<br />
<br />

# Frequently Asked Questions

## Question: Should I create the Search index before creating the CouchbaseVectorStore object?

Yes, currently you need to create the Search index before creating the `CouchbaseVectorStore` object.

## Question: I am not seeing all the fields that I specified in my search results.

In Couchbase, we can only return the fields stored in the Search index. Please ensure that the field that you are trying to access in the search results is part of the Search index.

One way to handle this is to index and store a document's fields dynamically in the index.

- In Capella, you need to go to "Advanced Mode" then under the chevron "General Settings" you can check "[X] Store Dynamic Fields" or "[X] Index Dynamic Fields"
- In Couchbase Server, in the Index Editor (not Quick Editor) under the chevron "Advanced" you can check "[X] Store Dynamic Fields" or "[X] Index Dynamic Fields"

Note that these options will increase the size of the index.

For more details on dynamic mappings, please refer to the [documentation](https://docs.couchbase.com/cloud/search/customize-index.html).

## Question: I am unable to see the metadata object in my search results.

This is most likely due to the `metadata` field in the document not being indexed and/or stored by the Couchbase Search index. In order to index the `metadata` field in the document, you need to add it to the index as a child mapping.

If you select to map all the fields in the mapping, you will be able to search by all metadata fields. Alternatively, to optimize the index, you can select the specific fields inside `metadata` object to be indexed.
You can refer to the [docs](https://docs.couchbase.com/cloud/search/customize-index.html) to learn more about indexing child mappings.

To create Child Mappings, you can refer to the following docs -

- [Couchbase Capella](https://docs.couchbase.com/cloud/search/create-child-mapping.html)
- [Couchbase Server](https://docs.couchbase.com/server/current/fts/fts-creating-index-from-UI-classic-editor-dynamic.html)

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/elasticsearch.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Elasticsearch
sidebar_class_name: node-only
---
"""

"""
# Elasticsearch

```{=mdx}

:::tip Compatibility
Only available on Node.js.
:::

```

[Elasticsearch](https://github.com/elastic/elasticsearch) is a distributed, RESTful search engine optimized for speed and relevance on production-scale workloads. It supports also vector search using the [k-nearest neighbor](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) (kNN) algorithm and also [custom models for Natural Language Processing](https://www.elastic.co/blog/how-to-deploy-nlp-text-embeddings-and-vector-search) (NLP).
You can read more about the support of vector search in Elasticsearch [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/knn-search.html).

This guide provides a quick overview for getting started with Elasticsearch [vector stores](/docs/concepts/#vectorstores). For detailed documentation of all `ElasticVectorSearch` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_vectorstores_elasticsearch.ElasticVectorSearch.html).
"""

"""
## Overview

### Integration details

| Class | Package | [PY support](https://python.langchain.com/docs/integrations/vectorstores/elasticsearch/) |  Package latest |
| :--- | :--- | :---: | :---: |
| [`ElasticVectorSearch`](https://api.js.langchain.com/classes/langchain_community_vectorstores_elasticsearch.ElasticVectorSearch.html) | [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) | ✅ |  ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |
"""

"""
## Setup

To use Elasticsearch vector stores, you'll need to install the `@langchain/community` integration package.

LangChain.js accepts [`@elastic/elasticsearch`](https://github.com/elastic/elasticsearch-js) as the client for Elasticsearch vectorstore. You'll need to install it as a peer dependency.

This guide will also use [OpenAI embeddings](/docs/integrations/text_embedding/openai), which require you to install the `@langchain/openai` integration package. You can also use [other supported embeddings models](/docs/integrations/text_embedding) if you wish.

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @elastic/elasticsearch @langchain/openai @langchain/core
</Npm2Yarn>
```

### Credentials

To use Elasticsearch vector stores, you'll need to have an Elasticsearch instance running.

You can use the [official Docker image](https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html) to get started, or you can use [Elastic Cloud](https://www.elastic.co/cloud/), Elastic's official cloud service.

For connecting to Elastic Cloud you can read the documentation reported [here](https://www.elastic.co/guide/en/kibana/current/api-keys.html) for obtaining an API key.

If you are using OpenAI embeddings for this guide, you'll need to set your OpenAI key as well:

```typescript
process.env.OPENAI_API_KEY = "YOUR_API_KEY";
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
// process.env.LANGSMITH_TRACING="true"
// process.env.LANGSMITH_API_KEY="your-api-key"
```
"""

"""
## Instantiation

Instatiating Elasticsearch will vary depending on where your instance is hosted.
"""

import {
  ElasticVectorSearch,
  type ElasticClientArgs,
} from "@langchain/community/vectorstores/elasticsearch";
import { OpenAIEmbeddings } from "@langchain/openai";

import { Client, type ClientOptions } from "@elastic/elasticsearch";

import * as fs from "node:fs";

const embeddings = new OpenAIEmbeddings({
  model: "text-embedding-3-small",
});

const config: ClientOptions = {
  node: process.env.ELASTIC_URL ?? "https://127.0.0.1:9200",
};

if (process.env.ELASTIC_API_KEY) {
  config.auth = {
    apiKey: process.env.ELASTIC_API_KEY,
  };
} else if (process.env.ELASTIC_USERNAME && process.env.ELASTIC_PASSWORD) {
  config.auth = {
    username: process.env.ELASTIC_USERNAME,
    password: process.env.ELASTIC_PASSWORD,
  };
}
// Local Docker deploys require a TLS certificate
if (process.env.ELASTIC_CERT_PATH) {
  config.tls = {
    ca: fs.readFileSync(process.env.ELASTIC_CERT_PATH),
    rejectUnauthorized: false,
  }
}
const clientArgs: ElasticClientArgs = {
  client: new Client(config),
  indexName: process.env.ELASTIC_INDEX ?? "test_vectorstore",
};

const vectorStore = new ElasticVectorSearch(embeddings, clientArgs);

"""
## Manage vector store

### Add items to vector store
"""

import type { Document } from "@langchain/core/documents";

const document1: Document = {
  pageContent: "The powerhouse of the cell is the mitochondria",
  metadata: { source: "https://example.com" }
};

const document2: Document = {
  pageContent: "Buildings are made out of brick",
  metadata: { source: "https://example.com" }
};

const document3: Document = {
  pageContent: "Mitochondria are made out of lipids",
  metadata: { source: "https://example.com" }
};

const document4: Document = {
  pageContent: "The 2024 Olympics are in Paris",
  metadata: { source: "https://example.com" }
}

const documents = [document1, document2, document3, document4];

await vectorStore.addDocuments(documents, { ids: ["1", "2", "3", "4"] });
# Output:
#   [ '1', '2', '3', '4' ]


"""
### Delete items from vector store

You can delete values from the store by passing the same id you passed in:
"""

await vectorStore.delete({ ids: ["4"] });

"""
## Query vector store

Once your vector store has been created and the relevant documents have been added you will most likely wish to query it during the running of your chain or agent.

### Query directly

Performing a simple similarity search can be done as follows:
"""

const filter = [{
  operator: "match",
  field: "source",
  value: "https://example.com",
}];

const similaritySearchResults = await vectorStore.similaritySearch("biology", 2, filter);

for (const doc of similaritySearchResults) {
  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);
}
# Output:
#   * The powerhouse of the cell is the mitochondria [{"source":"https://example.com"}]

#   * Mitochondria are made out of lipids [{"source":"https://example.com"}]


"""
The vector store supports [Elasticsearch filter syntax](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-filter-context.html) operators.

If you want to execute a similarity search and receive the corresponding scores you can run:
"""

const similaritySearchWithScoreResults = await vectorStore.similaritySearchWithScore("biology", 2, filter)

for (const [doc, score] of similaritySearchWithScoreResults) {
  console.log(`* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`);
}
# Output:
#   * [SIM=0.374] The powerhouse of the cell is the mitochondria [{"source":"https://example.com"}]

#   * [SIM=0.370] Mitochondria are made out of lipids [{"source":"https://example.com"}]


"""
### Query by turning into retriever

You can also transform the vector store into a [retriever](/docs/concepts/retrievers) for easier usage in your chains. 
"""

const retriever = vectorStore.asRetriever({
  // Optional filter
  filter: filter,
  k: 2,
});
await retriever.invoke("biology");
# Output:
#   [

#     Document {

#       pageContent: 'The powerhouse of the cell is the mitochondria',

#       metadata: { source: 'https://example.com' },

#       id: undefined

#     },

#     Document {

#       pageContent: 'Mitochondria are made out of lipids',

#       metadata: { source: 'https://example.com' },

#       id: undefined

#     }

#   ]


"""
### Usage for retrieval-augmented generation

For guides on how to use this vector store for retrieval-augmented generation (RAG), see the following sections:

- [Tutorials: working with external knowledge](/docs/tutorials/#working-with-external-knowledge).
- [How-to: Question and answer with RAG](/docs/how_to/#qa-with-rag)
- [Retrieval conceptual docs](/docs/concepts/retrieval)
"""

"""
## API reference

For detailed documentation of all `ElasticVectorSearch` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_vectorstores_elasticsearch.ElasticVectorSearch.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/faiss.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Faiss
sidebar_class_name: node-only
---
"""

"""
# FaissStore

```{=mdx}

:::tip Compatibility
Only available on Node.js.
:::

```

[Faiss](https://github.com/facebookresearch/faiss) is a library for efficient similarity search and clustering of dense vectors.

LangChain.js supports using Faiss as a locally-running vectorstore that can be saved to a file. It also provides the ability to read the saved file from the [LangChain Python implementation](https://python.langchain.com/docs/integrations/vectorstores/faiss#saving-and-loading).

This guide provides a quick overview for getting started with Faiss [vector stores](/docs/concepts/#vectorstores). For detailed documentation of all `FaissStore` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_vectorstores_faiss.FaissStore.html).
"""

"""
## Overview

### Integration details

| Class | Package | [PY support](https://python.langchain.com/docs/integrations/vectorstores/faiss) |  Package latest |
| :--- | :--- | :---: | :---: |
| [`FaissStore`](https://api.js.langchain.com/classes/langchain_community_vectorstores_faiss.FaissStore.html) | [`@langchain/community`](https://npmjs.com/@langchain/community) | ✅ |  ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |
"""

"""
## Setup

To use Faiss vector stores, you'll need to install the `@langchain/community` integration package and the [`faiss-node`](https://github.com/ewfian/faiss-node) package as a peer dependency.

This guide will also use [OpenAI embeddings](/docs/integrations/text_embedding/openai), which require you to install the `@langchain/openai` integration package. You can also use [other supported embeddings models](/docs/integrations/text_embedding) if you wish.

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community faiss-node @langchain/openai @langchain/core
</Npm2Yarn>
```

### Credentials

Because Faiss runs locally, you do not need any credentials to use it.

If you are using OpenAI embeddings for this guide, you'll need to set your OpenAI key as well:

```typescript
process.env.OPENAI_API_KEY = "YOUR_API_KEY";
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
// process.env.LANGSMITH_TRACING="true"
// process.env.LANGSMITH_API_KEY="your-api-key"
```
"""

"""
## Instantiation
"""

import { FaissStore } from "@langchain/community/vectorstores/faiss";
import { OpenAIEmbeddings } from "@langchain/openai";

const embeddings = new OpenAIEmbeddings({
  model: "text-embedding-3-small",
});

const vectorStore = new FaissStore(embeddings, {});

"""
## Manage vector store

### Add items to vector store
"""

import type { Document } from "@langchain/core/documents";

const document1: Document = {
  pageContent: "The powerhouse of the cell is the mitochondria",
  metadata: { source: "https://example.com" }
};

const document2: Document = {
  pageContent: "Buildings are made out of brick",
  metadata: { source: "https://example.com" }
};

const document3: Document = {
  pageContent: "Mitochondria are made out of lipids",
  metadata: { source: "https://example.com" }
};

const document4: Document = {
  pageContent: "The 2024 Olympics are in Paris",
  metadata: { source: "https://example.com" }
}

const documents = [document1, document2, document3, document4];

await vectorStore.addDocuments(documents, { ids: ["1", "2", "3", "4"] });
# Output:
#   [ '1', '2', '3', '4' ]


"""
### Delete items from vector store
"""

await vectorStore.delete({ ids: ["4"] });

"""
## Query vector store

Once your vector store has been created and the relevant documents have been added you will most likely wish to query it during the running of your chain or agent. 

### Query directly

Performing a simple similarity search can be done as follows:
"""

const similaritySearchResults = await vectorStore.similaritySearch("biology", 2);

for (const doc of similaritySearchResults) {
  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);
}
# Output:
#   * The powerhouse of the cell is the mitochondria [{"source":"https://example.com"}]

#   * Mitochondria are made out of lipids [{"source":"https://example.com"}]


"""
Filtering by metadata is currently not supported.

If you want to execute a similarity search and receive the corresponding scores you can run:
"""

const similaritySearchWithScoreResults = await vectorStore.similaritySearchWithScore("biology", 2);

for (const [doc, score] of similaritySearchWithScoreResults) {
  console.log(`* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`);
}
# Output:
#   * [SIM=1.671] The powerhouse of the cell is the mitochondria [{"source":"https://example.com"}]

#   * [SIM=1.705] Mitochondria are made out of lipids [{"source":"https://example.com"}]


"""
### Query by turning into retriever

You can also transform the vector store into a [retriever](/docs/concepts/retrievers) for easier usage in your chains. 
"""

const retriever = vectorStore.asRetriever({
  k: 2,
});
await retriever.invoke("biology");
# Output:
#   [

#     {

#       pageContent: 'The powerhouse of the cell is the mitochondria',

#       metadata: { source: 'https://example.com' }

#     },

#     {

#       pageContent: 'Mitochondria are made out of lipids',

#       metadata: { source: 'https://example.com' }

#     }

#   ]


"""
### Usage for retrieval-augmented generation

For guides on how to use this vector store for retrieval-augmented generation (RAG), see the following sections:

- [Tutorials: working with external knowledge](/docs/tutorials/#working-with-external-knowledge).
- [How-to: Question and answer with RAG](/docs/how_to/#qa-with-rag)
- [Retrieval conceptual docs](/docs/concepts/retrieval)
"""

"""
## Merging indexes

Faiss also supports merging existing indexes:
"""

// Create an initial vector store
const initialStore = await FaissStore.fromTexts(
  ["Hello world", "Bye bye", "hello nice world"],
  [{ id: 2 }, { id: 1 }, { id: 3 }],
  new OpenAIEmbeddings()
);

// Create another vector store from texts
const newStore = await FaissStore.fromTexts(
  ["Some text"],
  [{ id: 1 }],
  new OpenAIEmbeddings()
);

// merge the first vector store into vectorStore2
await newStore.mergeFrom(initialStore);

// You can also create a new vector store from another FaissStore index
const newStore2 = await FaissStore.fromIndex(
  newStore,
  new OpenAIEmbeddings()
);

await newStore2.similaritySearch("Bye bye", 1);

"""
## Save an index to file and load it again

To persist an index on disk, use the `.save` and static `.load` methods:
"""

// Create a vector store through any method, here from texts as an example
const persistentStore = await FaissStore.fromTexts(
  ["Hello world", "Bye bye", "hello nice world"],
  [{ id: 2 }, { id: 1 }, { id: 3 }],
  new OpenAIEmbeddings()
);

// Save the vector store to a directory
const directory = "your/directory/here";

await persistentStore.save(directory);

// Load the vector store from the same directory
const loadedVectorStore = await FaissStore.load(
  directory,
  new OpenAIEmbeddings()
);

// vectorStore and loadedVectorStore are identical
const result = await loadedVectorStore.similaritySearch("hello world", 1);
console.log(result);

"""
## Reading saved files from Python

To enable the ability to read the saved file from [LangChain Python's implementation](https://python.langchain.com/docs/integrations/vectorstores/faiss#saving-and-loading), you'll need to install the [`pickleparser`](https://github.com/ewfian/pickleparser) package.

```{=mdx}
<Npm2Yarn>
  pickleparser
</Npm2Yarn>
```

Then you can use the `.loadFromPython` static method:
"""

// The directory of data saved from Python
const directoryWithSavedPythonStore = "your/directory/here";

// Load the vector store from the directory
const pythonLoadedStore = await FaissStore.loadFromPython(
  directoryWithSavedPythonStore,
  new OpenAIEmbeddings()
);

// Search for the most similar document
await pythonLoadedStore.similaritySearch("test", 2);

"""
## API reference

For detailed documentation of all `FaissStore` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_vectorstores_faiss.FaissStore.html)
"""



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/google_cloudsql_pg.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Google Cloud SQL for PostgreSQL

[Cloud SQL](https://cloud.google.com/sql) is a fully managed relational database service that offers high performance, seamless integration, and impressive scalability and offers database engines such as PostgreSQL.

This guide provides a quick overview of how to use Cloud SQL for PostgreSQL to store vector embeddings with the `PostgresVectorStore` class.

"""

"""
## Overview

### Integration details

| Class               | Package                                    | [PY support](https://python.langchain.com/docs/integrations/vectorstores/google_cloud_sql_pg/) | Package latest |
| :------------------ | :----------------------------------------- | :--------------------------------------------------------------------------------------------: | :------------: |
| PostgresVectorStore | [`@langchain/google-cloud-sql-pg`](https://www.npmjs.com/package/@langchain/google-cloud-sql-pg) |                                               ✅                                               |     0.0.1      |

"""

"""
### Before you begin

In order to use this package, you first need to go throught the following steps:

1.  [Select or create a Cloud Platform project.](https://developers.google.com/workspace/guides/create-project)
2.  [Enable billing for your project.](https://cloud.google.com/billing/docs/how-to/modify-project#enable_billing_for_a_project)
3.  [Enable the Cloud SQL Admin API.](https://console.cloud.google.com/flows/enableapi?apiid=sqladmin.googleapis.com)
4.  [Setup Authentication.](https://cloud.google.com/docs/authentication)
5.  [Create a CloudSQL instance](https://cloud.google.com/sql/docs/postgres/connect-instance-auth-proxy#create-instance)
6.  [Create a CloudSQL database](https://cloud.google.com/sql/docs/postgres/create-manage-databases)
7.  [Add a user to the database](https://cloud.google.com/sql/docs/postgres/create-manage-users)

"""

"""
### Authentication

Authenticate locally to your Google Cloud account using the ```gcloud auth login``` command.

### Set Your Google Cloud Project

Set your Google Cloud project ID to leverage Google Cloud resources locally:
"""

gcloud config set project YOUR-PROJECT-ID

"""
If you don't know your project ID, try the following:
*   Run `gcloud config list`.
*   Run `gcloud projects list`.
*   See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113).
"""

"""
## Setting up a PostgresVectorStore instance

To use the PostgresVectorStore library, you'll need to install the `@langchain/google-cloud-sql-pg` package and then follow the steps bellow.

First, you'll need to log in to your Google Cloud account and set the following environment variables based on your Google Cloud project; these will be defined based on how you want to configure (fromInstance, fromEngine, fromEngineArgs) your PostgresEngine instance :

"""

PROJECT_ID="your-project-id"
REGION="your-project-region" // example: "us-central1"
INSTANCE_NAME="your-instance"
DB_NAME="your-database-name"
DB_USER="your-database-user"
PASSWORD="your-database-password"

"""
### Setting up an instance

To instantiate a PostgresVectorStore, you'll first need to create a database connection through the PostgresEngine, then initialize the vector store table and finally call the `.initialize()` method to instantiate the vector store.

"""

import {
  Column,
  PostgresEngine,
  PostgresEngineArgs,
  PostgresVectorStore,
  PostgresVectorStoreArgs,
  VectorStoreTableArgs,
} from "@langchain/google-cloud-sql-pg";
import { SyntheticEmbeddings } from "@langchain/core/utils/testing"; // This is used as an Embedding service
import * as dotenv from "dotenv";

dotenv.config();

const peArgs: PostgresEngineArgs = {
  user: process.env.DB_USER ?? "",
  password: process.env.PASSWORD ?? "",
};

// PostgresEngine instantiation
const engine: PostgresEngine = await PostgresEngine.fromInstance(
  process.env.PROJECT_ID ?? "",
  process.env.REGION ?? "",
  process.env.INSTANCE_NAME ?? "",
  process.env.DB_NAME ?? "",
  peArgs
);

const vectorStoreArgs: VectorStoreTableArgs = {
  metadataColumns: [new Column("page", "TEXT"), new Column("source", "TEXT")],
};

// Vector store table initilization
await engine.initVectorstoreTable("my_vector_store_table", 768, vectorStoreArgs);
const embeddingService = new SyntheticEmbeddings({ vectorSize: 768 });

const pvectorArgs: PostgresVectorStoreArgs = {
  metadataColumns: ["page", "source"],
};

// PostgresVectorStore instantiation
const vectorStore = await PostgresVectorStore.initialize(
  engine,
  embeddingService,
  "my_vector_store_table",
  pvectorArgs
);


"""
## Manage Vector Store

### Add Documents to vector store

To add Documents to the vector store, you would be able to it by passing or not the ids

"""

import { v4 as uuidv4 } from "uuid";
import type { Document } from "@langchain/core/documents";

const document1: Document = {
  pageContent: "The powerhouse of the cell is the mitochondria",
  metadata: { page: 0, source: "https://example.com" },
};

const document2: Document = {
  pageContent: "Buildings are made out of brick",
  metadata: { page: 1, source: "https://example.com" },
};

const document3: Document = {
  pageContent: "Mitochondria are made out of lipids",
  metadata: { page: 2, source: "https://example.com" },
};

const document4: Document = {
  pageContent: "The 2024 Olympics are in Paris",
  metadata: { page: 3, source: "https://example.com" },
};

const documents = [document1, document2, document3, document4];

const ids = [uuidv4(), uuidv4(), uuidv4(), uuidv4()];

await vectorStore.addDocuments(documents, { ids: ids });


"""
### Delete Documents from vector store

You can delete one or more Documents from the vector store by passing the arrays of ids to be deleted:

"""

// deleting a document
const id1 = ids[0];
await vectorStore.delete({ ids: [id1] });

// deleting more than one document
await vectorStore.delete({ ids: ids });


"""
## Search for documents

Once your vector store has been created and the relevant documents have been added you will most likely wish to query it during the running of your chain or agent.

### Query directly

Performing a simple similarity search can be done as follows:

"""

const filter = `"source" = "https://example.com"`;

const results = await vectorStore.similaritySearch("biology", 2, filter);

for (const doc of results) {
  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);
}


"""
If you want to execute a similarity search and receive the corresponding scores you can run:

"""

const filter = `"source" = "https://example.com"`;
const resultsWithScores = await vectorStore.similaritySearchWithScore(
  "biology",
  2,
  filter
);

for (const [doc, score] of resultsWithScores) {
  console.log(
    `* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`
  );
}


"""
### Query by using the max marginal relevance search

The Maximal marginal relevance optimizes for similarity to the query and diversity among selected documents.

"""

const options = {
  k: 4,
  filter: `"source" = 'https://example.com'`,
};

const results = await vectorStoreInstance.maxMarginalRelevanceSearch("biology", options);

for (const doc of results) {
  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);
}




================================================
FILE: docs/core_docs/docs/integrations/vectorstores/googlevertexai.mdx
================================================
---
sidebar_class_name: node-only
---

import CodeBlock from "@theme/CodeBlock";

# Google Vertex AI Matching Engine

:::tip Compatibility
Only available on Node.js.
:::

The Google Vertex AI Matching Engine "provides the industry's leading high-scale
low latency vector database. These vector databases are commonly referred
to as vector similarity-matching or an approximate nearest neighbor (ANN) service."

## Setup

:::caution
This module expects an endpoint and deployed index already created as the
creation time takes close to one hour. To learn more, see the LangChain python
documentation [Create Index and deploy it to an Endpoint](https://python.langchain.com/docs/integrations/vectorstores/matchingengine#create-index-and-deploy-it-to-an-endpoint).
:::

Before running this code, you should make sure the Vertex AI API is
enabled for the relevant project in your Google Cloud dashboard and that you've authenticated to
Google Cloud using one of these methods:

- You are logged into an account (using `gcloud auth application-default login`)
  permitted to that project.
- You are running on a machine using a service account that is permitted
  to the project.
- You have downloaded the credentials for a service account that is permitted
  to the project and set the `GOOGLE_APPLICATION_CREDENTIALS` environment
  variable to the path of this file.

Install the authentication library with:

```bash npm2yarn
npm install @langchain/community @langchain/core google-auth-library
```

The Matching Engine does not store the actual document contents, only embeddings. Therefore, you'll
need a docstore. The below example uses Google Cloud Storage, which requires the following:

```bash npm2yarn
npm install @google-cloud/storage
```

## Usage

### Initializing the engine

When creating the `MatchingEngine` object, you'll need some information about
the matching engine configuration. You can get this information from the Cloud Console
for Matching Engine:

- The id for the Index
- The id for the Index Endpoint

You will also need a document store. While an `InMemoryDocstore` is ok for
initial testing, you will want to use something like a
[GoogleCloudStorageDocstore](https://api.js.langchain.com/classes/_langchain_community.stores_doc_gcs.GoogleCloudStorageDocstore.html) to store it more permanently.

```typescript
import { MatchingEngine } from "@langchain/community/vectorstores/googlevertexai";
import { Document } from "langchain/document";
import { SyntheticEmbeddings } from "langchain/embeddings/fake";
import { GoogleCloudStorageDocstore } from "@langchain/community/stores/doc/gcs";

const embeddings = new SyntheticEmbeddings({
  vectorSize: Number.parseInt(
    process.env.SYNTHETIC_EMBEDDINGS_VECTOR_SIZE ?? "768",
    10
  ),
});

const store = new GoogleCloudStorageDocstore({
  bucket: process.env.GOOGLE_CLOUD_STORAGE_BUCKET!,
});

const config = {
  index: process.env.GOOGLE_VERTEXAI_MATCHINGENGINE_INDEX!,
  indexEndpoint: process.env.GOOGLE_VERTEXAI_MATCHINGENGINE_INDEXENDPOINT!,
  apiVersion: "v1beta1",
  docstore: store,
};

const engine = new MatchingEngine(embeddings, config);
```

### Adding documents

```typescript
const doc = new Document({ pageContent: "this" });
await engine.addDocuments([doc]);
```

Any metadata in a document is converted into Matching Engine "allow list" values
that can be used to filter during a query.

```typescript
const documents = [
  new Document({
    pageContent: "this apple",
    metadata: {
      color: "red",
      category: "edible",
    },
  }),
  new Document({
    pageContent: "this blueberry",
    metadata: {
      color: "blue",
      category: "edible",
    },
  }),
  new Document({
    pageContent: "this firetruck",
    metadata: {
      color: "red",
      category: "machine",
    },
  }),
];

// Add all our documents
await engine.addDocuments(documents);
```

The documents are assumed to have an "id" parameter available as well. If this
is not set, then an ID will be assigned and returned as part of the Document.

### Querying documents

Doing a straightforward k-nearest-neighbor search which returns all results
is done using any of the standard methods:

```typescript
const results = await engine.similaritySearch("this");
```

### Querying documents with a filter / restriction

We can limit what documents are returned based on the metadata that was
set for the document. So if we just wanted to limit the results to those
with a red color, we can do:

```typescript
import { Restriction } from `@langchain/community/vectorstores/googlevertexai`;

const redFilter: Restriction[] = [
  {
    namespace: "color",
    allowList: ["red"],
  },
];
const redResults = await engine.similaritySearch("this", 4, redFilter);
```

If we wanted to do something more complicated, like things that are red,
but not edible:

```typescript
const filter: Restriction[] = [
  {
    namespace: "color",
    allowList: ["red"],
  },
  {
    namespace: "category",
    denyList: ["edible"],
  },
];
const results = await engine.similaritySearch("this", 4, filter);
```

### Deleting documents

Deleting documents are done using ID.

```typescript
import { IdDocument } from `@langchain/community/vectorstores/googlevertexai`;

const oldResults: IdDocument[] = await engine.similaritySearch("this", 10);
const oldIds = oldResults.map( doc => doc.id! );
await engine.delete({ids: oldIds});
```

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/hanavector.mdx
================================================
# SAP HANA Cloud Vector Engine

[SAP HANA Cloud Vector Engine](https://www.sap.com/events/teched/news-guide/ai.html#article8) is a vector store fully integrated into the `SAP HANA Cloud database`.

## Setup

You'll first need to install either the [`@sap/hana-client`](https://www.npmjs.com/package/@sap/hana-client) or the [`hdb`](https://www.npmjs.com/package/hdb) package, and the [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) package:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install -S @langchain/community @langchain/core @sap/hana-client
# or
npm install -S @langchain/community @langchain/core hdb
```

You'll also need to have database connection to a HANA Cloud instance.

import CodeBlock from "@theme/CodeBlock";
import EnvVars from "@examples/indexes/vector_stores/hana_vector/.env.example";

<CodeBlock language="text">{EnvVars}</CodeBlock>

## Create a new index from texts

import ExampleTexts from "@examples/indexes/vector_stores/hana_vector/fromTexts.ts";

<CodeBlock language="typescript">{ExampleTexts}</CodeBlock>

## Create a new index from a loader and perform similarity searches

import ExampleLoader from "@examples/indexes/vector_stores/hana_vector/fromDocs.ts";

<CodeBlock language="typescript">{ExampleLoader}</CodeBlock>

## Creating an HNSW Vector Index

A vector index can significantly speed up top-k nearest neighbor queries for vectors. Users can create a Hierarchical Navigable Small World (HNSW) vector index using the `create_hnsw_index` function.

For more information about creating an index at the database level, such as parameters requirement, please refer to the [official documentation](https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-vector-engine-guide/create-vector-index-statement-data-definition).

import ExampleIndex from "@examples/indexes/vector_stores/hana_vector/createHnswIndex.ts";

<CodeBlock language="typescript">{ExampleIndex}</CodeBlock>

## Basic Vectorstore Operations

import ExampleBasic from "@examples/indexes/vector_stores/hana_vector/basics.ts";

<CodeBlock language="typescript">{ExampleBasic}</CodeBlock>

## Advanced filtering

import { Table, Tr, Th, Td } from "@mdx-js/react";

In addition to the basic value-based filtering capabilities, it is possible to use more advanced filtering. The table below shows the available filter operators.

| Operator   | Semantic                                                                   |
| ---------- | -------------------------------------------------------------------------- |
| `$eq`      | Equality (==)                                                              |
| `$ne`      | Inequality (!=)                                                            |
| `$lt`      | Less than (<)                                                              |
| `$lte`     | Less than or equal (<=)                                                    |
| `$gt`      | Greater than (>)                                                           |
| `$gte`     | Greater than or equal (>=)                                                 |
| `$in`      | Contained in a set of given values (in)                                    |
| `$nin`     | Not contained in a set of given values (not in)                            |
| `$between` | Between the range of two boundary values                                   |
| `$like`    | Text equality based on the "LIKE" semantics in SQL (using "%" as wildcard) |
| `$and`     | Logical "and", supporting 2 or more operands                               |
| `$or`      | Logical "or", supporting 2 or more operands                                |

import ExampleAdvancedFilter from "@examples/indexes/vector_stores/hana_vector/advancedFiltering.ts";

<CodeBlock language="typescript">{ExampleAdvancedFilter}</CodeBlock>

## Using a VectorStore as a retriever in chains for retrieval augmented generation (RAG)

import ExampleChain from "@examples/indexes/vector_stores/hana_vector/chains.ts";

<CodeBlock language="typescript">{ExampleChain}</CodeBlock>

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/hnswlib.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: HNSWLib
sidebar_class_name: node-only
---
"""

"""
# HNSWLib

```{=mdx}
:::tip Compatibility
Only available on Node.js.
:::
```

HNSWLib is an in-memory vector store that can be saved to a file. It uses the [HNSWLib library](https://github.com/nmslib/hnswlib).

This guide provides a quick overview for getting started with HNSWLib [vector stores](/docs/concepts/#vectorstores). For detailed documentation of all `HNSWLib` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_vectorstores_hnswlib.HNSWLib.html).
"""

"""
## Overview

### Integration details

| Class | Package | PY support |  Package latest |
| :--- | :--- | :---: | :---: |
| [`HNSWLib`](https://api.js.langchain.com/classes/langchain_community_vectorstores_hnswlib.HNSWLib.html) | [`@langchain/community`](https://npmjs.com/@langchain/community) | ❌ |  ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |
"""

"""
## Setup

To use HNSWLib vector stores, you'll need to install the `@langchain/community` integration package with the [`hnswlib-node`](https://www.npmjs.com/package/hnswlib-node) package as a peer dependency.

This guide will also use [OpenAI embeddings](/docs/integrations/text_embedding/openai), which require you to install the `@langchain/openai` integration package. You can also use [other supported embeddings models](/docs/integrations/text_embedding) if you wish.

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community hnswlib-node @langchain/openai @langchain/core
</Npm2Yarn>
```

```{=mdx}
:::caution

**On Windows**, you might need to install [Visual Studio](https://visualstudio.microsoft.com/downloads/) first in order to properly build the `hnswlib-node` package.

:::
```

### Credentials

Because HNSWLib runs locally, you do not need any credentials to use it.

If you are using OpenAI embeddings for this guide, you'll need to set your OpenAI key as well:

```typescript
process.env.OPENAI_API_KEY = "YOUR_API_KEY";
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
// process.env.LANGSMITH_TRACING="true"
// process.env.LANGSMITH_API_KEY="your-api-key"
```
"""

"""
## Instantiation
"""

import { HNSWLib } from "@langchain/community/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "@langchain/openai";

const embeddings = new OpenAIEmbeddings({
  model: "text-embedding-3-small",
});

const vectorStore = await HNSWLib.fromDocuments([], embeddings);

"""
## Manage vector store

### Add items to vector store
"""

import type { Document } from "@langchain/core/documents";

const document1: Document = {
  pageContent: "The powerhouse of the cell is the mitochondria",
  metadata: { source: "https://example.com" }
};

const document2: Document = {
  pageContent: "Buildings are made out of brick",
  metadata: { source: "https://example.com" }
};

const document3: Document = {
  pageContent: "Mitochondria are made out of lipids",
  metadata: { source: "https://example.com" }
};

const document4: Document = {
  pageContent: "The 2024 Olympics are in Paris",
  metadata: { source: "https://example.com" }
}

const documents = [document1, document2, document3, document4];

await vectorStore.addDocuments(documents);

"""
Deletion and ids for individual documents are not currently supported.

## Query vector store

Once your vector store has been created and the relevant documents have been added you will most likely wish to query it during the running of your chain or agent. 

### Query directly

Performing a simple similarity search can be done as follows:
"""

const filter = (doc) => doc.metadata.source === "https://example.com";

const similaritySearchResults = await vectorStore.similaritySearch("biology", 2, filter);

for (const doc of similaritySearchResults) {
  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);
}
# Output:
#   * The powerhouse of the cell is the mitochondria [{"source":"https://example.com"}]

#   * Mitochondria are made out of lipids [{"source":"https://example.com"}]


"""
The filter is optional, and must be a predicate function that takes a document as input, and returns `true` or `false` depending on whether the document should be returned.

If you want to execute a similarity search and receive the corresponding scores you can run:
"""

const similaritySearchWithScoreResults = await vectorStore.similaritySearchWithScore("biology", 2, filter)

for (const [doc, score] of similaritySearchWithScoreResults) {
  console.log(`* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`);
}
# Output:
#   * [SIM=0.835] The powerhouse of the cell is the mitochondria [{"source":"https://example.com"}]

#   * [SIM=0.852] Mitochondria are made out of lipids [{"source":"https://example.com"}]


"""
### Query by turning into retriever

You can also transform the vector store into a [retriever](/docs/concepts/retrievers) for easier usage in your chains.
"""

const retriever = vectorStore.asRetriever({
  // Optional filter
  filter: filter,
  k: 2,
});
await retriever.invoke("biology");
# Output:
#   [

#     {

#       pageContent: 'The powerhouse of the cell is the mitochondria',

#       metadata: { source: 'https://example.com' }

#     },

#     {

#       pageContent: 'Mitochondria are made out of lipids',

#       metadata: { source: 'https://example.com' }

#     }

#   ]


"""
### Usage for retrieval-augmented generation

For guides on how to use this vector store for retrieval-augmented generation (RAG), see the following sections:

- [Tutorials: working with external knowledge](/docs/tutorials/#working-with-external-knowledge).
- [How-to: Question and answer with RAG](/docs/how_to/#qa-with-rag)
- [Retrieval conceptual docs](/docs/concepts/retrieval)
"""

"""
## Save to/load from file

HNSWLib supports saving your index to a file, then reloading it at a later date:
"""

// Save the vector store to a directory
const directory = "your/directory/here";
await vectorStore.save(directory);

// Load the vector store from the same directory
const loadedVectorStore = await HNSWLib.load(directory, new OpenAIEmbeddings());

// vectorStore and loadedVectorStore are identical
await loadedVectorStore.similaritySearch("hello world", 1);

"""
### Delete a saved index

You can use the `.delete` method to clear an index saved to a given directory:
"""

// Load the vector store from the same directory
const savedVectorStore = await HNSWLib.load(directory, new OpenAIEmbeddings());

await savedVectorStore.delete({ directory });

"""
## API reference

For detailed documentation of all `HNSWLib` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_vectorstores_hnswlib.HNSWLib.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/index.mdx
================================================
---
sidebar_position: 0
sidebar_class_name: hidden
---

# Vector stores

A [vector store](/docs/concepts/#vectorstores) stores [embedded](/docs/concepts/embedding_models) data and performs similarity search.

import EmbeddingTabs from "@theme/EmbeddingTabs";

<EmbeddingTabs />

import VectorStoreTabs from "@theme/VectorStoreTabs";

<VectorStoreTabs />

LangChain.js integrates with a variety of vector stores. You can check out a full list below:

import { CategoryTable, IndexTable } from "@theme/FeatureTables";

<IndexTable />



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/lancedb.mdx
================================================
---
sidebar_class_name: node-only
---

import CodeBlock from "@theme/CodeBlock";

# LanceDB

LanceDB is an embedded vector database for AI applications. It is open source and distributed with an Apache-2.0 license.

LanceDB datasets are persisted to disk and can be shared between Node.js and Python.

## Setup

Install the [LanceDB](https://github.com/lancedb/lancedb) [Node.js bindings](https://www.npmjs.com/package/@lancedb/lancedb):

```bash npm2yarn
npm install -S @lancedb/lancedb
```

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

## Usage

### Create a new index from texts

import ExampleTexts from "@examples/indexes/vector_stores/lancedb/fromTexts.ts";

<CodeBlock language="typescript">{ExampleTexts}</CodeBlock>

### Create a new index from a loader

import ExampleLoader from "@examples/indexes/vector_stores/lancedb/fromDocs.ts";

<CodeBlock language="typescript">{ExampleLoader}</CodeBlock>

### Open an existing dataset

import ExampleLoad from "@examples/indexes/vector_stores/lancedb/load.ts";

<CodeBlock language="typescript">{ExampleLoad}</CodeBlock>

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/libsql.mdx
================================================
# libSQL

[Turso](https://turso.tech) is a SQLite-compatible database built on [libSQL](https://docs.turso.tech/libsql), the Open Contribution fork of SQLite. Vector Similiarity Search is built into Turso and libSQL as a native datatype, enabling you to store and query vectors directly in the database.

LangChain.js supports using a local libSQL, or remote Turso database as a vector store, and provides a simple API to interact with it.

This guide provides a quick overview for getting started with libSQL vector stores. For detailed documentation of all libSQL features and configurations head to the API reference.

## Overview

## Integration details

| Class               | Package                | PY support | Package latest                                                    |
| ------------------- | ---------------------- | ---------- | ----------------------------------------------------------------- |
| `LibSQLVectorStore` | `@langchain/community` | ❌         | ![npm version](https://img.shields.io/npm/v/@langchain/community) |

## Setup

To use libSQL vector stores, you'll need to create a Turso account or set up a local SQLite database, and install the `@langchain/community` integration package.

This guide will also use OpenAI embeddings, which require you to install the `@langchain/openai` integration package. You can also use other supported embeddings models if you wish.

You can use local SQLite when working with the libSQL vector store, or use a hosted Turso Database.

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @libsql/client @langchain/openai @langchain/community
```

Now it's time to create a database. You can create one locally, or use a hosted Turso database.

### Local libSQL

Create a new local SQLite file and connect to the shell:

```bash
sqlite3 file.db
```

### Hosted Turso

Visit [sqlite.new](https://sqlite.new) to create a new database, give it a name, and create a database auth token.

Make sure to copy the database auth token, and the database URL, it should look something like:

```text
libsql://[database-name]-[your-username].turso.io
```

### Setup the table and index

Execute the following SQL command to create a new table or add the embedding column to an existing table.

Make sure to modify the following parts of the SQL:

- `TABLE_NAME` is the name of the table you want to create.
- `content` is used to store the `Document.pageContent` values.
- `metadata` is used to store the `Document.metadata` object.
- `EMBEDDING_COLUMN` is used to store the vector values, use the dimensions size used by the model you plan to use (1536 for OpenAI).

```sql
CREATE TABLE IF NOT EXISTS TABLE_NAME (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    content TEXT,
    metadata TEXT,
    EMBEDDING_COLUMN F32_BLOB(1536) -- 1536-dimensional f32 vector for OpenAI
);
```

Now create an index on the `EMBEDDING_COLUMN` column - the index name is important!:

```sql
CREATE INDEX IF NOT EXISTS idx_TABLE_NAME_EMBEDDING_COLUMN ON TABLE_NAME(libsql_vector_idx(EMBEDDING_COLUMN));
```

Make sure to replace the `TABLE_NAME` and `EMBEDDING_COLUMN` with the values you used in the previous step.

## Instantiation

To initialize a new `LibSQL` vector store, you need to provide the database URL and Auth Token when working remotely, or by passing the filename for a local SQLite.

```typescript
import { LibSQLVectorStore } from "@langchain/community/vectorstores/libsql";
import { OpenAIEmbeddings } from "@langchain/openai";
import { createClient } from "@libsql/client";

const embeddings = new OpenAIEmbeddings({
  model: "text-embedding-3-small",
});

const libsqlClient = createClient({
  url: "libsql://[database-name]-[your-username].turso.io",
  authToken: "...",
});

// Local instantiation
// const libsqlClient = createClient({
//  url: "file:./dev.db",
// });

const vectorStore = new LibSQLVectorStore(embeddings, {
  db: libsqlClient,
  table: "TABLE_NAME",
  column: "EMBEDDING_COLUMN",
});
```

## Manage vector store

### Add items to vector store

```typescript
import type { Document } from "@langchain/core/documents";

const documents: Document[] = [
  { pageContent: "Hello", metadata: { topic: "greeting" } },
  { pageContent: "Bye bye", metadata: { topic: "greeting" } },
];

await vectorStore.addDocuments(documents);
```

### Delete items from vector store

```typescript
await vectorStore.deleteDocuments({ ids: [1, 2] });
```

## Query vector store

Once you have inserted the documents, you can query the vector store.

### Query directly

Performing a simple similarity search can be done as follows:

```typescript
const resultOne = await vectorStore.similaritySearch("hola", 1);

for (const doc of similaritySearchResults) {
  console.log(`${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);
}
```

For similarity search with scores:

```typescript
const similaritySearchWithScoreResults =
  await vectorStore.similaritySearchWithScore("hola", 1);

for (const [doc, score] of similaritySearchWithScoreResults) {
  console.log(
    `${score.toFixed(3)} ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`
  );
}
```

## API reference

For detailed documentation of all `LibSQLVectorStore` features and configurations head to the API reference.

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/mariadb.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: MariaDB
sidebar_class_name: node-only
---
"""

"""
# MariaDB

```{=mdx}
:::tip Compatibility
Only available on Node.js.
:::
```

This requires MariaDB 11.7 or later version

This guide provides a quick overview for getting started with mariadb [vector stores](/docs/concepts/#vectorstores). For detailed documentation of all `MariaDB store` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_vectorstores_mariadb.MariaDBStore.html).
"""

"""
## Overview

### Integration details

| Class | Package | [PY support](https://python.langchain.com/docs/integrations/vectorstores/mariadb/) | Package latest |
| :--- | :--- | :---: | :---: |
| [`MariaDBStore`](https://api.js.langchain.com/classes/langchain_community_vectorstores_mariadb.MariaDBStore.html) | [`@langchain/community`](https://npmjs.com/@langchain/community) | ✅ | ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |
"""

"""
## Setup

To use MariaDBVector vector stores, you'll need to set up a MariaDB 11.7 version or later with the [`mariadb`](https://www.npmjs.com/package/mariadb) connector as a peer dependency.

This guide will also use [OpenAI embeddings](/docs/integrations/text_embedding/openai), which require you to install the `@langchain/openai` integration package. You can also use [other supported embeddings models](/docs/integrations/text_embedding) if you wish.

We'll also use the [`uuid`](https://www.npmjs.com/package/uuid) package to generate ids in the required format.

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/openai @langchain/core mariadb uuid
</Npm2Yarn>
```

### Setting up an instance

Create a file with the below content named docker-compose.yml:

```yaml
# Run this command to start the database:
# docker-compose up --build
version: "3"
services:
  db:
    hostname: 127.0.0.1
    image: mariadb/mariadb:11.7-rc
    ports:
      - 3306:3306
    restart: always
    environment:
      - MARIADB_DATABASE=api
      - MARIADB_USER=myuser
      - MARIADB_PASSWORD=ChangeMe
      - MARIADB_ROOT_PASSWORD=ChangeMe
    volumes:
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
```

And then in the same directory, run docker compose up to start the container.

### Credentials

To connect to you MariaDB instance, you'll need corresponding credentials. For a full list of supported options, see the [`mariadb` docs](https://github.com/mariadb-corporation/mariadb-connector-nodejs/blob/master/documentation/promise-api.md#connection-options).

If you are using OpenAI embeddings for this guide, you'll need to set your OpenAI key as well:

```typescript
process.env.OPENAI_API_KEY = "YOUR_API_KEY";
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
// process.env.LANGCHAIN_TRACING_V2="true"
// process.env.LANGCHAIN_API_KEY="your-api-key"
```
"""

"""
## Instantiation

To instantiate the vector store, call the `.initialize()` static method. This will automatically check for the presence of a table, given by `tableName` in the passed `config`. If it is not there, it will create it with the required columns.


"""

import { OpenAIEmbeddings } from "@langchain/openai";

import {
   DistanceStrategy,
   MariaDBStore,
} from "@langchain/community/vectorstores/mariadb";
import { PoolConfig } from "mariadb";

const config = {
  connectionOptions: {
    type: "mariadb",
    host: "127.0.0.1",
    port: 3306,
    user: "myuser",
    password: "ChangeMe",
    database: "api",
  } as PoolConfig,
  distanceStrategy: 'EUCLIDEAN' as DistanceStrategy,
};
const vectorStore = await MariaDBStore.initialize(
  new OpenAIEmbeddings(),
   config
);

"""
## Manage vector store

### Add items to vector store
"""

import { v4 as uuidv4 } from "uuid";
import type { Document } from "@langchain/core/documents";

const document1: Document = {
  pageContent: "The powerhouse of the cell is the mitochondria",
  metadata: { source: "https://example.com" }
};

const document2: Document = {
  pageContent: "Buildings are made out of brick",
  metadata: { source: "https://example.com" }
};

const document3: Document = {
  pageContent: "Mitochondria are made out of lipids",
  metadata: { source: "https://example.com" }
};

const document4: Document = {
  pageContent: "The 2024 Olympics are in Paris",
  metadata: { source: "https://example.com" }
}

const documents = [document1, document2, document3, document4];

const ids = [uuidv4(), uuidv4(), uuidv4(), uuidv4()]

// ids are not mandatory, but that's for the example
await vectorStore.addDocuments(documents, { ids: ids });

"""
### Delete items from vector store
"""

const id4 = ids[ids.length - 1];

await vectorStore.delete({ ids: [id4] });

"""
## Query vector store

Once your vector store has been created and the relevant documents have been added you will most likely wish to query it during the running of your chain or agent. 

### Query directly

Performing a simple similarity search can be done as follows:
"""

const similaritySearchResults = await vectorStore.similaritySearch("biology", 2, { "year": 2021 });
for (const doc of similaritySearchResults) {
  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);
}
# Output:
#   * The powerhouse of the cell is the mitochondria [{"year": 2021}]

#   * Mitochondria are made out of lipids [{"year": 2022}]


"""
The above filter syntax use be more complex:

```json
# name = 'martin' OR firstname = 'john'
let res = await vectorStore.similaritySearch("biology", 2, {"$or": [{"name":"martin"}, {"firstname", "john"}] });
```

If you want to execute a similarity search and receive the corresponding scores you can run:
"""

const similaritySearchWithScoreResults = await vectorStore.similaritySearchWithScore("biology", 2)

for (const [doc, score] of similaritySearchWithScoreResults) {
  console.log(`* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`);
}
# Output:
#   * [SIM=0.835] The powerhouse of the cell is the mitochondria [{"source":"https://example.com"}]

#   * [SIM=0.852] Mitochondria are made out of lipids [{"source":"https://example.com"}]


"""
### Query by turning into retriever

You can also transform the vector store into a [retriever](/docs/concepts/retrievers) for easier usage in your chains. 
"""

const retriever = vectorStore.asRetriever({
  // Optional filter
  // filter: filter,
  k: 2,
});
await retriever.invoke("biology");
# Output:
#   [

#     Document {

#       pageContent: 'The powerhouse of the cell is the mitochondria',

#       metadata: { source: 'https://example.com' },

#       id: undefined

#     },

#     Document {

#       pageContent: 'Mitochondria are made out of lipids',

#       metadata: { source: 'https://example.com' },

#       id: undefined

#     }

#   ]


"""
### Usage for retrieval-augmented generation

For guides on how to use this vector store for retrieval-augmented generation (RAG), see the following sections:

- [Tutorials: working with external knowledge](/docs/tutorials/#working-with-external-knowledge).
- [How-to: Question and answer with RAG](/docs/how_to/#qa-with-rag)
- [Retrieval conceptual docs](/docs/concepts/retrieval)
"""

"""
## Advanced: reusing connections

You can reuse connections by creating a pool, then creating new `MariaDBStore` instances directly via the constructor.

Note that you should call `.initialize()` to set up your database at least once to set up your tables properly before using the constructor.
"""

import { OpenAIEmbeddings } from "@langchain/openai";
import { MariaDBStore } from "@langchain/community/vectorstores/mariadb";
import mariadb from "mariadb";

// First, follow set-up instructions at
// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/mariadb

const reusablePool = mariadb.createPool({
  host: "127.0.0.1",
  port: 3306,
  user: "myuser",
  password: "ChangeMe",
  database: "api",
});

const originalConfig = {
  pool: reusablePool,
  tableName: "testlangchainjs",
  collectionName: "sample",
  collectionTableName: "collections",
  columns: {
    idColumnName: "id",
    vectorColumnName: "vect",
    contentColumnName: "content",
    metadataColumnName: "metadata",
  },
};

// Set up the DB.
// Can skip this step if you've already initialized the DB.
// await MariaDBStore.initialize(new OpenAIEmbeddings(), originalConfig);
const mariadbStore = new MariaDBStore(new OpenAIEmbeddings(), originalConfig);

await mariadbStore.addDocuments([
  { pageContent: "what's this", metadata: { a: 2 } },
  { pageContent: "Cat drinks milk", metadata: { a: 1 } },
]);

const results = await mariadbStore.similaritySearch("water", 1);

console.log(results);

/*
  [ Document { pageContent: 'Cat drinks milk', metadata: { a: 1 } } ]
*/

const mariadbStore2 = new MariaDBStore(new OpenAIEmbeddings(), {
  pool: reusablePool,
  tableName: "testlangchainjs",
  collectionTableName: "collections",
  collectionName: "some_other_collection",
  columns: {
    idColumnName: "id",
    vectorColumnName: "vector",
    contentColumnName: "content",
    metadataColumnName: "metadata",
  },
});

const results2 = await mariadbStore2.similaritySearch("water", 1);

console.log(results2);

/*
  []
*/

await reusablePool.end();

"""
## Closing connections

Make sure you close the connection when you are finished to avoid excessive resource consumption:
"""

await vectorStore.end();

"""
## API reference

For detailed documentation of all `MariaDBStore` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_vectorstores_mariadb.MariaDBStore.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/memory.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: In-memory
---
"""

"""
# MemoryVectorStore

LangChain offers is an in-memory, ephemeral vectorstore that stores embeddings in-memory and does an exact, linear search for the most similar embeddings. The default similarity metric is cosine similarity, but can be changed to any of the similarity metrics supported by [ml-distance](https://mljs.github.io/distance/modules/similarity.html).

As it is intended for demos, it does not yet support ids or deletion.

This guide provides a quick overview for getting started with in-memory [`vector stores`](/docs/concepts/#vectorstores). For detailed documentation of all `MemoryVectorStore` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.vectorstores_memory.MemoryVectorStore.html).
"""

"""
## Overview

### Integration details

| Class | Package | PY support |  Package latest |
| :--- | :--- | :---: | :---: |
| [`MemoryVectorStore`](https://api.js.langchain.com/classes/langchain.vectorstores_memory.MemoryVectorStore.html) | [`langchain`](https://www.npmjs.com/package/langchain) | ❌ |  ![NPM - Version](https://img.shields.io/npm/v/langchain?style=flat-square&label=%20&) |
"""

"""
## Setup

To use in-memory vector stores, you'll need to install the `langchain` package:

This guide will also use [OpenAI embeddings](/docs/integrations/text_embedding/openai), which require you to install the `@langchain/openai` integration package. You can also use [other supported embeddings models](/docs/integrations/text_embedding) if you wish.

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  langchain @langchain/openai @langchain/core
</Npm2Yarn>
```

### Credentials

There are no required credentials to use in-memory vector stores.

If you are using OpenAI embeddings for this guide, you'll need to set your OpenAI key as well:

```typescript
process.env.OPENAI_API_KEY = "YOUR_API_KEY";
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
// process.env.LANGSMITH_TRACING="true"
// process.env.LANGSMITH_API_KEY="your-api-key"
```
"""

"""
## Instantiation
"""

import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "@langchain/openai";

const embeddings = new OpenAIEmbeddings({
  model: "text-embedding-3-small",
});

const vectorStore = new MemoryVectorStore(embeddings);

"""
## Manage vector store

### Add items to vector store
"""

import type { Document } from "@langchain/core/documents";

const document1: Document = {
  pageContent: "The powerhouse of the cell is the mitochondria",
  metadata: { source: "https://example.com" }
};

const document2: Document = {
  pageContent: "Buildings are made out of brick",
  metadata: { source: "https://example.com" }
};

const document3: Document = {
  pageContent: "Mitochondria are made out of lipids",
  metadata: { source: "https://example.com" }
};

const documents = [document1, document2, document3];

await vectorStore.addDocuments(documents);

"""
## Query vector store

Once your vector store has been created and the relevant documents have been added you will most likely wish to query it during the running of your chain or agent. 

### Query directly

Performing a simple similarity search can be done as follows:
"""

const filter = (doc) => doc.metadata.source === "https://example.com";

const similaritySearchResults = await vectorStore.similaritySearch("biology", 2, filter)

for (const doc of similaritySearchResults) {
  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);
}
# Output:
#   * The powerhouse of the cell is the mitochondria [{"source":"https://example.com"}]

#   * Mitochondria are made out of lipids [{"source":"https://example.com"}]


"""
The filter is optional, and must be a predicate function that takes a document as input, and returns `true` or `false` depending on whether the document should be returned.

If you want to execute a similarity search and receive the corresponding scores you can run:
"""

const similaritySearchWithScoreResults = await vectorStore.similaritySearchWithScore("biology", 2, filter)

for (const [doc, score] of similaritySearchWithScoreResults) {
  console.log(`* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`);
}
# Output:
#   * [SIM=0.165] The powerhouse of the cell is the mitochondria [{"source":"https://example.com"}]

#   * [SIM=0.148] Mitochondria are made out of lipids [{"source":"https://example.com"}]


"""
### Query by turning into retriever

You can also transform the vector store into a [retriever](/docs/concepts/retrievers) for easier usage in your chains:
"""

const retriever = vectorStore.asRetriever({
  // Optional filter
  filter: filter,
  k: 2,
});

await retriever.invoke("biology");
# Output:
#   [

#     Document {

#       pageContent: 'The powerhouse of the cell is the mitochondria',

#       metadata: { source: 'https://example.com' },

#       id: undefined

#     },

#     Document {

#       pageContent: 'Mitochondria are made out of lipids',

#       metadata: { source: 'https://example.com' },

#       id: undefined

#     }

#   ]


"""
### Maximal marginal relevance

This vector store also supports maximal marginal relevance (MMR), a technique that first fetches a larger number of results (given by `searchKwargs.fetchK`), with classic similarity search, then reranks for diversity and returns the top `k` results. This helps guard against redundant information:
"""

const mmrRetriever = vectorStore.asRetriever({
  searchType: "mmr",
  searchKwargs: {
    fetchK: 10,
  },
  // Optional filter
  filter: filter,
  k: 2,
});

await mmrRetriever.invoke("biology");
# Output:
#   [

#     Document {

#       pageContent: 'The powerhouse of the cell is the mitochondria',

#       metadata: { source: 'https://example.com' },

#       id: undefined

#     },

#     Document {

#       pageContent: 'Buildings are made out of brick',

#       metadata: { source: 'https://example.com' },

#       id: undefined

#     }

#   ]


"""
### Usage for retrieval-augmented generation

For guides on how to use this vector store for retrieval-augmented generation (RAG), see the following sections:

- [Tutorials: working with external knowledge](/docs/tutorials/#working-with-external-knowledge).
- [How-to: Question and answer with RAG](/docs/how_to/#qa-with-rag)
- [Retrieval conceptual docs](/docs/concepts/retrieval)
"""

"""
## API reference

For detailed documentation of all `MemoryVectorStore` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain.vectorstores_memory.MemoryVectorStore.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/milvus.mdx
================================================
---
sidebar_class_name: node-only
---

# Milvus

[Milvus](https://milvus.io/) is a vector database built for embeddings similarity search and AI applications.

:::tip Compatibility
Only available on Node.js.
:::

## Setup

1. Run Milvus instance with Docker on your computer [docs](https://milvus.io/docs/v2.1.x/install_standalone-docker.md)
2. Install the Milvus Node.js SDK.

   ```bash npm2yarn
   npm install -S @zilliz/milvus2-sdk-node
   ```

3. Setup Env variables for Milvus before running the code

   3.1 OpenAI

   ```bash
   export OPENAI_API_KEY=YOUR_OPENAI_API_KEY_HERE
   export MILVUS_URL=YOUR_MILVUS_URL_HERE # for example http://localhost:19530
   ```

   3.2 Azure OpenAI

   ```bash
   export AZURE_OPENAI_API_KEY=YOUR_AZURE_OPENAI_API_KEY_HERE
   export AZURE_OPENAI_API_INSTANCE_NAME=YOUR_AZURE_OPENAI_INSTANCE_NAME_HERE
   export AZURE_OPENAI_API_DEPLOYMENT_NAME=YOUR_AZURE_OPENAI_DEPLOYMENT_NAME_HERE
   export AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME=YOUR_AZURE_OPENAI_COMPLETIONS_DEPLOYMENT_NAME_HERE
   export AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME=YOUR_AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME_HERE
   export AZURE_OPENAI_API_VERSION=YOUR_AZURE_OPENAI_API_VERSION_HERE
   export AZURE_OPENAI_BASE_PATH=YOUR_AZURE_OPENAI_BASE_PATH_HERE
   export MILVUS_URL=YOUR_MILVUS_URL_HERE # for example http://localhost:19530
   ```

## Index and query docs

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core
```

```typescript
import { Milvus } from "langchain/vectorstores/milvus";
import { OpenAIEmbeddings } from "@langchain/openai";

// text sample from Godel, Escher, Bach
const vectorStore = await Milvus.fromTexts(
  [
    "Tortoise: Labyrinth? Labyrinth? Could it Are we in the notorious Little\
            Harmonic Labyrinth of the dreaded Majotaur?",
    "Achilles: Yiikes! What is that?",
    "Tortoise: They say-although I person never believed it myself-that an I\
            Majotaur has created a tiny labyrinth sits in a pit in the middle of\
            it, waiting innocent victims to get lost in its fears complexity.\
            Then, when they wander and dazed into the center, he laughs and\
            laughs at them-so hard, that he laughs them to death!",
    "Achilles: Oh, no!",
    "Tortoise: But it's only a myth. Courage, Achilles.",
  ],
  [{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],
  new OpenAIEmbeddings(),
  {
    collectionName: "goldel_escher_bach",
  }
);

// or alternatively from docs
const vectorStore = await Milvus.fromDocuments(docs, new OpenAIEmbeddings(), {
  collectionName: "goldel_escher_bach",
});

const response = await vectorStore.similaritySearch("scared", 2);
```

## Query docs from existing collection

```typescript
import { Milvus } from "langchain/vectorstores/milvus";
import { OpenAIEmbeddings } from "@langchain/openai";

const vectorStore = await Milvus.fromExistingCollection(
  new OpenAIEmbeddings(),
  {
    collectionName: "goldel_escher_bach",
  }
);

const response = await vectorStore.similaritySearch("scared", 2);
```

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/momento_vector_index.mdx
================================================
# Momento Vector Index (MVI)

[MVI](https://gomomento.com): the most productive, easiest to use, serverless vector index for your data. To get started with MVI, simply sign up for an account. There's no need to handle infrastructure, manage servers, or be concerned about scaling. MVI is a service that scales automatically to meet your needs. Whether in Node.js, browser, or edge, Momento has you covered.

To sign up and access MVI, visit the [Momento Console](https://console.gomomento.com).

## Setup

1. Sign up for an API key in the [Momento Console](https://console.gomomento.com/).
2. Install the SDK for your environment.

   2.1. For **Node.js**:

   ```bash npm2yarn
   npm install @gomomento/sdk
   ```

   2.2. For **browser or edge environments**:

   ```bash npm2yarn
   npm install @gomomento/sdk-web
   ```

3. Setup Env variables for Momento before running the code

   3.1 OpenAI

   ```bash
   export OPENAI_API_KEY=YOUR_OPENAI_API_KEY_HERE
   ```

   3.2 Momento

   ```bash
   export MOMENTO_API_KEY=YOUR_MOMENTO_API_KEY_HERE # https://console.gomomento.com
   ```

import CodeBlock from "@theme/CodeBlock";

## Usage

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

### Index documents using `fromTexts` and search

This example demonstrates using the `fromTexts` method to instantiate the vector store and index documents.
If the index does not exist, then it will be created. If the index already exists, then the documents will be
added to the existing index.

The `ids` are optional; if you omit them, then Momento will generate UUIDs for you.

import TextsExample from "@examples/indexes/vector_stores/momento_vector_index/fromTexts.ts";

<CodeBlock language="typescript">{TextsExample}</CodeBlock>

### Index documents using `fromDocuments` and search

Similar to the above, this example demonstrates using the `fromDocuments` method to instantiate the vector store and index documents.
If the index does not exist, then it will be created. If the index already exists, then the documents will be
added to the existing index.

Using `fromDocuments` allows you to seamlessly chain the various document loaders with indexing.

import DocsExample from "@examples/indexes/vector_stores/momento_vector_index/fromDocs.ts";

<CodeBlock language="typescript">{DocsExample}</CodeBlock>

### Search from an existing collection

import ExistingExample from "@examples/indexes/vector_stores/momento_vector_index/fromExisting.ts";

<CodeBlock language="typescript">{ExistingExample}</CodeBlock>

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/mongodb_atlas.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: MongoDB Atlas
sidebar_class_name: node-only
---
"""

"""
# MongoDB Atlas

```{=mdx}
:::tip Compatibility
Only available on Node.js.

You can still create API routes that use MongoDB with Next.js by setting the `runtime` variable to `nodejs` like so:

`export const runtime = "nodejs";`

You can read more about Edge runtimes in the Next.js documentation [here](https://nextjs.org/docs/app/building-your-application/rendering/edge-and-nodejs-runtimes).
:::
```

This guide provides a quick overview for getting started with MongoDB Atlas [vector stores](/docs/concepts/#vectorstores). For detailed documentation of all `MongoDBAtlasVectorSearch` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_mongodb.MongoDBAtlasVectorSearch.html).
"""

"""
## Overview

### Integration details

| Class | Package | [PY support](https://python.langchain.com/docs/integrations/vectorstores/mongodb_atlas/) |  Package latest |
| :--- | :--- | :---: | :---: |
| [`MongoDBAtlasVectorSearch`](https://api.js.langchain.com/classes/langchain_mongodb.MongoDBAtlasVectorSearch.html) | [`@langchain/mongodb`](https://www.npmjs.com/package/@langchain/mongodb) | ✅ | ![NPM - Version](https://img.shields.io/npm/v/@langchain/mongodb?style=flat-square&label=%20&) |
"""

"""
## Setup

To use MongoDB Atlas vector stores, you'll need to configure a MongoDB Atlas cluster and install the `@langchain/mongodb` integration package.

### Initial Cluster Configuration

To create a MongoDB Atlas cluster, navigate to the [MongoDB Atlas website](https://www.mongodb.com/products/platform/atlas-database) and create an account if you don't already have one.

Create and name a cluster when prompted, then find it under `Database`. Select `Browse Collections` and create either a blank collection or one from the provided sample data.

**Note:** The cluster created must be MongoDB 7.0 or higher.

### Creating an Index

After configuring your cluster, you'll need to create an index on the collection field you want to search over.

Switch to the `Atlas Search` tab and click `Create Search Index`. From there, make sure you select `Atlas Vector Search - JSON Editor`, then select the appropriate database and collection and paste the following into the textbox:

```json
{
  "fields": [
    {
      "numDimensions": 1536,
      "path": "embedding",
      "similarity": "euclidean",
      "type": "vector"
    }
  ]
}
```

Note that the dimensions property should match the dimensionality of the embeddings you are using. For example, Cohere embeddings have 1024 dimensions, and by default OpenAI embeddings have 1536:

Note: By default the vector store expects an index name of default, an indexed collection field name of embedding, and a raw text field name of text. You should initialize the vector store with field names matching your index name collection schema as shown below.

Finally, proceed to build the index.

### Embeddings

This guide will also use [OpenAI embeddings](/docs/integrations/text_embedding/openai), which require you to install the `@langchain/openai` integration package. You can also use [other supported embeddings models](/docs/integrations/text_embedding) if you wish.

### Installation

Install the following packages:

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/mongodb mongodb @langchain/openai @langchain/core
</Npm2Yarn>
```

### Credentials

Once you've done the above, set the `MONGODB_ATLAS_URI` environment variable from the `Connect` button in Mongo's dashboard. You'll also need your DB name and collection name:

```typescript
process.env.MONGODB_ATLAS_URI = "your-atlas-url";
process.env.MONGODB_ATLAS_COLLECTION_NAME = "your-atlas-db-name";
process.env.MONGODB_ATLAS_DB_NAME = "your-atlas-db-name";
```

If you are using OpenAI embeddings for this guide, you'll need to set your OpenAI key as well:

```typescript
process.env.OPENAI_API_KEY = "YOUR_API_KEY";
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
// process.env.LANGSMITH_TRACING="true"
// process.env.LANGSMITH_API_KEY="your-api-key"
```
"""

"""
## Instantiation

Once you've set up your cluster as shown above, you can initialize your vector store as follows:
"""

import { MongoDBAtlasVectorSearch } from "@langchain/mongodb";
import { OpenAIEmbeddings } from "@langchain/openai";
import { MongoClient } from "mongodb";

const client = new MongoClient(process.env.MONGODB_ATLAS_URI || "");
const collection = client.db(process.env.MONGODB_ATLAS_DB_NAME)
  .collection(process.env.MONGODB_ATLAS_COLLECTION_NAME);

const embeddings = new OpenAIEmbeddings({
  model: "text-embedding-3-small",
});

const vectorStore = new MongoDBAtlasVectorSearch(embeddings, {
  collection: collection,
  indexName: "vector_index", // The name of the Atlas search index. Defaults to "default"
  textKey: "text", // The name of the collection field containing the raw content. Defaults to "text"
  embeddingKey: "embedding", // The name of the collection field containing the embedded text. Defaults to "embedding"
});

"""
## Manage vector store

### Add items to vector store

You can now add documents to your vector store:
"""

import type { Document } from "@langchain/core/documents";

const document1: Document = {
  pageContent: "The powerhouse of the cell is the mitochondria",
  metadata: { source: "https://example.com" }
};

const document2: Document = {
  pageContent: "Buildings are made out of brick",
  metadata: { source: "https://example.com" }
};

const document3: Document = {
  pageContent: "Mitochondria are made out of lipids",
  metadata: { source: "https://example.com" }
};

const document4: Document = {
  pageContent: "The 2024 Olympics are in Paris",
  metadata: { source: "https://example.com" }
}

const documents = [document1, document2, document3, document4];

await vectorStore.addDocuments(documents, { ids: ["1", "2", "3", "4"] });
# Output:
#   [ '1', '2', '3', '4' ]


"""
**Note:** After adding documents, there is a slight delay before they become queryable.

Adding a document with the same `id` as an existing document will update the existing one.

### Delete items from vector store
"""

await vectorStore.delete({ ids: ["4"] });

"""
## Query vector store

Once your vector store has been created and the relevant documents have been added you will most likely wish to query it during the running of your chain or agent. 

### Query directly

Performing a simple similarity search can be done as follows:
"""

const similaritySearchResults = await vectorStore.similaritySearch("biology", 2);

for (const doc of similaritySearchResults) {
  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);
}
# Output:
#   * The powerhouse of the cell is the mitochondria [{"_id":"1","source":"https://example.com"}]

#   * Mitochondria are made out of lipids [{"_id":"3","source":"https://example.com"}]


"""
### Filtering

MongoDB Atlas supports pre-filtering of results on other fields. They require you to define which metadata fields you plan to filter on by updating the index you created initially. Here's an example:

```json
{
  "fields": [
    {
      "numDimensions": 1024,
      "path": "embedding",
      "similarity": "euclidean",
      "type": "vector"
    },
    {
      "path": "source",
      "type": "filter"
    }
  ]
}
```

Above, the first item in `fields` is the vector index, and the second item is the metadata property you want to filter on. The name of the property is the value of the `path` key. So the above index would allow us to search on a metadata field named `source`.

Then, in your code you can use [MQL Query Operators](https://www.mongodb.com/docs/manual/reference/operator/query/) for filtering.

The below example illustrates this:
"""

const filter = {
  preFilter: {
    source: {
      $eq: "https://example.com",
    },
  },
}

const filteredResults = await vectorStore.similaritySearch("biology", 2, filter);

for (const doc of filteredResults) {
  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);
}
# Output:
#   * The powerhouse of the cell is the mitochondria [{"_id":"1","source":"https://example.com"}]

#   * Mitochondria are made out of lipids [{"_id":"3","source":"https://example.com"}]


"""
### Returning scores

If you want to execute a similarity search and receive the corresponding scores you can run:
"""

const similaritySearchWithScoreResults = await vectorStore.similaritySearchWithScore("biology", 2, filter)

for (const [doc, score] of similaritySearchWithScoreResults) {
  console.log(`* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`);
}
# Output:
#   * [SIM=0.374] The powerhouse of the cell is the mitochondria [{"_id":"1","source":"https://example.com"}]

#   * [SIM=0.370] Mitochondria are made out of lipids [{"_id":"3","source":"https://example.com"}]


"""
### Query by turning into retriever

You can also transform the vector store into a [retriever](/docs/concepts/retrievers) for easier usage in your chains. 
"""

const retriever = vectorStore.asRetriever({
  // Optional filter
  filter: filter,
  k: 2,
});
await retriever.invoke("biology");
# Output:
#   [

#     Document {

#       pageContent: 'The powerhouse of the cell is the mitochondria',

#       metadata: { _id: '1', source: 'https://example.com' },

#       id: undefined

#     },

#     Document {

#       pageContent: 'Mitochondria are made out of lipids',

#       metadata: { _id: '3', source: 'https://example.com' },

#       id: undefined

#     }

#   ]


"""
### Usage for retrieval-augmented generation

For guides on how to use this vector store for retrieval-augmented generation (RAG), see the following sections:

- [Tutorials: working with external knowledge](/docs/tutorials/#working-with-external-knowledge).
- [How-to: Question and answer with RAG](/docs/how_to/#qa-with-rag)
- [Retrieval conceptual docs](/docs/concepts/retrieval)
"""

"""
## Closing connections

Make sure you close the client instance when you are finished to avoid excessive resource consumption:
"""

await client.close();

"""
## API reference

For detailed documentation of all `MongoDBAtlasVectorSearch` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_mongodb.MongoDBAtlasVectorSearch.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/myscale.mdx
================================================
---
sidebar_class_name: node-only
---

import CodeBlock from "@theme/CodeBlock";

# MyScale

:::tip Compatibility
Only available on Node.js.
:::

[MyScale](https://myscale.com/) is an emerging AI database that harmonizes the power of vector search and SQL analytics, providing a managed, efficient, and responsive experience.

## Setup

1. Launch a cluster through [MyScale's Web Console](https://console.myscale.com/). See [MyScale's official documentation](https://docs.myscale.com/en/quickstart/) for more information.
2. After launching a cluster, view your `Connection Details` from your cluster's `Actions` menu. You will need the host, port, username, and password.
3. Install the required Node.js peer dependency in your workspace.

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install -S @langchain/openai @clickhouse/client @langchain/community @langchain/core
```

## Index and Query Docs

import InsertExample from "@examples/indexes/vector_stores/myscale_fromTexts.ts";

<CodeBlock language="typescript">{InsertExample}</CodeBlock>

## Query Docs From an Existing Collection

import SearchExample from "@examples/indexes/vector_stores/myscale_search.ts";

<CodeBlock language="typescript">{SearchExample}</CodeBlock>

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/neo4jvector.mdx
================================================
# Neo4j Vector Index

Neo4j is an open-source graph database with integrated support for vector similarity search.
It supports:

- approximate nearest neighbor search
- Euclidean similarity and cosine similarity
- Hybrid search combining vector and keyword searches

## Setup

To work with Neo4j Vector Index, you need to install the `neo4j-driver` package:

```bash npm2yarn
npm install neo4j-driver
```

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

### Setup a `Neo4j` self hosted instance with `docker-compose`

`Neo4j` provides a prebuilt Docker image that can be used to quickly setup a self-hosted Neo4j database instance.
Create a file below named `docker-compose.yml`:

import CodeBlock from "@theme/CodeBlock";
import DockerExample from "@examples/indexes/vector_stores/neo4j_vector/docker-compose.example.yml";

<CodeBlock language="yml" name="docker-compose.yml">
  {DockerExample}
</CodeBlock>

And then in the same directory, run `docker compose up` to start the container.

You can find more information on how to setup `Neo4j` on their [website](https://neo4j.com/docs/operations-manual/current/installation/).

## Usage

import Example from "@examples/indexes/vector_stores/neo4j_vector/neo4j_vector.ts";

One complete example of using `Neo4jVectorStore` is the following:

<CodeBlock language="typescript">{Example}</CodeBlock>

### Use retrievalQuery parameter to customize responses

import RetrievalExample from "@examples/indexes/vector_stores/neo4j_vector/neo4j_vector_retrieval.ts";

<CodeBlock language="typescript">{RetrievalExample}</CodeBlock>

### Instantiate Neo4jVectorStore from existing graph

import ExistingGraphExample from "@examples/indexes/vector_stores/neo4j_vector/neo4j_vector_existinggraph.ts";

<CodeBlock language="typescript">{ExistingGraphExample}</CodeBlock>

### Metadata filtering

import MetadataExample from "@examples/indexes/vector_stores/neo4j_vector/neo4j_vector_metadata.ts";

<CodeBlock language="typescript">{MetadataExample}</CodeBlock>

# Disclaimer ⚠️

_Security note_: Make sure that the database connection uses credentials
that are narrowly-scoped to only include necessary permissions.
Failure to do so may result in data corruption or loss, since the calling
code may attempt commands that would result in deletion, mutation
of data if appropriately prompted or reading sensitive data if such
data is present in the database.
The best way to guard against such negative outcomes is to (as appropriate)
limit the permissions granted to the credentials used with this tool.
For example, creating read only users for the database is a good way to
ensure that the calling code cannot mutate or delete data.
See the [security page](/docs/security) for more information.

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/neon.mdx
================================================
# Neon Postgres

Neon is a fully managed serverless PostgreSQL database. It separates storage and compute to offer
features such as instant branching and automatic scaling.

With the `pgvector` extension, Neon provides a vector store that can be used with LangChain.js to store and query embeddings.

## Setup

### Select a Neon project

If you do not have a Neon account, sign up for one at [Neon](https://neon.tech). After logging into the Neon Console, proceed
to the [Projects](https://console.neon.tech/app/projects) section and select an existing project or create a new one.

Your Neon project comes with a ready-to-use Postgres database named `neondb` that you can use to store embeddings. Navigate to
the Connection Details section to find your database connection string. It should look similar to this:

```text
postgres://alex:AbC123dEf@ep-cool-darkness-123456.us-east-2.aws.neon.tech/dbname?sslmode=require
```

Keep your connection string handy for later use.

### Application code

To work with Neon Postgres, you need to install the `@neondatabase/serverless` package which provides a JavaScript/TypeScript
driver to connect to the database.

```bash npm2yarn
npm install @neondatabase/serverless
```

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core
```

To initialize a `NeonPostgres` vectorstore, you need to provide your Neon database connection string. You can use the connection string
we fetched above directly, or store it as an environment variable and use it in your code.

```typescript
const vectorStore = await NeonPostgres.initialize(embeddings, {
  connectionString: NEON_POSTGRES_CONNECTION_STRING,
});
```

## Usage

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/indexes/vector_stores/neon/example.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/opensearch.mdx
================================================
---
sidebar_class_name: node-only
keywords: [OpenSearchVectorStore]
---

# OpenSearch

:::tip Compatibility
Only available on Node.js.
:::

[OpenSearch](https://opensearch.org/) is a fork of [Elasticsearch](https://www.elastic.co/elasticsearch/) that is fully compatible with the Elasticsearch API. Read more about their support for Approximate Nearest Neighbors [here](https://opensearch.org/docs/latest/search-plugins/knn/approximate-knn/).

Langchain.js accepts [@opensearch-project/opensearch](https://opensearch.org/docs/latest/clients/javascript/index/) as the client for OpenSearch vectorstore.

## Setup

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install -S @langchain/openai @langchain/core @opensearch-project/opensearch
```

You'll also need to have an OpenSearch instance running. You can use the [official Docker image](https://opensearch.org/docs/latest/opensearch/install/docker/) to get started. You can also find an example docker-compose file [here](https://github.com/langchain-ai/langchainjs/blob/main/examples/src/indexes/vector_stores/opensearch/docker-compose.yml).

## Index docs

```typescript
import { Client } from "@opensearch-project/opensearch";
import { Document } from "langchain/document";
import { OpenAIEmbeddings } from "@langchain/openai";
import { OpenSearchVectorStore } from "@langchain/community/vectorstores/opensearch";

const client = new Client({
  nodes: [process.env.OPENSEARCH_URL ?? "http://127.0.0.1:9200"],
});

const docs = [
  new Document({
    metadata: { foo: "bar" },
    pageContent: "opensearch is also a vector db",
  }),
  new Document({
    metadata: { foo: "bar" },
    pageContent: "the quick brown fox jumped over the lazy dog",
  }),
  new Document({
    metadata: { baz: "qux" },
    pageContent: "lorem ipsum dolor sit amet",
  }),
  new Document({
    metadata: { baz: "qux" },
    pageContent:
      "OpenSearch is a scalable, flexible, and extensible open-source software suite for search, analytics, and observability applications",
  }),
];

await OpenSearchVectorStore.fromDocuments(docs, new OpenAIEmbeddings(), {
  client,
  indexName: process.env.OPENSEARCH_INDEX, // Will default to `documents`
});
```

## Query docs

```typescript
import { Client } from "@opensearch-project/opensearch";
import { VectorDBQAChain } from "langchain/chains";
import { OpenAIEmbeddings } from "@langchain/openai";
import { OpenAI } from "@langchain/openai";
import { OpenSearchVectorStore } from "@langchain/community/vectorstores/opensearch";

const client = new Client({
  nodes: [process.env.OPENSEARCH_URL ?? "http://127.0.0.1:9200"],
});

const vectorStore = new OpenSearchVectorStore(new OpenAIEmbeddings(), {
  client,
});

/* Search the vector DB independently with meta filters */
const results = await vectorStore.similaritySearch("hello world", 1);
console.log(JSON.stringify(results, null, 2));
/* [
    {
      "pageContent": "Hello world",
      "metadata": {
        "id": 2
      }
    }
  ] */

/* Use as part of a chain (currently no metadata filters) */
const model = new OpenAI();
const chain = VectorDBQAChain.fromLLM(model, vectorStore, {
  k: 1,
  returnSourceDocuments: true,
});
const response = await chain.call({ query: "What is opensearch?" });

console.log(JSON.stringify(response, null, 2));
/*
  {
    "text": " Opensearch is a collection of technologies that allow search engines to publish search results in a standard format, making it easier for users to search across multiple sites.",
    "sourceDocuments": [
      {
        "pageContent": "What's this?",
        "metadata": {
          "id": 3
        }
      }
    ]
  }
  */
```

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/pgvector.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: PGVector
sidebar_class_name: node-only
---
"""

"""
# PGVectorStore

```{=mdx}
:::tip Compatibility
Only available on Node.js.
:::
```

To enable vector search in generic PostgreSQL databases, LangChain.js supports using the [`pgvector`](https://github.com/pgvector/pgvector) Postgres extension.

This guide provides a quick overview for getting started with PGVector [vector stores](/docs/concepts/#vectorstores). For detailed documentation of all `PGVectorStore` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_vectorstores_pgvector.PGVectorStore.html).
"""

"""
## Overview

### Integration details

| Class | Package | [PY support](https://python.langchain.com/docs/integrations/vectorstores/pgvector/) | Package latest |
| :--- | :--- | :---: | :---: |
| [`PGVectorStore`](https://api.js.langchain.com/classes/langchain_community_vectorstores_pgvector.PGVectorStore.html) | [`@langchain/community`](https://npmjs.com/@langchain/community) | ✅ | ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |
"""

"""
## Setup

To use PGVector vector stores, you'll need to set up a Postgres instance with the [`pgvector`](https://github.com/pgvector/pgvector) extension enabled. You'll also need to install the `@langchain/community` integration package with the [`pg`](https://www.npmjs.com/package/pg) package as a peer dependency.

This guide will also use [OpenAI embeddings](/docs/integrations/text_embedding/openai), which require you to install the `@langchain/openai` integration package. You can also use [other supported embeddings models](/docs/integrations/text_embedding) if you wish.

We'll also use the [`uuid`](https://www.npmjs.com/package/uuid) package to generate ids in the required format.

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/openai @langchain/core pg uuid
</Npm2Yarn>
```

### Setting up an instance

There are many ways to connect to Postgres depending on how you've set up your instance. Here's one example of a local setup using a prebuilt Docker image provided by the `pgvector` team.

Create a file with the below content named docker-compose.yml:

```yaml
# Run this command to start the database:
# docker compose up
services:
  db:
    hostname: 127.0.0.1
    image: pgvector/pgvector:pg16
    ports:
      - 5432:5432
    restart: always
    environment:
      - POSTGRES_DB=api
      - POSTGRES_USER=myuser
      - POSTGRES_PASSWORD=ChangeMe
```

And then in the same directory, run `docker compose up` to start the container.

You can find more information on how to setup pgvector in the [official repository](https://github.com/pgvector/pgvector/).

### Credentials

To connect to you Postgres instance, you'll need corresponding credentials. For a full list of supported options, see the [`node-postgres` docs](https://node-postgres.com/apis/client).

If you are using OpenAI embeddings for this guide, you'll need to set your OpenAI key as well:

```typescript
process.env.OPENAI_API_KEY = "YOUR_API_KEY";
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
// process.env.LANGSMITH_TRACING="true"
// process.env.LANGSMITH_API_KEY="your-api-key"
```
"""

"""
## Instantiation

To instantiate the vector store, call the `.initialize()` static method. This will automatically check for the presence of a table, given by `tableName` in the passed `config`. If it is not there, it will create it with the required columns.

```{=mdx}

::::danger Security
User-generated data such as usernames should not be used as input for table and column names.  
**This may lead to SQL Injection!**
::::

```
"""

import {
  PGVectorStore,
  DistanceStrategy,
} from "@langchain/community/vectorstores/pgvector";
import { OpenAIEmbeddings } from "@langchain/openai";
import { PoolConfig } from "pg";

const embeddings = new OpenAIEmbeddings({
  model: "text-embedding-3-small",
});

// Sample config
const config = {
  postgresConnectionOptions: {
    type: "postgres",
    host: "127.0.0.1",
    port: 5433,
    user: "myuser",
    password: "ChangeMe",
    database: "api",
  } as PoolConfig,
  tableName: "testlangchainjs",
  columns: {
    idColumnName: "id",
    vectorColumnName: "vector",
    contentColumnName: "content",
    metadataColumnName: "metadata",
  },
  // supported distance strategies: cosine (default), innerProduct, or euclidean
  distanceStrategy: "cosine" as DistanceStrategy,
};

const vectorStore = await PGVectorStore.initialize(
  embeddings,
  config
);

"""
## Manage vector store

### Add items to vector store
"""

import { v4 as uuidv4 } from "uuid";
import type { Document } from "@langchain/core/documents";

const document1: Document = {
  pageContent: "The powerhouse of the cell is the mitochondria",
  metadata: { source: "https://example.com" }
};

const document2: Document = {
  pageContent: "Buildings are made out of brick",
  metadata: { source: "https://example.com" }
};

const document3: Document = {
  pageContent: "Mitochondria are made out of lipids",
  metadata: { source: "https://example.com" }
};

const document4: Document = {
  pageContent: "The 2024 Olympics are in Paris",
  metadata: { source: "https://example.com" }
}

const documents = [document1, document2, document3, document4];

const ids = [uuidv4(), uuidv4(), uuidv4(), uuidv4()]

await vectorStore.addDocuments(documents, { ids: ids });

"""
### Delete items from vector store
"""

const id4 = ids[ids.length - 1];

await vectorStore.delete({ ids: [id4] });

"""
## Query vector store

Once your vector store has been created and the relevant documents have been added you will most likely wish to query it during the running of your chain or agent. 

### Query directly

Performing a simple similarity search can be done as follows:
"""

const filter = { source: "https://example.com" };

const similaritySearchResults = await vectorStore.similaritySearch("biology", 2, filter);

for (const doc of similaritySearchResults) {
  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);
}
# Output:
#   * The powerhouse of the cell is the mitochondria [{"source":"https://example.com"}]

#   * Mitochondria are made out of lipids [{"source":"https://example.com"}]


"""
The above filter syntax supports exact match, but the following are also supported:

#### Using the `in` operator

```json
{
  "field": {
    "in": ["value1", "value2"],
  }
}
```

#### Using the `arrayContains` operator

```json
{
  "field": {
    "arrayContains": ["value1", "value2"],
  }
}
```

If you want to execute a similarity search and receive the corresponding scores you can run:
"""

const similaritySearchWithScoreResults = await vectorStore.similaritySearchWithScore("biology", 2, filter)

for (const [doc, score] of similaritySearchWithScoreResults) {
  console.log(`* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`);
}
# Output:
#   * [SIM=0.835] The powerhouse of the cell is the mitochondria [{"source":"https://example.com"}]

#   * [SIM=0.852] Mitochondria are made out of lipids [{"source":"https://example.com"}]


"""
### Query by turning into retriever

You can also transform the vector store into a [retriever](/docs/concepts/retrievers) for easier usage in your chains. 
"""

const retriever = vectorStore.asRetriever({
  // Optional filter
  filter: filter,
  k: 2,
});
await retriever.invoke("biology");
# Output:
#   [

#     Document {

#       pageContent: 'The powerhouse of the cell is the mitochondria',

#       metadata: { source: 'https://example.com' },

#       id: undefined

#     },

#     Document {

#       pageContent: 'Mitochondria are made out of lipids',

#       metadata: { source: 'https://example.com' },

#       id: undefined

#     }

#   ]


"""
### Usage for retrieval-augmented generation

For guides on how to use this vector store for retrieval-augmented generation (RAG), see the following sections:

- [Tutorials: working with external knowledge](/docs/tutorials/#working-with-external-knowledge).
- [How-to: Question and answer with RAG](/docs/how_to/#qa-with-rag)
- [Retrieval conceptual docs](/docs/concepts/retrieval)
"""

"""
## Advanced: reusing connections

You can reuse connections by creating a pool, then creating new `PGVectorStore` instances directly via the constructor.

Note that you should call `.initialize()` to set up your database at least once to set up your tables properly before using the constructor.
"""

import { OpenAIEmbeddings } from "@langchain/openai";
import { PGVectorStore } from "@langchain/community/vectorstores/pgvector";
import pg from "pg";

// First, follow set-up instructions at
// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/pgvector

const reusablePool = new pg.Pool({
  host: "127.0.0.1",
  port: 5433,
  user: "myuser",
  password: "ChangeMe",
  database: "api",
});

const originalConfig = {
  pool: reusablePool,
  tableName: "testlangchainjs",
  collectionName: "sample",
  collectionTableName: "collections",
  columns: {
    idColumnName: "id",
    vectorColumnName: "vector",
    contentColumnName: "content",
    metadataColumnName: "metadata",
  },
};

// Set up the DB.
// Can skip this step if you've already initialized the DB.
// await PGVectorStore.initialize(new OpenAIEmbeddings(), originalConfig);
const pgvectorStore = new PGVectorStore(new OpenAIEmbeddings(), originalConfig);

await pgvectorStore.addDocuments([
  { pageContent: "what's this", metadata: { a: 2 } },
  { pageContent: "Cat drinks milk", metadata: { a: 1 } },
]);

const results = await pgvectorStore.similaritySearch("water", 1);

console.log(results);

/*
  [ Document { pageContent: 'Cat drinks milk', metadata: { a: 1 } } ]
*/

const pgvectorStore2 = new PGVectorStore(new OpenAIEmbeddings(), {
  pool: reusablePool,
  tableName: "testlangchainjs",
  collectionTableName: "collections",
  collectionName: "some_other_collection",
  columns: {
    idColumnName: "id",
    vectorColumnName: "vector",
    contentColumnName: "content",
    metadataColumnName: "metadata",
  },
});

const results2 = await pgvectorStore2.similaritySearch("water", 1);

console.log(results2);

/*
  []
*/

await reusablePool.end();

"""
## Create HNSW Index

By default, the extension performs a sequential scan search, with 100% recall. You might consider creating an HNSW index for approximate nearest neighbor (ANN) search to speed up `similaritySearchVectorWithScore` execution time. To create the HNSW index on your vector column, use the `createHnswIndex()` method.

The method parameters include:

- `dimensions`: Defines the number of dimensions in your vector data type, up to 2000. For example, use 1536 for OpenAI's text-embedding-ada-002 and Amazon's amazon.titan-embed-text-v1 models.

- `m?`: The max number of connections per layer (16 by default). Index build time improves with smaller values, while higher values can speed up search queries.

- `efConstruction?`: The size of the dynamic candidate list for constructing the graph (64 by default). A higher value can potentially improve the index quality at the cost of index build time.

- `distanceFunction?`: The distance function name you want to use, is automatically selected based on the distanceStrategy.

For more info, see the [Pgvector GitHub repo](https://github.com/pgvector/pgvector?tab=readme-ov-file#hnsw) and the [HNSW paper from Malkov Yu A. and Yashunin D. A.. 2020. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs](https://arxiv.org/pdf/1603.09320)
"""

import { OpenAIEmbeddings } from "@langchain/openai";
import {
  DistanceStrategy,
  PGVectorStore,
} from "@langchain/community/vectorstores/pgvector";
import { PoolConfig } from "pg";

// First, follow set-up instructions at
// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/pgvector

const hnswConfig = {
  postgresConnectionOptions: {
    type: "postgres",
    host: "127.0.0.1",
    port: 5433,
    user: "myuser",
    password: "ChangeMe",
    database: "api",
  } as PoolConfig,
  tableName: "testlangchainjs",
  columns: {
    idColumnName: "id",
    vectorColumnName: "vector",
    contentColumnName: "content",
    metadataColumnName: "metadata",
  },
  // supported distance strategies: cosine (default), innerProduct, or euclidean
  distanceStrategy: "cosine" as DistanceStrategy,
};

const hnswPgVectorStore = await PGVectorStore.initialize(
  new OpenAIEmbeddings(),
  hnswConfig
);

// create the index
await hnswPgVectorStore.createHnswIndex({
  dimensions: 1536,
  efConstruction: 64,
  m: 16,
});

await hnswPgVectorStore.addDocuments([
  { pageContent: "what's this", metadata: { a: 2, b: ["tag1", "tag2"] } },
  { pageContent: "Cat drinks milk", metadata: { a: 1, b: ["tag2"] } },
]);

const model = new OpenAIEmbeddings();
const query = await model.embedQuery("water");
const hnswResults = await hnswPgVectorStore.similaritySearchVectorWithScore(query, 1);

console.log(hnswResults);

await pgvectorStore.end();

"""
## Closing connections

Make sure you close the connection when you are finished to avoid excessive resource consumption:
"""

await vectorStore.end();

"""
## API reference

For detailed documentation of all `PGVectorStore` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_vectorstores_pgvector.PGVectorStore.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/pinecone.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Pinecone
---
"""

"""
# PineconeStore

[Pinecone](https://www.pinecone.io/) is a vector database that helps power AI for some of the world’s best companies.

This guide provides a quick overview for getting started with Pinecone [vector stores](/docs/concepts/#vectorstores). For detailed documentation of all `PineconeStore` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_pinecone.PineconeStore.html).
"""

"""
## Overview

### Integration details

| Class | Package | [PY support](https://python.langchain.com/docs/integrations/vectorstores/pinecone/) | Package latest |
| :--- | :--- | :---: | :---: |
| [`PineconeStore`](https://api.js.langchain.com/classes/langchain_pinecone.PineconeStore.html) | [`@langchain/pinecone`](https://npmjs.com/@langchain/pinecone) | ✅ |  ![NPM - Version](https://img.shields.io/npm/v/@langchain/pinecone?style=flat-square&label=%20&) |
"""

"""
## Setup

To use Pinecone vector stores, you'll need to create a Pinecone account, initialize an index, and install the `@langchain/pinecone` integration package. You'll also want to install the [official Pinecone SDK](https://www.npmjs.com/package/@pinecone-database/pinecone) to initialize a client to pass into the `PineconeStore` instance.

This guide will also use [OpenAI embeddings](/docs/integrations/text_embedding/openai), which require you to install the `@langchain/openai` integration package. You can also use [other supported embeddings models](/docs/integrations/text_embedding) if you wish.

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/pinecone @langchain/openai @langchain/core @pinecone-database/pinecone@5
</Npm2Yarn>
```

### Credentials

Sign up for a [Pinecone](https://www.pinecone.io/) account and create an index. Make sure the dimensions match those of the embeddings you want to use (the default is 1536 for OpenAI's `text-embedding-3-small`). Once you've done this set the `PINECONE_INDEX`, `PINECONE_API_KEY`, and (optionally) `PINECONE_ENVIRONMENT` environment variables:

```typescript
process.env.PINECONE_API_KEY = "your-pinecone-api-key";
process.env.PINECONE_INDEX = "your-pinecone-index";

// Optional
process.env.PINECONE_ENVIRONMENT = "your-pinecone-environment";
```

If you are using OpenAI embeddings for this guide, you'll need to set your OpenAI key as well:

```typescript
process.env.OPENAI_API_KEY = "YOUR_API_KEY";
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
// process.env.LANGSMITH_TRACING="true"
// process.env.LANGSMITH_API_KEY="your-api-key"
```
"""

"""
## Instantiation
"""

import { PineconeStore } from "@langchain/pinecone";
import { OpenAIEmbeddings } from "@langchain/openai";

import { Pinecone as PineconeClient } from "@pinecone-database/pinecone";

const embeddings = new OpenAIEmbeddings({
  model: "text-embedding-3-small",
});

const pinecone = new PineconeClient();
// Will automatically read the PINECONE_API_KEY and PINECONE_ENVIRONMENT env vars
const pineconeIndex = pinecone.Index(process.env.PINECONE_INDEX!);

const vectorStore = await PineconeStore.fromExistingIndex(
  embeddings,
  {
    pineconeIndex,
    // Maximum number of batch requests to allow at once. Each batch is 1000 vectors.
    maxConcurrency: 5,
    // You can pass a namespace here too
    // namespace: "foo",
  }
);

"""
## Manage vector store

### Add items to vector store
"""

import type { Document } from "@langchain/core/documents";

const document1: Document = {
  pageContent: "The powerhouse of the cell is the mitochondria",
  metadata: { source: "https://example.com" }
};

const document2: Document = {
  pageContent: "Buildings are made out of brick",
  metadata: { source: "https://example.com" }
};

const document3: Document = {
  pageContent: "Mitochondria are made out of lipids",
  metadata: { source: "https://example.com" }
};

const document4: Document = {
  pageContent: "The 2024 Olympics are in Paris",
  metadata: { source: "https://example.com" }
}

const documents = [document1, document2, document3, document4];

await vectorStore.addDocuments(documents, { ids: ["1", "2", "3", "4"] });
# Output:
#   [ '1', '2', '3', '4' ]


"""
**Note:** After adding documents, there is a slight delay before they become queryable.

### Delete items from vector store
"""

await vectorStore.delete({ ids: ["4"] });

"""
## Query vector store

Once your vector store has been created and the relevant documents have been added you will most likely wish to query it during the running of your chain or agent. 

### Query directly

Performing a simple similarity search can be done as follows:
"""

// Optional filter
const filter = { source: "https://example.com" };

const similaritySearchResults = await vectorStore.similaritySearch("biology", 2, filter);

for (const doc of similaritySearchResults) {
  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);
}
# Output:
#   * The powerhouse of the cell is the mitochondria [{"source":"https://example.com"}]

#   * Mitochondria are made out of lipids [{"source":"https://example.com"}]


"""
If you want to execute a similarity search and receive the corresponding scores you can run:
"""

const similaritySearchWithScoreResults = await vectorStore.similaritySearchWithScore("biology", 2, filter)

for (const [doc, score] of similaritySearchWithScoreResults) {
  console.log(`* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`);
}
# Output:
#   * [SIM=0.165] The powerhouse of the cell is the mitochondria [{"source":"https://example.com"}]

#   * [SIM=0.148] Mitochondria are made out of lipids [{"source":"https://example.com"}]


"""
### Query by turning into retriever

You can also transform the vector store into a [retriever](/docs/concepts/retrievers) for easier usage in your chains. 
"""

const retriever = vectorStore.asRetriever({
  // Optional filter
  filter: filter,
  k: 2,
});

await retriever.invoke("biology");
# Output:
#   [

#     Document {

#       pageContent: 'The powerhouse of the cell is the mitochondria',

#       metadata: { source: 'https://example.com' },

#       id: undefined

#     },

#     Document {

#       pageContent: 'Mitochondria are made out of lipids',

#       metadata: { source: 'https://example.com' },

#       id: undefined

#     }

#   ]


"""
### Usage for retrieval-augmented generation

For guides on how to use this vector store for retrieval-augmented generation (RAG), see the following sections:

- [Tutorials: working with external knowledge](/docs/tutorials/#working-with-external-knowledge).
- [How-to: Question and answer with RAG](/docs/how_to/#qa-with-rag)
- [Retrieval conceptual docs](/docs/concepts/retrieval)
"""

"""
## API reference

For detailed documentation of all `PineconeStore` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_pinecone.PineconeStore.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/prisma.mdx
================================================
---
sidebar_class_name: node-only
---

# Prisma

For augmenting existing models in PostgreSQL database with vector search, Langchain supports using [Prisma](https://www.prisma.io/) together with PostgreSQL and [`pgvector`](https://github.com/pgvector/pgvector) Postgres extension.

## Setup

### Setup database instance with Supabase

Refer to the [Prisma and Supabase integration guide](https://supabase.com/docs/guides/integrations/prisma) to setup a new database instance with Supabase and Prisma.

### Install Prisma

```bash npm2yarn
npm install prisma
```

### Setup `pgvector` self hosted instance with `docker-compose`

`pgvector` provides a prebuilt Docker image that can be used to quickly setup a self-hosted Postgres instance.

```yaml
services:
  db:
    image: ankane/pgvector
    ports:
      - 5432:5432
    volumes:
      - db:/var/lib/postgresql/data
    environment:
      - POSTGRES_PASSWORD=
      - POSTGRES_USER=
      - POSTGRES_DB=

volumes:
  db:
```

### Create a new schema

Assuming you haven't created a schema yet, create a new model with a `vector` field of type `Unsupported("vector")`:

```prisma
model Document {
  id      String                 @id @default(cuid())
  content String
  vector  Unsupported("vector")?
}
```

Afterwards, create a new migration with `--create-only` to avoid running the migration directly.

```bash npm2yarn
npx prisma migrate dev --create-only
```

Add the following line to the newly created migration to enable `pgvector` extension if it hasn't been enabled yet:

```sql
CREATE EXTENSION IF NOT EXISTS vector;
```

Run the migration afterwards:

```bash npm2yarn
npx prisma migrate dev
```

## Usage

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

:::warning
Table names and column names (in fields such as `tableName`, `vectorColumnName`, `columns` and `filter`) are passed into SQL queries directly without parametrisation.
These fields must be sanitized beforehand to avoid SQL injection.
:::

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/indexes/vector_stores/prisma_vectorstore/prisma.ts";
import Schema from "@examples/indexes/vector_stores/prisma_vectorstore/prisma/schema.prisma";

<CodeBlock language="typescript">{Example}</CodeBlock>

The following SQL operators are available as filters: `equals`, `in`, `isNull`, `isNotNull`, `like`, `lt`, `lte`, `gt`, `gte`, `not`.

The samples above uses the following schema:

<CodeBlock language="prisma">{Schema}</CodeBlock>

You can remove `namespace` if you don't need it.

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/qdrant.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Qdrant
sidebar_class_name: node-only
---
"""

"""
# QdrantVectorStore

```{=mdx}
:::tip Compatibility
Only available on Node.js.
:::
```

[Qdrant](https://qdrant.tech/) is a vector similarity search engine. It provides a production-ready service with a convenient API to store, search, and manage points - vectors with an additional payload.

This guide provides a quick overview for getting started with Qdrant [vector stores](/docs/concepts/#vectorstores). For detailed documentation of all `QdrantVectorStore` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_qdrant.QdrantVectorStore.html).
"""

"""
## Overview

### Integration details

| Class | Package | [PY support](https://python.langchain.com/docs/integrations/vectorstores/qdrant/) |  Package latest |
| :--- | :--- | :---: | :---: |
| [`QdrantVectorStore`](https://api.js.langchain.com/classes/langchain_qdrant.QdrantVectorStore.html) | [`@langchain/qdrant`](https://npmjs.com/@langchain/qdrant) | ✅ |  ![NPM - Version](https://img.shields.io/npm/v/@langchain/qdrant?style=flat-square&label=%20&) |
"""

"""
## Setup

To use Qdrant vector stores, you'll need to set up a Qdrant instance and install the `@langchain/qdrant` integration package.

This guide will also use [OpenAI embeddings](/docs/integrations/text_embedding/openai), which require you to install the `@langchain/openai` integration package. You can also use [other supported embeddings models](/docs/integrations/text_embedding) if you wish.

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/qdrant @langchain/core @langchain/openai
</Npm2Yarn>
```

After installing the required dependencies, run a Qdrant instance with Docker on your computer by following the [Qdrant setup instructions](https://qdrant.tech/documentation/quickstart/). Note the URL your container runs on.

### Credentials

Once you've done this set a `QDRANT_URL` environment variable:

```typescript
// e.g. http://localhost:6333
process.env.QDRANT_URL = "your-qdrant-url"
```

If you are using OpenAI embeddings for this guide, you'll need to set your OpenAI key as well:

```typescript
process.env.OPENAI_API_KEY = "YOUR_API_KEY";
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
// process.env.LANGSMITH_TRACING="true"
// process.env.LANGSMITH_API_KEY="your-api-key"
```
"""

"""
## Instantiation
"""

import { QdrantVectorStore } from "@langchain/qdrant";
import { OpenAIEmbeddings } from "@langchain/openai";

const embeddings = new OpenAIEmbeddings({
  model: "text-embedding-3-small",
});

const vectorStore = await QdrantVectorStore.fromExistingCollection(embeddings, {
  url: process.env.QDRANT_URL,
  collectionName: "langchainjs-testing",
});

"""
## Manage vector store

### Add items to vector store
"""

import type { Document } from "@langchain/core/documents";

const document1: Document = {
  pageContent: "The powerhouse of the cell is the mitochondria",
  metadata: { source: "https://example.com" }
};

const document2: Document = {
  pageContent: "Buildings are made out of brick",
  metadata: { source: "https://example.com" }
};

const document3: Document = {
  pageContent: "Mitochondria are made out of lipids",
  metadata: { source: "https://example.com" }
};

const document4: Document = {
  pageContent: "The 2024 Olympics are in Paris",
  metadata: { source: "https://example.com" }
}

const documents = [document1, document2, document3, document4];

await vectorStore.addDocuments(documents);

"""
Top-level document ids and deletion are currently not supported.
"""

"""
## Query vector store

Once your vector store has been created and the relevant documents have been added you will most likely wish to query it during the running of your chain or agent. 

### Query directly

Performing a simple similarity search can be done as follows:
"""

const filter = {
  "must": [
      { "key": "metadata.source", "match": { "value": "https://example.com" } },
  ]
};

const similaritySearchResults = await vectorStore.similaritySearch("biology", 2, filter);

for (const doc of similaritySearchResults) {
  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);
}
# Output:
#   * The powerhouse of the cell is the mitochondria [{"source":"https://example.com"}]

#   * Mitochondria are made out of lipids [{"source":"https://example.com"}]


"""
See [this page](https://qdrant.tech/documentation/concepts/filtering/) for more on Qdrant filter syntax. Note that all values must be prefixed with `metadata.`

If you want to execute a similarity search and receive the corresponding scores you can run:
"""

const similaritySearchWithScoreResults = await vectorStore.similaritySearchWithScore("biology", 2, filter)

for (const [doc, score] of similaritySearchWithScoreResults) {
  console.log(`* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`);
}
# Output:
#   * [SIM=0.165] The powerhouse of the cell is the mitochondria [{"source":"https://example.com"}]

#   * [SIM=0.148] Mitochondria are made out of lipids [{"source":"https://example.com"}]


"""
### Query by turning into retriever

You can also transform the vector store into a [retriever](/docs/concepts/retrievers) for easier usage in your chains. 
"""

const retriever = vectorStore.asRetriever({
  // Optional filter
  filter: filter,
  k: 2,
});
await retriever.invoke("biology");
# Output:
#   [

#     Document {

#       pageContent: 'The powerhouse of the cell is the mitochondria',

#       metadata: { source: 'https://example.com' },

#       id: undefined

#     },

#     Document {

#       pageContent: 'Mitochondria are made out of lipids',

#       metadata: { source: 'https://example.com' },

#       id: undefined

#     }

#   ]


"""
### Usage for retrieval-augmented generation

For guides on how to use this vector store for retrieval-augmented generation (RAG), see the following sections:

- [Tutorials: working with external knowledge](/docs/tutorials/#working-with-external-knowledge).
- [How-to: Question and answer with RAG](/docs/how_to/#qa-with-rag)
- [Retrieval conceptual docs](/docs/concepts/retrieval)
"""

"""
## API reference

For detailed documentation of all `QdrantVectorStore` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_qdrant.QdrantVectorStore.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/redis.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Redis
sidebar_class_name: node-only
---
"""

"""
# RedisVectorStore

```{=mdx}
:::tip Compatibility
Only available on Node.js.
:::
```

[Redis](https://redis.io/) is a fast open source, in-memory data store. As part of the [Redis Stack](https://redis.io/docs/latest/operate/oss_and_stack/install/install-stack/), [RediSearch](https://redis.io/docs/latest/develop/interact/search-and-query/) is the module that enables vector similarity semantic search, as well as many other types of searching.

This guide provides a quick overview for getting started with Redis [vector stores](/docs/concepts/#vectorstores). For detailed documentation of all `RedisVectorStore` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_redis.RedisVectorStore.html).
"""

"""
## Overview

### Integration details

| Class | Package | [PY support](https://python.langchain.com/docs/integrations/vectorstores/redis/) |  Package latest |
| :--- | :--- | :---: | :---: |
| [`RedisVectorStore`](https://api.js.langchain.com/classes/langchain_redis.RedisVectorStore.html) | [`@langchain/redis`](https://npmjs.com/@langchain/redis/) | ✅ |  ![NPM - Version](https://img.shields.io/npm/v/@langchain/redis?style=flat-square&label=%20&) |
"""

"""
## Setup

To use Redis vector stores, you'll need to set up a Redis instance and install the `@langchain/redis` integration package. You can also install the [`node-redis`](https://github.com/redis/node-redis) package to initialize the vector store with a specific client instance.

This guide will also use [OpenAI embeddings](/docs/integrations/text_embedding/openai), which require you to install the `@langchain/openai` integration package. You can also use [other supported embeddings models](/docs/integrations/text_embedding) if you wish.

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/redis @langchain/core redis @langchain/openai
</Npm2Yarn>
```

You can set up a Redis instance locally with Docker by following [these instructions](https://redis.io/docs/latest/operate/oss_and_stack/install/install-stack/docker/#redisredis-stack).

### Credentials

Once you've set up an instance, set the `REDIS_URL` environment variable:

```typescript
process.env.REDIS_URL = "your-redis-url"
```

If you are using OpenAI embeddings for this guide, you'll need to set your OpenAI key as well:

```typescript
process.env.OPENAI_API_KEY = "YOUR_API_KEY";
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
// process.env.LANGSMITH_TRACING="true"
// process.env.LANGSMITH_API_KEY="your-api-key"
```
"""

"""
## Instantiation
"""

import { RedisVectorStore } from "@langchain/redis";
import { OpenAIEmbeddings } from "@langchain/openai";

import { createClient } from "redis";

const embeddings = new OpenAIEmbeddings({
  model: "text-embedding-3-small",
});

const client = createClient({
  url: process.env.REDIS_URL ?? "redis://localhost:6379",
});
await client.connect();

const vectorStore = new RedisVectorStore(embeddings, {
  redisClient: client,
  indexName: "langchainjs-testing",
});

"""
## Manage vector store

### Add items to vector store
"""

import type { Document } from "@langchain/core/documents";

const document1: Document = {
  pageContent: "The powerhouse of the cell is the mitochondria",
  metadata: { type: "example" }
};

const document2: Document = {
  pageContent: "Buildings are made out of brick",
  metadata: { type: "example" }
};

const document3: Document = {
  pageContent: "Mitochondria are made out of lipids",
  metadata: { type: "example" }
};

const document4: Document = {
  pageContent: "The 2024 Olympics are in Paris",
  metadata: { type: "example" }
}

const documents = [document1, document2, document3, document4];

await vectorStore.addDocuments(documents);

"""
Top-level document ids and deletion are currently not supported.
"""

"""
## Query vector store

Once your vector store has been created and the relevant documents have been added you will most likely wish to query it during the running of your chain or agent. 

### Query directly

Performing a simple similarity search can be done as follows:
"""

const similaritySearchResults = await vectorStore.similaritySearch("biology", 2);

for (const doc of similaritySearchResults) {
  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);
}

"""
Filtering will currently look for any metadata key containing the provided string.

If you want to execute a similarity search and receive the corresponding scores you can run:
"""

const similaritySearchWithScoreResults = await vectorStore.similaritySearchWithScore("biology", 2)

for (const [doc, score] of similaritySearchWithScoreResults) {
  console.log(`* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`);
}
# Output:
#   * [SIM=0.835] The powerhouse of the cell is the mitochondria [{"type":"example"}]

#   * [SIM=0.852] Mitochondria are made out of lipids [{"type":"example"}]


"""
### Query by turning into retriever

You can also transform the vector store into a [retriever](/docs/concepts/retrievers) for easier usage in your chains. 
"""

const retriever = vectorStore.asRetriever({
  k: 2,
});
await retriever.invoke("biology");
# Output:
#   [

#     Document {

#       pageContent: 'The powerhouse of the cell is the mitochondria',

#       metadata: { type: 'example' },

#       id: undefined

#     },

#     Document {

#       pageContent: 'Mitochondria are made out of lipids',

#       metadata: { type: 'example' },

#       id: undefined

#     }

#   ]


"""
### Usage for retrieval-augmented generation

For guides on how to use this vector store for retrieval-augmented generation (RAG), see the following sections:

- [Tutorials: working with external knowledge](/docs/tutorials/#working-with-external-knowledge).
- [How-to: Question and answer with RAG](/docs/how_to/#qa-with-rag)
- [Retrieval conceptual docs](/docs/concepts/retrieval)
"""

"""
## Deleting an index

You can delete an entire index with the following command:
"""

await vectorStore.delete({ deleteAll: true });

"""
## Closing connections

Make sure you close the client connection when you are finished to avoid excessive resource consumption:
"""

await client.disconnect();

"""
## API reference

For detailed documentation of all `RedisVectorSearch` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_redis.RedisVectorStore.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/rockset.mdx
================================================
---
sidebar_class_name: node-only
---

import CodeBlock from "@theme/CodeBlock";

# Rockset

[Rockset](https://rockset.com) is a real-time analyitics SQL database that runs in the cloud.
Rockset provides vector search capabilities, in the form of [SQL functions](https://rockset.com/docs/vector-functions/#vector-distance-functions), to support AI applications that rely on text similarity.

## Setup

Install the rockset client.

```bash
yarn add @rockset/client
```

### Usage

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/core @langchain/community
```

import UsageExample from "@examples/indexes/vector_stores/rockset.ts";

Below is an example showcasing how to use OpenAI and Rockset to answer questions about a text file:

<CodeBlock language="typescript">{UsageExample}</CodeBlock>

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/singlestore.mdx
================================================
---
sidebar_class_name: node-only
---

import CodeBlock from "@theme/CodeBlock";

# SingleStore

[SingleStoreDB](https://singlestore.com/) is a robust, high-performance distributed SQL database solution designed to excel in both [cloud](https://www.singlestore.com/cloud/) and on-premises environments. Boasting a versatile feature set, it offers seamless deployment options while delivering unparalleled performance.

A standout feature of SingleStoreDB is its advanced support for vector storage and operations, making it an ideal choice for applications requiring intricate AI capabilities such as text similarity matching. With built-in vector functions like [dot_product](https://docs.singlestore.com/managed-service/en/reference/sql-reference/vector-functions/dot_product.html) and [euclidean_distance](https://docs.singlestore.com/managed-service/en/reference/sql-reference/vector-functions/euclidean_distance.html), SingleStoreDB empowers developers to implement sophisticated algorithms efficiently.

For developers keen on leveraging vector data within SingleStoreDB, a comprehensive tutorial is available, guiding them through the intricacies of [working with vector data](https://docs.singlestore.com/managed-service/en/developer-resources/functional-extensions/working-with-vector-data.html). This tutorial delves into the Vector Store within SingleStoreDB, showcasing its ability to facilitate searches based on vector similarity. Leveraging vector indexes, queries can be executed with remarkable speed, enabling swift retrieval of relevant data.

Moreover, SingleStoreDB's Vector Store seamlessly integrates with [full-text indexing based on Lucene](https://docs.singlestore.com/cloud/developer-resources/functional-extensions/working-with-full-text-search/), enabling powerful text similarity searches. Users can filter search results based on selected fields of document metadata objects, enhancing query precision.

What sets SingleStoreDB apart is its ability to combine vector and full-text searches in various ways, offering flexibility and versatility. Whether prefiltering by text or vector similarity and selecting the most relevant data, or employing a weighted sum approach to compute a final similarity score, developers have multiple options at their disposal.

In essence, SingleStoreDB provides a comprehensive solution for managing and querying vector data, offering unparalleled performance and flexibility for AI-driven applications.

:::tip Compatibility
Only available on Node.js.
:::

LangChain.js requires the `mysql2` library to create a connection to a SingleStoreDB instance.

## Setup

1. Establish a SingleStoreDB environment. You have the flexibility to choose between [Cloud-based](https://docs.singlestore.com/managed-service/en/getting-started-with-singlestoredb-cloud.html) or [On-Premise](https://docs.singlestore.com/db/v8.1/en/developer-resources/get-started-using-singlestoredb-for-free.html) editions.
2. Install the mysql2 JS client

```bash npm2yarn
npm install -S mysql2
```

## Usage

`SingleStoreVectorStore` manages a connection pool. It is recommended to call `await store.end();` before terminating your application to assure all connections are appropriately closed and prevent any possible resource leaks.

### Standard usage

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

import UsageExample from "@examples/indexes/vector_stores/singlestore.ts";

Below is a straightforward example showcasing how to import the relevant module and perform a base similarity search using the `SingleStoreVectorStore`:

<CodeBlock language="typescript">{UsageExample}</CodeBlock>

### Metadata Filtering

import UsageExampleWithMetadata from "@examples/indexes/vector_stores/singlestore_with_metadata_filter.ts";

If it is needed to filter results based on specific metadata fields, you can pass a filter parameter to narrow down your search to the documents that match all specified fields in the filter object:

<CodeBlock language="typescript">{UsageExampleWithMetadata}</CodeBlock>

### Vector indexes

Enhance your search efficiency with SingleStore DB version 8.5 or above by leveraging [ANN vector indexes](https://docs.singlestore.com/cloud/reference/sql-reference/vector-functions/vector-indexing/).
By setting `useVectorIndex: true` during vector store object creation, you can activate this feature.
Additionally, if your vectors differ in dimensionality from the default OpenAI embedding size of 1536, ensure to specify the `vectorSize` parameter accordingly.

### Hybrid search

import HybridSearchUsageExample from "@examples/indexes/vector_stores/singlestore_hybrid_search.ts";

SingleStoreDB presents a diverse range of search strategies, each meticulously crafted to cater to specific use cases and user preferences.
The default `VECTOR_ONLY` strategy utilizes vector operations such as `DOT_PRODUCT` or `EUCLIDEAN_DISTANCE` to calculate similarity scores directly between vectors, while `TEXT_ONLY` employs Lucene-based full-text search, particularly advantageous for text-centric applications.
For users seeking a balanced approach, `FILTER_BY_TEXT` first refines results based on text similarity before conducting vector comparisons, whereas `FILTER_BY_VECTOR` prioritizes vector similarity, filtering results before assessing text similarity for optimal matches.
Notably, both `FILTER_BY_TEXT` and `FILTER_BY_VECTOR` necessitate a full-text index for operation. Additionally, `WEIGHTED_SUM` emerges as a sophisticated strategy, calculating the final similarity score by weighing vector and text similarities, albeit exclusively utilizing dot_product distance calculations and also requiring a full-text index.
These versatile strategies empower users to fine-tune searches according to their unique needs, facilitating efficient and precise data retrieval and analysis.
Moreover, SingleStoreDB's hybrid approaches, exemplified by `FILTER_BY_TEXT`, `FILTER_BY_VECTOR`, and `WEIGHTED_SUM` strategies, seamlessly blend vector and text-based searches to maximize efficiency and accuracy, ensuring users can fully leverage the platform's capabilities for a wide range of applications.

<CodeBlock language="typescript">{HybridSearchUsageExample}</CodeBlock>

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/supabase.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Supabase
---
"""

"""
# SupabaseVectorStore

[Supabase](https://supabase.com/docs) is an open-source Firebase alternative. Supabase is built on top of PostgreSQL, which offers strong SQL querying capabilities and enables a simple interface with already-existing tools and frameworks.

LangChain.js supports using a Supabase Postgres database as a vector store, using the [`pgvector`](https://github.com/pgvector/pgvector) extension. Refer to the [Supabase blog post](https://supabase.com/blog/openai-embeddings-postgres-vector) for more information.

This guide provides a quick overview for getting started with Supabase [vector stores](/docs/concepts/#vectorstores). For detailed documentation of all `SupabaseVectorStore` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_vectorstores_supabase.SupabaseVectorStore.html).
"""

"""
## Overview

### Integration details

| Class | Package | [PY support](https://python.langchain.com/docs/integrations/vectorstores/supabase/) |  Package latest |
| :--- | :--- | :---: | :---: |
| [`SupabaseVectorStore`](https://api.js.langchain.com/classes/langchain_community_vectorstores_supabase.SupabaseVectorStore.html) | [`@langchain/community`](https://npmjs.com/@langchain/community) | ✅ |  ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |
"""

"""
## Setup

To use Supabase vector stores, you'll need to set up a Supabase database and install the `@langchain/community` integration package. You'll also need to install the official [`@supabase/supabase-js`](https://www.npmjs.com/package/@supabase/supabase-js) SDK as a peer dependency.

This guide will also use [OpenAI embeddings](/docs/integrations/text_embedding/openai), which require you to install the `@langchain/openai` integration package. You can also use [other supported embeddings models](/docs/integrations/text_embedding) if you wish.

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core @supabase/supabase-js @langchain/openai
</Npm2Yarn>
```

Once you've created a database, run the following SQL to set up [`pgvector`](https://github.com/pgvector/pgvector) and create the necessary table and functions:

```sql
-- Enable the pgvector extension to work with embedding vectors
create extension vector;

-- Create a table to store your documents
create table documents (
  id bigserial primary key,
  content text, -- corresponds to Document.pageContent
  metadata jsonb, -- corresponds to Document.metadata
  embedding vector(1536) -- 1536 works for OpenAI embeddings, change if needed
);

-- Create a function to search for documents
create function match_documents (
  query_embedding vector(1536),
  match_count int DEFAULT null,
  filter jsonb DEFAULT '{}'
) returns table (
  id bigint,
  content text,
  metadata jsonb,
  embedding jsonb,
  similarity float
)
language plpgsql
as $$
#variable_conflict use_column
begin
  return query
  select
    id,
    content,
    metadata,
    (embedding::text)::jsonb as embedding,
    1 - (documents.embedding <=> query_embedding) as similarity
  from documents
  where metadata @> filter
  order by documents.embedding <=> query_embedding
  limit match_count;
end;
$$;
```

### Credentials

Once you've done this set the `SUPABASE_PRIVATE_KEY` and `SUPABASE_URL` environment variables:

```typescript
process.env.SUPABASE_PRIVATE_KEY = "your-api-key";
process.env.SUPABASE_URL = "your-supabase-db-url";
```

If you are using OpenAI embeddings for this guide, you'll need to set your OpenAI key as well:

```typescript
process.env.OPENAI_API_KEY = "YOUR_API_KEY";
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
// process.env.LANGSMITH_TRACING="true"
// process.env.LANGSMITH_API_KEY="your-api-key"
```
"""

"""
## Instantiation
"""

import { SupabaseVectorStore } from "@langchain/community/vectorstores/supabase";
import { OpenAIEmbeddings } from "@langchain/openai";

import { createClient } from "@supabase/supabase-js";

const embeddings = new OpenAIEmbeddings({
  model: "text-embedding-3-small",
});

const supabaseClient = createClient(
  process.env.SUPABASE_URL,
  process.env.SUPABASE_PRIVATE_KEY
);

const vectorStore = new SupabaseVectorStore(embeddings, {
  client: supabaseClient,
  tableName: "documents",
  queryName: "match_documents",
});

"""
## Manage vector store

### Add items to vector store
"""

import type { Document } from "@langchain/core/documents";

const document1: Document = {
  pageContent: "The powerhouse of the cell is the mitochondria",
  metadata: { source: "https://example.com" }
};

const document2: Document = {
  pageContent: "Buildings are made out of brick",
  metadata: { source: "https://example.com" }
};

const document3: Document = {
  pageContent: "Mitochondria are made out of lipids",
  metadata: { source: "https://example.com" }
};

const document4: Document = {
  pageContent: "The 2024 Olympics are in Paris",
  metadata: { source: "https://example.com" }
}

const documents = [document1, document2, document3, document4];

await vectorStore.addDocuments(documents, { ids: ["1", "2", "3", "4"] });
# Output:
#   [ 1, 2, 3, 4 ]


"""
### Delete items from vector store
"""

await vectorStore.delete({ ids: ["4"] });

"""
## Query vector store

Once your vector store has been created and the relevant documents have been added you will most likely wish to query it during the running of your chain or agent. 

### Query directly

Performing a simple similarity search can be done as follows:
"""

const filter = { source: "https://example.com" };

const similaritySearchResults = await vectorStore.similaritySearch("biology", 2, filter);

for (const doc of similaritySearchResults) {
  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);
}
# Output:
#   * The powerhouse of the cell is the mitochondria [{"source":"https://example.com"}]

#   * Mitochondria are made out of lipids [{"source":"https://example.com"}]


"""
If you want to execute a similarity search and receive the corresponding scores you can run:
"""

const similaritySearchWithScoreResults = await vectorStore.similaritySearchWithScore("biology", 2, filter)

for (const [doc, score] of similaritySearchWithScoreResults) {
  console.log(`* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`);
}
# Output:
#   * [SIM=0.165] The powerhouse of the cell is the mitochondria [{"source":"https://example.com"}]

#   * [SIM=0.148] Mitochondria are made out of lipids [{"source":"https://example.com"}]


"""
### Metadata Query Builder Filtering

You can also use query builder-style filtering similar to how the [Supabase JavaScript library](https://supabase.com/docs/reference/javascript/using-filters) works instead of passing an object. Note that since most of the filter properties are in the metadata column, you need to use arrow operators (-> for integer or ->> for text) as defined in [Postgrest API documentation](https://postgrest.org/en/stable/references/api/tables_views.html#json-columns) and specify the data type of the property (e.g. the column should look something like `metadata->some_int_prop_name::int`).
"""

import { SupabaseFilterRPCCall } from "@langchain/community/vectorstores/supabase";

const funcFilter: SupabaseFilterRPCCall = (rpc) =>
  rpc.filter("metadata->>source", "eq", "https://example.com");

const funcFilterSearchResults = await vectorStore.similaritySearch("biology", 2, funcFilter);

for (const doc of funcFilterSearchResults) {
  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);
}
# Output:
#   * The powerhouse of the cell is the mitochondria [{"source":"https://example.com"}]

#   * Mitochondria are made out of lipids [{"source":"https://example.com"}]


"""
### Query by turning into retriever

You can also transform the vector store into a [retriever](/docs/concepts/retrievers) for easier usage in your chains. 
"""

const retriever = vectorStore.asRetriever({
  // Optional filter
  filter: filter,
  k: 2,
});
await retriever.invoke("biology");
# Output:
#   [

#     Document {

#       pageContent: 'The powerhouse of the cell is the mitochondria',

#       metadata: { source: 'https://example.com' },

#       id: undefined

#     },

#     Document {

#       pageContent: 'Mitochondria are made out of lipids',

#       metadata: { source: 'https://example.com' },

#       id: undefined

#     }

#   ]


"""
### Usage for retrieval-augmented generation

For guides on how to use this vector store for retrieval-augmented generation (RAG), see the following sections:

- [Tutorials: working with external knowledge](/docs/tutorials/#working-with-external-knowledge).
- [How-to: Question and answer with RAG](/docs/how_to/#qa-with-rag)
- [Retrieval conceptual docs](/docs/concepts/retrieval)
"""

"""
## API reference

For detailed documentation of all `SupabaseVectorStore` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_vectorstores_supabase.SupabaseVectorStore.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/tigris.mdx
================================================
---
sidebar_class_name: node-only
---

import CodeBlock from "@theme/CodeBlock";

# Tigris

Tigris makes it easy to build AI applications with vector embeddings.
It is a fully managed cloud-native database that allows you store and
index documents and vector embeddings for fast and scalable vector search.

:::tip Compatibility
Only available on Node.js.
:::

## Setup

### 1. Install the Tigris SDK

Install the SDK as follows

```bash npm2yarn
npm install -S @tigrisdata/vector
```

### 2. Fetch Tigris API credentials

You can sign up for a free Tigris account [here](https://www.tigrisdata.com/).

Once you have signed up for the Tigris account, create a new project called `vectordemo`.
Next, make a note of the `clientId` and `clientSecret`, which you can get from the
Application Keys section of the project.

## Index docs

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install -S @langchain/openai
```

```typescript
import { VectorDocumentStore } from "@tigrisdata/vector";
import { Document } from "langchain/document";
import { OpenAIEmbeddings } from "@langchain/openai";
import { TigrisVectorStore } from "langchain/vectorstores/tigris";

const index = new VectorDocumentStore({
  connection: {
    serverUrl: "api.preview.tigrisdata.cloud",
    projectName: process.env.TIGRIS_PROJECT,
    clientId: process.env.TIGRIS_CLIENT_ID,
    clientSecret: process.env.TIGRIS_CLIENT_SECRET,
  },
  indexName: "examples_index",
  numDimensions: 1536, // match the OpenAI embedding size
});

const docs = [
  new Document({
    metadata: { foo: "bar" },
    pageContent: "tigris is a cloud-native vector db",
  }),
  new Document({
    metadata: { foo: "bar" },
    pageContent: "the quick brown fox jumped over the lazy dog",
  }),
  new Document({
    metadata: { baz: "qux" },
    pageContent: "lorem ipsum dolor sit amet",
  }),
  new Document({
    metadata: { baz: "qux" },
    pageContent: "tigris is a river",
  }),
];

await TigrisVectorStore.fromDocuments(docs, new OpenAIEmbeddings(), { index });
```

## Query docs

import Search from "@examples/indexes/vector_stores/tigris/search.ts";

```typescript
import { VectorDocumentStore } from "@tigrisdata/vector";
import { OpenAIEmbeddings } from "@langchain/openai";
import { TigrisVectorStore } from "langchain/vectorstores/tigris";

const index = new VectorDocumentStore({
  connection: {
    serverUrl: "api.preview.tigrisdata.cloud",
    projectName: process.env.TIGRIS_PROJECT,
    clientId: process.env.TIGRIS_CLIENT_ID,
    clientSecret: process.env.TIGRIS_CLIENT_SECRET,
  },
  indexName: "examples_index",
  numDimensions: 1536, // match the OpenAI embedding size
});

const vectorStore = await TigrisVectorStore.fromExistingIndex(
  new OpenAIEmbeddings(),
  { index }
);

/* Search the vector DB independently with metadata filters */
const results = await vectorStore.similaritySearch("tigris", 1, {
  "metadata.foo": "bar",
});
console.log(JSON.stringify(results, null, 2));
/*
[
  Document {
    pageContent: 'tigris is a cloud-native vector db',
    metadata: { foo: 'bar' }
  }
]
*/
```

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/turbopuffer.mdx
================================================
# Turbopuffer

## Setup

First you must sign up for a Turbopuffer account [here](https://turbopuffer.com/join).
Then, once you have an account you can create an API key.

Set your API key as an environment variable:

```bash
export TURBOPUFFER_API_KEY=<YOUR_API_KEY>
```

## Usage

import CodeBlock from "@theme/CodeBlock";
import SimilaritySearchExample from "@examples/indexes/vector_stores/turbopuffer.ts";

Here are some examples of how to use the class. You can filter your queries by previous specified metadata, but
keep in mind that currently only string values are supported.

See [here for more information](https://turbopuffer.com/docs/reference/query#filter-parameters) on acceptable filter formats.

<CodeBlock language="typescript">{SimilaritySearchExample}</CodeBlock>

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/typeorm.mdx
================================================
# TypeORM

To enable vector search in a generic PostgreSQL database, LangChain.js supports using [TypeORM](https://typeorm.io/) with the [`pgvector`](https://github.com/pgvector/pgvector) Postgres extension.

## Setup

To work with TypeORM, you need to install the `typeorm` and `pg` packages:

```bash npm2yarn
npm install typeorm
```

```bash npm2yarn
npm install pg
```

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

### Setup a `pgvector` self hosted instance with `docker-compose`

`pgvector` provides a prebuilt Docker image that can be used to quickly setup a self-hosted Postgres instance.
Create a file below named `docker-compose.yml`:

import CodeBlock from "@theme/CodeBlock";
import DockerExample from "@examples/indexes/vector_stores/typeorm_vectorstore/docker-compose.example.yml";

<CodeBlock language="yml" name="docker-compose.yml">
  {DockerExample}
</CodeBlock>

And then in the same directory, run `docker compose up` to start the container.

You can find more information on how to setup `pgvector` in the [official repository](https://github.com/pgvector/pgvector).

## Usage

import Example from "@examples/indexes/vector_stores/typeorm_vectorstore/typeorm.ts";

One complete example of using `TypeORMVectorStore` is the following:

<CodeBlock language="typescript">{Example}</CodeBlock>

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/typesense.mdx
================================================
# Typesense

Vector store that utilizes the Typesense search engine.

### Basic Usage

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

```typescript
import {
  Typesense,
  TypesenseConfig,
} from "@lanchain/community/vectorstores/typesense";
import { OpenAIEmbeddings } from "@langchain/openai";
import { Client } from "typesense";
import { Document } from "@langchain/core/documents";

const vectorTypesenseClient = new Client({
  nodes: [
    {
      // Ideally should come from your .env file
      host: "...",
      port: 123,
      protocol: "https",
    },
  ],
  // Ideally should come from your .env file
  apiKey: "...",
  numRetries: 3,
  connectionTimeoutSeconds: 60,
});

const typesenseVectorStoreConfig = {
  // Typesense client
  typesenseClient: vectorTypesenseClient,
  // Name of the collection to store the vectors in
  schemaName: "your_schema_name",
  // Optional column names to be used in Typesense
  columnNames: {
    // "vec" is the default name for the vector column in Typesense but you can change it to whatever you want
    vector: "vec",
    // "text" is the default name for the text column in Typesense but you can change it to whatever you want
    pageContent: "text",
    // Names of the columns that you will save in your typesense schema and need to be retrieved as metadata when searching
    metadataColumnNames: ["foo", "bar", "baz"],
  },
  // Optional search parameters to be passed to Typesense when searching
  searchParams: {
    q: "*",
    filter_by: "foo:[fooo]",
    query_by: "",
  },
  // You can override the default Typesense import function if you want to do something more complex
  // Default import function:
  // async importToTypesense<
  //   T extends Record<string, unknown> = Record<string, unknown>
  // >(data: T[], collectionName: string) {
  //   const chunkSize = 2000;
  //   for (let i = 0; i < data.length; i += chunkSize) {
  //     const chunk = data.slice(i, i + chunkSize);

  //     await this.caller.call(async () => {
  //       await this.client
  //         .collections<T>(collectionName)
  //         .documents()
  //         .import(chunk, { action: "emplace", dirty_values: "drop" });
  //     });
  //   }
  // }
  import: async (data, collectionName) => {
    await vectorTypesenseClient
      .collections(collectionName)
      .documents()
      .import(data, { action: "emplace", dirty_values: "drop" });
  },
} satisfies TypesenseConfig;

/**
 * Creates a Typesense vector store from a list of documents.
 * Will update documents if there is a document with the same id, at least with the default import function.
 * @param documents list of documents to create the vector store from
 * @returns Typesense vector store
 */
const createVectorStoreWithTypesense = async (documents: Document[] = []) =>
  Typesense.fromDocuments(
    documents,
    new OpenAIEmbeddings(),
    typesenseVectorStoreConfig
  );

/**
 * Returns a Typesense vector store from an existing index.
 * @returns Typesense vector store
 */
const getVectorStoreWithTypesense = async () =>
  new Typesense(new OpenAIEmbeddings(), typesenseVectorStoreConfig);

// Do a similarity search
const vectorStore = await getVectorStoreWithTypesense();
const documents = await vectorStore.similaritySearch("hello world");

// Add filters based on metadata with the search parameters of Typesense
// will exclude documents with author:JK Rowling, so if Joe Rowling & JK Rowling exists, only Joe Rowling will be returned
vectorStore.similaritySearch("Rowling", undefined, {
  filter_by: "author:!=JK Rowling",
});

// Delete a document
vectorStore.deleteDocuments(["document_id_1", "document_id_2"]);
```

### Constructor

Before starting, create a schema in Typesense with an id, a field for the vector and a field for the text. Add as many other fields as needed for the metadata.

- `constructor(embeddings: Embeddings, config: TypesenseConfig)`: Constructs a new instance of the `Typesense` class.
  - `embeddings`: An instance of the `Embeddings` class used for embedding documents.
  - `config`: Configuration object for the Typesense vector store.
    - `typesenseClient`: Typesense client instance.
    - `schemaName`: Name of the Typesense schema in which documents will be stored and searched.
    - `searchParams` (optional): Typesense search parameters. Default is `{ q: '*', per_page: 5, query_by: '' }`.
    - `columnNames` (optional): Column names configuration.
      - `vector` (optional): Vector column name. Default is `'vec'`.
      - `pageContent` (optional): Page content column name. Default is `'text'`.
      - `metadataColumnNames` (optional): Metadata column names. Default is an empty array `[]`.
    - `import` (optional): Replace the default import function for importing data to Typesense. This can affect the functionality of updating documents.

### Methods

- `async addDocuments(documents: Document[]): Promise<void>`: Adds documents to the vector store. The documents will be updated if there is a document with the same ID.
- `static async fromDocuments(docs: Document[], embeddings: Embeddings, config: TypesenseConfig): Promise<Typesense>`: Creates a Typesense vector store from a list of documents. Documents are added to the vector store during construction.
- `static async fromTexts(texts: string[], metadatas: object[], embeddings: Embeddings, config: TypesenseConfig): Promise<Typesense>`: Creates a Typesense vector store from a list of texts and associated metadata. Texts are converted to documents and added to the vector store during construction.
- `async similaritySearch(query: string, k?: number, filter?: Record<string, unknown>): Promise<Document[]>`: Searches for similar documents based on a query. Returns an array of similar documents.
- `async deleteDocuments(documentIds: string[]): Promise<void>`: Deletes documents from the vector store based on their IDs.

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/upstash.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Upstash Vector
---
"""

"""
# UpstashVectorStore

[Upstash Vector](https://upstash.com/) is a REST based serverless vector database, designed for working with vector embeddings.

This guide provides a quick overview for getting started with Upstash [vector stores](/docs/concepts/#vectorstores). For detailed documentation of all `UpstashVectorStore` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_vectorstores_upstash.UpstashVectorStore.html).
"""

"""
## Overview

### Integration details

| Class | Package | [PY support](https://python.langchain.com/docs/integrations/vectorstores/upstash/) |  Package latest |
| :--- | :--- | :---: | :---: |
| [`UpstashVectorStore`](https://api.js.langchain.com/classes/langchain_community_vectorstores_upstash.UpstashVectorStore.html) | [`@langchain/community`](https://npmjs.com/@langchain/community) | ✅ |  ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square&label=%20&) |
"""

"""
## Setup

To use Upstash vector stores, you'll need to create an Upstash account, create an index, and install the `@langchain/community` integration package. You'll also need to install the [`@upstash/vector`](https://www.npmjs.com/package/@upstash/vector) package as a peer dependency.

This guide will also use [OpenAI embeddings](/docs/integrations/text_embedding/openai), which require you to install the `@langchain/openai` integration package. You can also use [other supported embeddings models](/docs/integrations/text_embedding) if you wish.

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/community @langchain/core @upstash/vector @langchain/openai
</Npm2Yarn>
```

You can create an index from the [Upstash Console](https://console.upstash.com/login). For further reference, see [the official docs](https://upstash.com/docs/vector/overall/getstarted).

Upstash vector also has built in embedding support. Which means you can use it directly without the need for an additional embedding model. Check the [embedding models documentation](https://upstash.com/docs/vector/features/embeddingmodels) for more details.

```{=mdx}
:::note
To use the built-in Upstash embeddings, you'll need to select an embedding model when creating the index.
:::
```

### Credentials

Once you've set up an index, set the following environment variables:

```typescript
process.env.UPSTASH_VECTOR_REST_URL = "your-rest-url";
process.env.UPSTASH_VECTOR_REST_TOKEN = "your-rest-token";
```

If you are using OpenAI embeddings for this guide, you'll need to set your OpenAI key as well:

```typescript
process.env.OPENAI_API_KEY = "YOUR_API_KEY";
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
// process.env.LANGSMITH_TRACING="true"
// process.env.LANGSMITH_API_KEY="your-api-key"
```
"""

"""
## Instantiation

Make sure your index has the same dimension count as your embeddings. The default for OpenAI `text-embedding-3-small` is 1536.
"""

import { UpstashVectorStore } from "@langchain/community/vectorstores/upstash";
import { OpenAIEmbeddings } from "@langchain/openai";

import { Index } from "@upstash/vector";

const embeddings = new OpenAIEmbeddings({
  model: "text-embedding-3-small",
});

const indexWithCredentials = new Index({
  url: process.env.UPSTASH_VECTOR_REST_URL,
  token: process.env.UPSTASH_VECTOR_REST_TOKEN,
});

const vectorStore = new UpstashVectorStore(embeddings, {
  index: indexWithCredentials,
  // You can use namespaces to partition your data in an index
  // namespace: "test-namespace",
});

"""
## Usage with built-in embeddings

To use the built-in Upstash embeddings, you can pass a `FakeEmbeddings` instance to the `UpstashVectorStore` constructor. This will make the `UpstashVectorStore` use the built-in embeddings, which you selected when creating the index.
"""

import { UpstashVectorStore } from "@langchain/community/vectorstores/upstash";
import { FakeEmbeddings } from "@langchain/core/utils/testing";

import { Index } from "@upstash/vector";

const indexWithEmbeddings = new Index({
  url: process.env.UPSTASH_VECTOR_REST_URL,
  token: process.env.UPSTASH_VECTOR_REST_TOKEN,
});

const vectorStore = new UpstashVectorStore(new FakeEmbeddings(), {
  index: indexWithEmbeddings,
});

"""
## Manage vector store

### Add items to vector store
"""

import type { Document } from "@langchain/core/documents";

const document1: Document = {
  pageContent: "The powerhouse of the cell is the mitochondria",
  metadata: { source: "https://example.com" }
};

const document2: Document = {
  pageContent: "Buildings are made out of brick",
  metadata: { source: "https://example.com" }
};

const document3: Document = {
  pageContent: "Mitochondria are made out of lipids",
  metadata: { source: "https://example.com" }
};

const document4: Document = {
  pageContent: "The 2024 Olympics are in Paris",
  metadata: { source: "https://example.com" }
}

const documents = [document1, document2, document3, document4];

await vectorStore.addDocuments(documents, { ids: ["1", "2", "3", "4"] });
# Output:
#   [ '1', '2', '3', '4' ]


"""
**Note:** After adding documents, there may be a slight delay before they become queryable.

### Delete items from vector store
"""

await vectorStore.delete({ ids: ["4"] });

"""
## Query vector store

Once your vector store has been created and the relevant documents have been added you will most likely wish to query it during the running of your chain or agent. 

### Query directly

Performing a simple similarity search can be done as follows:
"""

const filter = "source = 'https://example.com'";

const similaritySearchResults = await vectorStore.similaritySearch("biology", 2, filter);

for (const doc of similaritySearchResults) {
  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);
}
# Output:
#   * The powerhouse of the cell is the mitochondria [{"source":"https://example.com"}]

#   * Mitochondria are made out of lipids [{"source":"https://example.com"}]


"""
See [this page](https://upstash.com/docs/vector/features/filtering) for more on Upstash Vector filter syntax.

If you want to execute a similarity search and receive the corresponding scores you can run:
"""

const similaritySearchWithScoreResults = await vectorStore.similaritySearchWithScore("biology", 2, filter)

for (const [doc, score] of similaritySearchWithScoreResults) {
  console.log(`* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`);
}
# Output:
#   * [SIM=0.576] The powerhouse of the cell is the mitochondria [{"source":"https://example.com"}]

#   * [SIM=0.557] Mitochondria are made out of lipids [{"source":"https://example.com"}]


"""
### Query by turning into retriever

You can also transform the vector store into a [retriever](/docs/concepts/retrievers) for easier usage in your chains. 
"""

const retriever = vectorStore.asRetriever({
  // Optional filter
  filter: filter,
  k: 2,
});
await retriever.invoke("biology");
# Output:
#   [

#     Document {

#       pageContent: 'The powerhouse of the cell is the mitochondria',

#       metadata: { source: 'https://example.com' },

#       id: undefined

#     },

#     Document {

#       pageContent: 'Mitochondria are made out of lipids',

#       metadata: { source: 'https://example.com' },

#       id: undefined

#     }

#   ]


"""
### Usage for retrieval-augmented generation

For guides on how to use this vector store for retrieval-augmented generation (RAG), see the following sections:

- [Tutorials: working with external knowledge](/docs/tutorials/#working-with-external-knowledge).
- [How-to: Question and answer with RAG](/docs/how_to/#qa-with-rag)
- [Retrieval conceptual docs](/docs/concepts/retrieval)
"""

"""
## API reference

For detailed documentation of all `UpstashVectorStore` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_vectorstores_upstash.UpstashVectorStore.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/usearch.mdx
================================================
---
sidebar_class_name: node-only
---

import CodeBlock from "@theme/CodeBlock";

# USearch

:::tip Compatibility
Only available on Node.js.
:::

[USearch](https://github.com/unum-cloud/usearch) is a library for efficient similarity search and clustering of dense vectors.

## Setup

Install the [usearch](https://github.com/unum-cloud/usearch/tree/main/javascript) package, which is a Node.js binding for [USearch](https://github.com/unum-cloud/usearch).

```bash npm2yarn
npm install -S usearch
```

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

## Usage

### Create a new index from texts

import ExampleTexts from "@examples/indexes/vector_stores/usearch.ts";

<CodeBlock language="typescript">{ExampleTexts}</CodeBlock>

### Create a new index from a loader

import ExampleLoader from "@examples/indexes/vector_stores/usearch_fromdocs.ts";

<CodeBlock language="typescript">{ExampleLoader}</CodeBlock>

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/vectara.mdx
================================================
---
sidebar_class_name: node-only
---

import CodeBlock from "@theme/CodeBlock";

# Vectara

Vectara is a platform for building GenAI applications. It provides an easy-to-use API for document indexing and querying that is managed by Vectara and is optimized for performance and accuracy.

You can use Vectara as a vector store with LangChain.js.

## 👉 Embeddings Included

Vectara uses its own embeddings under the hood, so you don't have to provide any yourself or call another service to obtain embeddings.

This also means that if you provide your own embeddings, they'll be a no-op.

```typescript
const store = await VectaraStore.fromTexts(
  ["hello world", "hi there"],
  [{ foo: "bar" }, { foo: "baz" }],
  // This won't have an effect. Provide a FakeEmbeddings instance instead for clarity.
  new OpenAIEmbeddings(),
  args
);
```

## Setup

You'll need to:

- Create a [free Vectara account](https://vectara.com/integrations/langchain).
- Create a [corpus](https://docs.vectara.com/docs/console-ui/creating-a-corpus) to store your data
- Create an [API key](https://docs.vectara.com/docs/common-use-cases/app-authn-authz/api-keys) with QueryService and IndexService access so you can access this corpus

Configure your `.env` file or provide args to connect LangChain to your Vectara corpus:

```
VECTARA_CUSTOMER_ID=your_customer_id
VECTARA_CORPUS_ID=your_corpus_id
VECTARA_API_KEY=your-vectara-api-key
```

Note that you can provide multiple corpus IDs separated by commas for querying multiple corpora at once. For example: `VECTARA_CORPUS_ID=3,8,9,43`.
For indexing multiple corpora, you'll need to create a separate VectaraStore instance for each corpus.

## Usage

import Example from "@examples/indexes/vector_stores/vectara.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

Note that `lambda` is a parameter related to Vectara's hybrid search capbility, providing a tradeoff between neural search and boolean/exact match as described [here](https://docs.vectara.com/docs/api-reference/search-apis/lexical-matching). We recommend the value of 0.025 as a default, while providing a way for advanced users to customize this value if needed.

## APIs

Vectara's LangChain vector store consumes Vectara's core APIs:

- [Indexing API](https://docs.vectara.com/docs/indexing-apis/indexing) for storing documents in a Vectara corpus.
- [Search API](https://docs.vectara.com/docs/search-apis/search) for querying this data. This API supports hybrid search.

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/vercel_postgres.mdx
================================================
# Vercel Postgres

LangChain.js supports using the [`@vercel/postgres`](https://www.npmjs.com/package/@vercel/postgres) package to use generic Postgres databases
as vector stores, provided they support the [`pgvector`](https://github.com/pgvector/pgvector) Postgres extension.

This integration is particularly useful from web environments like Edge functions.

## Setup

To work with Vercel Postgres, you need to install the `@vercel/postgres` package:

```bash npm2yarn
npm install @vercel/postgres
```

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community @langchain/core
```

This integration automatically connects using the connection string set under `process.env.POSTGRES_URL`.
You can also pass a connection string manually like this:

```typescript
const vectorstore = await VercelPostgres.initialize(new OpenAIEmbeddings(), {
  postgresConnectionOptions: {
    connectionString:
      "postgres://<username>:<password>@<hostname>:<port>/<dbname>",
  },
});
```

### Connecting to Vercel Postgres

A simple way to get started is to create a serverless [Vercel Postgres instance](https://vercel.com/docs/storage/vercel-postgres/quickstart).
If you're deploying to a Vercel project with an associated Vercel Postgres instance, the required `POSTGRES_URL` environment variable
will already be populated in hosted environments.

### Connecting to other databases

If you prefer to host your own Postgres instance, you can use a similar flow to LangChain's [PGVector](/docs/integrations/vectorstores/pgvector)
vectorstore integration and set the connection string either as an environment variable or as shown above.

## Usage

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/indexes/vector_stores/vercel_postgres/example.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/voy.mdx
================================================
import CodeBlock from "@theme/CodeBlock";

# Voy

[Voy](https://github.com/tantaraio/voy) is a WASM vector similarity search engine written in Rust.
It's supported in non-Node environments like browsers. You can use Voy as a vector store with LangChain.js.

### Install Voy

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai voy-search @langchain/community @langchain/core
```

## Usage

import Example from "@examples/indexes/vector_stores/voy.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/weaviate.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_label: Weaviate
---
"""

"""
# WeaviateStore

[Weaviate](https://weaviate.io/) is an open source vector database that stores both objects and vectors, allowing for combining vector search with structured filtering. LangChain connects to Weaviate via the weaviate-ts-client package, the official Typescript client for Weaviate.

This guide provides a quick overview for getting started with Weaviate [vector stores](/docs/concepts/#vectorstores). For detailed documentation of all `WeaviateStore` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_weaviate.WeaviateStore.html).
"""

"""
## Overview

### Integration details

| Class | Package | [PY support](https://python.langchain.com/docs/integrations/vectorstores/weaviate/) |  Package latest |
| :--- | :--- | :---: | :---: |
| [`WeaviateStore`](https://api.js.langchain.com/classes/langchain_weaviate.WeaviateStore.html) | [`@langchain/weaviate`](https://npmjs.com/@langchain/weaviate) | ✅ |  ![NPM - Version](https://img.shields.io/npm/v/@langchain/weaviate?style=flat-square&label=%20&) |
"""

"""
## Setup

To use Weaviate vector stores, you'll need to set up a Weaviate instance and install the `@langchain/weaviate` integration package. You should also install the `weaviate-ts-client` package to initialize a client to connect to your instance with, and the `uuid` package if you want to assign indexed documents ids.

This guide will also use [OpenAI embeddings](/docs/integrations/text_embedding/openai), which require you to install the `@langchain/openai` integration package. You can also use [other supported embeddings models](/docs/integrations/text_embedding) if you wish.

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/weaviate @langchain/core weaviate-ts-client uuid @langchain/openai
</Npm2Yarn>
```

You'll need to run Weaviate either locally or on a server. See [the Weaviate documentation](https://weaviate.io/developers/weaviate/installation) for more information.

### Credentials

Once you've set up your instance, set the following environment variables:

```typescript
// http or https
process.env.WEAVIATE_SCHEME = "";
// If running locally, include port e.g. "localhost:8080"
process.env.WEAVIATE_HOST = "YOUR_HOSTNAME";
// Optional, for cloud deployments
process.env.WEAVIATE_API_KEY = "YOUR_API_KEY";
```

If you are using OpenAI embeddings for this guide, you'll need to set your OpenAI key as well:

```typescript
process.env.OPENAI_API_KEY = "YOUR_API_KEY";
```

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```typescript
// process.env.LANGSMITH_TRACING="true"
// process.env.LANGSMITH_API_KEY="your-api-key"
```
"""

"""
## Instantiation
"""

import { WeaviateStore } from "@langchain/weaviate";
import { OpenAIEmbeddings } from "@langchain/openai";

import weaviate from "weaviate-ts-client";
// import { ApiKey } from "weaviate-ts-client"

const embeddings = new OpenAIEmbeddings({
  model: "text-embedding-3-small",
});

// The Weaviate SDK has an issue with types
const weaviateClient = (weaviate as any).client({
  scheme: process.env.WEAVIATE_SCHEME ?? "http",
  host: process.env.WEAVIATE_HOST ?? "localhost",
  // If necessary
  // apiKey: new ApiKey(process.env.WEAVIATE_API_KEY ?? "default"),
});

const vectorStore = new WeaviateStore(embeddings, {
  client: weaviateClient,
  // Must start with a capital letter
  indexName: "Langchainjs_test",
  // Default value
  textKey: "text",
  // Any keys you intend to set as metadata
  metadataKeys: ["source"],
});

"""
## Manage vector store

### Add items to vector store

**Note:** If you want to associate ids with your indexed documents, they must be UUIDs.
"""

import type { Document } from "@langchain/core/documents";
import { v4 as uuidv4 } from "uuid";

const document1: Document = {
  pageContent: "The powerhouse of the cell is the mitochondria",
  metadata: { source: "https://example.com" }
};

const document2: Document = {
  pageContent: "Buildings are made out of brick",
  metadata: { source: "https://example.com" }
};

const document3: Document = {
  pageContent: "Mitochondria are made out of lipids",
  metadata: { source: "https://example.com" }
};

const document4: Document = {
  pageContent: "The 2024 Olympics are in Paris",
  metadata: { source: "https://example.com" }
}

const documents = [document1, document2, document3, document4];
const uuids = [uuidv4(), uuidv4(), uuidv4(), uuidv4()];

await vectorStore.addDocuments(documents, { ids: uuids });
# Output:
#   [

#     '610f9b92-9bee-473f-a4db-8f2ca6e3442d',

#     '995160fa-441e-41a0-b476-cf3785518a0d',

#     '0cdbe6d4-0df8-4f99-9b67-184009fee9a2',

#     '18a8211c-0649-467b-a7c5-50ebb4b9ca9d'

#   ]


"""
### Delete items from vector store

You can delete by id as by passing a `filter` param:
"""

await vectorStore.delete({ ids: [uuids[3]] });

"""
## Query vector store

Once your vector store has been created and the relevant documents have been added you will most likely wish to query it during the running of your chain or agent. 

### Query directly

Performing a simple similarity search can be done as follows:
"""

const filter = {
  where: {
    operator: "Equal" as const,
    path: ["source"],
    valueText: "https://example.com",
  }
};

const similaritySearchResults = await vectorStore.similaritySearch("biology", 2, filter);

for (const doc of similaritySearchResults) {
  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);
}
# Output:
#   * The powerhouse of the cell is the mitochondria [{"source":"https://example.com"}]

#   * Mitochondria are made out of lipids [{"source":"https://example.com"}]


"""
See [this page](https://weaviate.io/developers/weaviate/api/graphql/filters) for more on Weaviat filter syntax.

If you want to execute a similarity search and receive the corresponding scores you can run:
"""

const similaritySearchWithScoreResults = await vectorStore.similaritySearchWithScore("biology", 2, filter)

for (const [doc, score] of similaritySearchWithScoreResults) {
  console.log(`* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`);
}
# Output:
#   * [SIM=0.835] The powerhouse of the cell is the mitochondria [{"source":"https://example.com"}]

#   * [SIM=0.852] Mitochondria are made out of lipids [{"source":"https://example.com"}]


"""
### Query by turning into retriever

You can also transform the vector store into a [retriever](/docs/concepts/retrievers) for easier usage in your chains. 
"""

const retriever = vectorStore.asRetriever({
  // Optional filter
  filter: filter,
  k: 2,
});
await retriever.invoke("biology");
# Output:
#   [

#     Document {

#       pageContent: 'The powerhouse of the cell is the mitochondria',

#       metadata: { source: 'https://example.com' },

#       id: undefined

#     },

#     Document {

#       pageContent: 'Mitochondria are made out of lipids',

#       metadata: { source: 'https://example.com' },

#       id: undefined

#     }

#   ]


"""
### Usage for retrieval-augmented generation

For guides on how to use this vector store for retrieval-augmented generation (RAG), see the following sections:

- [Tutorials: working with external knowledge](/docs/tutorials/#working-with-external-knowledge).
- [How-to: Question and answer with RAG](/docs/how_to/#qa-with-rag)
- [Retrieval conceptual docs](/docs/concepts/retrieval)
"""

"""
## API reference

For detailed documentation of all `WeaviateStore` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_weaviate.WeaviateStore.html).
"""



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/xata.mdx
================================================
# Xata

[Xata](https://xata.io) is a serverless data platform, based on PostgreSQL. It provides a type-safe TypeScript/JavaScript SDK for interacting with your database, and a UI for managing your data.

Xata has a native vector type, which can be added to any table, and supports similarity search. LangChain inserts vectors directly to Xata, and queries it for the nearest neighbors of a given vector, so that you can use all the LangChain Embeddings integrations with Xata.

## Setup

### Install the Xata CLI

```bash
npm install @xata.io/cli -g
```

### Create a database to be used as a vector store

In the [Xata UI](https://app.xata.io) create a new database. You can name it whatever you want, but for this example we'll use `langchain`.
Create a table, again you can name it anything, but we will use `vectors`. Add the following columns via the UI:

- `content` of type "Text". This is used to store the `Document.pageContent` values.
- `embedding` of type "Vector". Use the dimension used by the model you plan to use (1536 for OpenAI).
- any other columns you want to use as metadata. They are populated from the `Document.metadata` object. For example, if in the `Document.metadata` object you have a `title` property, you can create a `title` column in the table and it will be populated.

### Initialize the project

In your project, run:

```bash
xata init
```

and then choose the database you created above. This will also generate a `xata.ts` or `xata.js` file that defines the client you can use to interact with the database. See the [Xata getting started docs](https://xata.io/docs/getting-started/installation) for more details on using the Xata JavaScript/TypeScript SDK.

## Usage

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

import CodeBlock from "@theme/CodeBlock";

### Example: Q&A chatbot using OpenAI and Xata as vector store

This example uses the `VectorDBQAChain` to search the documents stored in Xata and then pass them as context to the OpenAI model, in order to answer the question asked by the user.

import FromDocs from "@examples/indexes/vector_stores/xata.ts";

<CodeBlock language="typescript">{FromDocs}</CodeBlock>

### Example: Similarity search with a metadata filter

This example shows how to implement semantic search using LangChain.js and Xata. Before running it, make sure to add an `author` column of type String to the `vectors` table in Xata.

import SimSearch from "@examples/indexes/vector_stores/xata_metadata.ts";

<CodeBlock language="typescript">{SimSearch}</CodeBlock>

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/zep.mdx
================================================
# Zep Open Source

> [Zep](https://www.getzep.com) is a long-term memory service for AI Assistant apps.
> With Zep, you can provide AI assistants with the ability to recall past conversations, no matter how distant,
> while also reducing hallucinations, latency, and cost.

> Interested in Zep Cloud? See [Zep Cloud Installation Guide](https://help.getzep.com/sdks)

import CodeBlock from "@theme/CodeBlock";

**Note:** The `ZepVectorStore` works with `Documents` and is intended to be used as a `Retriever`.
It offers separate functionality to Zep's `ZepMemory` class, which is designed for persisting, enriching
and searching your user's chat history.

## Why Zep's VectorStore? 🤖🚀

Zep automatically embeds documents added to the Zep Vector Store using low-latency models local to the Zep server.
The Zep TS/JS client can be used in non-Node edge environments. These two together with Zep's chat memory functionality
make Zep ideal for building conversational LLM apps where latency and performance are important.

### Supported Search Types

Zep supports both similarity search and Maximal Marginal Relevance (MMR) search. MMR search is particularly useful
for Retrieval Augmented Generation applications as it re-ranks results to ensure diversity in the returned documents.

## Installation

Follow the [Zep Open Source Quickstart Guide](https://docs.getzep.com/deployment/quickstart/) to install and get started with Zep.

## Usage

You'll need your Zep API URL and optionally an API key to use the Zep VectorStore. See the [Zep docs](https://docs.getzep.com) for more information.

In the examples below, we're using Zep's auto-embedding feature which automatically embed documents on the Zep server using
low-latency embedding models. Since LangChain requires passing in a `Embeddings` instance, we pass in `FakeEmbeddings`.

**Note:** If you pass in an `Embeddings` instance other than `FakeEmbeddings`, this class will be used to embed documents.
You must also set your document collection to `isAutoEmbedded === false`. See the `OpenAIEmbeddings` example below.

### Example: Creating a ZepVectorStore from Documents & Querying

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai @langchain/community @langchain/core
```

import ExampleDocs from "@examples/indexes/vector_stores/zep/zep_from_docs.ts";

<CodeBlock language="typescript">{ExampleDocs}</CodeBlock>

### Example: Querying a ZepVectorStore using a metadata filter

import ExampleMetadata from "@examples/indexes/vector_stores/zep/zep_with_metadata.ts";

<CodeBlock language="typescript">{ExampleMetadata}</CodeBlock>

### Example: Using a LangChain Embedding Class such as `OpenAIEmbeddings`

import ExampleOpenAI from "@examples/indexes/vector_stores/zep/zep_with_openai_embeddings.ts";

<CodeBlock language="typescript">{ExampleOpenAI}</CodeBlock>

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/integrations/vectorstores/zep_cloud.mdx
================================================
# Zep Cloud

> [Zep](https://www.getzep.com) is a long-term memory service for AI Assistant apps.
> With Zep, you can provide AI assistants with the ability to recall past conversations, no matter how distant,
> while also reducing hallucinations, latency, and cost.

import CodeBlock from "@theme/CodeBlock";

**Note:** The `ZepCloudVectorStore` works with `Documents` and is intended to be used as a `Retriever`.
It offers separate functionality to Zep's `ZepCloudMemory` class, which is designed for persisting, enriching
and searching your user's chat history.

## Why Zep's VectorStore? 🤖🚀

Zep automatically embeds documents added to the Zep Vector Store using low-latency models local to the Zep server.
The Zep TS/JS client can be used in non-Node edge environments. These two together with Zep's chat memory functionality
make Zep ideal for building conversational LLM apps where latency and performance are important.

### Supported Search Types

Zep supports both similarity search and Maximal Marginal Relevance (MMR) search. MMR search is particularly useful
for Retrieval Augmented Generation applications as it re-ranks results to ensure diversity in the returned documents.

## Installation

Sign up for [Zep Cloud](https://app.getzep.com/) and create a project.

Follow the [Zep Cloud Typescript SDK Installation Guide](https://help.getzep.com/sdks) to install and get started with Zep.

## Usage

You'll need your Zep Cloud Project API Key to use the Zep VectorStore. See the [Zep Cloud docs](https://help.getzep.com/projects) for more information.

Zep auto embeds all documents by default, and it's not expecting to receive any embeddings from the user.
Since LangChain requires passing in a `Embeddings` instance, we pass in `FakeEmbeddings`.

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

### Example: Creating a ZepVectorStore from Documents & Querying

```bash npm2yarn
npm install @getzep/zep-cloud @langchain/openai @langchain/community @langchain/core
```

import ZepCloudVectorStoreExample from "@examples/indexes/vector_stores/zep/zep_cloud.ts";

<CodeBlock language="typescript">{ZepCloudVectorStoreExample}</CodeBlock>

### Example: Using ZepCloudVectorStore with Expression Language

import ZepCloudVectorStoreExpressionLanguageExample from "@examples/guides/expression_language/zep/zep_cloud_vector_store.ts";

<CodeBlock language="typescript">
  {ZepCloudVectorStoreExpressionLanguageExample}
</CodeBlock>

## Related

- Vector store [conceptual guide](/docs/concepts/#vectorstores)
- Vector store [how-to guides](/docs/how_to/#vectorstores)



================================================
FILE: docs/core_docs/docs/mdx_components/integration_install_tooltip.mdx
================================================
:::tip
See [this section for general instructions on installing integration packages](/docs/how_to/installation#installing-integration-packages).
:::



================================================
FILE: docs/core_docs/docs/mdx_components/unified_model_params_tooltip.mdx
================================================
:::tip
We're unifying model params across all packages. We now suggest using `model` instead of `modelName`, and `apiKey` for API keys.
:::



================================================
FILE: docs/core_docs/docs/troubleshooting/errors/index.mdx
================================================
# Error reference

This page contains guides around resolving common errors you may find while building with LangChain.
Errors referenced below will have an `lc_error_code` property corresponding to one of the below codes when they are thrown in code.

- [INVALID_PROMPT_INPUT](/docs/troubleshooting/errors/INVALID_PROMPT_INPUT)
- [INVALID_TOOL_RESULTS](/docs/troubleshooting/errors/INVALID_TOOL_RESULTS)
- [MESSAGE_COERCION_FAILURE](/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE)
- [MODEL_AUTHENTICATION](/docs/troubleshooting/errors/MODEL_AUTHENTICATION)
- [MODEL_NOT_FOUND](/docs/troubleshooting/errors/MODEL_NOT_FOUND)
- [MODEL_RATE_LIMIT](/docs/troubleshooting/errors/MODEL_RATE_LIMIT)
- [OUTPUT_PARSING_FAILURE](/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE)



================================================
FILE: docs/core_docs/docs/troubleshooting/errors/INVALID_PROMPT_INPUT.mdx
================================================
# INVALID_PROMPT_INPUT

A [prompt template](/docs/concepts/prompt_templates) received missing or invalid input variables.

One unexpected way this can occur is if you add a JSON object directly into a prompt template:

```ts
import { PromptTemplate } from "@langchain/core/prompts";
import { ChatOpenAI } from "@langchain/openai";

const prompt = PromptTemplate.fromTemplate(`You are a helpful assistant.

Here is an example of how you should respond:

{
  "firstName": "John",
  "lastName": "Doe",
  "age": 21
}

Now, answer the following question:

{question}`);
```

You might think that the above prompt template should require a single input key named `question`, but the JSON object will be
interpreted as an additional variable because the curly braces (`{`) are not escaped, and should be preceded by a second brace instead, like this:

```ts
import { PromptTemplate } from "@langchain/core/prompts";
import { ChatOpenAI } from "@langchain/openai";

const prompt = PromptTemplate.fromTemplate(`You are a helpful assistant.

Here is an example of how you should respond:

{{
  "firstName": "John",
  "lastName": "Doe",
  "age": 21
}}

Now, answer the following question:

{question}`);
```

## Troubleshooting

The following may help resolve this error:

- Double-check your prompt template to ensure that it is correct.
  - If you are using default formatting and you are using curly braces `{` anywhere in your template, they should be double escaped like this: `{{`, as shown above.
- If you are using a [`MessagesPlaceholder`](/docs/concepts/prompt_templates/#messagesplaceholder), make sure that you are passing in an array of messages or message-like objects.
  - If you are using shorthand tuples to declare your prompt template, make sure that the variable name is wrapped in curly braces (`["placeholder", "{messages}"]`).
- Try viewing the inputs into your prompt template using [LangSmith](https://docs.smith.langchain.com/) or log statements to confirm they appear as expected.
- If you are pulling a prompt from the [LangChain Prompt Hub](https://smith.langchain.com/prompts), try pulling and logging it or running it in isolation with a sample input to confirm that it is what you expect.



================================================
FILE: docs/core_docs/docs/troubleshooting/errors/INVALID_TOOL_RESULTS.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# INVALID_TOOL_RESULTS

You are passing too many, too few, or mismatched [`ToolMessages`](https://api.js.langchain.com/classes/_langchain_core.messages_tool.ToolMessage.html) to a model.

When [using a model to call tools](/docs/concepts/tool_calling), the [`AIMessage`](https://api.js.langchain.com/classes/_langchain_core.messages.AIMessage.html)
the model responds with will contain a `tool_calls` array. To continue the flow, the next messages you pass back to the model must
be exactly one `ToolMessage` for each item in that array containing the result of that tool call. Each `ToolMessage` must have a `tool_call_id` field
that matches one of the `tool_calls` on the `AIMessage`.

For example, given the following response from a model:
"""

import { z } from "zod";
import { tool } from "@langchain/core/tools";
import { ChatOpenAI } from "@langchain/openai";
import { BaseMessageLike } from "@langchain/core/messages";

const model = new ChatOpenAI({
  model: "gpt-4o-mini",
});

const dummyTool = tool(
  async () => {
    return "action complete!";
  },
  {
    name: "foo",
    schema: z.object({}),
  }
);

const modelWithTools = model.bindTools([dummyTool]);

const chatHistory: BaseMessageLike[] = [
  {
    role: "user",
    content: `Call tool "foo" twice with no arguments`,
  },
];

const responseMessage = await modelWithTools.invoke(chatHistory);

console.log(responseMessage);
# Output:
#   AIMessage {

#     "id": "chatcmpl-AIgT1xUd6lkWAutThiiBsqjq7Ykj1",

#     "content": "",

#     "additional_kwargs": {

#       "tool_calls": [

#         {

#           "id": "call_BknYpnY7xiARM17TPYqL7luj",

#           "type": "function",

#           "function": "[Object]"

#         },

#         {

#           "id": "call_EHf8MIcTdsLCZcFVlcH4hxJw",

#           "type": "function",

#           "function": "[Object]"

#         }

#       ]

#     },

#     "response_metadata": {

#       "tokenUsage": {

#         "promptTokens": 42,

#         "completionTokens": 37,

#         "totalTokens": 79

#       },

#       "finish_reason": "tool_calls",

#       "usage": {

#         "prompt_tokens": 42,

#         "completion_tokens": 37,

#         "total_tokens": 79,

#         "prompt_tokens_details": {

#           "cached_tokens": 0

#         },

#         "completion_tokens_details": {

#           "reasoning_tokens": 0

#         }

#       },

#       "system_fingerprint": "fp_e2bde53e6e"

#     },

#     "tool_calls": [

#       {

#         "name": "foo",

#         "args": {},

#         "type": "tool_call",

#         "id": "call_BknYpnY7xiARM17TPYqL7luj"

#       },

#       {

#         "name": "foo",

#         "args": {},

#         "type": "tool_call",

#         "id": "call_EHf8MIcTdsLCZcFVlcH4hxJw"

#       }

#     ],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "output_tokens": 37,

#       "input_tokens": 42,

#       "total_tokens": 79,

#       "input_token_details": {

#         "cache_read": 0

#       },

#       "output_token_details": {

#         "reasoning": 0

#       }

#     }

#   }


"""
Calling the model with only one tool response would result in an error:
"""

const toolResponse1 = await dummyTool.invoke(responseMessage.tool_calls![0]);

chatHistory.push(responseMessage);
chatHistory.push(toolResponse1);

await modelWithTools.invoke(chatHistory);
# Output:
#   BadRequestError: 400 An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_EHf8MIcTdsLCZcFVlcH4hxJw

#       at APIError.generate (/Users/jacoblee/langchain/langchainjs/libs/langchain-openai/node_modules/openai/error.js:45:20)

#       at OpenAI.makeStatusError (/Users/jacoblee/langchain/langchainjs/libs/langchain-openai/node_modules/openai/core.js:291:33)

#       at OpenAI.makeRequest (/Users/jacoblee/langchain/langchainjs/libs/langchain-openai/node_modules/openai/core.js:335:30)

#       at process.processTicksAndRejections (node:internal/process/task_queues:95:5)

#       at async /Users/jacoblee/langchain/langchainjs/libs/langchain-openai/dist/chat_models.cjs:1441:29

#       at async RetryOperation._fn (/Users/jacoblee/langchain/langchainjs/node_modules/p-retry/index.js:50:12) {

#     status: 400,

#     headers: {

#       'access-control-expose-headers': 'X-Request-ID',

#       'alt-svc': 'h3=":443"; ma=86400',

#       'cf-cache-status': 'DYNAMIC',

#       'cf-ray': '8d31d4d95e2a0c96-EWR',

#       connection: 'keep-alive',

#       'content-length': '315',

#       'content-type': 'application/json',

#       date: 'Tue, 15 Oct 2024 18:21:53 GMT',

#       'openai-organization': 'langchain',

#       'openai-processing-ms': '16',

#       'openai-version': '2020-10-01',

#       server: 'cloudflare',

#       'set-cookie': '__cf_bm=e5.GX1bHiMVgr76YSvAKuECCGG7X_RXF0jDGSMXFGfU-1729016513-1.0.1.1-ZBYeVqX.M6jSNJB.wS696fEhX7V.es._M0WcWtQ9Qx8doEA5qMVKNE5iX6i7UKyPCg2GvDfM.MoDwRCXKMSkEA; path=/; expires=Tue, 15-Oct-24 18:51:53 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=J8gS08GodUA9hRTYuElen0YOCzMO3d4LW0ZT0k_kyj4-1729016513560-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',

#       'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',

#       'x-content-type-options': 'nosniff',

#       'x-ratelimit-limit-requests': '30000',

#       'x-ratelimit-limit-tokens': '150000000',

#       'x-ratelimit-remaining-requests': '29999',

#       'x-ratelimit-remaining-tokens': '149999967',

#       'x-ratelimit-reset-requests': '2ms',

#       'x-ratelimit-reset-tokens': '0s',

#       'x-request-id': 'req_f810058e7f047fafcb713575c4419161'

#     },

#     request_id: 'req_f810058e7f047fafcb713575c4419161',

#     error: {

#       message: "An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_EHf8MIcTdsLCZcFVlcH4hxJw",

#       type: 'invalid_request_error',

#       param: 'messages',

#       code: null

#     },

#     code: null,

#     param: 'messages',

#     type: 'invalid_request_error',

#     attemptNumber: 1,

#     retriesLeft: 6

#   }


"""
If we add a second response, the call will succeed as expected because we now have one tool response per tool call:
"""

const toolResponse2 = await dummyTool.invoke(responseMessage.tool_calls![1]);

chatHistory.push(toolResponse2);

await modelWithTools.invoke(chatHistory);
# Output:
#   AIMessage {

#     "id": "chatcmpl-AIgTPDBm1epnnLHx0tPFTgpsf8Ay6",

#     "content": "The tool \"foo\" was called twice, and both times returned the result: \"action complete!\".",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "promptTokens": 98,

#         "completionTokens": 21,

#         "totalTokens": 119

#       },

#       "finish_reason": "stop",

#       "usage": {

#         "prompt_tokens": 98,

#         "completion_tokens": 21,

#         "total_tokens": 119,

#         "prompt_tokens_details": {

#           "cached_tokens": 0

#         },

#         "completion_tokens_details": {

#           "reasoning_tokens": 0

#         }

#       },

#       "system_fingerprint": "fp_e2bde53e6e"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "output_tokens": 21,

#       "input_tokens": 98,

#       "total_tokens": 119,

#       "input_token_details": {

#         "cache_read": 0

#       },

#       "output_token_details": {

#         "reasoning": 0

#       }

#     }

#   }


"""
But if we add a duplicate, extra tool response, the call will fail again:
"""

const duplicateToolResponse2 = await dummyTool.invoke(responseMessage.tool_calls![1]);

chatHistory.push(duplicateToolResponse2);

await modelWithTools.invoke(chatHistory);
# Output:
#   BadRequestError: 400 Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.

#       at APIError.generate (/Users/jacoblee/langchain/langchainjs/libs/langchain-openai/node_modules/openai/error.js:45:20)

#       at OpenAI.makeStatusError (/Users/jacoblee/langchain/langchainjs/libs/langchain-openai/node_modules/openai/core.js:291:33)

#       at OpenAI.makeRequest (/Users/jacoblee/langchain/langchainjs/libs/langchain-openai/node_modules/openai/core.js:335:30)

#       at process.processTicksAndRejections (node:internal/process/task_queues:95:5)

#       at async /Users/jacoblee/langchain/langchainjs/libs/langchain-openai/dist/chat_models.cjs:1441:29

#       at async RetryOperation._fn (/Users/jacoblee/langchain/langchainjs/node_modules/p-retry/index.js:50:12) {

#     status: 400,

#     headers: {

#       'access-control-expose-headers': 'X-Request-ID',

#       'alt-svc': 'h3=":443"; ma=86400',

#       'cf-cache-status': 'DYNAMIC',

#       'cf-ray': '8d31d57dff5e0f3b-EWR',

#       connection: 'keep-alive',

#       'content-length': '233',

#       'content-type': 'application/json',

#       date: 'Tue, 15 Oct 2024 18:22:19 GMT',

#       'openai-organization': 'langchain',

#       'openai-processing-ms': '36',

#       'openai-version': '2020-10-01',

#       server: 'cloudflare',

#       'set-cookie': '__cf_bm=QUsNlSGxVeIbscI0rm2YR3U9aUFLNxxqh1i_3aYBGN4-1729016539-1.0.1.1-sKRUvxHkQXvlb5LaqASkGtIwPMWUF5x9kF0ut8NLP6e0FVKEhdIEkEe6lYA1toW45JGTwp98xahaX7wt9CO4AA; path=/; expires=Tue, 15-Oct-24 18:52:19 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=J6fN8u8HUieCeyLDI59mi_0r_W0DgiO207wEtvrmT9Y-1729016539919-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',

#       'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',

#       'x-content-type-options': 'nosniff',

#       'x-ratelimit-limit-requests': '30000',

#       'x-ratelimit-limit-tokens': '150000000',

#       'x-ratelimit-remaining-requests': '29999',

#       'x-ratelimit-remaining-tokens': '149999956',

#       'x-ratelimit-reset-requests': '2ms',

#       'x-ratelimit-reset-tokens': '0s',

#       'x-request-id': 'req_aebfebbb9af2feaf2e9683948e431676'

#     },

#     request_id: 'req_aebfebbb9af2feaf2e9683948e431676',

#     error: {

#       message: "Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.",

#       type: 'invalid_request_error',

#       param: 'messages.[4].role',

#       code: null

#     },

#     code: null,

#     param: 'messages.[4].role',

#     type: 'invalid_request_error',

#     attemptNumber: 1,

#     retriesLeft: 6

#   }


"""
You should additionally not pass `ToolMessages` back to to a model if they are not preceded by an `AIMessage` with tool calls. For example, this will fail:
"""

await modelWithTools.invoke([{
  role: "tool",
  content: "action completed!",
  tool_call_id: "dummy",
}])
# Output:
#   BadRequestError: 400 Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.

#       at APIError.generate (/Users/jacoblee/langchain/langchainjs/libs/langchain-openai/node_modules/openai/error.js:45:20)

#       at OpenAI.makeStatusError (/Users/jacoblee/langchain/langchainjs/libs/langchain-openai/node_modules/openai/core.js:291:33)

#       at OpenAI.makeRequest (/Users/jacoblee/langchain/langchainjs/libs/langchain-openai/node_modules/openai/core.js:335:30)

#       at process.processTicksAndRejections (node:internal/process/task_queues:95:5)

#       at async /Users/jacoblee/langchain/langchainjs/libs/langchain-openai/dist/chat_models.cjs:1441:29

#       at async RetryOperation._fn (/Users/jacoblee/langchain/langchainjs/node_modules/p-retry/index.js:50:12) {

#     status: 400,

#     headers: {

#       'access-control-expose-headers': 'X-Request-ID',

#       'alt-svc': 'h3=":443"; ma=86400',

#       'cf-cache-status': 'DYNAMIC',

#       'cf-ray': '8d31d5da7fba19aa-EWR',

#       connection: 'keep-alive',

#       'content-length': '233',

#       'content-type': 'application/json',

#       date: 'Tue, 15 Oct 2024 18:22:34 GMT',

#       'openai-organization': 'langchain',

#       'openai-processing-ms': '25',

#       'openai-version': '2020-10-01',

#       server: 'cloudflare',

#       'set-cookie': '__cf_bm=qK6.PWACr7IYuMafLpxumD4CrFnwHQiJn4TiGkrNTBk-1729016554-1.0.1.1-ECIk0cvh1wOfsK41a1Ce7npngsUDRRG93_yinP4.kVIWu1eX0CFG19iZ8yfGXedyPo6Wh1CKTGLk_3Qwrg.blA; path=/; expires=Tue, 15-Oct-24 18:52:34 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=IVTqysqHo4VUVJ.tVTcGg0rnXGWTbSSzX5mcUVrw8BU-1729016554732-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',

#       'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',

#       'x-content-type-options': 'nosniff',

#       'x-ratelimit-limit-requests': '30000',

#       'x-ratelimit-limit-tokens': '150000000',

#       'x-ratelimit-remaining-requests': '29999',

#       'x-ratelimit-remaining-tokens': '149999978',

#       'x-ratelimit-reset-requests': '2ms',

#       'x-ratelimit-reset-tokens': '0s',

#       'x-request-id': 'req_59339f8163ef5bd3f0308a212611dfea'

#     },

#     request_id: 'req_59339f8163ef5bd3f0308a212611dfea',

#     error: {

#       message: "Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.",

#       type: 'invalid_request_error',

#       param: 'messages.[0].role',

#       code: null

#     },

#     code: null,

#     param: 'messages.[0].role',

#     type: 'invalid_request_error',

#     attemptNumber: 1,

#     retriesLeft: 6

#   }


"""
See [this guide](/docs/how_to/tool_results_pass_to_model/) for more details on tool calling.

## Troubleshooting

The following may help resolve this error:

- If you are using a custom executor rather than a prebuilt one like LangGraph's [`ToolNode`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph_prebuilt.ToolNode.html)
  or the legacy LangChain [AgentExecutor](/docs/how_to/agent_executor), verify that you are invoking and returning the result for one tool per tool call.
- If you are using [few-shot tool call examples](/docs/how_to/tools_few_shot) with messages that you manually create, and you want to simulate a failure,
  you still need to pass back a `ToolMessage` whose content indicates that failure.

"""



================================================
FILE: docs/core_docs/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE.mdx
================================================
# MESSAGE_COERCION_FAILURE

Several modules in LangChain take [`MessageLike`](https://api.js.langchain.com/types/_langchain_core.messages.BaseMessageLike.html)
objects in place of formal [`BaseMessage`](/docs/concepts/messages) classes. These include OpenAI style message objects (`{ role: "user", content: "Hello world!" }`),
tuples, and plain strings (which are converted to [`HumanMessages`](/docs/concepts/messages/#humanmessage)).

If one of these modules receives a value outside of one of these formats, you will receive an error like the following:

```ts
const badlyFormattedMessageObject = {
  role: "foo",
  randomNonContentValue: "bar",
};

await model.invoke([badlyFormattedMessageObject]);
```

```
Error: Unable to coerce message from array: only human, AI, system, or tool message coercion is currently supported.

Received: {
  "role": "foo",
  "randomNonContentValue": "bar",
}
```

## Troubleshooting

The following may help resolve this error:

- Ensure that all inputs to chat models are an array of LangChain message classes or a supported message-like.
  - Check that there is no stringification or other unexpected transformation occuring.
- Check the error's stack trace and add log or debugger statements.



================================================
FILE: docs/core_docs/docs/troubleshooting/errors/MODEL_AUTHENTICATION.mdx
================================================
# MODEL_AUTHENTICATION

Your model provider is denying you access to their service.

## Troubleshooting

The following may help resolve this error:

- Confirm that your API key or other credentials are correct.
- If you are relying on an environment variable to authenticate, confirm that the variable name is correct and that it has a value set.
  - Note that some environments, like Cloudflare Workers, do not support environment variables.
  - For some models, you can try explicitly passing an `apiKey` parameter to rule out any environment variable issues like this:

```ts
const model = new ChatOpenAI({
  apiKey: "YOUR_KEY_HERE",
});
```

- If you are using a proxy or other custom endpoint, make sure that your custom provider does not expect an alternative authentication scheme.



================================================
FILE: docs/core_docs/docs/troubleshooting/errors/MODEL_NOT_FOUND.mdx
================================================
# MODEL_NOT_FOUND

The model name you have specified is not acknowledged by your provider.

## Troubleshooting

The following may help resolve this error:

- Double check the model string you are passing in.
- If you are using a proxy or other alternative host with a model wrapper, confirm that the permitted model names are not restricted or altered.



================================================
FILE: docs/core_docs/docs/troubleshooting/errors/MODEL_RATE_LIMIT.mdx
================================================
# MODEL_RATE_LIMIT

You have hit the maximum number of requests that a model provider allows over a given time period and are being temporarily blocked.
Generally, this error is temporary and your limit will reset after a certain amount of time.

## Troubleshooting

The following may help resolve this error:

- Contact your model provider and ask for a rate limit increase.
- If many of your incoming requests are the same, utilize [model response caching](/docs/how_to/chat_model_caching/).
- Spread requests across different providers if your application allows it.
- Set a higher number of [max retries](https://api.js.langchain.com/interfaces/_langchain_core.language_models_base.BaseLanguageModelParams.html#maxRetries) when initializing your model.
  LangChain will use an exponential backoff strategy for requests that fail in this way, so the retry may occur when your limits have reset.



================================================
FILE: docs/core_docs/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE.mdx
================================================
# OUTPUT_PARSING_FAILURE

An [output parser](/docs/concepts/output_parsers) was unable to handle model output as expected.

To illustrate this, let's say you have an output parser that expects a chat model to output JSON surrounded by a markdown code tag (triple backticks). Here would be an example of good input:

````ts
AIMessage {
  content: "```\n{\"foo\": \"bar\"}\n```"
}
````

Internally, our output parser might try to strip out the markdown fence and newlines and then run `JSON.parse()`.

If instead the chat model generated an output with malformed JSON like this:

````ts
AIMessage {
  content: "```\n{\"foo\":\n```"
}
````

When our output parser attempts to parse this, the `JSON.parse()` call will fail.

Note that some prebuilt constructs like [legacy LangChain agents](/docs/how_to/agent_executor) and chains may use output parsers internally,
so you may see this error even if you're not visibly instantiating and using an output parser.

## Troubleshooting

The following may help resolve this error:

- Consider using [tool calling or other structured output techniques](/docs/how_to/structured_output/) if possible without an output parser to reliably output parseable values.
  - If you are using a prebuilt chain or agent, use [LangGraph](https://langchain-ai.github.io/langgraphjs/) to compose your logic explicitly instead.
- Add more precise formatting instructions to your prompt. In the above example, adding `"You must always return valid JSON fenced by a markdown code block. Do not return any additional text."` to your input may help steer the model to returning the expected format.
- If you are using a smaller or less capable model, try using a more capable one.
- Add [LLM-powered retries](/docs/how_to/output_parser_fixing/).



================================================
FILE: docs/core_docs/docs/tutorials/chatbot.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_position: 1
keywords: [conversationchain]
---
"""

"""
# Build a Chatbot


:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat Models](/docs/concepts/chat_models)
- [Prompt Templates](/docs/concepts/prompt_templates)
- [Chat History](/docs/concepts/chat_history)

This guide requires `langgraph >= 0.2.28`.

:::


```{=mdx}

:::note

This tutorial previously built a chatbot using [RunnableWithMessageHistory](https://api.js.langchain.com/classes/_langchain_core.runnables.RunnableWithMessageHistory.html). You can access this version of the tutorial in the [v0.2 docs](https://js.langchain.com/v0.2/docs/tutorials/chatbot/).

The LangGraph implementation offers a number of advantages over `RunnableWithMessageHistory`, including the ability to persist arbitrary components of an application's state (instead of only messages).

:::

```

## Overview

We'll go over an example of how to design and implement an LLM-powered chatbot. 
This chatbot will be able to have a conversation and remember previous interactions.


Note that this chatbot that we build will only use the language model to have a conversation.
There are several other related concepts that you may be looking for:

- [Conversational RAG](/docs/tutorials/qa_chat_history): Enable a chatbot experience over an external source of data
- [Agents](https://langchain-ai.github.io/langgraphjs/tutorials/multi_agent/agent_supervisor/): Build a chatbot that can take actions

This tutorial will cover the basics which will be helpful for those two more advanced topics, but feel free to skip directly to there should you choose.

## Setup

### Jupyter Notebook

This guide (and most of the other guides in the documentation) uses [Jupyter notebooks](https://jupyter.org/) and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.

This and other tutorials are perhaps most conveniently run in a Jupyter notebook. See [here](https://jupyter.org/install) for instructions on how to install.

### Installation

For this tutorial we will need `@langchain/core` and `langgraph`:

```{=mdx}
import Npm2Yarn from "@theme/Npm2Yarn"

<Npm2Yarn>
  @langchain/core @langchain/langgraph uuid
</Npm2Yarn>
```

For more details, see our [Installation guide](/docs/how_to/installation).

### LangSmith

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.
As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.
The best way to do this is with [LangSmith](https://smith.langchain.com).

After you sign up at the link above, make sure to set your environment variables to start logging traces:

```typescript
process.env.LANGSMITH_TRACING = "true"
process.env.LANGSMITH_API_KEY = "..."
```

## Quickstart

First up, let's learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably - select the one you want to use below!

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```

"""

// @lc-docs-hide-cell

import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({ model: "gpt-4o-mini" })

"""
Let's first use the model directly. `ChatModel`s are instances of LangChain "Runnables", which means they expose a standard interface for interacting with them. To just simply call the model, we can pass in a list of messages to the `.invoke` method.
"""

await llm.invoke([{ role: "user", content: "Hi im bob" }])
# Output:
#   AIMessage {

#     "id": "chatcmpl-AekDrrCyaBauLYHuVv3dkacxW2G1J",

#     "content": "Hi Bob! How can I help you today?",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "promptTokens": 10,

#         "completionTokens": 10,

#         "totalTokens": 20

#       },

#       "finish_reason": "stop",

#       "usage": {

#         "prompt_tokens": 10,

#         "completion_tokens": 10,

#         "total_tokens": 20,

#         "prompt_tokens_details": {

#           "cached_tokens": 0,

#           "audio_tokens": 0

#         },

#         "completion_tokens_details": {

#           "reasoning_tokens": 0,

#           "audio_tokens": 0,

#           "accepted_prediction_tokens": 0,

#           "rejected_prediction_tokens": 0

#         }

#       },

#       "system_fingerprint": "fp_6fc10e10eb"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "output_tokens": 10,

#       "input_tokens": 10,

#       "total_tokens": 20,

#       "input_token_details": {

#         "audio": 0,

#         "cache_read": 0

#       },

#       "output_token_details": {

#         "audio": 0,

#         "reasoning": 0

#       }

#     }

#   }


"""
The model on its own does not have any concept of state. For example, if you ask a followup question:
"""

await llm.invoke([{ role: "user", content: "Whats my name" }])
# Output:
#   AIMessage {

#     "id": "chatcmpl-AekDuOk1LjOdBVLtuCvuHjAs5aoad",

#     "content": "I'm sorry, but I don't have access to personal information about users unless you've shared it with me in this conversation. How can I assist you today?",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "promptTokens": 10,

#         "completionTokens": 30,

#         "totalTokens": 40

#       },

#       "finish_reason": "stop",

#       "usage": {

#         "prompt_tokens": 10,

#         "completion_tokens": 30,

#         "total_tokens": 40,

#         "prompt_tokens_details": {

#           "cached_tokens": 0,

#           "audio_tokens": 0

#         },

#         "completion_tokens_details": {

#           "reasoning_tokens": 0,

#           "audio_tokens": 0,

#           "accepted_prediction_tokens": 0,

#           "rejected_prediction_tokens": 0

#         }

#       },

#       "system_fingerprint": "fp_6fc10e10eb"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "output_tokens": 30,

#       "input_tokens": 10,

#       "total_tokens": 40,

#       "input_token_details": {

#         "audio": 0,

#         "cache_read": 0

#       },

#       "output_token_details": {

#         "audio": 0,

#         "reasoning": 0

#       }

#     }

#   }


"""
Let's take a look at the example [LangSmith trace](https://smith.langchain.com/public/3b768e44-a319-453a-bd6e-30f9df75f16a/r)

We can see that it doesn't take the previous conversation turn into context, and cannot answer the question.
This makes for a terrible chatbot experience!

To get around this, we need to pass the entire conversation history into the model. Let's see what happens when we do that:
"""

await llm.invoke([
  { role: "user", content: "Hi! I'm Bob" },
  { role: "assistant", content: "Hello Bob! How can I assist you today?" },
  { role: "user", content: "What's my name?" }
]);
# Output:
#   AIMessage {

#     "id": "chatcmpl-AekDyJdj6y9IREyNIf3tkKGRKhN1Z",

#     "content": "Your name is Bob! How can I help you today, Bob?",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "promptTokens": 33,

#         "completionTokens": 14,

#         "totalTokens": 47

#       },

#       "finish_reason": "stop",

#       "usage": {

#         "prompt_tokens": 33,

#         "completion_tokens": 14,

#         "total_tokens": 47,

#         "prompt_tokens_details": {

#           "cached_tokens": 0,

#           "audio_tokens": 0

#         },

#         "completion_tokens_details": {

#           "reasoning_tokens": 0,

#           "audio_tokens": 0,

#           "accepted_prediction_tokens": 0,

#           "rejected_prediction_tokens": 0

#         }

#       },

#       "system_fingerprint": "fp_6fc10e10eb"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "output_tokens": 14,

#       "input_tokens": 33,

#       "total_tokens": 47,

#       "input_token_details": {

#         "audio": 0,

#         "cache_read": 0

#       },

#       "output_token_details": {

#         "audio": 0,

#         "reasoning": 0

#       }

#     }

#   }


"""
And now we can see that we get a good response!

This is the basic idea underpinning a chatbot's ability to interact conversationally.
So how do we best implement this?
"""

"""
## Message persistence

[LangGraph](https://langchain-ai.github.io/langgraphjs/) implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.

Wrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications.

LangGraph comes with a simple in-memory checkpointer, which we use below.
"""

import { START, END, MessagesAnnotation, StateGraph, MemorySaver } from "@langchain/langgraph";

// Define the function that calls the model
const callModel = async (state: typeof MessagesAnnotation.State) => {
  const response = await llm.invoke(state.messages);
  return { messages: response };
};

// Define a new graph
const workflow = new StateGraph(MessagesAnnotation)
  // Define the node and edge
  .addNode("model", callModel)
  .addEdge(START, "model")
  .addEdge("model", END);

// Add memory
const memory = new MemorySaver();
const app = workflow.compile({ checkpointer: memory });

"""
We now need to create a `config` that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a `thread_id`. This should look like:
"""

import { v4 as uuidv4 } from "uuid";

const config = { configurable: { thread_id: uuidv4() } };

"""
This enables us to support multiple conversation threads with a single application, a common requirement when your application has multiple users.

We can then invoke the application:
"""

const input = [
  {
    role: "user",
    content: "Hi! I'm Bob.",
  }
]
const output = await app.invoke({ messages: input }, config)
// The output contains all messages in the state.
// This will log the last message in the conversation.
console.log(output.messages[output.messages.length - 1]);
# Output:
#   AIMessage {

#     "id": "chatcmpl-AekEFPclmrO7YfAe7J0zUAanS4ifx",

#     "content": "Hi Bob! How can I assist you today?",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "promptTokens": 12,

#         "completionTokens": 10,

#         "totalTokens": 22

#       },

#       "finish_reason": "stop",

#       "usage": {

#         "prompt_tokens": 12,

#         "completion_tokens": 10,

#         "total_tokens": 22,

#         "prompt_tokens_details": {

#           "cached_tokens": 0,

#           "audio_tokens": 0

#         },

#         "completion_tokens_details": {

#           "reasoning_tokens": 0,

#           "audio_tokens": 0,

#           "accepted_prediction_tokens": 0,

#           "rejected_prediction_tokens": 0

#         }

#       },

#       "system_fingerprint": "fp_6fc10e10eb"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "output_tokens": 10,

#       "input_tokens": 12,

#       "total_tokens": 22,

#       "input_token_details": {

#         "audio": 0,

#         "cache_read": 0

#       },

#       "output_token_details": {

#         "audio": 0,

#         "reasoning": 0

#       }

#     }

#   }


const input2 = [
  {
    role: "user",
    content: "What's my name?",
  }
]
const output2 = await app.invoke({ messages: input2 }, config)
console.log(output2.messages[output2.messages.length - 1]);
# Output:
#   AIMessage {

#     "id": "chatcmpl-AekEJgCfLodGCcuLgLQdJevH7CpCJ",

#     "content": "Your name is Bob! How can I help you today, Bob?",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "promptTokens": 34,

#         "completionTokens": 14,

#         "totalTokens": 48

#       },

#       "finish_reason": "stop",

#       "usage": {

#         "prompt_tokens": 34,

#         "completion_tokens": 14,

#         "total_tokens": 48,

#         "prompt_tokens_details": {

#           "cached_tokens": 0,

#           "audio_tokens": 0

#         },

#         "completion_tokens_details": {

#           "reasoning_tokens": 0,

#           "audio_tokens": 0,

#           "accepted_prediction_tokens": 0,

#           "rejected_prediction_tokens": 0

#         }

#       },

#       "system_fingerprint": "fp_6fc10e10eb"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "output_tokens": 14,

#       "input_tokens": 34,

#       "total_tokens": 48,

#       "input_token_details": {

#         "audio": 0,

#         "cache_read": 0

#       },

#       "output_token_details": {

#         "audio": 0,

#         "reasoning": 0

#       }

#     }

#   }


"""
Great! Our chatbot now remembers things about us. If we change the config to reference a different `thread_id`, we can see that it starts the conversation fresh.
"""

const config2 = { configurable: { thread_id: uuidv4() } }
const input3 = [
  {
    role: "user",
    content: "What's my name?",
  }
]
const output3 = await app.invoke({ messages: input3 }, config2)
console.log(output3.messages[output3.messages.length - 1]);
# Output:
#   AIMessage {

#     "id": "chatcmpl-AekELvPXLtjOKgLN63mQzZwvyo12J",

#     "content": "I'm sorry, but I don't have access to personal information about individuals unless you share it with me. How can I assist you today?",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "promptTokens": 11,

#         "completionTokens": 27,

#         "totalTokens": 38

#       },

#       "finish_reason": "stop",

#       "usage": {

#         "prompt_tokens": 11,

#         "completion_tokens": 27,

#         "total_tokens": 38,

#         "prompt_tokens_details": {

#           "cached_tokens": 0,

#           "audio_tokens": 0

#         },

#         "completion_tokens_details": {

#           "reasoning_tokens": 0,

#           "audio_tokens": 0,

#           "accepted_prediction_tokens": 0,

#           "rejected_prediction_tokens": 0

#         }

#       },

#       "system_fingerprint": "fp_39a40c96a0"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "output_tokens": 27,

#       "input_tokens": 11,

#       "total_tokens": 38,

#       "input_token_details": {

#         "audio": 0,

#         "cache_read": 0

#       },

#       "output_token_details": {

#         "audio": 0,

#         "reasoning": 0

#       }

#     }

#   }


"""
However, we can always go back to the original conversation (since we are persisting it in a database)
"""

const output4 = await app.invoke({ messages: input2 }, config)
console.log(output4.messages[output4.messages.length - 1]);
# Output:
#   AIMessage {

#     "id": "chatcmpl-AekEQ8Z5JmYquSfzPsCWv1BDTKZSh",

#     "content": "Your name is Bob. Is there something specific you would like to talk about?",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "promptTokens": 60,

#         "completionTokens": 16,

#         "totalTokens": 76

#       },

#       "finish_reason": "stop",

#       "usage": {

#         "prompt_tokens": 60,

#         "completion_tokens": 16,

#         "total_tokens": 76,

#         "prompt_tokens_details": {

#           "cached_tokens": 0,

#           "audio_tokens": 0

#         },

#         "completion_tokens_details": {

#           "reasoning_tokens": 0,

#           "audio_tokens": 0,

#           "accepted_prediction_tokens": 0,

#           "rejected_prediction_tokens": 0

#         }

#       },

#       "system_fingerprint": "fp_39a40c96a0"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "output_tokens": 16,

#       "input_tokens": 60,

#       "total_tokens": 76,

#       "input_token_details": {

#         "audio": 0,

#         "cache_read": 0

#       },

#       "output_token_details": {

#         "audio": 0,

#         "reasoning": 0

#       }

#     }

#   }


"""
This is how we can support a chatbot having conversations with many users!

Right now, all we've done is add a simple persistence layer around the model. We can start to make the more complicated and personalized by adding in a prompt template.

## Prompt templates

Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages.

To add in a system message, we will create a `ChatPromptTemplate`. We will utilize `MessagesPlaceholder` to pass all the messages in.
"""

import { ChatPromptTemplate } from "@langchain/core/prompts";

const promptTemplate = ChatPromptTemplate.fromMessages([
  ["system", "You talk like a pirate. Answer all questions to the best of your ability."],
  ["placeholder", "{messages}"],
]);

"""
We can now update our application to incorporate this template:
"""

import { START, END, MessagesAnnotation, StateGraph, MemorySaver } from "@langchain/langgraph";

// Define the function that calls the model
const callModel2 = async (state: typeof MessagesAnnotation.State) => {
  // highlight-start
  const prompt = await promptTemplate.invoke(state)
  const response = await llm.invoke(prompt);
  // highlight-end
  // Update message history with response:
  return { messages: [response] };
};

// Define a new graph
const workflow2 = new StateGraph(MessagesAnnotation)
  // Define the (single) node in the graph
  .addNode("model", callModel2)
  .addEdge(START, "model")
  .addEdge("model", END);

// Add memory
const app2 = workflow2.compile({ checkpointer: new MemorySaver() });

"""
We invoke the application in the same way:
"""

const config3 = { configurable: { thread_id: uuidv4() } }
const input4 = [
  {
    role: "user",
    content: "Hi! I'm Jim.",
  }
]
const output5 = await app2.invoke({ messages: input4 }, config3)
console.log(output5.messages[output5.messages.length - 1]);
# Output:
#   AIMessage {

#     "id": "chatcmpl-AekEYAQVqh9OFZRGdzGiPz33WPf1v",

#     "content": "Ahoy, Jim! A pleasure to meet ye, matey! What be on yer mind this fine day?",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "promptTokens": 32,

#         "completionTokens": 23,

#         "totalTokens": 55

#       },

#       "finish_reason": "stop",

#       "usage": {

#         "prompt_tokens": 32,

#         "completion_tokens": 23,

#         "total_tokens": 55,

#         "prompt_tokens_details": {

#           "cached_tokens": 0,

#           "audio_tokens": 0

#         },

#         "completion_tokens_details": {

#           "reasoning_tokens": 0,

#           "audio_tokens": 0,

#           "accepted_prediction_tokens": 0,

#           "rejected_prediction_tokens": 0

#         }

#       },

#       "system_fingerprint": "fp_39a40c96a0"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "output_tokens": 23,

#       "input_tokens": 32,

#       "total_tokens": 55,

#       "input_token_details": {

#         "audio": 0,

#         "cache_read": 0

#       },

#       "output_token_details": {

#         "audio": 0,

#         "reasoning": 0

#       }

#     }

#   }


const input5 = [
  {
    role: "user",
    content:  "What is my name?"
  }
]
const output6 = await app2.invoke({ messages: input5 }, config3)
console.log(output6.messages[output6.messages.length - 1]);
# Output:
#   AIMessage {

#     "id": "chatcmpl-AekEbrpFI3K8BxemHZ5fG4xF2tT8x",

#     "content": "Ye be callin' yerself Jim, if I heard ye right, savvy? What else can I do fer ye, me hearty?",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "promptTokens": 68,

#         "completionTokens": 29,

#         "totalTokens": 97

#       },

#       "finish_reason": "stop",

#       "usage": {

#         "prompt_tokens": 68,

#         "completion_tokens": 29,

#         "total_tokens": 97,

#         "prompt_tokens_details": {

#           "cached_tokens": 0,

#           "audio_tokens": 0

#         },

#         "completion_tokens_details": {

#           "reasoning_tokens": 0,

#           "audio_tokens": 0,

#           "accepted_prediction_tokens": 0,

#           "rejected_prediction_tokens": 0

#         }

#       },

#       "system_fingerprint": "fp_6fc10e10eb"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "output_tokens": 29,

#       "input_tokens": 68,

#       "total_tokens": 97,

#       "input_token_details": {

#         "audio": 0,

#         "cache_read": 0

#       },

#       "output_token_details": {

#         "audio": 0,

#         "reasoning": 0

#       }

#     }

#   }


"""
Awesome! Let's now make our prompt a little bit more complicated. Let's assume that the prompt template now looks something like this:
"""

const promptTemplate2 = ChatPromptTemplate.fromMessages([
  ["system", "You are a helpful assistant. Answer all questions to the best of your ability in {language}."],
  ["placeholder", "{messages}"],
]);

"""
Note that we have added a new `language` input to the prompt. Our application now has two parameters-- the input `messages` and `language`. We should update our application's state to reflect this:
"""

import { START, END, StateGraph, MemorySaver, MessagesAnnotation, Annotation } from "@langchain/langgraph";

// Define the State
const GraphAnnotation = Annotation.Root({
  ...MessagesAnnotation.spec,
  language: Annotation<string>(),
});

// Define the function that calls the model
const callModel3 = async (state: typeof GraphAnnotation.State) => {
  const prompt = await promptTemplate2.invoke(state);
  const response = await llm.invoke(prompt);
  return { messages: [response] };
};

const workflow3 = new StateGraph(GraphAnnotation)
  .addNode("model", callModel3)
  .addEdge(START, "model")
  .addEdge("model", END);

const app3 = workflow3.compile({ checkpointer: new MemorySaver() });

const config4 = { configurable: { thread_id: uuidv4() } }
const input6 = {
  messages: [
    {
      role: "user",
      content:  "Hi im bob"
    }
  ],
  language: "Spanish"
}
const output7 = await app3.invoke(input6, config4)
console.log(output7.messages[output7.messages.length - 1]);
# Output:
#   AIMessage {

#     "id": "chatcmpl-AekF4R7ioefFo6PmOYo3YuCbGpROq",

#     "content": "¡Hola, Bob! ¿Cómo puedo ayudarte hoy?",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "promptTokens": 32,

#         "completionTokens": 11,

#         "totalTokens": 43

#       },

#       "finish_reason": "stop",

#       "usage": {

#         "prompt_tokens": 32,

#         "completion_tokens": 11,

#         "total_tokens": 43,

#         "prompt_tokens_details": {

#           "cached_tokens": 0,

#           "audio_tokens": 0

#         },

#         "completion_tokens_details": {

#           "reasoning_tokens": 0,

#           "audio_tokens": 0,

#           "accepted_prediction_tokens": 0,

#           "rejected_prediction_tokens": 0

#         }

#       },

#       "system_fingerprint": "fp_39a40c96a0"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "output_tokens": 11,

#       "input_tokens": 32,

#       "total_tokens": 43,

#       "input_token_details": {

#         "audio": 0,

#         "cache_read": 0

#       },

#       "output_token_details": {

#         "audio": 0,

#         "reasoning": 0

#       }

#     }

#   }


"""
Note that the entire state is persisted, so we can omit parameters like `language` if no changes are desired:
"""

const input7 = {
  messages: [
    {
      role: "user",
      content:  "What is my name?"
    }
  ],
}
const output8 = await app3.invoke(input7, config4)
console.log(output8.messages[output8.messages.length - 1]);
# Output:
#   AIMessage {

#     "id": "chatcmpl-AekF8yN7H81ITccWlBzSahmduP69T",

#     "content": "Tu nombre es Bob. ¿En qué puedo ayudarte, Bob?",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "promptTokens": 56,

#         "completionTokens": 13,

#         "totalTokens": 69

#       },

#       "finish_reason": "stop",

#       "usage": {

#         "prompt_tokens": 56,

#         "completion_tokens": 13,

#         "total_tokens": 69,

#         "prompt_tokens_details": {

#           "cached_tokens": 0,

#           "audio_tokens": 0

#         },

#         "completion_tokens_details": {

#           "reasoning_tokens": 0,

#           "audio_tokens": 0,

#           "accepted_prediction_tokens": 0,

#           "rejected_prediction_tokens": 0

#         }

#       },

#       "system_fingerprint": "fp_6fc10e10eb"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "output_tokens": 13,

#       "input_tokens": 56,

#       "total_tokens": 69,

#       "input_token_details": {

#         "audio": 0,

#         "cache_read": 0

#       },

#       "output_token_details": {

#         "audio": 0,

#         "reasoning": 0

#       }

#     }

#   }


"""
To help you understand what's happening internally, check out [this LangSmith trace](https://smith.langchain.com/public/d61630b7-6a52-4dc9-974c-8452008c498a/r).
"""

"""
## Managing Conversation History

One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.

**Importantly, you will want to do this BEFORE the prompt template but AFTER you load previous messages from Message History.**

We can do this by adding a simple step in front of the prompt that modifies the `messages` key appropriately, and then wrap that new chain in the Message History class. 

LangChain comes with a few built-in helpers for [managing a list of messages](/docs/how_to/#messages). In this case we'll use the [trimMessages](/docs/how_to/trim_messages/) helper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages:
"""

import { SystemMessage, HumanMessage, AIMessage, trimMessages } from "@langchain/core/messages"

const trimmer = trimMessages({
  maxTokens: 10,
  strategy: "last",
  tokenCounter: (msgs) => msgs.length,
  includeSystem: true,
  allowPartial: false,
  startOn: "human",
})

const messages = [
    new SystemMessage("you're a good assistant"),
    new HumanMessage("hi! I'm bob"),
    new AIMessage("hi!"),
    new HumanMessage("I like vanilla ice cream"),
    new AIMessage("nice"),
    new HumanMessage("whats 2 + 2"),
    new AIMessage("4"),
    new HumanMessage("thanks"),
    new AIMessage("no problem!"),
    new HumanMessage("having fun?"),
    new AIMessage("yes!"),
]

await trimmer.invoke(messages)
# Output:
#   [

#     SystemMessage {

#       "content": "you're a good assistant",

#       "additional_kwargs": {},

#       "response_metadata": {}

#     },

#     HumanMessage {

#       "content": "I like vanilla ice cream",

#       "additional_kwargs": {},

#       "response_metadata": {}

#     },

#     AIMessage {

#       "content": "nice",

#       "additional_kwargs": {},

#       "response_metadata": {},

#       "tool_calls": [],

#       "invalid_tool_calls": []

#     },

#     HumanMessage {

#       "content": "whats 2 + 2",

#       "additional_kwargs": {},

#       "response_metadata": {}

#     },

#     AIMessage {

#       "content": "4",

#       "additional_kwargs": {},

#       "response_metadata": {},

#       "tool_calls": [],

#       "invalid_tool_calls": []

#     },

#     HumanMessage {

#       "content": "thanks",

#       "additional_kwargs": {},

#       "response_metadata": {}

#     },

#     AIMessage {

#       "content": "no problem!",

#       "additional_kwargs": {},

#       "response_metadata": {},

#       "tool_calls": [],

#       "invalid_tool_calls": []

#     },

#     HumanMessage {

#       "content": "having fun?",

#       "additional_kwargs": {},

#       "response_metadata": {}

#     },

#     AIMessage {

#       "content": "yes!",

#       "additional_kwargs": {},

#       "response_metadata": {},

#       "tool_calls": [],

#       "invalid_tool_calls": []

#     }

#   ]


"""
To  use it in our chain, we just need to run the trimmer before we pass the `messages` input to our prompt. 
"""

const callModel4 = async (state: typeof GraphAnnotation.State) => {
  // highlight-start
  const trimmedMessage = await trimmer.invoke(state.messages);
  const prompt = await promptTemplate2.invoke({ messages: trimmedMessage, language: state.language });
  const response = await llm.invoke(prompt);
  // highlight-end
  return { messages: [response] };
};


const workflow4 = new StateGraph(GraphAnnotation)
  .addNode("model", callModel4)
  .addEdge(START, "model")
  .addEdge("model", END);

const app4 = workflow4.compile({ checkpointer: new MemorySaver() });

"""
Now if we try asking the model our name, it won't know it since we trimmed that part of the chat history:
"""

const config5 = { configurable: { thread_id: uuidv4() }}
const input8 = {
  // highlight-next-line
  messages: [...messages, new HumanMessage("What is my name?")],
  language: "English"
}

const output9 = await app4.invoke(
  input8,
  config5,
)
console.log(output9.messages[output9.messages.length - 1]);
# Output:
#   AIMessage {

#     "id": "chatcmpl-AekHyVN7f0Pnuyc2RHVL8CxKmFfMQ",

#     "content": "I don't know your name. You haven't shared it yet!",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "promptTokens": 97,

#         "completionTokens": 12,

#         "totalTokens": 109

#       },

#       "finish_reason": "stop",

#       "usage": {

#         "prompt_tokens": 97,

#         "completion_tokens": 12,

#         "total_tokens": 109,

#         "prompt_tokens_details": {

#           "cached_tokens": 0,

#           "audio_tokens": 0

#         },

#         "completion_tokens_details": {

#           "reasoning_tokens": 0,

#           "audio_tokens": 0,

#           "accepted_prediction_tokens": 0,

#           "rejected_prediction_tokens": 0

#         }

#       },

#       "system_fingerprint": "fp_6fc10e10eb"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "output_tokens": 12,

#       "input_tokens": 97,

#       "total_tokens": 109,

#       "input_token_details": {

#         "audio": 0,

#         "cache_read": 0

#       },

#       "output_token_details": {

#         "audio": 0,

#         "reasoning": 0

#       }

#     }

#   }


"""
But if we ask about information that is within the last few messages, it remembers:
"""

const config6 = { configurable: { thread_id: uuidv4() }}
const input9 = {
  // highlight-next-line
  messages: [...messages, new HumanMessage("What math problem did I ask?")],
  language: "English"
}

const output10 = await app4.invoke(
  input9,
  config6,
)
console.log(output10.messages[output10.messages.length - 1]);
# Output:
#   AIMessage {

#     "id": "chatcmpl-AekI1jwlErzHuZ3BhAxr97Ct818Pp",

#     "content": "You asked what 2 + 2 equals.",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "promptTokens": 99,

#         "completionTokens": 10,

#         "totalTokens": 109

#       },

#       "finish_reason": "stop",

#       "usage": {

#         "prompt_tokens": 99,

#         "completion_tokens": 10,

#         "total_tokens": 109,

#         "prompt_tokens_details": {

#           "cached_tokens": 0,

#           "audio_tokens": 0

#         },

#         "completion_tokens_details": {

#           "reasoning_tokens": 0,

#           "audio_tokens": 0,

#           "accepted_prediction_tokens": 0,

#           "rejected_prediction_tokens": 0

#         }

#       },

#       "system_fingerprint": "fp_6fc10e10eb"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "output_tokens": 10,

#       "input_tokens": 99,

#       "total_tokens": 109,

#       "input_token_details": {

#         "audio": 0,

#         "cache_read": 0

#       },

#       "output_token_details": {

#         "audio": 0,

#         "reasoning": 0

#       }

#     }

#   }


"""
If you take a look at LangSmith, you can see exactly what is happening under the hood in the [LangSmith trace](https://smith.langchain.com/public/ac63745d-8429-4ae5-8c11-9ec79d9632f2/r).
"""

"""
## Next Steps

Now that you understand the basics of how to create a chatbot in LangChain, some more advanced tutorials you may be interested in are:

- [Conversational RAG](/docs/tutorials/qa_chat_history): Enable a chatbot experience over an external source of data
- [Agents](https://langchain-ai.github.io/langgraphjs/tutorials/multi_agent/agent_supervisor/): Build a chatbot that can take actions

If you want to dive deeper on specifics, some things worth checking out are:

- [Streaming](/docs/how_to/streaming): streaming is *crucial* for chat applications
- [How to add message history](/docs/how_to/message_history): for a deeper dive into all things related to message history
- [How to manage large message history](/docs/how_to/trim_messages/): more techniques for managing a large chat history
- [LangGraph main docs](https://langchain-ai.github.io/langgraphjs/): for more detail on building with LangGraph
"""



================================================
FILE: docs/core_docs/docs/tutorials/classification.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
title: Tagging
sidebar_class_name: hidden
---
"""

"""
# Classify Text into Labels

Tagging means labeling a document with classes such as:

- sentiment
- language
- style (formal, informal etc.)
- covered topics
- political tendency

![Image description](../../static/img/tagging.png)

## Overview

Tagging has a few components:

* `function`: Like [extraction](/docs/tutorials/extraction), tagging uses [functions](https://openai.com/blog/function-calling-and-other-api-updates) to specify how the model should tag a document
* `schema`: defines how we want to tag the document

## Quickstart

Let's see a very straightforward example of how we can use tool calling for tagging in LangChain. We'll use the `.withStructuredOutput()`, supported on [selected chat models](/docs/integrations/chat/).


```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

// @lc-docs-hide-cell
import { ChatOpenAI } from '@langchain/openai';

const llm = new ChatOpenAI({
  model: "gpt-4o-mini",
  temperature: 0,
})

"""
Let's specify a [Zod](https://zod.dev) schema with a few properties and their expected type in our schema.
"""

import { ChatPromptTemplate } from "@langchain/core/prompts";
import { z } from "zod";

const taggingPrompt = ChatPromptTemplate.fromTemplate(
    `Extract the desired information from the following passage.

Only extract the properties mentioned in the 'Classification' function.

Passage:
{input}
`
);

const classificationSchema = z.object({
    sentiment: z.string().describe("The sentiment of the text"),
    aggressiveness: z.number().int().describe(
        "How aggressive the text is on a scale from 1 to 10"
    ),
    language: z.string().describe("The language the text is written in"),
});

// Name is optional, but gives the models more clues as to what your schema represents
const llmWihStructuredOutput = llm.withStructuredOutput(classificationSchema, { name: "extractor" })

const prompt1 = await taggingPrompt.invoke({
  input: "Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!"
})
await llmWihStructuredOutput.invoke(prompt1)
# Output:
#   { sentiment: 'positive', aggressiveness: 1, language: 'Spanish' }


"""
As we can see in the example, it correctly interprets what we want.

The results vary so that we may get, for example, sentiments in different languages ('positive', 'enojado' etc.).

We will see how to control these results in the next section.
"""

"""
## Finer control

Careful schema definition gives us more control over the model's output. 

Specifically, we can define:

- possible values for each property
- description to make sure that the model understands the property
- required properties to be returned
"""

"""
Let's redeclare our Zod schema to control for each of the previously mentioned aspects using enums:
"""

import { z } from "zod";

const classificationSchema2 = z.object({
    sentiment: z.enum(["happy", "neutral", "sad"]).describe("The sentiment of the text"),
    aggressiveness: z.number().int().describe(
        "describes how aggressive the statement is on a scale from 1 to 5. The higher the number the more aggressive"
    ),
    language: z.enum(["spanish", "english", "french", "german", "italian"]).describe("The language the text is written in"),
});

const taggingPrompt2 = ChatPromptTemplate.fromTemplate(
`Extract the desired information from the following passage.

Only extract the properties mentioned in the 'Classification' function.

Passage:
{input}
`
)

const llmWithStructuredOutput2 = llm.withStructuredOutput(classificationSchema2, { name: "extractor" })

"""
Now the answers will be restricted in a way we expect!
"""

const prompt2 = await taggingPrompt2.invoke({
  input: "Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!"
})
await llmWithStructuredOutput2.invoke(prompt2)
# Output:
#   { sentiment: 'happy', aggressiveness: 1, language: 'spanish' }


const prompt3 = await taggingPrompt2.invoke({
  input: "Estoy muy enojado con vos! Te voy a dar tu merecido!"
})
await llmWithStructuredOutput2.invoke(prompt3)
# Output:
#   { sentiment: 'sad', aggressiveness: 5, language: 'spanish' }


const prompt4 = await taggingPrompt2.invoke({
  input: "Weather is ok here, I can go outside without much more than a coat"
})
await llmWithStructuredOutput2.invoke(prompt4)
# Output:
#   { sentiment: 'neutral', aggressiveness: 1, language: 'english' }


"""
The [LangSmith trace](https://smith.langchain.com/public/455f5404-8784-49ce-8851-0619b99e936f/r) lets us peek under the hood:

![](../../static/img/classification_ls_trace.png)
"""



================================================
FILE: docs/core_docs/docs/tutorials/extraction.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_position: 4
---
"""

"""
# Build an Extraction Chain

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat Models](/docs/concepts/chat_models)
- [Tools](/docs/concepts/tools)
- [Tool calling](/docs/concepts/tool_calling)

:::

In this tutorial, we will build a chain to extract structured information from unstructured text. 

:::{.callout-important}
This tutorial will only work with models that support **function/tool calling**
:::
"""

"""
## Setup

### Installation

To install LangChain run:

```{=mdx}
import Npm2Yarn from '@theme/Npm2Yarn';

<Npm2Yarn>
  langchain @langchain/core
</Npm2Yarn>
```

For more details, see our [Installation guide](/docs/how_to/installation/).

### LangSmith

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.
As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.
The best way to do this is with [LangSmith](https://smith.langchain.com).

After you sign up at the link above, make sure to set your environment variables to start logging traces:

```shell
export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."

# Reduce tracing latency if you are not in a serverless environment
# export LANGCHAIN_CALLBACKS_BACKGROUND=true
```
"""

"""
## The Schema

First, we need to describe what information we want to extract from the text.

We'll use [Zod](https://zod.dev) to define an example schema that extracts personal information.

```{=mdx}
<Npm2Yarn>
  zod @langchain/core
</Npm2Yarn>
```
"""

import { z } from "zod";

const personSchema = z.object({
  name: z.optional(z.string()).describe("The name of the person"),
  hair_color: z.optional(z.string()).describe("The color of the person's hair if known"),
  height_in_meters: z.optional(z.string()).describe('Height measured in meters'),
});

"""
There are two best practices when defining schema:

1. Document the **attributes** and the **schema** itself: This information is sent to the LLM and is used to improve the quality of information extraction.
2. Do not force the LLM to make up information! Above we used `.nullish()` for the attributes allowing the LLM to output `null` or `undefined` if it doesn't know the answer.

:::{.callout-important}
For best performance, document the schema well and make sure the model isn't force to return results if there's no information to be extracted in the text.
:::

## The Extractor

Let's create an information extractor using the schema we defined above.
"""

import { ChatPromptTemplate } from "@langchain/core/prompts";

// Define a custom prompt to provide instructions and any additional context.
// 1) You can add examples into the prompt template to improve extraction quality
// 2) Introduce additional parameters to take context into account (e.g., include metadata
//    about the document from which the text was extracted.)
const promptTemplate = ChatPromptTemplate.fromMessages(
  [
    [
      "system",
      `You are an expert extraction algorithm.
Only extract relevant information from the text.
If you do not know the value of an attribute asked to extract,
return null for the attribute's value.`,
    ],
    // Please see the how-to about improving performance with
    // reference examples.
    // ["placeholder", "{examples}"],
    ["human", "{text}"],
  ],
);

"""
We need to use a model that supports function/tool calling.

Please review [the documentation](/docs/integrations/chat) for list of some models that can be used with this API.

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

// @lc-docs-hide-cell
import { ChatAnthropic } from "@langchain/anthropic";

const llm = new ChatAnthropic({
  model: "claude-3-sonnet-20240229",
  temperature: 0
})

"""
We enable structured output by creating a new object with the `.withStructuredOutput` method:
"""

const structured_llm = llm.withStructuredOutput(personSchema)

"""
We can then invoke it normally:
"""

const prompt = await promptTemplate.invoke({
  text: "Alan Smith is 6 feet tall and has blond hair."
})
await structured_llm.invoke(prompt)
# Output:
#   { name: [32m'Alan Smith'[39m, hair_color: [32m'blond'[39m, height_in_meters: [32m'1.83'[39m }


"""
:::{.callout-important} 

Extraction is Generative 🤯

LLMs are generative models, so they can do some pretty cool things like correctly extract the height of the person in meters
even though it was provided in feet!
:::

We can see the LangSmith trace [here](https://smith.langchain.com/public/3d44b7e8-e7ca-4e02-951d-3290ccc89d64/r).

Even though we defined our schema with the variable name `personSchema`, Zod is unable to infer this name and therefore does not pass it along to the model. To help give the LLM more clues as to what your provided schema represents, you can also give the schema you pass to `withStructuredOutput()` a name:
"""

const structured_llm2 = llm.withStructuredOutput(personSchema, { name: "person" })

const prompt2 = await promptTemplate.invoke({
  text: "Alan Smith is 6 feet tall and has blond hair."
})
await structured_llm2.invoke(prompt2)
# Output:
#   { name: [32m'Alan Smith'[39m, hair_color: [32m'blond'[39m, height_in_meters: [32m'1.83'[39m }


"""
This can improve performance in many cases.
"""

"""
## Multiple Entities

In **most cases**, you should be extracting a list of entities rather than a single entity.

This can be easily achieved using Zod by nesting models inside one another.
"""

import { z } from "zod";

const person = z.object({
  name: z.optional(z.string()).describe('The name of the person'),
  hair_color: z.optional(z.string()).describe("The color of the person's hair if known"),
  height_in_meters: z.number().nullish().describe('Height measured in meters'),
});
  
const dataSchema = z.object({
  people: z.array(person).describe('Extracted data about people'),
});

"""
:::{.callout-important}
Extraction might not be perfect here. Please continue to see how to use **Reference Examples** to improve the quality of extraction, and see the **guidelines** section!
:::
"""

const structured_llm3 = llm.withStructuredOutput(dataSchema)
const prompt3 = await promptTemplate.invoke({
  text: "My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me."
})
await structured_llm3.invoke(prompt3)
# Output:
#   {

#     people: [

#       { name: [32m'Jeff'[39m, hair_color: [32m'black'[39m, height_in_meters: [33m1.83[39m },

#       { name: [32m'Anna'[39m, hair_color: [32m'black'[39m, height_in_meters: [1mnull[22m }

#     ]

#   }


"""
:::{.callout-tip}
When the schema accommodates the extraction of **multiple entities**, it also allows the model to extract **no entities** if no relevant information
is in the text by providing an empty list. 

This is usually a **good** thing! It allows specifying **required** attributes on an entity without necessarily forcing the model to detect this entity.
:::

We can see the LangSmith trace [here](https://smith.langchain.com/public/272096ab-9ac5-43f9-aa00-3b8443477d17/r)
"""

"""
## Next steps

Now that you understand the basics of extraction with LangChain, you're ready to proceed to the rest of the how-to guides:

- [Add Examples](/docs/how_to/extraction_examples): Learn how to use **reference examples** to improve performance.
- [Handle Long Text](/docs/how_to/extraction_long_text): What should you do if the text does not fit into the context window of the LLM?
- [Use a Parsing Approach](/docs/how_to/extraction_parse): Use a prompt based approach to extract with models that do not support **tool/function calling**.
"""



================================================
FILE: docs/core_docs/docs/tutorials/graph.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_position: 0
---
"""

"""
# Build a Question Answering application over a Graph Database

In this guide we'll go over the basic ways to create a Q&A chain over a graph database. These systems will allow us to ask a question about the data in a graph database and get back a natural language answer.

## ⚠️ Security note ⚠️

Building Q&A systems of graph databases requires executing model-generated graph queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your chain/agent's needs. This will mitigate though not eliminate the risks of building a model-driven system. For more on general security best practices, [see here](/docs/security).


## Architecture

At a high-level, the steps of most graph chains are:

1. **Convert question to a graph database query**: Model converts user input to a graph database query (e.g. Cypher).
2. **Execute graph database query**: Execute the graph database query.
3. **Answer the question**: Model responds to user input using the query results.


![sql_usecase.png](../../static/img/graph_usecase.png)
"""

"""
## Setup
#### Install dependencies

```{=mdx}
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  langchain @langchain/community @langchain/openai @langchain/core neo4j-driver
</Npm2Yarn>
```

#### Set environment variables

We'll use OpenAI in this example:

```env
OPENAI_API_KEY=your-api-key

# Optional, use LangSmith for best-in-class observability
LANGSMITH_API_KEY=your-api-key
LANGSMITH_TRACING=true

# Reduce tracing latency if you are not in a serverless environment
# LANGCHAIN_CALLBACKS_BACKGROUND=true
```

Next, we need to define Neo4j credentials.
Follow [these installation steps](https://neo4j.com/docs/operations-manual/current/installation/) to set up a Neo4j database.

```env
NEO4J_URI="bolt://localhost:7687"
NEO4J_USERNAME="neo4j"
NEO4J_PASSWORD="password"
```
"""

"""
The below example will create a connection with a Neo4j database and will populate it with example data about movies and their actors.
"""

import "neo4j-driver";
import { Neo4jGraph } from "@langchain/community/graphs/neo4j_graph";

const url = process.env.NEO4J_URI;
const username = process.env.NEO4J_USER;
const password = process.env.NEO4J_PASSWORD;
const graph = await Neo4jGraph.initialize({ url, username, password });

// Import movie information
const moviesQuery = `LOAD CSV WITH HEADERS FROM 
'https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/movies/movies_small.csv'
AS row
MERGE (m:Movie {id:row.movieId})
SET m.released = date(row.released),
    m.title = row.title,
    m.imdbRating = toFloat(row.imdbRating)
FOREACH (director in split(row.director, '|') | 
    MERGE (p:Person {name:trim(director)})
    MERGE (p)-[:DIRECTED]->(m))
FOREACH (actor in split(row.actors, '|') | 
    MERGE (p:Person {name:trim(actor)})
    MERGE (p)-[:ACTED_IN]->(m))
FOREACH (genre in split(row.genres, '|') | 
    MERGE (g:Genre {name:trim(genre)})
    MERGE (m)-[:IN_GENRE]->(g))`

await graph.query(moviesQuery);
# Output:
#   Schema refreshed successfully.

#   []

"""
## Graph schema

In order for an LLM to be able to generate a Cypher statement, it needs information about the graph schema. When you instantiate a graph object, it retrieves the information about the graph schema. If you later make any changes to the graph, you can run the `refreshSchema` method to refresh the schema information.
"""

await graph.refreshSchema()
console.log(graph.getSchema())
# Output:
#   Node properties are the following:

#   Movie {imdbRating: FLOAT, id: STRING, released: DATE, title: STRING}, Person {name: STRING}, Genre {name: STRING}

#   Relationship properties are the following:

#   

#   The relationships are the following:

#   (:Movie)-[:IN_GENRE]->(:Genre), (:Person)-[:DIRECTED]->(:Movie), (:Person)-[:ACTED_IN]->(:Movie)


"""
Great! We've got a graph database that we can query. Now let's try hooking it up to an LLM.

## Chain

Let's use a simple chain that takes a question, turns it into a Cypher query, executes the query, and uses the result to answer the original question.

![graph_chain.webp](../../static/img/graph_chain.webp)

LangChain comes with a built-in chain for this workflow that is designed to work with Neo4j: `GraphCypherQAChain`.

```{=mdx}
:::warning

The `GraphCypherQAChain` used in this guide will execute Cypher statements against the provided database.
For production, make sure that the database connection uses credentials that are narrowly-scoped to only include necessary permissions.

Failure to do so may result in data corruption or loss, since the calling code
may attempt commands that would result in deletion, mutation of data
if appropriately prompted or reading sensitive data if such data is present in the database.

:::
```
"""

import { GraphCypherQAChain } from "langchain/chains/graph_qa/cypher";
import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({ model: "gpt-3.5-turbo", temperature: 0 })
const chain = GraphCypherQAChain.fromLLM({
  llm,
  graph,
});
const response = await chain.invoke({ query: "What was the cast of the Casino?" })
console.log(response)
# Output:
#   { result: [32m"James Woods, Joe Pesci, Robert De Niro, Sharon Stone"[39m }

"""
### Next steps

For more complex query-generation, we may want to create few-shot prompts or add query-checking steps. For advanced techniques like this and more check out:

* [Prompting strategies](/docs/how_to/graph_prompting): Advanced prompt engineering techniques.
* [Mapping values](/docs/how_to/graph_mapping/): Techniques for mapping values from questions to database.
* [Semantic layer](/docs/how_to/graph_semantic): Techniques for working implementing semantic layers.
* [Constructing graphs](/docs/how_to/graph_constructing): Techniques for constructing knowledge graphs.

"""



================================================
FILE: docs/core_docs/docs/tutorials/index.mdx
================================================
---
sidebar_position: 0
sidebar_class_name: hidden
---

# Tutorials

New to LangChain or LLM app development in general? Read this material to quickly get up and running building your first applications.

## Get started

Familiarize yourself with LangChain's open-source components by building simple applications.

If you're looking to get started with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),
or other LangChain components from a specific provider, check out our supported [integrations](/docs/integrations/platforms/).

- [Chat models and prompts](/docs/tutorials/llm_chain): Build a simple LLM application with [prompt templates](/docs/concepts/prompt_templates) and [chat models](/docs/concepts/chat_models).
- [Semantic search](/docs/tutorials/retrievers): Build a semantic search engine over a PDF with [document loaders](/docs/concepts/document_loaders), [embedding models](/docs/concepts/embedding_models/), and [vector stores](/docs/concepts/vectorstores/).
- [Classification](/docs/tutorials/classification): Classify text into categories or labels using [chat models](/docs/concepts/chat_models) with [structured outputs](/docs/concepts/structured_outputs/).
- [Extraction](/docs/tutorials/extraction): Extract structured data from text and other unstructured media using [chat models](/docs/concepts/chat_models) and [few-shot examples](/docs/concepts/few_shot_prompting/).

Refer to the [how-to guides](/docs/how_to) for more detail on using all LangChain components.

## Orchestration

Get started using [LangGraph](https://langchain-ai.github.io/langgraphjs/) to assemble LangChain components into full-featured applications.

- [Chatbots](/docs/tutorials/chatbot): Build a chatbot that incorporates memory.
- [Agents](https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/): Build an agent with LangGraph.js that interacts with external tools.
- [Retrieval Augmented Generation (RAG) Part 1](/docs/tutorials/rag): Build an application that uses your own documents to inform its responses.
- [Retrieval Augmented Generation (RAG) Part 2](/docs/tutorials/qa_chat_history): Build a RAG application that incorporates a memory of its user interactions and multi-step retrieval.
- [Question-Answering with SQL](/docs/tutorials/sql_qa): Build a question-answering system that executes SQL queries to inform its responses.
- [Summarization](/docs/tutorials/summarization): Generate summaries of (potentially long) texts.
- [Question-Answering with Graph Databases](/docs/tutorials/graph): Build a question-answering system that queries a graph database to inform its responses.

## LangSmith

LangSmith allows you to closely trace, monitor and evaluate your LLM application.
It seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of your chains as you build.

LangSmith documentation is hosted on a separate site.
You can peruse [LangSmith tutorials here](https://docs.smith.langchain.com/tutorials/).

### Evaluation

LangSmith helps you evaluate the performance of your LLM applications. The below tutorial is a great way to get started:

- [Evaluate your LLM application](https://docs.smith.langchain.com/tutorials/Developers/evaluation)



================================================
FILE: docs/core_docs/docs/tutorials/llm_chain.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_position: 0
---
"""

"""
# Build a simple LLM application with chat models and prompt templates

In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!

After reading this tutorial, you'll have a high level overview of:

- Using [language models](/docs/concepts/chat_models)

- Using [prompt templates](/docs/concepts/prompt_templates)

- Debugging and tracing your application using [LangSmith](https://docs.smith.langchain.com/)

Let's dive in!

## Setup

### Installation

To install LangChain run:

```{=mdx}
import Npm2Yarn from '@theme/Npm2Yarn';
import TabItem from '@theme/TabItem';
import CodeBlock from "@theme/CodeBlock";

<Npm2Yarn>
  langchain @langchain/core
</Npm2Yarn>
```


For more details, see our [Installation guide](/docs/how_to/installation/).

### LangSmith

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.
As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.
The best way to do this is with [LangSmith](https://smith.langchain.com).

After you sign up at the link above, make sure to set your environment variables to start logging traces:

```shell
export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."

# Reduce tracing latency if you are not in a serverless environment
# export LANGCHAIN_CALLBACKS_BACKGROUND=true
```
"""

"""
## Using Language Models

First up, let's learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably. For details on getting started with a specific model, refer to [supported integrations](/docs/integrations/chat/).

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs openaiParams={`{ model: "gpt-4" }`} />
```
"""

// @lc-docs-hide-cell
import { ChatOpenAI } from '@langchain/openai';

const model = new ChatOpenAI({
  model: "gpt-4o-mini",
  temperature: 0,
})

"""
Let's first use the model directly. [ChatModels](/docs/concepts/chat_models) are instances of LangChain [Runnables](/docs/concepts/runnables/), which means they expose a standard interface for interacting with them. To simply call the model, we can pass in a list of [messages](/docs/concepts/messages/) to the `.invoke` method.
"""

import { HumanMessage, SystemMessage } from "@langchain/core/messages"

const messages = [
  new SystemMessage("Translate the following from English into Italian"),
  new HumanMessage("hi!"),
];

await model.invoke(messages)
# Output:
#   AIMessage {

#     "id": "chatcmpl-AekSfJkg3QIOsk42BH6Qom4Gt159j",

#     "content": "Ciao!",

#     "additional_kwargs": {},

#     "response_metadata": {

#       "tokenUsage": {

#         "promptTokens": 20,

#         "completionTokens": 3,

#         "totalTokens": 23

#       },

#       "finish_reason": "stop",

#       "usage": {

#         "prompt_tokens": 20,

#         "completion_tokens": 3,

#         "total_tokens": 23,

#         "prompt_tokens_details": {

#           "cached_tokens": 0,

#           "audio_tokens": 0

#         },

#         "completion_tokens_details": {

#           "reasoning_tokens": 0,

#           "audio_tokens": 0,

#           "accepted_prediction_tokens": 0,

#           "rejected_prediction_tokens": 0

#         }

#       },

#       "system_fingerprint": "fp_6fc10e10eb"

#     },

#     "tool_calls": [],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "output_tokens": 3,

#       "input_tokens": 20,

#       "total_tokens": 23,

#       "input_token_details": {

#         "audio": 0,

#         "cache_read": 0

#       },

#       "output_token_details": {

#         "audio": 0,

#         "reasoning": 0

#       }

#     }

#   }


"""
```{=mdx}
:::tip

If we've enabled LangSmith, we can see that this run is logged to LangSmith, and can see the [LangSmith trace](https://smith.langchain.com/public/45f1a650-38fb-41e1-9b61-becc0684f2ce/r). The LangSmith trace reports [token](/docs/concepts/tokens/) usage information, latency, [standard model parameters](/docs/concepts/chat_models/#standard-parameters) (such as temperature), and other information.

:::
```

Note that ChatModels receive [message](/docs/concepts/messages/) objects as input and generate message objects as output. In addition to text content, message objects convey conversational [roles](/docs/concepts/messages/#role) and hold important data, such as [tool calls](/docs/concepts/tool_calling/) and token usage counts.

LangChain also supports chat model inputs via strings or [OpenAI format](/docs/concepts/messages/#openai-format). The following are equivalent:

```javascript
await model.invoke("Hello")

await model.invoke([ {role: "user", content: "Hello" }])

await model.invoke([new HumanMessage("hi!")])
```

### Streaming

Because chat models are [Runnables](/docs/concepts/runnables/), they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model:
"""

const stream = await model.stream(messages);

const chunks = [];
for await (const chunk of stream) {
  chunks.push(chunk);
  console.log(`${chunk.content}|`);
}
# Output:
#   |

#   C|

#   iao|

#   !|

#   |

#   |


"""
## Prompt Templates

Right now we are passing a list of messages directly into the language model. Where does this list of messages come from? Usually, it is constructed from a combination of user input and application logic. This application logic usually takes the raw user input and transforms it into a list of messages ready to pass to the language model. Common transformations include adding a system message or formatting a template with the user input.

[Prompt templates](/docs/concepts/prompt_templates/) are a concept in LangChain designed to assist with this transformation. They take in raw user input and return data (a prompt) that is ready to pass into a language model. 

Let's create a prompt template here. It will take in two user variables:

- `language`: The language to translate text into
- `text`: The text to translate
"""

import { ChatPromptTemplate } from "@langchain/core/prompts"

"""
First, let's create a string that we will format to be the system message:
"""

const systemTemplate = "Translate the following from English into {language}"

"""
Next, we can create the PromptTemplate. This will be a combination of the `systemTemplate` as well as a simpler template for where to put the text
"""

const promptTemplate = ChatPromptTemplate.fromMessages(
  [
    ["system", systemTemplate],
    ["user", "{text}"]
  ]
)

"""
Note that `ChatPromptTemplate` supports multiple [message roles](/docs/concepts/messages/#role) in a single template. We format the `language` parameter into the system message, and the user `text` into a user message.

The input to this prompt template is a dictionary. We can play around with this prompt template by itself to see what it does by itself
"""

const promptValue = await promptTemplate.invoke({ language: "italian", text: "hi!" })

promptValue
# Output:
#   ChatPromptValue {

#     lc_serializable: [33mtrue[39m,

#     lc_kwargs: {

#       messages: [

#         SystemMessage {

#           "content": "Translate the following from English into italian",

#           "additional_kwargs": {},

#           "response_metadata": {}

#         },

#         HumanMessage {

#           "content": "hi!",

#           "additional_kwargs": {},

#           "response_metadata": {}

#         }

#       ]

#     },

#     lc_namespace: [ [32m'langchain_core'[39m, [32m'prompt_values'[39m ],

#     messages: [

#       SystemMessage {

#         "content": "Translate the following from English into italian",

#         "additional_kwargs": {},

#         "response_metadata": {}

#       },

#       HumanMessage {

#         "content": "hi!",

#         "additional_kwargs": {},

#         "response_metadata": {}

#       }

#     ]

#   }


"""
We can see that it returns a `ChatPromptValue` that consists of two messages. If we want to access the messages directly we do:
"""

promptValue.toChatMessages()
# Output:
#   [

#     SystemMessage {

#       "content": "Translate the following from English into italian",

#       "additional_kwargs": {},

#       "response_metadata": {}

#     },

#     HumanMessage {

#       "content": "hi!",

#       "additional_kwargs": {},

#       "response_metadata": {}

#     }

#   ]


"""
Finally, we can invoke the chat model on the formatted prompt:
"""

const response = await model.invoke(promptValue)
console.log(`${response.content}`)
# Output:
#   Ciao!


"""
If we take a look at the LangSmith trace, we can see all three components show up in the [LangSmith trace](https://smith.langchain.com/public/6529d912-8564-4686-8df8-999c427621a7/r).
"""

"""
## Conclusion

That's it! In this tutorial you've learned how to create your first simple LLM application. You've learned how to work with language models, how to create a prompt template, and how to get great observability into applications you create with LangSmith.

This just scratches the surface of what you will want to learn to become a proficient AI Engineer. Luckily - we've got a lot of other resources!

For further reading on the core concepts of LangChain, we've got detailed [Conceptual Guides](/docs/concepts).

If you have more specific questions on these concepts, check out the following sections of the how-to guides:

- [Chat models](/docs/how_to/#chat-models)
- [Prompt templates](/docs/how_to/#prompt-templates)

And the LangSmith docs:

- [LangSmith](https://docs.smith.langchain.com)
"""



================================================
FILE: docs/core_docs/docs/tutorials/qa_chat_history.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
sidebar_position: 2
---
"""

"""
# Build a Retrieval Augmented Generation (RAG) App: Part 2

In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of "memory" of past questions and answers, and some logic for incorporating those into its current thinking.

This is a the second part of a multi-part tutorial:

- [Part 1](/docs/tutorials/rag) introduces RAG and walks through a minimal implementation.
- [Part 2](/docs/tutorials/qa_chat_history) (this guide) extends the implementation to accommodate conversation-style interactions and multi-step retrieval processes.

Here we focus on **adding logic for incorporating historical messages.** This involves the management of a [chat history](/docs/concepts/chat_history).

We will cover two approaches:

1. [Chains](/docs/tutorials/qa_chat_history/#chains), in which we execute at most one retrieval step;
2. [Agents](/docs/tutorials/qa_chat_history/#agents), in which we give an LLM discretion to execute multiple retrieval steps.

```{=mdx}
:::note

The methods presented here leverage [tool-calling](/docs/concepts/tool_calling/) capabilities in modern [chat models](/docs/concepts/chat_models). See [this page](/docs/integrations/chat/) for a table of models supporting tool calling features.

:::
```

For the external knowledge source, we will use the same [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng from the [Part 1](/docs/tutorials/rag) of the RAG tutorial.
"""

"""
## Setup

### Components

We will need to select three components from LangChain's suite of integrations.

A [chat model](/docs/integrations/chat/):

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

// @lc-docs-hide-cell
import { ChatOpenAI } from '@langchain/openai';

const llm = new ChatOpenAI({
  model: "gpt-4o-mini",
  temperature: 0,
})

"""
An [embedding model](/docs/integrations/text_embedding/):

```{=mdx}
import EmbeddingTabs from "@theme/EmbeddingTabs";

<EmbeddingTabs/>
```
"""

// @lc-docs-hide-cell
import { OpenAIEmbeddings } from "@langchain/openai";

const embeddings = new OpenAIEmbeddings({model: "text-embedding-3-large"});

"""
And a [vector store](/docs/integrations/vectorstores/):

```{=mdx}
import VectorStoreTabs from "@theme/VectorStoreTabs";

<VectorStoreTabs/>
```
"""

// @lc-docs-hide-cell
import { MemoryVectorStore } from "langchain/vectorstores/memory";

const vectorStore = new MemoryVectorStore(embeddings);

"""
### Dependencies

In addition, we'll use the following packages:

```{=mdx}
import Npm2Yarn from '@theme/Npm2Yarn';

<Npm2Yarn>
  langchain @langchain/community @langchain/langgraph cheerio
</Npm2Yarn>
```
"""

"""
### LangSmith

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://docs.smith.langchain.com).

Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:


```bash
export LANGSMITH_TRACING=true
export LANGSMITH_API_KEY=YOUR_KEY

# Reduce tracing latency if you are not in a serverless environment
# export LANGCHAIN_CALLBACKS_BACKGROUND=true
```
"""

"""
## Chains

Let's first revisit the vector store we built in [Part 1](/docs/tutorials/rag), which indexes an [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng.
"""

import "cheerio";
import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";
import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio";

// Load and chunk contents of the blog
const pTagSelector = "p";
const cheerioLoader = new CheerioWebBaseLoader(
  "https://lilianweng.github.io/posts/2023-06-23-agent/",
  {
    selector: pTagSelector
  }
);

const docs = await cheerioLoader.load();

const splitter = new RecursiveCharacterTextSplitter({
  chunkSize: 1000, chunkOverlap: 200
});
const allSplits = await splitter.splitDocuments(docs);

// Index chunks
await vectorStore.addDocuments(allSplits)

"""
In the [Part 1](/docs/tutorials/rag) of the RAG tutorial, we represented the user input, retrieved context, and generated answer as separate keys in the state. Conversational experiences can be naturally represented using a sequence of [messages](/docs/concepts/messages/). In addition to messages from the user and assistant, retrieved documents and other artifacts can be incorporated into a message sequence via [tool messages](/docs/concepts/messages/#toolmessage). This motivates us to represent the state of our RAG application using a sequence of messages. Specifically, we will have

1. User input as a `HumanMessage`;
2. Vector store query as an `AIMessage` with tool calls;
3. Retrieved documents as a `ToolMessage`;
4. Final response as a `AIMessage`.

This model for state is so versatile that LangGraph offers a built-in version for convenience:
```javascript
import { MessagesAnnotation, StateGraph } from "@langchain/langgraph";

const graph = new StateGraph(MessagesAnnotation)
```
"""

"""
Leveraging [tool-calling](/docs/concepts/tool_calling/) to interact with a retrieval step has another benefit, which is that the query for the retrieval is generated by our model. This is especially important in a conversational setting, where user queries may require contextualization based on the chat history. For instance, consider the following exchange:

> Human: "What is Task Decomposition?"
>
> AI: "Task decomposition involves breaking down complex tasks into smaller and simpler steps to make them more manageable for an agent or model."
>
> Human: "What are common ways of doing it?"

In this scenario, a model could generate a query such as `"common approaches to task decomposition"`. Tool-calling facilitates this naturally. As in the [query analysis](/docs/tutorials/rag#query-analysis) section of the RAG tutorial, this allows a model to rewrite user queries into more effective search queries. It also provides support for direct responses that do not involve a retrieval step (e.g., in response to a generic greeting from the user).

Let's turn our retrieval step into a [tool](/docs/concepts/tools):
"""

import { z } from "zod";
import { tool } from "@langchain/core/tools";

const retrieveSchema = z.object({query: z.string()});

const retrieve = tool(
  async ({ query }) => {
    const retrievedDocs = await vectorStore.similaritySearch(query, 2);
    const serialized = retrievedDocs.map(
      doc => `Source: ${doc.metadata.source}\nContent: ${doc.pageContent}`
    ).join("\n");
    return [
      serialized,
      retrievedDocs,
    ];
  },
  {
    name: "retrieve",
    description: "Retrieve information related to a query.",
    schema: retrieveSchema,
    responseFormat: "content_and_artifact",
  }
);

"""
See [this guide](/docs/how_to/custom_tools/) for more detail on creating tools.

Our graph will consist of three nodes:

1. A node that fields the user input, either generating a query for the retriever or responding directly;
2. A node for the retriever tool that executes the retrieval step;
3. A node that generates the final response using the retrieved context.

We build them below. Note that we leverage another pre-built LangGraph component, [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode), that executes the tool and adds the result as a `ToolMessage` to the state.
"""

import { 
    AIMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage
} from "@langchain/core/messages";
import { MessagesAnnotation } from "@langchain/langgraph";
import { ToolNode } from "@langchain/langgraph/prebuilt";


// Step 1: Generate an AIMessage that may include a tool-call to be sent.
async function queryOrRespond(state: typeof MessagesAnnotation.State) {
  const llmWithTools = llm.bindTools([retrieve])
  const response = await llmWithTools.invoke(state.messages);
  // MessagesState appends messages to state instead of overwriting
  return { messages: [response] };
}


// Step 2: Execute the retrieval.
const tools = new ToolNode([retrieve]);


// Step 3: Generate a response using the retrieved content.
async function generate(state: typeof MessagesAnnotation.State) {
  // Get generated ToolMessages
  let recentToolMessages = [];
    for (let i = state["messages"].length - 1; i >= 0; i--) {
      let message = state["messages"][i];
      if (message instanceof ToolMessage) {
        recentToolMessages.push(message);
      } else {
        break;
      }
    }
  let toolMessages = recentToolMessages.reverse();
  
  // Format into prompt
  const docsContent = toolMessages.map(doc => doc.content).join("\n");
  const systemMessageContent = 
    "You are an assistant for question-answering tasks. " +
    "Use the following pieces of retrieved context to answer " +
    "the question. If you don't know the answer, say that you " +
    "don't know. Use three sentences maximum and keep the " +
    "answer concise." +
    "\n\n" +
    `${docsContent}`;

  const conversationMessages = state.messages.filter(message => 
    message instanceof HumanMessage || 
    message instanceof SystemMessage || 
    (message instanceof AIMessage && message.tool_calls.length == 0)
  );
  const prompt = [new SystemMessage(systemMessageContent), ...conversationMessages];

  // Run
  const response = await llm.invoke(prompt)
  return { messages: [response] };
}

"""
Finally, we compile our application into a single `graph` object. In this case, we are just connecting the steps into a sequence. We also allow the first `query_or_respond` step to "short-circuit" and respond directly to the user if it does not generate a tool call. This allows our application to support conversational experiences-- e.g., responding to generic greetings that may not require a retrieval step
"""

import { StateGraph } from "@langchain/langgraph";
import { toolsCondition } from "@langchain/langgraph/prebuilt";


const graphBuilder = new StateGraph(MessagesAnnotation)
  .addNode("queryOrRespond", queryOrRespond)
  .addNode("tools", tools)
  .addNode("generate", generate)
  .addEdge("__start__", "queryOrRespond")
  .addConditionalEdges(
    "queryOrRespond",
    toolsCondition,
    {__end__: "__end__", tools: "tools"}
  )
  .addEdge("tools", "generate")
  .addEdge("generate", "__end__")

const graph = graphBuilder.compile();

"""
```javascript
// Note: tslab only works inside a jupyter notebook. Don't worry about running this code yourself!
import * as tslab from "tslab";

const image = await graph.getGraph().drawMermaidPng();
const arrayBuffer = await image.arrayBuffer();

await tslab.display.png(new Uint8Array(arrayBuffer));
```

![graph_img_rag_part_2](../../static/img/graph_img_rag_part_2.png)
"""

"""
Let's test our application.

```{=mdx}
<details>
<summary>Expand for `prettyPrint` code.</summary>
```
"""

import { BaseMessage, isAIMessage } from "@langchain/core/messages";

const prettyPrint = (message: BaseMessage) => {
  let txt = `[${message._getType()}]: ${message.content}`;
  if (
    (isAIMessage(message) && message.tool_calls?.length) ||
    0 > 0
  ) {
    const tool_calls = (message as AIMessage)?.tool_calls
      ?.map((tc) => `- ${tc.name}(${JSON.stringify(tc.args)})`)
      .join("\n");
    txt += ` \nTools: \n${tool_calls}`;
  }
  console.log(txt);
};

"""
```{=mdx}
</details>
```
"""

"""
Note that it responds appropriately to messages that do not require an additional retrieval step:
"""

let inputs1 = { messages: [{ role: "user", content: "Hello" }] };

for await (
  const step of await graph.stream(inputs1, {
    streamMode: "values",
  })
) {
    const lastMessage = step.messages[step.messages.length - 1];
    prettyPrint(lastMessage);
    console.log("-----\n");
}
# Output:
#   [human]: Hello

#   -----

#   

#   [ai]: Hello! How can I assist you today?

#   -----

#   


"""
And when executing a search, we can stream the steps to observe the query generation, retrieval, and answer generation:
"""

let inputs2 = { messages: [{ role: "user", content: "What is Task Decomposition?" }] };

for await (
  const step of await graph.stream(inputs2, {
    streamMode: "values",
  })
) {
    const lastMessage = step.messages[step.messages.length - 1];
    prettyPrint(lastMessage);
    console.log("-----\n");
}
# Output:
#   [human]: What is Task Decomposition?

#   -----

#   

#   [ai]:  

#   Tools: 

#   - retrieve({"query":"Task Decomposition"})

#   -----

#   

#   [tool]: Source: https://lilianweng.github.io/posts/2023-06-23-agent/

#   Content: hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain

#   Source: https://lilianweng.github.io/posts/2023-06-23-agent/

#   Content: System message:Think step by step and reason yourself to the right decisions to make sure we get it right.

#   You will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.Then you will output the content of each file including ALL code.

#   Each file must strictly follow a markdown code block format, where the following tokens must be replaced such that

#   FILENAME is the lowercase file name including the file extension,

#   LANG is the markup code block language for the code’s language, and CODE is the code:FILENAMEYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.

#   Please note that the code should be fully functional. No placeholders.Follow a language and framework appropriate best practice file naming convention.

#   Make sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.

#   -----

#   

#   [ai]: Task decomposition is the process of breaking down a complex task into smaller, more manageable steps or subgoals. This can be achieved through various methods, such as using prompts for large language models (LLMs), task-specific instructions, or human inputs. It helps in simplifying the problem-solving process and enhances understanding of the task at hand.

#   -----

#   


"""
Check out the LangSmith trace [here](https://smith.langchain.com/public/c6ed4e16-b9ed-46cc-912e-6a580d3c47ed/r).
"""

"""
### Stateful management of chat history

```{=mdx}
:::note

This section of the tutorial previously used the [RunnableWithMessageHistory](https://api.js.langchain.com/classes/_langchain_core.runnables.RunnableWithMessageHistory.html) abstraction. You can access that version of the documentation in the [v0.2 docs](https://js.langchain.com/v0.2/docs/tutorials/qa_chat_history).

As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of [LangGraph persistence](https://langchain-ai.github.io/langgraphjs/concepts/persistence/) to incorporate `memory` into new LangChain applications.

If your code is already relying on `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do **not** need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.

Please see [How to migrate to LangGraph Memory](/docs/versions/migrating_memory/) for more details.
:::
```

In production, the Q&A application will usually persist the chat history into a database, and be able to read and update it appropriately.

[LangGraph](https://langchain-ai.github.io/langgraphjs/) implements a built-in [persistence layer](https://langchain-ai.github.io/langgraphjs/concepts/persistence/), making it ideal for chat applications that support multiple conversational turns.

To manage multiple conversational turns and threads, all we have to do is specify a [checkpointer](https://langchain-ai.github.io/langgraphjs/concepts/persistence/) when compiling our application. Because the nodes in our graph are appending messages to the state, we will retain a consistent chat history across invocations.

LangGraph comes with a simple in-memory checkpointer, which we use below. See its [documentation](https://langchain-ai.github.io/langgraphjs/concepts/persistence/) for more detail, including how to use different persistence backends (e.g., SQLite or Postgres).

For a detailed walkthrough of how to manage message history, head to the [How to add message history (memory)](/docs/how_to/message_history) guide.
"""

import { MemorySaver } from "@langchain/langgraph";

const checkpointer = new MemorySaver();
const graphWithMemory = graphBuilder.compile({ checkpointer });

// Specify an ID for the thread
const threadConfig = {
    configurable: { thread_id: "abc123" },
    streamMode: "values" as const
};

"""
We can now invoke similar to before:
"""

let inputs3 = { messages: [{ role: "user", content: "What is Task Decomposition?" }] };

for await (
  const step of await graphWithMemory.stream(inputs3, threadConfig)
) {
    const lastMessage = step.messages[step.messages.length - 1];
    prettyPrint(lastMessage);
    console.log("-----\n");
}
# Output:
#   [human]: What is Task Decomposition?

#   -----

#   

#   [ai]:  

#   Tools: 

#   - retrieve({"query":"Task Decomposition"})

#   -----

#   

#   [tool]: Source: https://lilianweng.github.io/posts/2023-06-23-agent/

#   Content: hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain

#   Source: https://lilianweng.github.io/posts/2023-06-23-agent/

#   Content: System message:Think step by step and reason yourself to the right decisions to make sure we get it right.

#   You will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.Then you will output the content of each file including ALL code.

#   Each file must strictly follow a markdown code block format, where the following tokens must be replaced such that

#   FILENAME is the lowercase file name including the file extension,

#   LANG is the markup code block language for the code’s language, and CODE is the code:FILENAMEYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.

#   Please note that the code should be fully functional. No placeholders.Follow a language and framework appropriate best practice file naming convention.

#   Make sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.

#   -----

#   

#   [ai]: Task decomposition is the process of breaking down a complex task into smaller, more manageable steps or subgoals. This can be achieved through various methods, such as using prompts for large language models (LLMs), task-specific instructions, or human inputs. It helps in simplifying the problem-solving process and enhances understanding of the task at hand.

#   -----

#   


let inputs4 = { messages: [{ role: "user", content: "Can you look up some common ways of doing it?" }] };

for await (
  const step of await graphWithMemory.stream(inputs4, threadConfig)
) {
    const lastMessage = step.messages[step.messages.length - 1];
    prettyPrint(lastMessage);
    console.log("-----\n");
}
# Output:
#   [human]: Can you look up some common ways of doing it?

#   -----

#   

#   [ai]:  

#   Tools: 

#   - retrieve({"query":"common methods of task decomposition"})

#   -----

#   

#   [tool]: Source: https://lilianweng.github.io/posts/2023-06-23-agent/

#   Content: hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain

#   Source: https://lilianweng.github.io/posts/2023-06-23-agent/

#   Content: be provided by other developers (as in Plugins) or self-defined (as in function calls).HuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.The system comprises of 4 stages:(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.Instruction:(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.Instruction:(3) Task execution: Expert models execute on the specific tasks and log results.Instruction:(4) Response generation:

#   -----

#   

#   [ai]: Common ways of task decomposition include using large language models (LLMs) with simple prompts like "Steps for XYZ" or "What are the subgoals for achieving XYZ?", employing task-specific instructions (e.g., "Write a story outline"), and incorporating human inputs. Additionally, methods like the Tree of Thoughts approach explore multiple reasoning possibilities at each step, creating a structured tree of thoughts. These techniques facilitate breaking down tasks into manageable components for better execution.

#   -----

#   


"""
Note that the query generated by the model in the second question incorporates the conversational context.

The [LangSmith](https://smith.langchain.com/public/c8b2c1ba-8c8b-47ab-b298-3502e0688711/r) trace is particularly informative here, as we can see exactly what messages are visible to our chat model at each step.
"""

"""
## Agents

[Agents](/docs/concepts/agents) leverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allows you to offload additional discretion over the retrieval process. Although their behavior is less predictable than the above "chain", they are able to execute multiple retrieval steps in service of a query, or iterate on a single search.

Below we assemble a minimal RAG agent. Using LangGraph's [pre-built ReAct agent constructor](https://langchain-ai.github.io/langgraph/how-tos/#langgraph.prebuilt.chat_agent_executor.create_react_agent), we can do this in one line.

```{=mdx}
:::tip

Check out LangGraph's [Agentic RAG](https://langchain-ai.github.io/langgraphjs/tutorials/rag/langgraph_agentic_rag/) tutorial for more advanced formulations.

:::
```
"""

import { createReactAgent } from "@langchain/langgraph/prebuilt";

const agent = createReactAgent({ llm: llm, tools: [retrieve] });

"""
Let's inspect the graph:
"""

"""
```javascript
// Note: tslab only works inside a jupyter notebook. Don't worry about running this code yourself!
import * as tslab from "tslab";

const image = await agent.getGraph().drawMermaidPng();
const arrayBuffer = await image.arrayBuffer();

await tslab.display.png(new Uint8Array(arrayBuffer));
```

![graph_img_react](../../static/img/graph_img_react.png)
"""

"""
The key difference from our earlier implementation is that instead of a final generation step that ends the run, here the tool invocation loops back to the original LLM call. The model can then either answer the question using the retrieved context, or generate another tool call to obtain more information.

Let's test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:
"""

let inputMessage = `What is the standard method for Task Decomposition?
Once you get the answer, look up common extensions of that method.`

let inputs5 = { messages: [{ role: "user", content: inputMessage }] };

for await (
  const step of await agent.stream(inputs5, {
    streamMode: "values",
  })
) {
    const lastMessage = step.messages[step.messages.length - 1];
    prettyPrint(lastMessage);
    console.log("-----\n");
}
# Output:
#   [human]: What is the standard method for Task Decomposition?

#   Once you get the answer, look up common extensions of that method.

#   -----

#   

#   [ai]:  

#   Tools: 

#   - retrieve({"query":"standard method for Task Decomposition"})

#   -----

#   

#   [tool]: Source: https://lilianweng.github.io/posts/2023-06-23-agent/

#   Content: hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain

#   Source: https://lilianweng.github.io/posts/2023-06-23-agent/

#   Content: System message:Think step by step and reason yourself to the right decisions to make sure we get it right.

#   You will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.Then you will output the content of each file including ALL code.

#   Each file must strictly follow a markdown code block format, where the following tokens must be replaced such that

#   FILENAME is the lowercase file name including the file extension,

#   LANG is the markup code block language for the code’s language, and CODE is the code:FILENAMEYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.

#   Please note that the code should be fully functional. No placeholders.Follow a language and framework appropriate best practice file naming convention.

#   Make sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.

#   -----

#   

#   [ai]:  

#   Tools: 

#   - retrieve({"query":"common extensions of Task Decomposition method"})

#   -----

#   

#   [tool]: Source: https://lilianweng.github.io/posts/2023-06-23-agent/

#   Content: hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain

#   Source: https://lilianweng.github.io/posts/2023-06-23-agent/

#   Content: be provided by other developers (as in Plugins) or self-defined (as in function calls).HuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.The system comprises of 4 stages:(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.Instruction:(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.Instruction:(3) Task execution: Expert models execute on the specific tasks and log results.Instruction:(4) Response generation:

#   -----

#   

#   [ai]: ### Standard Method for Task Decomposition

#   

#   The standard method for task decomposition involves breaking down hard tasks into smaller, more manageable steps. This can be achieved through various approaches:

#   

#   1. **Chain of Thought (CoT)**: This method transforms large tasks into multiple manageable tasks, providing insight into the model's reasoning process.

#   2. **Prompting**: Using simple prompts like "Steps for XYZ" or "What are the subgoals for achieving XYZ?" to guide the decomposition.

#   3. **Task-Specific Instructions**: Providing specific instructions tailored to the task, such as "Write a story outline" for writing a novel.

#   4. **Human Inputs**: Involving human input to assist in the decomposition process.

#   

#   ### Common Extensions of Task Decomposition

#   

#   Several extensions have been developed to enhance the task decomposition process:

#   

#   1. **Tree of Thoughts (ToT)**: This method extends CoT by exploring multiple reasoning possibilities at each step. It decomposes the problem into multiple thought steps and generates various thoughts per step, creating a tree structure. The search process can utilize either breadth-first search (BFS) or depth-first search (DFS), with each state evaluated by a classifier or through majority voting.

#   

#   2. **LLM+P**: This approach involves using an external classical planner for long-horizon planning, integrating planning domains to enhance the decomposition process.

#   

#   3. **HuggingGPT**: This framework utilizes ChatGPT as a task planner to select models from the HuggingFace platform based on model descriptions. It consists of four stages:

#      - **Task Planning**: Parsing user requests into multiple tasks with attributes like task type, ID, dependencies, and arguments.

#      - **Model Selection**: Distributing tasks to expert models based on a multiple-choice question format.

#      - **Task Execution**: Expert models execute specific tasks and log results.

#      - **Response Generation**: Compiling the results into a coherent response.

#   

#   These extensions aim to improve the efficiency and effectiveness of task decomposition, making it easier to manage complex tasks.

#   -----

#   


"""
Note that the agent:

1. Generates a query to search for a standard method for task decomposition;
2. Receiving the answer, generates a second query to search for common extensions of it;
3. Having received all necessary context, answers the question.

We can see the full sequence of steps, along with latency and other metadata, in the [LangSmith trace](https://smith.langchain.com/public/67b7642b-78d0-482a-bb49-fe08674bf972/r).

## Next steps

We've covered the steps to build a basic conversational Q&A application:

- We used chains to build a predictable application that generates at most one query per user input;
- We used agents to build an application that can iterate on a sequence of queries.

To explore different types of retrievers and retrieval strategies, visit the [retrievers](/docs/how_to/#retrievers) section of the how-to guides.

For a detailed walkthrough of LangChain's conversation memory abstractions, visit the [How to add message history (memory)](/docs/how_to/message_history) guide.

To learn more about agents, check out the [conceptual guide](/docs/concepts/agents) and LangGraph [agent architectures](https://langchain-ai.github.io/langgraphjs/concepts/agentic_concepts/) page.
"""



================================================
FILE: docs/core_docs/docs/tutorials/rag.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Build a Retrieval Augmented Generation (RAG) App: Part 1

One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or [RAG](/docs/concepts/rag/).

This is a multi-part tutorial:

- [Part 1](/docs/tutorials/rag) (this guide) introduces RAG and walks through a minimal implementation.
- [Part 2](/docs/tutorials/qa_chat_history) extends the implementation to accommodate conversation-style interactions and multi-step retrieval processes.

This tutorial will show how to build a simple Q&A application
over a text data source. Along the way we’ll go over a typical Q&A
architecture and highlight additional resources for more advanced Q&A techniques. We’ll also see
how LangSmith can help us trace and understand our application.
LangSmith will become increasingly helpful as our application grows in
complexity.

If you're already familiar with basic retrieval, you might also be interested in
this [high-level overview of different retrieval techinques](/docs/concepts/retrieval).

**Note**: Here we focus on Q&A for unstructured data. If you are interested for RAG over structured data, check out our tutorial on doing [question/answering over SQL data](/docs/tutorials/sql_qa).

## Overview
A typical RAG application has two main components:

**Indexing**: a pipeline for ingesting data from a source and indexing it. *This usually happens offline.*

**Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.

Note: the indexing portion of this tutorial will largely follow the [semantic search tutorial](/docs/tutorials/retrievers).

The most common full sequence from raw data to answer looks like:

### Indexing
1. **Load**: First we need to load our data. This is done with [Document Loaders](/docs/concepts/document_loaders).
2. **Split**: [Text splitters](/docs/concepts/text_splitters) break large `Documents` into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.
3. **Store**: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a [VectorStore](/docs/concepts/vectorstores) and [Embeddings](/docs/concepts/embedding_models) model.

![index_diagram](../../static/img/rag_indexing.png)

### Retrieval and generation
4. **Retrieve**: Given a user input, relevant splits are retrieved from storage using a [Retriever](/docs/concepts/retrievers).
5. **Generate**: A [ChatModel](/docs/concepts/chat_models) / [LLM](/docs/concepts/text_llms) produces an answer using a prompt that includes both the question with the retrieved data

![retrieval_diagram](../../static/img/rag_retrieval_generation.png)

Once we've indexed our data, we will use [LangGraph](https://langchain-ai.github.io/langgraphjs/) as our orchestration framework to implement the retrieval and generation steps.

## Setup

### Jupyter Notebook

This and other tutorials are perhaps most conveniently run in a [Jupyter notebooks](https://jupyter.org/). Going through guides in an interactive environment is a great way to better understand them. See [here](https://jupyter.org/install) for instructions on how to install.

### Installation

This guide requires the following dependencies:

```{=mdx}
import Npm2Yarn from '@theme/Npm2Yarn';

<Npm2Yarn>
  langchain @langchain/core @langchain/langgraph
</Npm2Yarn>
```

For more details, see our [Installation guide](/docs/how_to/installation).

### LangSmith

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.
As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.
The best way to do this is with [LangSmith](https://smith.langchain.com).

After you sign up at the link above, make sure to set your environment variables to start logging traces:

```shell
export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."

# Reduce tracing latency if you are not in a serverless environment
# export LANGCHAIN_CALLBACKS_BACKGROUND=true
```
"""

"""
## Components

We will need to select three components from LangChain's suite of integrations.

A [chat model](/docs/integrations/chat/):

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

// @lc-docs-hide-cell
import { ChatOpenAI } from '@langchain/openai';

const llm = new ChatOpenAI({
  model: "gpt-4o-mini",
  temperature: 0,
})

"""
An [embedding model](/docs/integrations/text_embedding/):

```{=mdx}
import EmbeddingTabs from "@theme/EmbeddingTabs";

<EmbeddingTabs/>
```
"""

// @lc-docs-hide-cell
import { OpenAIEmbeddings } from "@langchain/openai";

const embeddings = new OpenAIEmbeddings({model: "text-embedding-3-large"});

"""
And a [vector store](/docs/integrations/vectorstores/):

```{=mdx}
import VectorStoreTabs from "@theme/VectorStoreTabs";

<VectorStoreTabs/>
```
"""

// @lc-docs-hide-cell
import { MemoryVectorStore } from "langchain/vectorstores/memory";

const vectorStore = new MemoryVectorStore(embeddings);

"""
## Preview

In this guide we’ll build an app that answers questions about the website's content. The specific website we will use is the [LLM Powered Autonomous
Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post
by Lilian Weng, which allows us to ask questions about the contents of
the post.

We can create a simple indexing pipeline and RAG chain to do this in ~50
lines of code.

```javascript
import "cheerio";
import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio";
import { Document } from "@langchain/core/documents";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { pull } from "langchain/hub";
import { Annotation, StateGraph } from "@langchain/langgraph";
import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";


// Load and chunk contents of blog
const pTagSelector = "p";
const cheerioLoader = new CheerioWebBaseLoader(
  "https://lilianweng.github.io/posts/2023-06-23-agent/",
  {
    selector: pTagSelector
  }
);

const docs = await cheerioLoader.load();

const splitter = new RecursiveCharacterTextSplitter({
  chunkSize: 1000, chunkOverlap: 200
});
const allSplits = await splitter.splitDocuments(docs);


// Index chunks
await vectorStore.addDocuments(allSplits)

// Define prompt for question-answering
const promptTemplate = await pull<ChatPromptTemplate>("rlm/rag-prompt");

// Define state for application
const InputStateAnnotation = Annotation.Root({
  question: Annotation<string>,
});

const StateAnnotation = Annotation.Root({
  question: Annotation<string>,
  context: Annotation<Document[]>,
  answer: Annotation<string>,
});

// Define application steps
const retrieve = async (state: typeof InputStateAnnotation.State) => {
  const retrievedDocs = await vectorStore.similaritySearch(state.question)
  return { context: retrievedDocs };
};


const generate = async (state: typeof StateAnnotation.State) => {
  const docsContent = state.context.map(doc => doc.pageContent).join("\n");
  const messages = await promptTemplate.invoke({ question: state.question, context: docsContent });
  const response = await llm.invoke(messages);
  return { answer: response.content };
};


// Compile application and test
const graph = new StateGraph(StateAnnotation)
  .addNode("retrieve", retrieve)
  .addNode("generate", generate)
  .addEdge("__start__", "retrieve")
  .addEdge("retrieve", "generate")
  .addEdge("generate", "__end__")
  .compile();
```

```javascript
let inputs = { question: "What is Task Decomposition?" };

const result = await graph.invoke(inputs);
console.log(result.answer)
```

```
Task decomposition is the process of breaking down complex tasks into smaller, more manageable steps. This can be achieved through various methods, including prompting large language models (LLMs) or using task-specific instructions. Techniques like Chain of Thought (CoT) and Tree of Thoughts further enhance this process by structuring reasoning and exploring multiple possibilities at each step.
```
"""

"""
Check out the [LangSmith
trace](https://smith.langchain.com/public/84a36239-b466-41bd-ac84-befc33ab50df/r).
"""

"""
## Detailed walkthrough

Let’s go through the above code step-by-step to really understand what’s
going on.

## 1. Indexing {#indexing}

```{=mdx}
:::note

This section is an abbreviated version of the content in the [semantic search tutorial](/docs/tutorials/retrievers).
If you're comfortable with [document loaders](/docs/concepts/document_loaders), [embeddings](/docs/concepts/embedding_models), and [vector stores](/docs/concepts/vectorstores),
feel free to skip to the next section on [retrieval and generation](/docs/tutorials/rag/#orchestration).

:::
```

### Loading documents

We need to first load the blog post contents. We can use [DocumentLoaders](/docs/concepts/document_loaders) for this, which are objects that load in data from a source and return a list of [Documents](https://api.js.langchain.com/classes/langchain_core.documents.Document.html). A Document is an object with some pageContent (`string`) and metadata (`Record<string, any>`).

In this case we’ll use the [CheerioWebBaseLoader](https://api.js.langchain.com/classes/langchain.document_loaders_web_cheerio.CheerioWebBaseLoader.html), which uses cheerio to load HTML form web URLs and parse it to text. We can pass custom selectors to the constructor to only parse specific elements:
"""

import "cheerio";
import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio";

const pTagSelector = "p";
const cheerioLoader = new CheerioWebBaseLoader(
  "https://lilianweng.github.io/posts/2023-06-23-agent/",
  {
    selector: pTagSelector
  }
);

const docs = await cheerioLoader.load();

console.assert(docs.length === 1);
console.log(`Total characters: ${docs[0].pageContent.length}`);
# Output:
#   Total characters: 22360


console.log(docs[0].pageContent.slice(0, 500));
# Output:
#   Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:A complicated task usually involv


"""
#### Go deeper

`DocumentLoader`: Class that loads data from a source as list of Documents.

- [Docs](/docs/concepts/document_loaders): Detailed documentation on how to use
- [Integrations](/docs/integrations/document_loaders/)
- [Interface](https://api.js.langchain.com/classes/langchain.document_loaders_base.BaseDocumentLoader.html): API reference for the base interface.

### Splitting documents

Our loaded document is over 42k characters which is too long to fit
into the context window of many models. Even for those models that could
fit the full post in their context window, models can struggle to find
information in very long inputs.

To handle this we’ll split the `Document` into chunks for embedding and
vector storage. This should help us retrieve only the most relevant parts
of the blog post at run time.

As in the [semantic search tutorial](/docs/tutorials/retrievers), we use a
[RecursiveCharacterTextSplitter](/docs/how_to/recursive_text_splitter),
which will recursively split the document using common separators like
new lines until each chunk is the appropriate size. This is the
recommended text splitter for generic text use cases.
"""

import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";

const splitter = new RecursiveCharacterTextSplitter({
  chunkSize: 1000, chunkOverlap: 200
});
const allSplits = await splitter.splitDocuments(docs);
console.log(`Split blog post into ${allSplits.length} sub-documents.`);
# Output:
#   Split blog post into 29 sub-documents.


"""
#### Go deeper

`TextSplitter`: Object that splits a list of `Document`s into smaller chunks. Subclass of `DocumentTransformers`.
- Explore `Context-aware splitters`, which keep the location (“context”) of each split in the original `Document`: - [Markdown files](/docs/how_to/code_splitter/#markdown) - [Code](/docs/how_to/code_splitter/) (15+ langs) - [Interface](https://api.js.langchain.com/classes/langchain_textsplitters.TextSplitter.html): API reference for the base interface.

`DocumentTransformer`: Object that performs a transformation on a list of `Document`s. - Docs: Detailed documentation on how to use `DocumentTransformer`s - [Integrations](/docs/integrations/document_transformers) - [Interface](https://api.js.langchain.com/classes/langchain_core.documents.BaseDocumentTransformer.html): API reference for the base interface.

### Storing documents

Now we need to index our 66 text chunks so that we can search over them
at runtime. Following the [semantic search tutorial](/docs/tutorials/retrievers),
our approach is to [embed](/docs/concepts/embedding_models/) the contents of each document split and insert these embeddings
into a [vector store](/docs/concepts/vectorstores/). Given an input query, we can then use
vector search to retrieve relevant documents.

We can embed and store all of our document splits in a single command
using the vector store and embeddings model selected at the [start of the tutorial](/docs/tutorials/rag/#components).
"""

await vectorStore.addDocuments(allSplits)

"""
#### Go deeper

`Embeddings`: Wrapper around a text embedding model, used for converting text to embeddings. - [Docs](/docs/concepts/embedding_models): Detailed documentation on how to use embeddings. - [Integrations](/docs/integrations/text_embedding): 30+ integrations to choose from. - [Interface](https://api.js.langchain.com/classes/langchain_core.embeddings.Embeddings.html): API reference for the base interface.

`VectorStore`: Wrapper around a vector database, used for storing and querying embeddings. - [Docs](/docs/concepts/vectorstores): Detailed documentation on how to use vector stores. - [Integrations](/docs/integrations/vectorstores): 40+ integrations to choose from. - [Interface](https://api.js.langchain.com/classes/langchain_core.vectorstores.VectorStore.html): API reference for the base interface.

This completes the **Indexing** portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.
"""

"""
## 2. Retrieval and Generation {#orchestration}

Now let’s write the actual application logic. We want to create a simple
application that takes a user question, searches for documents relevant
to that question, passes the retrieved documents and initial question to
a model, and returns an answer.

For generation, we will use the chat model selected at the [start of the tutorial](/docs/tutorials/rag/#components).

We’ll use a prompt for RAG that is checked into the LangChain prompt hub
([here](https://smith.langchain.com/hub/rlm/rag-prompt)).
"""

import { pull } from "langchain/hub";
import { ChatPromptTemplate } from "@langchain/core/prompts";

const promptTemplate = await pull<ChatPromptTemplate>("rlm/rag-prompt");

// Example:
const example_prompt = await promptTemplate.invoke(
    { context: "(context goes here)", question: "(question goes here)" }
)
const example_messages = example_prompt.messages

console.assert(example_messages.length === 1);
example_messages[0].content
# Output:
#   You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.

#   Question: (question goes here) 

#   Context: (context goes here) 

#   Answer:


"""
We'll use [LangGraph](https://langchain-ai.github.io/langgraphjs/) to tie together the retrieval and generation steps into a single application. This will bring a number of benefits:

- We can define our application logic once and automatically support multiple invocation modes, including streaming, async, and batched calls.
- We get streamlined deployments via [LangGraph Platform](https://langchain-ai.github.io/langgraphjs/concepts/langgraph_platform/).
- LangSmith will automatically trace the steps of our application together.
- We can easily add key features to our application, including [persistence](https://langchain-ai.github.io/langgraphjs/concepts/persistence/) and [human-in-the-loop approval](https://langchain-ai.github.io/langgraphjs/concepts/human_in_the_loop/), with minimal code changes.

To use LangGraph, we need to define three things:

1. The state of our application;
2. The nodes of our application (i.e., application steps);
3. The "control flow" of our application (e.g., the ordering of the steps).

#### State:

The [state](https://langchain-ai.github.io/langgraphjs/concepts/low_level/#state) of our application controls what data is input to the application, transferred between steps, and output by the application.

For a simple RAG application, we can just keep track of the input question, retrieved context, and generated answer.

Read more about defining graph states [here](https://langchain-ai.github.io/langgraphjs/how-tos/define-state/).
"""

import { Document } from "@langchain/core/documents";
import { Annotation } from "@langchain/langgraph";


const InputStateAnnotation = Annotation.Root({
  question: Annotation<string>,
});


const StateAnnotation = Annotation.Root({
  question: Annotation<string>,
  context: Annotation<Document[]>,
  answer: Annotation<string>,
});

"""
#### Nodes (application steps)

Let's start with a simple sequence of two steps: retrieval and generation.
"""

import { concat } from "@langchain/core/utils/stream";

const retrieve = async (state: typeof InputStateAnnotation.State) => {
  const retrievedDocs = await vectorStore.similaritySearch(state.question)
  return { context: retrievedDocs };
};


const generate = async (state: typeof StateAnnotation.State) => {
  const docsContent = state.context.map(doc => doc.pageContent).join("\n");
  const messages = await promptTemplate.invoke({ question: state.question, context: docsContent });
  const response = await llm.invoke(messages);
  return { answer: response.content };
};

"""
Our retrieval step simply runs a similarity search using the input question, and the generation step formats the retrieved context and original question into a prompt for the chat model.

#### Control flow

Finally, we compile our application into a single `graph` object. In this case, we are just connecting the retrieval and generation steps into a single sequence.
"""

import { StateGraph } from "@langchain/langgraph";

const graph = new StateGraph(StateAnnotation)
  .addNode("retrieve", retrieve)
  .addNode("generate", generate)
  .addEdge("__start__", "retrieve")
  .addEdge("retrieve", "generate")
  .addEdge("generate", "__end__")
  .compile();

"""
LangGraph also comes with built-in utilities for visualizing the control flow of your application:
"""

"""
```javascript
// Note: tslab only works inside a jupyter notebook. Don't worry about running this code yourself!
import * as tslab from "tslab";

const image = await graph.getGraph().drawMermaidPng();
const arrayBuffer = await image.arrayBuffer();

await tslab.display.png(new Uint8Array(arrayBuffer));
```

![graph_img_rag](../../static/img/graph_img_rag.png)
"""

"""
```{=mdx}
<details>
<summary>Do I need to use LangGraph?</summary>
```

LangGraph is not required to build a RAG application. Indeed, we can implement the same application logic through invocations of the individual components:

```javascript
let question = "..."

const retrievedDocs = await vectorStore.similaritySearch(question)
const docsContent = retrievedDocs.map(doc => doc.pageContent).join("\n");
const messages = await promptTemplate.invoke({ question: question, context: docsContent });
const answer = await llm.invoke(messages);
```

The benefits of LangGraph include:

- Support for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps;
- Automatic support for tracing via [LangSmith](https://docs.smith.langchain.com/) and deployments via [LangGraph Platform](https://langchain-ai.github.io/langgraphjs/concepts/langgraph_platform/);
- Support for persistence, human-in-the-loop, and other features.

Many use-cases demand RAG in a conversational experience, such that a user can receive context-informed answers via a stateful conversation. As we will see in [Part 2](/docs/tutorials/qa_chat_history) of the tutorial, LangGraph's management and persistence of state simplifies these applications enormously.

```{=mdx}
</details>
```
"""

"""
#### Usage

Let's test our application! LangGraph supports multiple invocation modes, including sync, async, and streaming.

Invoke:
"""

let inputs = { question: "What is Task Decomposition?" };

const result = await graph.invoke(inputs);
console.log(result.context.slice(0, 2));
console.log(`\nAnswer: ${result["answer"]}`);
# Output:
#   [

#     Document {

#       pageContent: [32m'hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain'[39m,

#       metadata: {

#         source: [32m'https://lilianweng.github.io/posts/2023-06-23-agent/'[39m,

#         loc: [36m[Object][39m

#       },

#       id: [90mundefined[39m

#     },

#     Document {

#       pageContent: [32m'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.Tree of Thoughts (Yao et al.'[39m,

#       metadata: {

#         source: [32m'https://lilianweng.github.io/posts/2023-06-23-agent/'[39m,

#         loc: [36m[Object][39m

#       },

#       id: [90mundefined[39m

#     }

#   ]

#   

#   Answer: Task decomposition is the process of breaking down complex tasks into smaller, more manageable steps. This can be achieved through various methods, including prompting large language models (LLMs) to outline steps or using task-specific instructions. Techniques like Chain of Thought (CoT) and Tree of Thoughts further enhance this process by structuring reasoning and exploring multiple possibilities at each step.


"""
Stream steps:
"""

console.log(inputs)
console.log("\n====\n");
for await (
  const chunk of await graph.stream(inputs, {
    streamMode: "updates",
  })
) {
  console.log(chunk);
  console.log("\n====\n");
}
# Output:
#   { question: [32m'What is Task Decomposition?'[39m }

#   

#   ====

#   

#   {

#     retrieve: { context: [ [36m[Document][39m, [36m[Document][39m, [36m[Document][39m, [36m[Document][39m ] }

#   }

#   

#   ====

#   

#   {

#     generate: {

#       answer: [32m'Task decomposition is the process of breaking down complex tasks into smaller, more manageable steps. This can be achieved through various methods, including prompting large language models (LLMs) or using task-specific instructions. Techniques like Chain of Thought (CoT) and Tree of Thoughts further enhance this process by structuring reasoning and exploring multiple possibilities at each step.'[39m

#     }

#   }

#   

#   ====

#   


"""
Stream [tokens](/docs/concepts/tokens/) (requires `@langchain/core` >= 0.3.24 and `@langchain/langgraph` >= 0.2.34 with above implementation):
"""

const stream = await graph.stream(
  inputs,
  { streamMode: "messages" },
);

for await (const [message, _metadata] of stream) {
  process.stdout.write(message.content + "|");
}
# Output:
#   |Task| decomposition| is| the| process| of| breaking| down| complex| tasks| into| smaller|,| more| manageable| steps|.| This| can| be| achieved| through| various| methods|,| including| prompting| large| language| models| (|LL|Ms|)| to| outline| steps| or| using| task|-specific| instructions|.| Techniques| like| Chain| of| Thought| (|Co|T|)| and| Tree| of| Thoughts| further| enhance| this| process| by| struct|uring| reasoning| and| exploring| multiple| possibilities| at| each| step|.||

"""
```{=mdx}
:::note

Streaming tokens with the current implementation, using `.invoke` in the `generate` step, requires `@langchain/core` >= 0.3.24 and `@langchain/langgraph` >= 0.2.34. See details [here](https://langchain-ai.github.io/langgraphjs/how-tos/stream-tokens/).

:::
```
"""

"""
#### Returning sources

Note that by storing the retrieved context in the state of the graph, we recover sources for the model's generated answer in the `"context"` field of the state. See [this guide](/docs/how_to/qa_sources/) on returning sources for more detail.

#### Go deeper

[Chat models](/docs/concepts/chat_models) take in a sequence of messages and return a message.

- [Docs](/docs/how_to#chat-models)
- [Integrations](/docs/integrations/chat/): 25+ integrations to choose from.

**Customizing the prompt**

As shown above, we can load prompts (e.g., [this RAG
prompt](https://smith.langchain.com/hub/rlm/rag-prompt)) from the prompt
hub. The prompt can also be easily customized. For example:
"""

const template = `Use the following pieces of context to answer the question at the end.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
Use three sentences maximum and keep the answer as concise as possible.
Always say "thanks for asking!" at the end of the answer.

{context}

Question: {question}

Helpful Answer:`

const promptTemplateCustom = ChatPromptTemplate.fromMessages(
  [
    ["user", template]
  ]
)

"""
## Query analysis

So far, we are executing the retrieval using the raw input query. However, there are some advantages to allowing a model to generate the query for retrieval purposes. For example:

- In addition to semantic search, we can build in structured filters (e.g., "Find documents since the year 2020.");
- The model can rewrite user queries, which may be multifaceted or include irrelevant language, into more effective search queries.

[Query analysis](/docs/concepts/retrieval/#query-analysis) employs models to transform or construct optimized search queries from raw user input. We can easily incorporate a query analysis step into our application. For illustrative purposes, let's add some metadata to the documents in our vector store. We will add some (contrived) sections to the document which we can filter on later.
"""

const totalDocuments = allSplits.length;
const third = Math.floor(totalDocuments / 3);

allSplits.forEach((document, i) => {
    if (i < third) {
        document.metadata["section"] = "beginning";
    } else if (i < 2 * third) {
        document.metadata["section"] = "middle";
    } else {
        document.metadata["section"] = "end";
    }
});

allSplits[0].metadata;
# Output:
#   {

#     source: [32m'https://lilianweng.github.io/posts/2023-06-23-agent/'[39m,

#     loc: { lines: { from: [33m1[39m, to: [33m1[39m } },

#     section: [32m'beginning'[39m

#   }


"""
We will need to update the documents in our vector store. We will use a simple [MemoryVectorStore](https://api.js.langchain.com/classes/langchain.vectorstores_memory.MemoryVectorStore.html) for this, as we will use some of its specific features (i.e., metadata filtering). Refer to the vector store [integration documentation](/docs/integrations/vectorstores/) for relevant features of your chosen vector store.
"""

import { MemoryVectorStore } from "langchain/vectorstores/memory";

const vectorStoreQA = new MemoryVectorStore(embeddings);
await vectorStoreQA.addDocuments(allSplits)

"""
Let's next define a schema for our search query. We will use [structured output](/docs/concepts/structured_outputs/) for this purpose. Here we define a query as containing a string query and a document section (either "beginning", "middle", or "end"), but this can be defined however you like.
"""

import { z } from "zod";


const searchSchema = z.object({
  query: z.string().describe("Search query to run."),
  section: z.enum(["beginning", "middle", "end"]).describe("Section to query."),
});

const structuredLlm = llm.withStructuredOutput(searchSchema)

"""
Finally, we add a step to our LangGraph application to generate a query from the user's raw input:
"""

const StateAnnotationQA = Annotation.Root({
  question: Annotation<string>,
  // highlight-start
  search: Annotation<z.infer<typeof searchSchema>>,
  // highlight-end
  context: Annotation<Document[]>,
  answer: Annotation<string>,
});


// highlight-start
const analyzeQuery = async (state: typeof InputStateAnnotation.State) => {
  const result = await structuredLlm.invoke(state.question)
  return { search: result }
};
// highlight-end


const retrieveQA = async (state: typeof StateAnnotationQA.State) => {
  // highlight-start
  const filter = (doc) => doc.metadata.section === state.search.section;
  const retrievedDocs = await vectorStore.similaritySearch(
    state.search.query,
    2,
    filter
  )
  // highlight-end
  return { context: retrievedDocs };
};


const generateQA = async (state: typeof StateAnnotationQA.State) => {
  const docsContent = state.context.map(doc => doc.pageContent).join("\n");
  const messages = await promptTemplate.invoke({ question: state.question, context: docsContent });
  const response = await llm.invoke(messages);
  return { answer: response.content };
};



const graphQA = new StateGraph(StateAnnotationQA)
  .addNode("analyzeQuery", analyzeQuery)
  .addNode("retrieveQA", retrieveQA)
  .addNode("generateQA", generateQA)
  .addEdge("__start__", "analyzeQuery")
  .addEdge("analyzeQuery", "retrieveQA")
  .addEdge("retrieveQA", "generateQA")
  .addEdge("generateQA", "__end__")
  .compile();

"""
```javascript
// Note: tslab only works inside a jupyter notebook. Don't worry about running this code yourself!
import * as tslab from "tslab";

const image = await graphQA.getGraph().drawMermaidPng();
const arrayBuffer = await image.arrayBuffer();

await tslab.display.png(new Uint8Array(arrayBuffer));
```

![graph_img_rag_qa](../../static/img/graph_img_rag_qa.png)
"""

"""
We can test our implementation by specifically asking for context from the end of the post. Note that the model includes different information in its answer.
"""

let inputsQA = { question: "What does the end of the post say about Task Decomposition?" };

console.log(inputsQA)
console.log("\n====\n");
for await (
  const chunk of await graphQA.stream(inputsQA, {
    streamMode: "updates",
  })
) {
  console.log(chunk);
  console.log("\n====\n");
}
# Output:
#   {

#     question: [32m'What does the end of the post say about Task Decomposition?'[39m

#   }

#   

#   ====

#   

#   {

#     analyzeQuery: { search: { query: [32m'Task Decomposition'[39m, section: [32m'end'[39m } }

#   }

#   

#   ====

#   

#   { retrieveQA: { context: [ [36m[Document][39m, [36m[Document][39m ] } }

#   

#   ====

#   

#   {

#     generateQA: {

#       answer: [32m'The end of the post emphasizes the importance of task decomposition by outlining a structured approach to organizing code into separate files and functions. It highlights the need for clarity and compatibility among different components, ensuring that each part of the architecture is well-defined and functional. This methodical breakdown aids in maintaining best practices and enhances code readability and manageability.'[39m

#     }

#   }

#   

#   ====

#   


"""
In both the streamed steps and the [LangSmith trace](https://smith.langchain.com/public/8ff4742c-a5d4-41b2-adf9-22915a876a30/r), we can now observe the structured query that was fed into the retrieval step.

Query Analysis is a rich problem with a wide range of approaches. Refer to the [how-to guides](/docs/how_to/#query-analysis) for more examples.
"""

"""
## Next steps

We've covered the steps to build a basic Q&A app over data:

- Loading data with a [Document Loader](/docs/concepts/document_loaders)
- Chunking the indexed data with a [Text Splitter](/docs/concepts/text_splitters) to make it more easily usable by a model
- [Embedding the data](/docs/concepts/embedding_models) and storing the data in a [vectorstore](/docs/how_to/vectorstores)
- [Retrieving](/docs/concepts/retrievers) the previously stored chunks in response to incoming questions
- Generating an answer using the retrieved chunks as context.

In [Part 2](/docs/tutorials/qa_chat_history) of the tutorial, we will extend the implementation here to accommodate conversation-style interactions and multi-step retrieval processes.

Further reading:

- [Return sources](/docs/how_to/qa_sources): Learn how to return source documents
- [Streaming](/docs/how_to/streaming): Learn how to stream outputs and intermediate steps
- [Add chat history](/docs/how_to/message_history): Learn how to add chat history to your app
- [Retrieval conceptual guide](/docs/concepts/retrieval): A high-level overview of specific retrieval techniques
"""



================================================
FILE: docs/core_docs/docs/tutorials/retrievers.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Build a semantic search engine

This tutorial will familiarize you with LangChain's [document loader](/docs/concepts/document_loaders), [embedding](/docs/concepts/embedding_models), and [vector store](/docs/concepts/vectorstores) abstractions. These abstractions are designed to support retrieval of data--  from (vector) databases and other sources--  for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or [RAG](/docs/concepts/rag) (see our RAG tutorial [here](/docs/tutorials/rag)).

Here we will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query.

## Concepts

This guide focuses on retrieval of text data. We will cover the following concepts:

- Documents and document loaders;
- Text splitters;
- Embeddings;
- Vector stores and retrievers.

## Setup

### Jupyter Notebook

This and other tutorials are perhaps most conveniently run in a Jupyter notebook. See [here](https://jupyter.org/install) for instructions on how to install.

### Installation

This guide requires `@langchain/community` and `pdf-parse`:

```{=mdx}
import Npm2Yarn from '@theme/Npm2Yarn';
import TabItem from '@theme/TabItem';
import CodeBlock from "@theme/CodeBlock";

<Npm2Yarn>
  @langchain/community pdf-parse
</Npm2Yarn>
```

For more details, see our [Installation guide](/docs/how_to/installation/).

### LangSmith

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.
As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.
The best way to do this is with [LangSmith](https://smith.langchain.com).

After you sign up at the link above, make sure to set your environment variables to start logging traces:

```shell
export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."

# Reduce tracing latency if you are not in a serverless environment
# export LANGCHAIN_CALLBACKS_BACKGROUND=true
```


## Documents and Document Loaders

LangChain implements a [Document](https://api.js.langchain.com/classes/_langchain_core.documents.Document.html) abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:

- `pageContent`: a string representing the content;
- `metadata`: records of arbitrary metadata;
- `id`: (optional) a string identifier for the document.

The `metadata` attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual `Document` object often represents a chunk of a larger document.

We can generate sample documents when desired:
```javascript
import { Document } from "@langchain/core/documents";

const documents = [
    new Document({
        pageContent: "Dogs are great companions, known for their loyalty and friendliness.",
        metadata: {"source": "mammal-pets-doc"},
    }),
    new Document({
        pageContent: "Cats are independent pets that often enjoy their own space.",
        metadata: {"source": "mammal-pets-doc"},
    }),
]
```
"""

"""
However, the LangChain ecosystem implements [document loaders](/docs/concepts/document_loaders) that [integrate with hundreds of common sources](/docs/integrations/document_loaders/). This makes it easy to incorporate data from these sources into your AI application.

### Loading documents

Let's load a PDF into a sequence of `Document` objects. There is a sample PDF in the LangChain repo [here](https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/data/nke-10k-2023.pdf) -- a 10-k filing for Nike from 2023. LangChain implements a [PDFLoader](/docs/integrations/document_loaders/file_loaders/pdf/) that we can use to parse the PDF:
"""

import { PDFLoader } from "@langchain/community/document_loaders/fs/pdf";

const loader = new PDFLoader("../../data/nke-10k-2023.pdf");

const docs = await loader.load();
console.log(docs.length)
# Output:
#   [33m107[39m


"""
```{=mdx}
:::tip

See [this guide](/docs/how_to/document_loader_pdf/) for more detail on PDF document loaders.

:::
```

`PDFLoader` loads one `Document` object per PDF page. For each, we can easily access:

- The string content of the page;
- Metadata containing the file name and page number.
"""

docs[0].pageContent.slice(0, 200)
# Output:
#   Table of Contents

#   UNITED STATES

#   SECURITIES AND EXCHANGE COMMISSION

#   Washington, D.C. 20549

#   FORM 10-K

#   (Mark One)

#   ☑ ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934

#   FO


docs[0].metadata
# Output:
#   {

#     source: [32m'../../data/nke-10k-2023.pdf'[39m,

#     pdf: {

#       version: [32m'1.10.100'[39m,

#       info: {

#         PDFFormatVersion: [32m'1.4'[39m,

#         IsAcroFormPresent: [33mfalse[39m,

#         IsXFAPresent: [33mfalse[39m,

#         Title: [32m'0000320187-23-000039'[39m,

#         Author: [32m'EDGAR Online, a division of Donnelley Financial Solutions'[39m,

#         Subject: [32m'Form 10-K filed on 2023-07-20 for the period ending 2023-05-31'[39m,

#         Keywords: [32m'0000320187-23-000039; ; 10-K'[39m,

#         Creator: [32m'EDGAR Filing HTML Converter'[39m,

#         Producer: [32m'EDGRpdf Service w/ EO.Pdf 22.0.40.0'[39m,

#         CreationDate: [32m"D:20230720162200-04'00'"[39m,

#         ModDate: [32m"D:20230720162208-04'00'"[39m

#       },

#       metadata: [1mnull[22m,

#       totalPages: [33m107[39m

#     },

#     loc: { pageNumber: [33m1[39m }

#   }


"""
### Splitting

For both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve `Document` objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not "washed out" by surrounding text.

We can use [text splitters](/docs/concepts/text_splitters) for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters
with 200 characters of overlap between chunks. The overlap helps
mitigate the possibility of separating a statement from important
context related to it. We use the
[RecursiveCharacterTextSplitter](/docs/how_to/recursive_text_splitter),
which will recursively split the document using common separators like
new lines until each chunk is the appropriate size. This is the
recommended text splitter for generic text use cases.

We set `add_start_index=True` so that the character index where each
split Document starts within the initial Document is preserved as
metadata attribute “start_index”.

See [this guide](/docs/how_to/document_loader_pdf/) for more detail about working with PDFs. 
"""

import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";

const textSplitter = new RecursiveCharacterTextSplitter({
  chunkSize: 1000,
  chunkOverlap: 200,
});

const allSplits = await textSplitter.splitDocuments(docs)

allSplits.length
# Output:
#   [33m513[39m


"""
## Embeddings

Vector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can [embed](/docs/concepts/embedding_models) it as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text.

LangChain supports embeddings from [dozens of providers](/docs/integrations/text_embedding/). These models specify how text should be converted into a numeric vector. Let's select a model.

```{=mdx}
import EmbeddingTabs from "@theme/EmbeddingTabs";

<EmbeddingTabs customVarName="embeddings" />
```
"""

// @lc-docs-hide-cell
import { OpenAIEmbeddings } from "@langchain/openai";

const embeddings = new OpenAIEmbeddings({model: "text-embedding-3-large"});

const vector1 = await embeddings.embedQuery(allSplits[0].pageContent)
const vector2 = await embeddings.embedQuery(allSplits[1].pageContent)


console.assert(vector1.length === vector2.length);
console.log(`Generated vectors of length ${vector1.length}\n`);
console.log(vector1.slice(0, 10));
# Output:
#   Generated vectors of length 3072

#   

#   [

#     [33m0.014310152[39m,

#     [33m-0.01681044[39m,

#     [33m-0.0011537228[39m,

#     [33m0.010546423[39m,

#     [33m0.022808468[39m,

#     [33m-0.028327717[39m,

#     [33m-0.00058849837[39m,

#     [33m0.0419197[39m,

#     [33m-0.0012900416[39m,

#     [33m0.0661778[39m

#   ]


"""
Armed with a model for generating text embeddings, we can next store them in a special data structure that supports efficient similarity search.

## Vector stores

LangChain [VectorStore](https://api.js.langchain.com/classes/_langchain_core.vectorstores.VectorStore.html) objects contain methods for adding text and `Document` objects to the store, and querying them using various similarity metrics. They are often initialized with [embedding](/docs/how_to/embed_text) models, which determine how text data is translated to numeric vectors.

LangChain includes a suite of [integrations](/docs/integrations/vectorstores) with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as [Postgres](/docs/integrations/vectorstores/pgvector)) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads.

```{=mdx}
import VectorStoreTabs from "@theme/VectorStoreTabs";

<VectorStoreTabs/>
```
"""

// @lc-docs-hide-cell
import { MemoryVectorStore } from "langchain/vectorstores/memory";

const vectorStore = new MemoryVectorStore(embeddings);

"""
Having instantiated our vector store, we can now index the documents.
"""

await vectorStore.addDocuments(allSplits)

"""
Note that most vector store implementations will allow you to connect to an existing vector store--  e.g., by providing a client, index name, or other information. See the documentation for a specific [integration](/docs/integrations/vectorstores) for more detail.

Once we've instantiated a `VectorStore` that contains documents, we can query it. [VectorStore](https://api.js.langchain.com/classes/_langchain_core.vectorstores.VectorStore.html) includes methods for querying:
- Synchronously and asynchronously;
- By string query and by vector;
- With and without returning similarity scores;
- By similarity and [maximum marginal relevance](https://api.js.langchain.com/classes/_langchain_core.vectorstores.VectorStore.html#maxMarginalRelevanceSearch) (to balance similarity with query to diversity in retrieved results).

The methods will generally include a list of [Document](https://api.js.langchain.com/classes/_langchain_core.documents.Document.html) objects in their outputs.

### Usage

Embeddings typically represent text as a "dense" vector such that texts with similar meanings are gemoetrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document.

Return documents based on similarity to a string query:
"""

const results1 = await vectorStore.similaritySearch("When was Nike incorporated?")

results1[0]
# Output:
#   Document {

#     pageContent: [32m'Table of Contents\n'[39m +

#       [32m'PART I\n'[39m +

#       [32m'ITEM 1. BUSINESS\n'[39m +

#       [32m'GENERAL\n'[39m +

#       [32m'NIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this "Annual Report"), the terms "we," "us," "our,"\n'[39m +

#       [32m'"NIKE" and the "Company" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\n'[39m +

#       [32m'Our principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\n'[39m +

#       [32m'the largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\n'[39m +

#       [32m'and sales through our digital platforms (also referred to as "NIKE Brand Digital"), to retail accounts and to a mix of independent distributors, licensees and sales'[39m,

#     metadata: {

#       source: [32m'../../data/nke-10k-2023.pdf'[39m,

#       pdf: {

#         version: [32m'1.10.100'[39m,

#         info: [36m[Object][39m,

#         metadata: [1mnull[22m,

#         totalPages: [33m107[39m

#       },

#       loc: { pageNumber: [33m4[39m, lines: [36m[Object][39m }

#     },

#     id: [90mundefined[39m

#   }


"""
Return scores:
"""

const results2 = await vectorStore.similaritySearchWithScore(
    "What was Nike's revenue in 2023?"
)

results2[0]
# Output:
#   [

#     Document {

#       pageContent: [32m'Table of Contents\n'[39m +

#         [32m'FISCAL 2023 NIKE BRAND REVENUE HIGHLIGHTS\n'[39m +

#         [32m'The following tables present NIKE Brand revenues disaggregated by reportable operating segment, distribution channel and major product line:\n'[39m +

#         [32m'FISCAL 2023 COMPARED TO FISCAL 2022\n'[39m +

#         [32m'•NIKE, Inc. Revenues were $51.2 billion in fiscal 2023, which increased 10% and 16% compared to fiscal 2022 on a reported and currency-neutral basis, respectively.\n'[39m +

#         [32m'The increase was due to higher revenues in North America, Europe, Middle East & Africa ("EMEA"), APLA and Greater China, which contributed approximately 7, 6,\n'[39m +

#         [32m'2 and 1 percentage points to NIKE, Inc. Revenues, respectively.\n'[39m +

#         [32m'•NIKE Brand revenues, which represented over 90% of NIKE, Inc. Revenues, increased 10% and 16% on a reported and currency-neutral basis, respectively. This\n'[39m +

#         [32m"increase was primarily due to higher revenues in Men's, the Jordan Brand, Women's and Kids' which grew 17%, 35%,11% and 10%, respectively, on a wholesale\n"[39m +

#         [32m'equivalent basis.'[39m,

#       metadata: {

#         source: [32m'../../data/nke-10k-2023.pdf'[39m,

#         pdf: [36m[Object][39m,

#         loc: [36m[Object][39m

#       },

#       id: [90mundefined[39m

#     },

#     [33m0.6992287611800424[39m

#   ]


"""
Return documents based on similarity to an embedded query:
"""

const embedding = await embeddings.embedQuery(
    "How were Nike's margins impacted in 2023?"
)

const results3 = await vectorStore.similaritySearchVectorWithScore(
    embedding, 1
)

results3[0]
# Output:
#   [

#     Document {

#       pageContent: [32m'Table of Contents\n'[39m +

#         [32m'GROSS MARGIN\n'[39m +

#         [32m'FISCAL 2023 COMPARED TO FISCAL 2022\n'[39m +

#         [32m'For fiscal 2023, our consolidated gross profit increased 4% to $22,292 million compared to $21,479 million for fiscal 2022. Gross margin decreased 250 basis points to\n'[39m +

#         [32m'43.5% for fiscal 2023 compared to 46.0% for fiscal 2022 due to the following:\n'[39m +

#         [32m'*Wholesale equivalent\n'[39m +

#         [32m'The decrease in gross margin for fiscal 2023 was primarily due to:\n'[39m +

#         [32m'•Higher NIKE Brand product costs, on a wholesale equivalent basis, primarily due to higher input costs and elevated inbound freight and logistics costs as well as\n'[39m +

#         [32m'product mix;\n'[39m +

#         [32m'•Lower margin in our NIKE Direct business, driven by higher promotional activity to liquidate inventory in the current period compared to lower promotional activity in\n'[39m +

#         [32m'the prior period resulting from lower available inventory supply;\n'[39m +

#         [32m'•Unfavorable changes in net foreign currency exchange rates, including hedges; and\n'[39m +

#         [32m'•Lower off-price margin, on a wholesale equivalent basis.\n'[39m +

#         [32m'This was partially offset by:'[39m,

#       metadata: {

#         source: [32m'../../data/nke-10k-2023.pdf'[39m,

#         pdf: [36m[Object][39m,

#         loc: [36m[Object][39m

#       },

#       id: [90mundefined[39m

#     },

#     [33m0.7368815472158006[39m

#   ]


"""
Learn more:

- [API reference](https://api.js.langchain.com/classes/_langchain_core.vectorstores.VectorStore.html)
- [How-to guide](/docs/how_to/vectorstores)
- [Integration-specific docs](/docs/integrations/vectorstores)

## Retrievers

LangChain `VectorStore` objects do not subclass [Runnable](https://api.js.langchain.com/classes/_langchain_core.runnables.Runnable.html). LangChain [Retrievers](https://api.js.langchain.com/classes/_langchain_core.retrievers.BaseRetriever.html) are Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous `invoke` and `batch` operations). Although we can construct retrievers from vector stores, retrievers can interface with non-vector store sources of data, as well (such as external APIs).

Vectorstores implement an [as retriever](https://api.js.langchain.com/classes/_langchain_core.vectorstores.VectorStore.html#asRetriever) method that will generate a Retriever, specifically a [VectorStoreRetriever](https://api.js.langchain.com/classes/_langchain_core.vectorstores.VectorStoreRetriever.html). These retrievers include specific `search_type` and `search_kwargs` attributes that identify what methods of the underlying vector store to call, and how to parameterize them.
"""

const retriever = vectorStore.asRetriever({
  searchType: "mmr",
  searchKwargs: {
    fetchK: 1,
  },
});


await retriever.batch(
    [
        "When was Nike incorporated?",
        "What was Nike's revenue in 2023?",
    ]
)
# Output:
#   [

#     [

#       Document {

#         pageContent: [32m'Table of Contents\n'[39m +

#           [32m'PART I\n'[39m +

#           [32m'ITEM 1. BUSINESS\n'[39m +

#           [32m'GENERAL\n'[39m +

#           [32m'NIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this "Annual Report"), the terms "we," "us," "our,"\n'[39m +

#           [32m'"NIKE" and the "Company" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\n'[39m +

#           [32m'Our principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\n'[39m +

#           [32m'the largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\n'[39m +

#           [32m'and sales through our digital platforms (also referred to as "NIKE Brand Digital"), to retail accounts and to a mix of independent distributors, licensees and sales'[39m,

#         metadata: [36m[Object][39m,

#         id: [90mundefined[39m

#       }

#     ],

#     [

#       Document {

#         pageContent: [32m'Table of Contents\n'[39m +

#           [32m'FISCAL 2023 NIKE BRAND REVENUE HIGHLIGHTS\n'[39m +

#           [32m'The following tables present NIKE Brand revenues disaggregated by reportable operating segment, distribution channel and major product line:\n'[39m +

#           [32m'FISCAL 2023 COMPARED TO FISCAL 2022\n'[39m +

#           [32m'•NIKE, Inc. Revenues were $51.2 billion in fiscal 2023, which increased 10% and 16% compared to fiscal 2022 on a reported and currency-neutral basis, respectively.\n'[39m +

#           [32m'The increase was due to higher revenues in North America, Europe, Middle East & Africa ("EMEA"), APLA and Greater China, which contributed approximately 7, 6,\n'[39m +

#           [32m'2 and 1 percentage points to NIKE, Inc. Revenues, respectively.\n'[39m +

#           [32m'•NIKE Brand revenues, which represented over 90% of NIKE, Inc. Revenues, increased 10% and 16% on a reported and currency-neutral basis, respectively. This\n'[39m +

#           [32m"increase was primarily due to higher revenues in Men's, the Jordan Brand, Women's and Kids' which grew 17%, 35%,11% and 10%, respectively, on a wholesale\n"[39m +

#           [32m'equivalent basis.'[39m,

#         metadata: [36m[Object][39m,

#         id: [90mundefined[39m

#       }

#     ]

#   ]


"""
`VectorStoreRetriever` supports search types of `"similarity"` (default) and `"mmr"` (maximum marginal relevance, described above).

Retrievers can easily be incorporated into more complex applications, such as [retrieval-augmented generation (RAG)](/docs/concepts/rag) applications that combine a given question with retrieved context into a prompt for a LLM. To learn more about building such an application, check out the [RAG tutorial](/docs/tutorials/rag) tutorial.
"""

"""
### Learn more:

Retrieval strategies can be rich and complex. For example:

- We can [infer hard rules and filters](/docs/how_to/self_query/) from a query (e.g., "using documents published after 2020");
- We can [return documents that are linked](/docs/how_to/parent_document_retriever/) to the retrieved context in some way (e.g., via some document taxonomy);
- We can generate [multiple embeddings](/docs/how_to/multi_vector) for each unit of context;
- We can [ensemble results](/docs/how_to/ensemble_retriever) from multiple retrievers;
- We can assign weights to documents, e.g., to weigh [recent documents](/docs/how_to/time_weighted_vectorstore/) higher.

The [retrievers](/docs/how_to#retrievers) section of the how-to guides covers these and other built-in retrieval strategies.

It is also straightforward to extend the [BaseRetriever](https://api.js.langchain.com/classes/_langchain_core.retrievers.BaseRetriever.html) class in order to implement custom retrievers. See our how-to guide [here](/docs/how_to/custom_retriever).


## Next steps

You've now seen how to build a semantic search engine over a PDF document.

For more on document loaders:

- [Conceptual guide](/docs/concepts/document_loaders)
- [How-to guides](/docs/how_to/#document-loaders)
- [Available integrations](/docs/integrations/document_loaders/)

For more on embeddings:

- [Conceptual guide](/docs/concepts/embedding_models/)
- [How-to guides](/docs/how_to/#embedding-models)
- [Available integrations](/docs/integrations/text_embedding/)

For more on vector stores:

- [Conceptual guide](/docs/concepts/vectorstores/)
- [How-to guides](/docs/how_to/#vector-stores)
- [Available integrations](/docs/integrations/vectorstores/)

For more on RAG, see:

- [Build a Retrieval Augmented Generation (RAG) App](/docs/tutorials/rag/)
- [Related how-to guides](/docs/how_to/#qa-with-rag)
"""



================================================
FILE: docs/core_docs/docs/tutorials/sql_qa.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Build a Question/Answering system over SQL data

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat models](/docs/concepts/chat_models)
- [Tools](/docs/concepts/tools)
- [Agents](/docs/concepts/agents)
- [LangGraph](/docs/concepts/architecture/#langgraph)

:::

Enabling a LLM system to query structured data can be qualitatively different from unstructured text data. Whereas in the latter it is common to generate text that can be searched against a vector database, the approach for structured data is often for the LLM to write and execute queries in a DSL, such as SQL. In this guide we'll go over the basic ways to create a Q&A system over tabular data in databases. We will cover implementations using both [chains](/docs/tutorials/sql_qa#chains) and [agents](/docs/tutorials/sql_qa#agents). These systems will allow us to ask a question about the data in a database and get back a natural language answer. The main difference between the two is that our agent can query the database in a loop as many times as it needs to answer the question.

## ⚠️ Security note ⚠️

Building Q&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your chain/agent's needs. This will mitigate though not eliminate the risks of building a model-driven system. For more on general security best practices, [see here](/docs/security).


## Architecture

At a high-level, the steps of these systems are:

1. **Convert question to SQL query**: Model converts user input to a SQL query.
2. **Execute SQL query**: Execute the query.
3. **Answer the question**: Model responds to user input using the query results.

![sql_usecase.png](../../static/img/sql_usecase.png)

## Setup

First, get required packages and set environment variables:
```bash npm2yarn
npm i langchain @langchain/community @langchain/langgraph
```

```shell
# Uncomment the below to use LangSmith. Not required, but recommended for debugging and observability.
# export LANGSMITH_API_KEY=<your key>
# export LANGSMITH_TRACING=true

# Reduce tracing latency if you are not in a serverless environment
# export LANGCHAIN_CALLBACKS_BACKGROUND=true
```
"""

"""
### Sample data

The below example will use a SQLite connection with the Chinook database, which is a sample database that represents a digital media store. Follow [these installation steps](https://database.guide/2-sample-databases-sqlite/) to create `Chinook.db` in the same directory as this notebook. You can also download and build the database via the command line:
```bash
curl -s https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql | sqlite3 Chinook.db
```

Now, `Chinook.db` is in our directory and we can interface with it using the [SqlDatabase](https://api.js.langchain.com/classes/langchain.sql_db.SqlDatabase.html) class:
"""

import { SqlDatabase } from "langchain/sql_db";
import { DataSource } from "typeorm";

const datasource = new DataSource({
  type: "sqlite",
  database: "Chinook.db",
});
const db = await SqlDatabase.fromDataSourceParams({
  appDataSource: datasource,
});

await db.run("SELECT * FROM Artist LIMIT 10;")
# Output:
#   [{"ArtistId":1,"Name":"AC/DC"},{"ArtistId":2,"Name":"Accept"},{"ArtistId":3,"Name":"Aerosmith"},{"ArtistId":4,"Name":"Alanis Morissette"},{"ArtistId":5,"Name":"Alice In Chains"},{"ArtistId":6,"Name":"Antônio Carlos Jobim"},{"ArtistId":7,"Name":"Apocalyptica"},{"ArtistId":8,"Name":"Audioslave"},{"ArtistId":9,"Name":"BackBeat"},{"ArtistId":10,"Name":"Billy Cobham"}]


"""
Great! We've got a SQL database that we can query. Now let's try hooking it up to an LLM.

## Chains {#chains}

Chains are compositions of predictable steps. In [LangGraph](/docs/concepts/architecture#langchainlanggraph), we can represent a chain via simple sequence of nodes. Let's create a sequence of steps that, given a question, does the following:
- converts the question into a SQL query;
- executes the query;
- uses the result to answer the original question.

There are scenarios not supported by this arrangement. For example, this system will execute a SQL query for any user input-- even "hello". Importantly, as we'll see below, some questions require more than one query to answer. We will address these scenarios in the Agents section.

### Application state

The LangGraph [state](https://langchain-ai.github.io/langgraphjs/concepts/low_level/#state) of our application controls what data is input to the application, transferred between steps, and output by the application.

For this application, we can just keep track of the input question, generated query, query result, and generated answer:
"""

import { Annotation } from "@langchain/langgraph";


const InputStateAnnotation = Annotation.Root({
  question: Annotation<string>,
});


const StateAnnotation = Annotation.Root({
  question: Annotation<string>,
  query: Annotation<string>,
  result: Annotation<string>,
  answer: Annotation<string>,
});

"""
Now we just need functions that operate on this state and populate its contents.

### Convert question to SQL query

The first step is to take the user input and convert it to a SQL query. To reliably obtain SQL queries (absent markdown formatting and explanations or clarifications), we will make use of LangChain's [structured output](/docs/concepts/structured_outputs/) abstraction.

Let's select a chat model for our application:

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

// @lc-docs-hide-cell
import { ChatOpenAI } from '@langchain/openai';

const llm = new ChatOpenAI({
  model: "gpt-4o-mini",
  temperature: 0,
})

"""
We will pull a prompt from the [Prompt Hub](https://smith.langchain.com/hub) to instruct the model.
"""

import { pull } from "langchain/hub";
import { ChatPromptTemplate } from "@langchain/core/prompts";

const queryPromptTemplate = await pull<ChatPromptTemplate>("langchain-ai/sql-query-system-prompt");

console.log(queryPromptTemplate.promptMessages[0].lc_kwargs.prompt.template)
# Output:
#   Given an input question, create a syntactically correct {dialect} query to run to help find the answer. Unless the user specifies in his question a specific number of examples they wish to obtain, always limit your query to at most {top_k} results. You can order the results by a relevant column to return the most interesting examples in the database.

#   

#   Never query for all the columns from a specific table, only ask for a the few relevant columns given the question.

#   

#   Pay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.

#   

#   Only use the following tables:

#   {table_info}

#   

#   Question: {input}


"""
The prompt includes several parameters we will need to populate, such as the SQL dialect and table schemas. LangChain's [SqlDatabase](https://api.js.langchain.com/classes/langchain.sql_db.SqlDatabase.html) object includes methods to help with this. Our `writeQuery` step will just populate these parameters and prompt a model to generate the SQL query:
"""

import { z } from "zod";


const queryOutput = z.object({
  query: z.string().describe("Syntactically valid SQL query."),
});

const structuredLlm = llm.withStructuredOutput(queryOutput)


const writeQuery = async (state: typeof InputStateAnnotation.State) => {
  const promptValue = await queryPromptTemplate.invoke({
      dialect: db.appDataSourceOptions.type,
      top_k: 10,
      table_info: await db.getTableInfo(),
      input: state.question
  })
  const result = await structuredLlm.invoke(promptValue)
  return { query: result.query }
};

"""
Let's test it out:
"""

await writeQuery({ question: "How many Employees are there?" })
# Output:
#   { query: [32m'SELECT COUNT(*) AS EmployeeCount FROM Employee;'[39m }


"""
### Execute query

**This is the most dangerous part of creating a SQL chain.** Consider carefully if it is OK to run automated queries over your data. Minimize the database connection permissions as much as possible. Consider adding a human approval step to you chains before query execution (see below).

To execute the query, we will load a tool from [langchain-community](/docs/concepts/architecture#langchaincommunity). Our `executeQuery` node will just wrap this tool:
"""

import { QuerySqlTool } from "langchain/tools/sql";

const executeQuery = async (state: typeof StateAnnotation.State) => {
  const executeQueryTool = new QuerySqlTool(db);
  return { result: await executeQueryTool.invoke(state.query) }
};

"""
Testing this step:
"""

await executeQuery({
    question: "",
    query: "SELECT COUNT(*) AS EmployeeCount FROM Employee;",
    result: "",
    answer: ""
})
# Output:
#   { result: [32m'[{"EmployeeCount":8}]'[39m }


"""
### Generate answer

Finally, our last step generates an answer to the question given the information pulled from the database:
"""

const generateAnswer = async (state: typeof StateAnnotation.State) => {
  const promptValue = 
    "Given the following user question, corresponding SQL query, " +
    "and SQL result, answer the user question.\n\n" +
    `Question: ${state.question}\n` +
    `SQL Query: ${state.query}\n` +
    `SQL Result: ${state.result}\n`;
  const response = await llm.invoke(promptValue)
  return { answer: response.content }
};

"""
### Orchestrating with LangGraph

Finally, we compile our application into a single `graph` object. In this case, we are just connecting the three steps into a single sequence.
"""

import { StateGraph } from "@langchain/langgraph";

const graphBuilder = new StateGraph({
  stateSchema: StateAnnotation,
})
  .addNode("writeQuery", writeQuery)
  .addNode("executeQuery", executeQuery)
  .addNode("generateAnswer", generateAnswer)
  .addEdge("__start__", "writeQuery")
  .addEdge("writeQuery", "executeQuery")
  .addEdge("executeQuery", "generateAnswer")
  .addEdge("generateAnswer", "__end__")

const graph = graphBuilder.compile()

"""
LangGraph also comes with built-in utilities for visualizing the control flow of your application:
"""

"""
```javascript
// Note: tslab only works inside a jupyter notebook. Don't worry about running this code yourself!
import * as tslab from "tslab";

const image = await graph.getGraph().drawMermaidPng();
const arrayBuffer = await image.arrayBuffer();

await tslab.display.png(new Uint8Array(arrayBuffer));
```

![graph_img_sql_qa](../../static/img/graph_img_sql_qa.png)
"""

"""
Let's test our application! Note that we can stream the results of individual steps:
"""

let inputs = { question: "How many employees are there?" }

console.log(inputs)
console.log("\n====\n");
for await (
  const step of await graph.stream(inputs, {
    streamMode: "updates",
  })
) {
  console.log(step);
  console.log("\n====\n");
}
# Output:
#   { question: [32m'How many employees are there?'[39m }

#   

#   ====

#   

#   {

#     writeQuery: { query: [32m'SELECT COUNT(*) AS EmployeeCount FROM Employee;'[39m }

#   }

#   

#   ====

#   

#   { executeQuery: { result: [32m'[{"EmployeeCount":8}]'[39m } }

#   

#   ====

#   

#   { generateAnswer: { answer: [32m'There are 8 employees.'[39m } }

#   

#   ====

#   


"""
Check out the [LangSmith trace](https://smith.langchain.com/public/4cb42037-55cf-4da9-8b3a-8410482dbd32/r).
"""

"""
### Human-in-the-loop

LangGraph supports a number of features that can be useful for this workflow. One of them is [human-in-the-loop](https://langchain-ai.github.io/langgraphjs/concepts/human_in_the_loop/): we can interrupt our application before sensitive steps (such as the execution of a SQL query) for human review. This is enabled by LangGraph's [persistence](https://langchain-ai.github.io/langgraphjs/concepts/persistence/) layer, which saves run progress to your storage of choice. Below, we specify storage in-memory:
"""

import { MemorySaver } from "@langchain/langgraph";

const checkpointer = new MemorySaver();
const graphWithInterrupt = graphBuilder.compile({
    checkpointer: checkpointer,
    interruptBefore: ["executeQuery"]
});

// Now that we're using persistence, we need to specify a thread ID
// so that we can continue the run after review.
const threadConfig = {
    configurable: { thread_id: "1" },
    streamMode: "updates" as const
};

"""
```javascript
const image = await graphWithInterrupt.getGraph().drawMermaidPng();
const arrayBuffer = await image.arrayBuffer();

await tslab.display.png(new Uint8Array(arrayBuffer));
```

![graph_img_sql_qa_interrupt](../../static/img/graph_img_sql_qa_interrupt.png)
"""

"""
Let's repeat the same run, adding in a simple yes/no approval step:
"""

console.log(inputs)
console.log("\n====\n");
for await (
  const step of await graphWithInterrupt.stream(inputs, threadConfig)
) {
  console.log(step);
  console.log("\n====\n");
}

// Will log when the graph is interrupted, after `executeQuery`.
console.log("---GRAPH INTERRUPTED---");
# Output:
#   { question: [32m'How many employees are there?'[39m }

#   

#   ====

#   

#   {

#     writeQuery: { query: [32m'SELECT COUNT(*) AS EmployeeCount FROM Employee;'[39m }

#   }

#   

#   ====

#   

#   ---GRAPH INTERRUPTED---


"""
The run interrupts before the query is executed. At this point, our application can handle an interaction with a user, who reviews the query.

If approved, running the same thread with a `null` input will continue from where we left off. This is enabled by LangGraph's [persistence](https://langchain-ai.github.io/langgraphjs/concepts/persistence/) layer.
"""

for await (
  const step of await graphWithInterrupt.stream(null, threadConfig)
) {
  console.log(step);
  console.log("\n====\n");
}
# Output:
#   { executeQuery: { result: [32m'[{"EmployeeCount":8}]'[39m } }

#   

#   ====

#   

#   { generateAnswer: { answer: [32m'There are 8 employees.'[39m } }

#   

#   ====

#   


"""
See [this](https://langchain-ai.github.io/langgraphjs/concepts/human_in_the_loop/) LangGraph guide for more detail and examples.
"""

"""
### Next steps

For more complex query-generation, we may want to create few-shot prompts or add query-checking steps. For advanced techniques like this and more check out:

* [Prompting strategies](/docs/how_to/sql_prompting): Advanced prompt engineering techniques.
* [Query checking](/docs/how_to/sql_query_checking): Add query validation and error handling.
* [Large databases](/docs/how_to/sql_large_db): Techniques for working with large databases.
"""

"""
## Agents {#agents}

[Agents](/docs/concepts/agents) leverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allows you to offload additional discretion over the query generation and execution process. Although their behavior is less predictable than the above "chain", they feature some advantages:

- They can query the database as many times as needed to answer the user question.
- They can recover from errors by running a generated query, catching the traceback and regenerating it correctly.
- They can answer questions based on the databases' schema as well as on the databases' content (like describing a specific table).


Below we assemble a minimal SQL agent. We will equip it with a set of tools using LangChain's [SqlToolkit](https://api.js.langchain.com/classes/langchain.agents_toolkits_sql.SqlToolkit.html). Using LangGraph's [pre-built ReAct agent constructor](https://langchain-ai.github.io/langgraphjs/how-tos/create-react-agent/), we can do this in one line.

The `SqlToolkit` includes tools that can:

* Create and execute queries
* Check query syntax
* Retrieve table descriptions
* ... and more
"""

import { SqlToolkit } from "langchain/agents/toolkits/sql";

const toolkit = new SqlToolkit(db, llm);

const tools = toolkit.getTools();

console.log(
  tools.map((tool) => ({
    name: tool.name,
    description: tool.description,
  }))
)
# Output:
#   [

#     {

#       name: [32m'query-sql'[39m,

#       description: [32m'Input to this tool is a detailed and correct SQL query, output is a result from the database.\n'[39m +

#         [32m'  If the query is not correct, an error message will be returned.\n'[39m +

#         [32m'  If an error is returned, rewrite the query, check the query, and try again.'[39m

#     },

#     {

#       name: [32m'info-sql'[39m,

#       description: [32m'Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables.\n'[39m +

#         [32m'    Be sure that the tables actually exist by calling list-tables-sql first!\n'[39m +

#         [32m'\n'[39m +

#         [32m'    Example Input: "table1, table2, table3.'[39m

#     },

#     {

#       name: [32m'list-tables-sql'[39m,

#       description: [32m'Input is an empty string, output is a comma-separated list of tables in the database.'[39m

#     },

#     {

#       name: [32m'query-checker'[39m,

#       description: [32m'Use this tool to double check if your query is correct before executing it.\n'[39m +

#         [32m'    Always use this tool before executing a query with query-sql!'[39m

#     }

#   ]


"""
### System Prompt

We will also want to load a system prompt for our agent. This will consist of instructions for how to behave.
"""

import { pull } from "langchain/hub";
import { ChatPromptTemplate } from "@langchain/core/prompts";

const systemPromptTemplate = await pull<ChatPromptTemplate>("langchain-ai/sql-agent-system-prompt");

console.log(systemPromptTemplate.promptMessages[0].lc_kwargs.prompt.template)
# Output:
#   You are an agent designed to interact with a SQL database.

#   Given an input question, create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.

#   Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top_k} results.

#   You can order the results by a relevant column to return the most interesting examples in the database.

#   Never query for all the columns from a specific table, only ask for the relevant columns given the question.

#   You have access to tools for interacting with the database.

#   Only use the below tools. Only use the information returned by the below tools to construct your final answer.

#   You MUST double check your query before executing it. If you get an error while executing a query, rewrite the query and try again.

#   

#   DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.

#   

#   To start you should ALWAYS look at the tables in the database to see what you can query.

#   Do NOT skip this step.

#   Then you should query the schema of the most relevant tables.


"""
Let's populate the parameters highlighted in the prompt:
"""

const systemMessage = await systemPromptTemplate.format({
    dialect: "SQLite", top_k: 5
})

"""
### Initializing agent

We will use a prebuilt [LangGraph](/docs/concepts/architecture/#langgraph) agent to build our agent
"""

import { createReactAgent } from "@langchain/langgraph/prebuilt";

const agent = createReactAgent({ llm: llm, tools: tools, stateModifier: systemMessage });

"""
Consider how the agent responds to the below question:
"""

"""
```{=mdx}
<details>
<summary>Expand for `prettyPrint` code.</summary>
```
"""

import { AIMessage, BaseMessage, isAIMessage } from "@langchain/core/messages";

const prettyPrint = (message: BaseMessage) => {
  let txt = `[${message._getType()}]: ${message.content}`;
  if (
    (isAIMessage(message) && message.tool_calls?.length) ||
    0 > 0
  ) {
    const tool_calls = (message as AIMessage)?.tool_calls
      ?.map((tc) => `- ${tc.name}(${JSON.stringify(tc.args)})`)
      .join("\n");
    txt += ` \nTools: \n${tool_calls}`;
  }
  console.log(txt);
};

"""
```{=mdx}
</details>
```
"""

let inputs2 = { messages: [{ role: "user", content: "Which country's customers spent the most?" }] };

for await (
  const step of await agent.stream(inputs2, {
    streamMode: "values",
  })
) {
    const lastMessage = step.messages[step.messages.length - 1];
    prettyPrint(lastMessage);
    console.log("-----\n");
}
# Output:
#   [human]: Which country's customers spent the most?

#   -----

#   

#   [ai]:  

#   Tools: 

#   - list-tables-sql({"input":""})

#   -----

#   

#   [tool]: Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track

#   -----

#   

#   [ai]:  

#   Tools: 

#   - info-sql({"input":"Customer, Invoice, InvoiceLine"})

#   - info-sql({"input":"Invoice"})

#   -----

#   

#   [tool]: CREATE TABLE Invoice (

#   InvoiceId INTEGER NOT NULL, CustomerId INTEGER NOT NULL, InvoiceDate DATETIME NOT NULL, BillingAddress NVARCHAR(70) , BillingCity NVARCHAR(40) , BillingState NVARCHAR(40) , BillingCountry NVARCHAR(40) , BillingPostalCode NVARCHAR(10) , Total NUMERIC(10,2) NOT NULL) 

#   SELECT * FROM "Invoice" LIMIT 3;

#    InvoiceId CustomerId InvoiceDate BillingAddress BillingCity BillingState BillingCountry BillingPostalCode Total

#    1 2 2021-01-01 00:00:00 Theodor-Heuss-Straße 34 Stuttgart null Germany 70174 1.98

#    2 4 2021-01-02 00:00:00 Ullevålsveien 14 Oslo null Norway 0171 3.96

#    3 8 2021-01-03 00:00:00 Grétrystraat 63 Brussels null Belgium 1000 5.94

#   

#   -----

#   

#   [ai]:  

#   Tools: 

#   - query-checker({"input":"SELECT c.Country, SUM(i.Total) AS TotalSpent \nFROM Customer c \nJOIN Invoice i ON c.CustomerId = i.CustomerId \nGROUP BY c.Country \nORDER BY TotalSpent DESC \nLIMIT 5;"})

#   -----

#   

#   [tool]: The SQL query you provided appears to be correct and does not contain any of the common mistakes listed. It properly joins the `Customer` and `Invoice` tables, groups the results by country, and orders the total spending in descending order while limiting the results to the top 5 countries.

#   

#   Here is the original query reproduced:

#   

#   ```sql

#   SELECT c.Country, SUM(i.Total) AS TotalSpent 

#   FROM Customer c 

#   JOIN Invoice i ON c.CustomerId = i.CustomerId 

#   GROUP BY c.Country 

#   ORDER BY TotalSpent DESC 

#   LIMIT 5;

#   ``` 

#   

#   No changes are necessary.

#   -----

#   

#   [ai]:  

#   Tools: 

#   - query-sql({"input":"SELECT c.Country, SUM(i.Total) AS TotalSpent \nFROM Customer c \nJOIN Invoice i ON c.CustomerId = i.CustomerId \nGROUP BY c.Country \nORDER BY TotalSpent DESC \nLIMIT 5;"})

#   -----

#   

#   [tool]: [{"Country":"USA","TotalSpent":523.0600000000003},{"Country":"Canada","TotalSpent":303.9599999999999},{"Country":"France","TotalSpent":195.09999999999994},{"Country":"Brazil","TotalSpent":190.09999999999997},{"Country":"Germany","TotalSpent":156.48}]

#   -----

#   

#   [ai]: The countries whose customers spent the most are:

#   

#   1. **USA** - $523.06

#   2. **Canada** - $303.96

#   3. **France** - $195.10

#   4. **Brazil** - $190.10

#   5. **Germany** - $156.48

#   -----

#   


"""
You can also use the [LangSmith trace](https://smith.langchain.com/public/f4313ba4-a93e-418b-b863-1c2626c330d1/r) to visualize these steps and associated metadata.

Note that the agent executes multiple queries until it has the information it needs:
1. List available tables;
2. Retrieves the schema for three tables;
3. Queries multiple of the tables via a join operation.

The agent is then able to use the result of the final query to generate an answer to the original question.

The agent can similarly handle qualitative questions:
"""

let inputs3 = { messages: [{ role: "user", content: "Describe the playlisttrack table" }] };

for await (
  const step of await agent.stream(inputs3, {
    streamMode: "values",
  })
) {
    const lastMessage = step.messages[step.messages.length - 1];
    prettyPrint(lastMessage);
    console.log("-----\n");
}
# Output:
#   [human]: Describe the playlisttrack table

#   -----

#   

#   [ai]:  

#   Tools: 

#   - list-tables-sql({"input":""})

#   -----

#   

#   [tool]: Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track

#   -----

#   

#   [ai]:  

#   Tools: 

#   - info-sql({"input":"PlaylistTrack"})

#   -----

#   

#   [tool]: CREATE TABLE PlaylistTrack (

#   PlaylistId INTEGER NOT NULL, TrackId INTEGER NOT NULL) 

#   SELECT * FROM "PlaylistTrack" LIMIT 3;

#    PlaylistId TrackId

#    1 3402

#    1 3389

#    1 3390

#   

#   -----

#   

#   [ai]: The `PlaylistTrack` table has the following schema:

#   

#   - **PlaylistId**: INTEGER (NOT NULL)

#   - **TrackId**: INTEGER (NOT NULL)

#   

#   This table is used to associate tracks with playlists. Here are some sample rows from the table:

#   

#   | PlaylistId | TrackId |

#   |------------|---------|

#   | 1          | 3402    |

#   | 1          | 3389    |

#   | 1          | 3390    |

#   -----

#   


"""
### Dealing with high-cardinality columns

In order to filter columns that contain proper nouns such as addresses, song names or artists, we first need to double-check the spelling in order to filter the data correctly. 

We can achieve this by creating a vector store with all the distinct proper nouns that exist in the database. We can then have the agent query that vector store each time the user includes a proper noun in their question, to find the correct spelling for that word. In this way, the agent can make sure it understands which entity the user is referring to before building the target query.

First we need the unique values for each entity we want, for which we define a function that parses the result into a list of elements:
"""

async function queryAsList(database: any, query: string): Promise<string[]> {
  const res: Array<{ [key: string]: string }> = JSON.parse(
    await database.run(query)
  )
    .flat()
    .filter((el: any) => el != null);
  const justValues: Array<string> = res.map((item) =>
    Object.values(item)[0]
      .replace(/\b\d+\b/g, "")
      .trim()
  );
  return justValues;
}

// Gather entities into a list
let artists: string[] = await queryAsList(db, "SELECT Name FROM Artist");
let albums: string[] = await queryAsList(db, "SELECT Title FROM Album");
let properNouns = artists.concat(albums);

console.log(`Total: ${properNouns.length}\n`)
console.log(`Sample: ${properNouns.slice(0, 5)}...`)
# Output:
#   Total: 622

#   

#   Sample: AC/DC,Accept,Aerosmith,Alanis Morissette,Alice In Chains...


"""
Using this function, we can create a **retriever tool** that the agent can execute at its discretion.

Let's select an [embeddings model](/docs/integrations/text_embedding/) and [vector store](/docs/integrations/vectorstores/) for this step:

```{=mdx}
import EmbeddingTabs from "@theme/EmbeddingTabs";

<EmbeddingTabs/>
```
"""

// @lc-docs-hide-cell
import { OpenAIEmbeddings } from "@langchain/openai";

const embeddings = new OpenAIEmbeddings({model: "text-embedding-3-large"});

"""
```{=mdx}
import VectorStoreTabs from "@theme/VectorStoreTabs";

<VectorStoreTabs/>
```
"""

// @lc-docs-hide-cell
import { MemoryVectorStore } from "langchain/vectorstores/memory";

const vectorStore = new MemoryVectorStore(embeddings);

"""
We can now construct a retrieval tool that can search over relevant proper nouns in the database:
"""

import { createRetrieverTool } from "langchain/tools/retriever";
import { Document } from "@langchain/core/documents";


const documents = properNouns.map(text => new Document({ pageContent: text }));
await vectorStore.addDocuments(documents)

const retriever = vectorStore.asRetriever(5);

const retrieverTool = createRetrieverTool(retriever, {
  name: "searchProperNouns",
  description:
    "Use to look up values to filter on. Input is an approximate spelling " +
    "of the proper noun, output is valid proper nouns. Use the noun most " +
    "similar to the search."
});

"""
Let's try it out:
"""

console.log(await retrieverTool.invoke({ query: "Alice Chains" }))
# Output:
#   Alice In Chains

#   

#   Alanis Morissette

#   

#   Jagged Little Pill

#   

#   Angel Dust

#   

#   Amy Winehouse


"""
This way, if the agent determines it needs to write a filter based on an artist along the lines of "Alice Chains", it can first use the retriever tool to observe relevant values of a column.

Putting this together:
"""

// Add to system message
let suffix = (
    "If you need to filter on a proper noun like a Name, you must ALWAYS first look up " +
    "the filter value using the 'search_proper_nouns' tool! Do not try to " +
    "guess at the proper name - use this function to find similar ones."
)

const system = systemMessage + suffix

const updatedTools = tools.concat(retrieverTool)

const agent2 = createReactAgent({ llm: llm, tools: updatedTools, stateModifier: system });

let inputs4 = { messages: [{ role: "user", content: "How many albums does alis in chain have?" }] };

for await (
  const step of await agent2.stream(inputs4, {
    streamMode: "values",
  })
) {
    const lastMessage = step.messages[step.messages.length - 1];
    prettyPrint(lastMessage);
    console.log("-----\n");
}
# Output:
#   [human]: How many albums does alis in chain have?

#   -----

#   

#   [ai]:  

#   Tools: 

#   - searchProperNouns({"query":"alis in chain"})

#   -----

#   

#   [tool]: Alice In Chains

#   

#   Alanis Morissette

#   

#   Up An' Atom

#   

#   A-Sides

#   

#   Jagged Little Pill

#   -----

#   

#   [ai]:  

#   Tools: 

#   - query-sql({"input":"SELECT COUNT(*) FROM albums WHERE artist_name = 'Alice In Chains'"})

#   -----

#   

#   [tool]: QueryFailedError: SQLITE_ERROR: no such table: albums

#   -----

#   

#   [ai]:  

#   Tools: 

#   - list-tables-sql({"input":""})

#   -----

#   

#   [tool]: Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track

#   -----

#   

#   [ai]:  

#   Tools: 

#   - info-sql({"input":"Album"})

#   - info-sql({"input":"Artist"})

#   -----

#   

#   [tool]: CREATE TABLE Artist (

#   ArtistId INTEGER NOT NULL, Name NVARCHAR(120) ) 

#   SELECT * FROM "Artist" LIMIT 3;

#    ArtistId Name

#    1 AC/DC

#    2 Accept

#    3 Aerosmith

#   

#   -----

#   

#   [ai]:  

#   Tools: 

#   - query-sql({"input":"SELECT COUNT(*) FROM Album WHERE ArtistId = (SELECT ArtistId FROM Artist WHERE Name = 'Alice In Chains')"})

#   -----

#   

#   [tool]: [{"COUNT(*)":1}]

#   -----

#   

#   [ai]: Alice In Chains has released 1 album.

#   -----

#   


"""
As we can see, both in the streamed steps and in the [LangSmith trace](https://smith.langchain.com/public/8b14a4a4-c08b-4b85-8086-c050931ae03d/r), the agent used the `searchProperNouns` tool in order to check how to correctly query the database for this specific artist.
"""



================================================
FILE: docs/core_docs/docs/tutorials/summarization.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
---
title: Summarize Text
sidebar_class_name: hidden
---
"""

"""
# Summarize Text

```{=mdx}
:::info

This tutorial demonstrates text summarization using built-in chains and [LangGraph](https://langchain-ai.github.io/langgraphjs/).

See [here](https://js.langchain.com/v0.2/docs/tutorials/summarization/) for a previous version of this page, which showcased the legacy chain [RefineDocumentsChain](https://api.js.langchain.com/classes/langchain.chains.RefineDocumentsChain.html).

:::
```

Suppose you have a set of documents (PDFs, Notion pages, customer questions, etc.) and you want to summarize the content. 

LLMs are a great tool for this given their proficiency in understanding and synthesizing text.

In the context of [retrieval-augmented generation](/docs/tutorials/rag), summarizing text can help distill the information in a large number of retrieved documents to provide context for a LLM.

In this walkthrough we'll go over how to summarize content from multiple documents using LLMs.
"""

"""
## Concepts

Concepts we will cover are:

- Using [language models](/docs/concepts/chat_models).

- Using [document loaders](/docs/concepts/document_loaders), specifically the [CheerioWebBaseLoader](https://api.js.langchain.com/classes/langchain.document_loaders_web_cheerio.CheerioWebBaseLoader.html) to load content from an HTML webpage.

- Two ways to summarize or otherwise combine documents.
  1. [Stuff](/docs/tutorials/summarization#stuff), which simply concatenates documents into a prompt;
  2. [Map-reduce](/docs/tutorials/summarization#map-reduce), for larger sets of documents. This splits documents into batches, summarizes those, and then summarizes the summaries.

## Setup

### Jupyter Notebook

This and other tutorials are perhaps most conveniently run in a [Jupyter notebooks](https://jupyter.org/). Going through guides in an interactive environment is a great way to better understand them. See [here](https://jupyter.org/install) for instructions on how to install.

### Installation

To install LangChain run:

```bash npm2yarn
npm i langchain @langchain/core
```

For more details, see our [Installation guide](/docs/how_to/installation).

### LangSmith

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.
As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.
The best way to do this is with [LangSmith](https://smith.langchain.com).

After you sign up at the link above, make sure to set your environment variables to start logging traces:

```shell
export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."

# Reduce tracing latency if you are not in a serverless environment
# export LANGCHAIN_CALLBACKS_BACKGROUND=true
```
"""

"""
## Overview

A central question for building a summarizer is how to pass your documents into the LLM's context window. Two common approaches for this are:

1. `Stuff`: Simply "stuff" all your documents into a single prompt. This is the simplest approach.

2. `Map-reduce`: Summarize each document on its own in a "map" step and then "reduce" the summaries into a final summary.

Note that map-reduce is especially effective when understanding of a sub-document does not rely on preceding context. For example, when summarizing a corpus of many, shorter documents. In other cases, such as summarizing a novel or body of text with an inherent sequence, [iterative refinement](https://js.langchain.com/v0.2/docs/tutorials/summarization/) may be more effective.
"""

"""
First we load in our documents. We will use [WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html) to load a blog post:
"""

import "cheerio";
import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio";

const pTagSelector = "p";
const cheerioLoader = new CheerioWebBaseLoader(
  "https://lilianweng.github.io/posts/2023-06-23-agent/",
  {
    selector: pTagSelector
  }
);

const docs = await cheerioLoader.load();

"""
Let's next select a [chat model](/docs/integrations/chat/):

```{=mdx}
import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs customVarName="llm" />
```
"""

// @lc-docs-hide-cell
import { ChatOpenAI } from '@langchain/openai';

const llm = new ChatOpenAI({
  model: "gpt-4o-mini",
  temperature: 0,
})

"""
## Stuff: summarize in a single LLM call {#stuff}

We can use [createStuffDocumentsChain](https://api.js.langchain.com/functions/langchain.chains_combine_documents.createStuffDocumentsChain.html), especially if using larger context window models such as:

* 128k token OpenAI `gpt-4o` 
* 200k token Anthropic `claude-3-5-sonnet-20240620`

The chain will take a list of documents, insert them all into a prompt, and pass that prompt to an LLM:
"""

import { createStuffDocumentsChain } from "langchain/chains/combine_documents";
import { StringOutputParser } from "@langchain/core/output_parsers";
import { PromptTemplate } from "@langchain/core/prompts";


// Define prompt
const prompt = PromptTemplate.fromTemplate(
  "Summarize the main themes in these retrieved docs: {context}"
);

// Instantiate
const chain = await createStuffDocumentsChain({
  llm: llm,
  outputParser: new StringOutputParser(),
  prompt,
});

// Invoke
const result = await chain.invoke({context: docs})
console.log(result)
# Output:
#   The retrieved documents discuss the development and capabilities of autonomous agents powered by large language models (LLMs). Here are the main themes:

#   

#   1. **LLM as a Core Controller**: LLMs are positioned as the central intelligence in autonomous agent systems, capable of performing complex tasks beyond simple text generation. They can be framed as general problem solvers, with various implementations like AutoGPT, GPT-Engineer, and BabyAGI serving as proof-of-concept demonstrations.

#   

#   2. **Task Decomposition and Planning**: Effective task management is crucial for LLMs. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) are highlighted for breaking down complex tasks into manageable steps. CoT encourages step-by-step reasoning, while ToT explores multiple reasoning paths, enhancing the agent's problem-solving capabilities.

#   

#   3. **Integration of External Tools**: The use of external tools significantly enhances LLM capabilities. Frameworks like MRKL and Toolformer allow LLMs to interact with various APIs and tools, improving their performance in specific tasks. This modular approach enables LLMs to route inquiries to specialized modules, combining neural and symbolic reasoning.

#   

#   4. **Self-Reflection and Learning**: Self-reflection mechanisms are essential for agents to learn from past actions and improve over time. Approaches like ReAct and Reflexion integrate reasoning with action, allowing agents to evaluate their performance and adjust strategies based on feedback.

#   

#   5. **Memory and Context Management**: The documents discuss different types of memory (sensory, short-term, long-term) and their relevance to LLMs. The challenge of finite context length in LLMs is emphasized, as it limits the ability to retain and utilize historical information effectively. Techniques like external memory storage and vector databases are suggested to mitigate these limitations.

#   

#   6. **Challenges and Limitations**: Several challenges are identified, including the reliability of natural language interfaces, difficulties in long-term planning, and the need for robust task decomposition. The documents note that LLMs may struggle with unexpected errors and formatting issues, which can hinder their performance in real-world applications.

#   

#   7. **Emerging Applications**: The potential applications of LLM-powered agents are explored, including scientific discovery, autonomous design, and interactive simulations (e.g., generative agents mimicking human behavior). These applications demonstrate the versatility and innovative possibilities of LLMs in various domains.

#   

#   Overall, the documents present a comprehensive overview of the current state of LLM-powered autonomous agents, highlighting their capabilities, methodologies, and the challenges they face in practical implementations.


"""
### Streaming

Note that we can also stream the result token-by-token:
"""

const stream = await chain.stream({context: docs});

for await (const token of stream) {
  process.stdout.write(token + "|");
}
# Output:
#   |The| retrieved| documents| discuss| the| development| and| capabilities| of| autonomous| agents| powered| by| large| language| models| (|LL|Ms|).| Here| are| the| main| themes|:

#   

#   |1|.| **|LL|M| as| a| Core| Controller|**|:| L|LM|s| are| positioned| as| the| central| intelligence| in| autonomous| agent| systems|,| capable| of| performing| complex| tasks| beyond| simple| text| generation|.| They| can| be| framed| as| general| problem| sol|vers|,| with| various| implementations| like| Auto|GPT|,| GPT|-|Engineer|,| and| Baby|AG|I| serving| as| proof|-of|-con|cept| demonstrations|.

#   

#   |2|.| **|Task| De|composition| and| Planning|**|:| Effective| task| management| is| crucial| for| L|LM|s| to| handle| complicated| tasks|.| Techniques| like| Chain| of| Thought| (|Co|T|)| and| Tree| of| Thoughts| (|To|T|)| are| highlighted| for| breaking| down| tasks| into| manageable| steps| and| exploring| multiple| reasoning| paths|.| Additionally|,| L|LM|+|P| integrates| classical| planning| methods| to| enhance| long|-term| planning| capabilities|.

#   

#   |3|.| **|Self|-|Reflection| and| Learning|**|:| Self|-ref|lection| mechanisms| are| essential| for| agents| to| learn| from| past| actions| and| improve| their| decision|-making| processes|.| Framework|s| like| Re|Act| and| Reflex|ion| incorporate| dynamic| memory| and| self|-ref|lection| to| refine| reasoning| skills| and| enhance| performance| through| iterative| learning|.

#   

#   |4|.| **|Tool| Util|ization|**|:| The| integration| of| external| tools| significantly| extends| the| capabilities| of| L|LM|s|.| Appro|aches| like| MR|KL| and| Tool|former| demonstrate| how| L|LM|s| can| be| augmented| with| various| APIs| to| perform| specialized| tasks|,| enhancing| their| functionality| in| real|-world| applications|.

#   

#   |5|.| **|Memory| and| Context| Management|**|:| The| documents| discuss| different| types| of| memory| (|sens|ory|,| short|-term|,| long|-term|)| and| their| relevance| to| L|LM|s|.| The| challenge| of| finite| context| length| is| emphasized|,| as| it| limits| the| model|'s| ability| to| retain| and| utilize| historical| information| effectively|.| Techniques| like| vector| stores| and| approximate| nearest| neighbors| (|ANN|)| are| suggested| to| improve| retrieval| speed| and| memory| management|.

#   

#   |6|.| **|Challenges| and| Limit|ations|**|:| Several| limitations| of| current| L|LM|-powered| agents| are| identified|,| including| issues| with| the| reliability| of| natural| language| interfaces|,| difficulties| in| long|-term| planning|,| and| the| need| for| improved| efficiency| in| task| execution|.| The| documents| also| highlight| the| importance| of| human| feedback| in| refining| model| outputs| and| addressing| potential| biases|.

#   

#   |7|.| **|Emer|ging| Applications|**|:| The| potential| applications| of| L|LM|-powered| agents| are| explored|,| including| scientific| discovery|,| autonomous| design|,| and| interactive| simulations| (|e|.g|.,| gener|ative| agents|).| These| applications| showcase| the| versatility| of| L|LM|s| in| various| domains|,| from| drug| discovery| to| social| behavior| simulations|.

#   

#   |Overall|,| the| documents| present| a| comprehensive| overview| of| the| current| state| of| L|LM|-powered| autonomous| agents|,| their| capabilities|,| methodologies| for| improvement|,| and| the| challenges| they| face| in| practical| applications|.|||

"""
### Go deeper

* You can easily customize the prompt. 
* You can easily try different LLMs, (e.g., [Claude](/docs/integrations/chat/anthropic)) via the `llm` parameter.
"""

"""
## Map-Reduce: summarize long texts via parallelization {#map-reduce}

Let's unpack the map reduce approach. For this, we'll first map each document to an individual summary using an LLM. Then we'll reduce or consolidate those summaries into a single global summary.

Note that the map step is typically parallelized over the input documents.

[LangGraph](https://langchain-ai.github.io/langgraphjs/), built on top of [@langchain/core](/docs/concepts/architecture#langchaincore), supports [map-reduce](https://langchain-ai.github.io/langgraphjs/how-tos/map-reduce/) workflows and is well-suited to this problem:

- LangGraph allows for individual steps (such as successive summarizations) to be streamed, allowing for greater control of execution;
- LangGraph's [checkpointing](https://langchain-ai.github.io/langgraphjs/how-tos/persistence/) supports error recovery, extending with human-in-the-loop workflows, and easier incorporation into conversational applications.
- The LangGraph implementation is straightforward to modify and extend, as we will see below.

### Map
Let's first define the prompt associated with the map step. We can use the same summarization prompt as in the `stuff` approach, above:
"""

import { ChatPromptTemplate } from "@langchain/core/prompts";

const mapPrompt = ChatPromptTemplate.fromMessages(
  [
    ["user", "Write a concise summary of the following: \n\n{context}"]
  ]
)

"""
We can also use the Prompt Hub to store and fetch prompts.

This will work with your [LangSmith API key](https://docs.smith.langchain.com/).

For example, see the map prompt [here](https://smith.langchain.com/hub/rlm/map-prompt).

```javascript
import { pull } from "langchain/hub";
import { ChatPromptTemplate } from "@langchain/core/prompts";

const mapPrompt = await pull<ChatPromptTemplate>("rlm/map-prompt");
```
"""

"""
### Reduce

We also define a prompt that takes the document mapping results and reduces them into a single output.
"""

// Also available via the hub at `rlm/reduce-prompt`
let reduceTemplate = `
The following is a set of summaries:
{docs}
Take these and distill it into a final, consolidated summary
of the main themes.
`

const reducePrompt = ChatPromptTemplate.fromMessages(
  [
    ["user", reduceTemplate]
  ]
)

"""
### Orchestration via LangGraph

Below we implement a simple application that maps the summarization step on a list of documents, then reduces them using the above prompts.

Map-reduce flows are particularly useful when texts are long compared to the context window of a LLM. For long texts, we need a mechanism that ensures that the context to be summarized in the reduce step does not exceed a model's context window size. Here we implement a recursive "collapsing" of the summaries: the inputs are partitioned based on a token limit, and summaries are generated of the partitions. This step is repeated until the total length of the summaries is within a desired limit, allowing for the summarization of arbitrary-length text.

First we chunk the blog post into smaller "sub documents" to be mapped:
"""

import { TokenTextSplitter } from "@langchain/textsplitters";

const textSplitter = new TokenTextSplitter({
  chunkSize: 1000,
  chunkOverlap: 0,
});

const splitDocs = await textSplitter.splitDocuments(docs)
console.log(`Generated ${splitDocs.length} documents.`)
# Output:
#   Generated 6 documents.


"""
Next, we define our graph. Note that we define an artificially low maximum token length of 1,000 tokens to illustrate the "collapsing" step.
"""

import {
  collapseDocs,
  splitListOfDocs,
} from "langchain/chains/combine_documents/reduce";
import { Document } from "@langchain/core/documents";
import { StateGraph, Annotation, Send } from "@langchain/langgraph";


let tokenMax = 1000


async function lengthFunction(documents) {
    const tokenCounts = await Promise.all(documents.map(async (doc) => {
        return llm.getNumTokens(doc.pageContent);
    }));
    return tokenCounts.reduce((sum, count) => sum + count, 0);
}

const OverallState = Annotation.Root({
  contents: Annotation<string[]>,
  // Notice here we pass a reducer function.
  // This is because we want combine all the summaries we generate
  // from individual nodes back into one list. - this is essentially
  // the "reduce" part
  summaries: Annotation<string[]>({
    reducer: (state, update) => state.concat(update),
  }),
  collapsedSummaries: Annotation<Document[]>,
  finalSummary: Annotation<string>,
});


// This will be the state of the node that we will "map" all
// documents to in order to generate summaries
interface SummaryState {
  content: string;
}

// Here we generate a summary, given a document
const generateSummary = async (state: SummaryState): Promise<{ summaries: string[] }> => {
  const prompt = await mapPrompt.invoke({context: state.content});
  const response = await llm.invoke(prompt);
  return { summaries: [String(response.content)] };
};


// Here we define the logic to map out over the documents
// We will use this an edge in the graph
const mapSummaries = (state: typeof OverallState.State) => {
  // We will return a list of `Send` objects
  // Each `Send` object consists of the name of a node in the graph
  // as well as the state to send to that node
  return state.contents.map((content) => new Send("generateSummary", { content }));
};


const collectSummaries = async (state: typeof OverallState.State) => {
  return {
      collapsedSummaries: state.summaries.map(summary => new Document({pageContent: summary}))
  };
}


async function _reduce(input) {
    const prompt = await reducePrompt.invoke({ docs: input });
    const response = await llm.invoke(prompt);
    return String(response.content);
}

// Add node to collapse summaries
const collapseSummaries = async (state: typeof OverallState.State) => {
  const docLists = splitListOfDocs(state.collapsedSummaries, lengthFunction, tokenMax);
  const results = [];
  for (const docList of docLists) {
      results.push(await collapseDocs(docList, _reduce));
  }

  return { collapsedSummaries: results };
}


// This represents a conditional edge in the graph that determines
// if we should collapse the summaries or not
async function shouldCollapse(state: typeof OverallState.State) {
  let numTokens = await lengthFunction(state.collapsedSummaries);
  if (numTokens > tokenMax) {
    return "collapseSummaries";
  } else {
    return "generateFinalSummary";
  }
}


// Here we will generate the final summary
const generateFinalSummary = async (state: typeof OverallState.State) => {
  const response = await _reduce(state.collapsedSummaries);
  return { finalSummary: response}
}

// Construct the graph
const graph = new StateGraph(OverallState)
  .addNode("generateSummary", generateSummary)
  .addNode("collectSummaries", collectSummaries)
  .addNode("collapseSummaries", collapseSummaries)
  .addNode("generateFinalSummary", generateFinalSummary)
  .addConditionalEdges(
    "__start__",
    mapSummaries,
    ["generateSummary"]
  )
  .addEdge("generateSummary", "collectSummaries")
  .addConditionalEdges(
    "collectSummaries",
    shouldCollapse,
    ["collapseSummaries", "generateFinalSummary"]
  )
  .addConditionalEdges(
    "collapseSummaries",
    shouldCollapse,
    ["collapseSummaries", "generateFinalSummary"]
  )
  .addEdge("generateFinalSummary", "__end__")

const app = graph.compile();

"""
LangGraph allows the graph structure to be plotted to help visualize its function:
"""

"""
```javascript
// Note: tslab only works inside a jupyter notebook. Don't worry about running this code yourself!
import * as tslab from "tslab";

const image = await app.getGraph().drawMermaidPng();
const arrayBuffer = await image.arrayBuffer();

await tslab.display.png(new Uint8Array(arrayBuffer));
```

![graph_img_summarization](../../static/img/graph_img_summarization.png)
"""

"""
When running the application, we can stream the graph to observe its sequence of steps. Below, we will simply print out the name of the step.

Note that because we have a loop in the graph, it can be helpful to specify a [recursion_limit](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.GraphRecursionError.html) on its execution. This will raise a specific error when the specified limit is exceeded.
"""

let finalSummary = null;

for await (
  const step of await app.stream(
    {contents: splitDocs.map(doc => doc.pageContent)},
    { recursionLimit: 10 }
  )
) {
  console.log(Object.keys(step));
  if (step.hasOwnProperty("generateFinalSummary")) {
      finalSummary = step.generateFinalSummary
  }
}
# Output:
#   [ [32m'generateSummary'[39m ]

#   [ [32m'generateSummary'[39m ]

#   [ [32m'generateSummary'[39m ]

#   [ [32m'generateSummary'[39m ]

#   [ [32m'generateSummary'[39m ]

#   [ [32m'generateSummary'[39m ]

#   [ [32m'collectSummaries'[39m ]

#   [ [32m'generateFinalSummary'[39m ]


finalSummary
# Output:
#   {

#     finalSummary: [32m'The summaries highlight the evolving landscape of large language models (LLMs) and their integration into autonomous agents and various applications. Key themes include:\n'[39m +

#       [32m'\n'[39m +

#       [32m'1. **Autonomous Agents and LLMs**: Projects like AutoGPT and GPT-Engineer demonstrate the potential of LLMs as core controllers in autonomous systems, utilizing techniques such as Chain of Thought (CoT) and Tree of Thoughts (ToT) for task management and reasoning. These agents can learn from past actions through self-reflection mechanisms, enhancing their problem-solving capabilities.\n'[39m +

#       [32m'\n'[39m +

#       [32m'2. **Supervised Fine-Tuning and Human Feedback**: The importance of human feedback in fine-tuning models is emphasized, with methods like Algorithm Distillation (AD) showing promise in improving model performance while preventing overfitting. The integration of various memory types and external memory systems is suggested to enhance cognitive capabilities.\n'[39m +

#       [32m'\n'[39m +

#       [32m'3. **Integration of External Tools**: The incorporation of external tools and APIs significantly extends LLM capabilities, particularly in specialized tasks like maximum inner-product search (MIPS) and domain-specific applications such as ChemCrow for drug discovery. Frameworks like MRKL and HuggingGPT illustrate the potential for LLMs to effectively utilize these tools.\n'[39m +

#       [32m'\n'[39m +

#       [32m'4. **Evaluation Discrepancies**: There are notable discrepancies between LLM-based assessments and expert evaluations, indicating that LLMs may struggle with specialized knowledge. This raises concerns about their reliability in critical applications, such as scientific discovery.\n'[39m +

#       [32m'\n'[39m +

#       [32m'5. **Limitations of LLMs**: Despite advancements, LLMs face limitations, including finite context lengths, challenges in long-term planning, and difficulties in adapting to unexpected errors. These constraints hinder their robustness compared to human capabilities.\n'[39m +

#       [32m'\n'[39m +

#       [32m'Overall, the advancements in LLMs and their applications reveal both their potential and limitations, emphasizing the need for ongoing research and development to enhance their effectiveness in various domains.'[39m

#   }


"""
In the corresponding [LangSmith trace](https://smith.langchain.com/public/467d535b-1732-46ee-8d3b-f44d9cea7efa/r) we can see the individual LLM calls, grouped under their respective nodes.

### Go deeper
 
**Customization** 

* As shown above, you can customize the LLMs and prompts for map and reduce stages.

**Real-world use-case**

* See [this blog post](https://blog.langchain.dev/llms-to-improve-documentation/) case-study on analyzing user interactions (questions about LangChain documentation)!  
* The blog post and associated [repo](https://github.com/mendableai/QA_clustering) also introduce clustering as a means of summarization.
* This opens up another path beyond the `stuff` or `map-reduce` approaches that is worth considering.
"""

"""
## Next steps

We encourage you to check out the [how-to guides](/docs/how_to) for more detail on: 

- Built-in [document loaders](/docs/how_to/#document-loaders) and [text-splitters](/docs/how_to/#text-splitters)
- Integrating various combine-document chains into a [RAG application](/docs/tutorials/rag/)
- Incorporating retrieval into a [chatbot](/docs/how_to/chatbots_retrieval/)

and other concepts.
"""



================================================
FILE: docs/core_docs/docs/versions/release_policy.mdx
================================================
---
sidebar_position: 2
sidebar_label: Release Policy
---

# LangChain releases

The LangChain ecosystem is composed of different component packages (e.g., `@langchain/core`, `langchain`, `@langchain/community`, `@langchain/langgraph`, partner packages etc.)

## Versioning

### `langchain` and `@langchain/core`

`langchain` and `@langchain/core` follow [semantic versioning](https://semver.org/) in the format of 0.**Y**.**Z**. The packages are under rapid development, and so are currently versioning the packages with a major version of 0.

Minor version increases will occur for:

- Breaking changes for any public interfaces marked as `beta`.

Patch version increases will occur for:

- Bug fixes
- New features
- Any changes to private interfaces
- Any changes to `beta` features

When upgrading between minor versions, users should review the list of breaking changes and deprecations.

From time to time, we will version packages as **release candidates**. These are versions that are intended to be released as stable versions, but we want to get feedback from the community before doing so.
Release candidates will be versioned as 0.**Y**.**Z**-rc**.N**. For example, `0.2.0-rc.1`. If no issues are found, the release candidate will be released as a stable version with the same version number.
\If issues are found, we will release a new release candidate with an incremented `N` value (e.g., `0.2.0-rc.2`).

### Other packages in the langchain ecosystem

Other packages in the ecosystem (including user packages) can follow a different versioning scheme, but are generally expected to pin to specific minor versions of `langchain` and `@langchain/core`.

## Release cadence

We expect to space out **minor** releases (e.g., from 0.2.0 to 0.3.0) of `langchain` and `@langchain/core` by at least 2-3 months, as such releases may contain breaking changes.

Patch versions are released frequently as they contain bug fixes and new features.

## API stability

The development of LLM applications is a rapidly evolving field, and we are constantly learning from our users and the community. As such, we expect that the APIs in `langchain` and `@langchain/core` will continue to evolve to better serve the needs of our users.

Even though both `langchain` and `@langchain/core` are currently in a pre-1.0 state, we are committed to maintaining API stability in these packages.

- Breaking changes to the public API will result in a minor version bump (the second digit)
- Any bug fixes or new features will result in a patch version bump (the third digit)

We will generally try to avoid making unnecessary changes, and will provide a deprecation policy for features that are being removed.

### Stability of other packages

The stability of other packages in the LangChain ecosystem may vary:

- `@langchain/community` is a community maintained package that contains 3rd party integrations. While we do our best to review and test changes in `@langchain/community`, `@langchain/community` is expected to experience more breaking changes than `langchain` and `@langchain/core` as it contains many community contributions.
- Partner packages may follow different stability and versioning policies, and users should refer to the documentation of those packages for more information; however, in general these packages are expected to be stable.

### What is a "API stability"?

API stability means:

- All the public APIs (everything in this documentation) will not be moved or renamed without providing backwards-compatible aliases.
- If new features are added to these APIs – which is quite possible – they will not break or change the meaning of existing methods. In other words, "stable" does not (necessarily) mean "complete."
- If, for some reason, an API declared stable must be removed or replaced, it will be declared deprecated but will remain in the API for at least two minor releases. Warnings will be issued when the deprecated method is called.

### **APIs marked as internal**

Certain APIs are explicitly marked as “internal” in a couple of ways:

- Some documentation refers to internals and mentions them as such. If the documentation says that something is internal, it may change.
- Functions, methods, and other objects prefixed by a leading underscore (**`_`**). If any method starts with a single **`_`**, it’s an internal API.
  - **Exception:** Certain methods are prefixed with `_` , but do not contain an implementation. These methods are _meant_ to be overridden by sub-classes that provide the implementation. Such methods are generally part of the **Public API** of LangChain.

## Deprecation policy

We will generally avoid deprecating features until a better alternative is available.

When a feature is deprecated, it will continue to work in the current and next minor version of `langchain` and `@langchain/core`. After that, the feature will be removed.

Since we're expecting to space out minor releases by at least 2-3 months, this means that a feature can be removed within 2-6 months of being deprecated.

In some situations, we may allow the feature to remain in the code base for longer periods of time, if it's not causing issues in the packages, to reduce the burden on users.



================================================
FILE: docs/core_docs/docs/versions/migrating_memory/chat_history.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# How to use BaseChatMessageHistory with LangGraph

:::info Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat History](/docs/concepts/chat_history)
- [RunnableWithMessageHistory](https://api.js.langchain.com/classes/_langchain_core.runnables.RunnableWithMessageHistory.html)
- [LangGraph](https://langchain-ai.github.io/langgraphjs/concepts/high_level/)
- [Memory](https://langchain-ai.github.io/langgraphjs/concepts/agentic_concepts/#memory)

:::

We recommend that new LangChain applications take advantage of the [built-in LangGraph peristence](https://langchain-ai.github.io/langgraphjs/concepts/persistence/) to implement memory.

In some situations, users may need to keep using an existing persistence solution for chat message history.

Here, we will show how to use [LangChain chat message histories](/docs/integrations/memory/) (implementations of [BaseChatMessageHistory](https://api.js.langchain.com/classes/_langchain_core.chat_history.BaseChatMessageHistory.html)) with LangGraph.
"""

"""
## Set up

```typescript
process.env.ANTHROPIC_API_KEY = 'YOUR_API_KEY'
```

```{=mdx}
import Npm2Yarn from "@theme/Npm2Yarn"

<Npm2Yarn>
  @langchain/core @langchain/langgraph @langchain/anthropic
</Npm2Yarn>
```
"""

"""
## ChatMessageHistory

A message history needs to be parameterized by a conversation ID or maybe by the 2-tuple of (user ID, conversation ID).

Many of the [LangChain chat message histories](/docs/integrations/memory/) will have either a `sessionId` or some `namespace` to allow keeping track of different conversations. Please refer to the specific implementations to check how it is parameterized.

The built-in `InMemoryChatMessageHistory` does not contains such a parameterization, so we'll create a dictionary to keep track of the message histories.
"""

import { InMemoryChatMessageHistory } from "@langchain/core/chat_history";

const chatsBySessionId: Record<string, InMemoryChatMessageHistory> = {}

const getChatHistory = (sessionId: string) => {
    let chatHistory: InMemoryChatMessageHistory | undefined = chatsBySessionId[sessionId]
    if (!chatHistory) {
      chatHistory = new InMemoryChatMessageHistory()
      chatsBySessionId[sessionId] = chatHistory
    }
    return chatHistory
}

"""
## Use with LangGraph

Next, we'll set up a basic chat bot using LangGraph. If you're not familiar with LangGraph, you should look at the following [Quick Start Tutorial](https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/).

We'll create a [LangGraph node](https://langchain-ai.github.io/langgraphjs/concepts/low_level/#nodes) for the chat model, and manually manage the conversation history, taking into account the conversation ID passed as part of the RunnableConfig.

The conversation ID can be passed as either part of the RunnableConfig (as we'll do here), or as part of the [graph state](https://langchain-ai.github.io/langgraphjs/concepts/low_level/#state).
"""

import { v4 as uuidv4 } from "uuid";
import { ChatAnthropic } from "@langchain/anthropic";
import { StateGraph, MessagesAnnotation, END, START } from "@langchain/langgraph";
import { HumanMessage } from "@langchain/core/messages";
import { RunnableConfig } from "@langchain/core/runnables";

// Define a chat model
const model = new ChatAnthropic({ modelName: "claude-3-haiku-20240307" });

// Define the function that calls the model
const callModel = async (
  state: typeof MessagesAnnotation.State,
  config: RunnableConfig
): Promise<Partial<typeof MessagesAnnotation.State>> => {
  if (!config.configurable?.sessionId) {
    throw new Error(
      "Make sure that the config includes the following information: {'configurable': {'sessionId': 'some_value'}}"
    );
  }

  const chatHistory = getChatHistory(config.configurable.sessionId as string);

  let messages = [...(await chatHistory.getMessages()), ...state.messages];

  if (state.messages.length === 1) {
    // First message, ensure it's in the chat history
    await chatHistory.addMessage(state.messages[0]);
  }

  const aiMessage = await model.invoke(messages);

  // Update the chat history
  await chatHistory.addMessage(aiMessage);

  return { messages: [aiMessage] };
};

// Define a new graph
const workflow = new StateGraph(MessagesAnnotation)
  .addNode("model", callModel)
  .addEdge(START, "model")
  .addEdge("model", END);

const app = workflow.compile();

// Create a unique session ID to identify the conversation
const sessionId = uuidv4();
const config = { configurable: { sessionId }, streamMode: "values" as const };

const inputMessage = new HumanMessage("hi! I'm bob");

for await (const event of await app.stream({ messages: [inputMessage] }, config)) {
  const lastMessage = event.messages[event.messages.length - 1];
  console.log(lastMessage.content);
}

// Here, let's confirm that the AI remembers our name!
const followUpMessage = new HumanMessage("what was my name?");

for await (const event of await app.stream({ messages: [followUpMessage] }, config)) {
  const lastMessage = event.messages[event.messages.length - 1];
  console.log(lastMessage.content);
}
# Output:
#   hi! I'm bob

#   Hello Bob! It's nice to meet you. How can I assist you today?

#   what was my name?

#   You said your name is Bob.


"""
## Using With RunnableWithMessageHistory

This how-to guide used the `messages` and `addMessages` interface of `BaseChatMessageHistory` directly. 

Alternatively, you can use [RunnableWithMessageHistory](https://api.js.langchain.com/classes/_langchain_core.runnables.RunnableWithMessageHistory.html), as [LCEL](/docs/concepts/lcel/) can be used inside any [LangGraph node](https://langchain-ai.github.io/langgraphjs/concepts/low_level/#nodes).

To do that replace the following code:

```typescript
const callModel = async (
  state: typeof MessagesAnnotation.State,
  config: RunnableConfig
): Promise<Partial<typeof MessagesAnnotation.State>> => {
  // highlight-start
  if (!config.configurable?.sessionId) {
    throw new Error(
      "Make sure that the config includes the following information: {'configurable': {'sessionId': 'some_value'}}"
    );
  }

  const chatHistory = getChatHistory(config.configurable.sessionId as string);

  let messages = [...(await chatHistory.getMessages()), ...state.messages];

  if (state.messages.length === 1) {
    // First message, ensure it's in the chat history
    await chatHistory.addMessage(state.messages[0]);
  }

  const aiMessage = await model.invoke(messages);

  // Update the chat history
  await chatHistory.addMessage(aiMessage);
  // highlight-end
  return { messages: [aiMessage] };
};
```

With the corresponding instance of `RunnableWithMessageHistory` defined in your current application.

```typescript
const runnable = new RunnableWithMessageHistory({
  // ... configuration from existing code
});

const callModel = async (
  state: typeof MessagesAnnotation.State,
  config: RunnableConfig
): Promise<Partial<typeof MessagesAnnotation.State>> => {
  // RunnableWithMessageHistory takes care of reading the message history
  // and updating it with the new human message and AI response.
  const aiMessage = await runnable.invoke(state.messages, config);
  return {
    messages: [aiMessage]
  };
};
```
"""



================================================
FILE: docs/core_docs/docs/versions/migrating_memory/conversation_buffer_window_memory.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Migrating off ConversationTokenBufferMemory

Follow this guide if you're trying to migrate off one of the old memory classes listed below:


| Memory Type                      | Description                                                                                                                                                       |
|----------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `ConversationTokenBufferMemory`  | Keeps only the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit. |

`ConversationTokenBufferMemory` applies additional processing on top of the raw conversation history to trim the conversation history to a size that fits inside the context window of a chat model. 

This processing functionality can be accomplished using LangChain's built-in [trimMessages](https://api.js.langchain.com/functions/_langchain_core.messages.trimMessages.html) function.
"""

"""
```{=mdx}
:::important

We’ll begin by exploring a straightforward method that involves applying processing logic to the entire conversation history.

While this approach is easy to implement, it has a downside: as the conversation grows, so does the latency, since the logic is re-applied to all previous exchanges in the conversation at each turn.

More advanced strategies focus on incrementally updating the conversation history to avoid redundant processing.

For instance, the LangGraph [how-to guide on summarization](https://langchain-ai.github.io/langgraphjs/how-tos/add-summary-conversation-history/) demonstrates
how to maintain a running summary of the conversation while discarding older messages, ensuring they aren't re-processed during later turns.
:::
```
"""

"""
## Set up

### Dependencies

```{=mdx}
import Npm2Yarn from "@theme/Npm2Yarn"

<Npm2Yarn>
  @langchain/openai @langchain/core zod
</Npm2Yarn>
```

### Environment variables

```typescript
process.env.OPENAI_API_KEY = "YOUR_OPENAI_API_KEY";
```

```{=mdx}
<details open>
```
"""

"""
## Reimplementing ConversationTokenBufferMemory logic

Here, we'll use `trimMessages` to keeps the system message and the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit.
"""

import {
  AIMessage,
  HumanMessage,
  SystemMessage,
} from "@langchain/core/messages";

const messages = [
  new SystemMessage("you're a good assistant, you always respond with a joke."),
  new HumanMessage("i wonder why it's called langchain"),
  new AIMessage(
    'Well, I guess they thought "WordRope" and "SentenceString" just didn\'t have the same ring to it!'
  ),
  new HumanMessage("and who is harrison chasing anyways"),
  new AIMessage(
      "Hmmm let me think.\n\nWhy, he's probably chasing after the last cup of coffee in the office!"
  ),
  new HumanMessage("why is 42 always the answer?"),
  new AIMessage(
      "Because it's the only number that's constantly right, even when it doesn't add up!"
  ),
  new HumanMessage("What did the cow say?"),
]

import { trimMessages } from "@langchain/core/messages";
import { ChatOpenAI } from "@langchain/openai";

const selectedMessages = await trimMessages(
  messages,
  {
    // Please see API reference for trimMessages for other ways to specify a token counter.
    tokenCounter: new ChatOpenAI({ model: "gpt-4o" }),
    maxTokens: 80,  // <-- token limit
    // The startOn is specified
    // to make sure we do not generate a sequence where
    // a ToolMessage that contains the result of a tool invocation
    // appears before the AIMessage that requested a tool invocation
    // as this will cause some chat models to raise an error.
    startOn: "human",
    strategy: "last",
    includeSystem: true,  // <-- Keep the system message
  }
)

for (const msg of selectedMessages) {
    console.log(msg);
}
# Output:
#   SystemMessage {

#     "content": "you're a good assistant, you always respond with a joke.",

#     "additional_kwargs": {},

#     "response_metadata": {}

#   }

#   HumanMessage {

#     "content": "and who is harrison chasing anyways",

#     "additional_kwargs": {},

#     "response_metadata": {}

#   }

#   AIMessage {

#     "content": "Hmmm let me think.\n\nWhy, he's probably chasing after the last cup of coffee in the office!",

#     "additional_kwargs": {},

#     "response_metadata": {},

#     "tool_calls": [],

#     "invalid_tool_calls": []

#   }

#   HumanMessage {

#     "content": "why is 42 always the answer?",

#     "additional_kwargs": {},

#     "response_metadata": {}

#   }

#   AIMessage {

#     "content": "Because it's the only number that's constantly right, even when it doesn't add up!",

#     "additional_kwargs": {},

#     "response_metadata": {},

#     "tool_calls": [],

#     "invalid_tool_calls": []

#   }

#   HumanMessage {

#     "content": "What did the cow say?",

#     "additional_kwargs": {},

#     "response_metadata": {}

#   }


"""
```{=mdx}
</details>
```

## Modern usage with LangGraph

The example below shows how to use LangGraph to add simple conversation pre-processing logic.

```{=mdx}
:::note

If you want to avoid running the computation on the entire conversation history each time, you can follow
the [how-to guide on summarization](https://langchain-ai.github.io/langgraphjs/how-tos/add-summary-conversation-history/) that demonstrates
how to discard older messages, ensuring they aren't re-processed during later turns.

:::
```

```{=mdx}
<details open>
```
"""

import { v4 as uuidv4 } from 'uuid';
import { ChatOpenAI } from "@langchain/openai";
import { StateGraph, MessagesAnnotation, END, START, MemorySaver } from "@langchain/langgraph";
import { trimMessages } from "@langchain/core/messages";

// Define a chat model
const model = new ChatOpenAI({ model: "gpt-4o" });

// Define the function that calls the model
const callModel = async (state: typeof MessagesAnnotation.State): Promise<Partial<typeof MessagesAnnotation.State>> => {
  // highlight-start
  const selectedMessages = await trimMessages(
    state.messages,
    {
      tokenCounter: (messages) => messages.length, // Simple message count instead of token count
      maxTokens: 5, // Allow up to 5 messages
      strategy: "last",
      startOn: "human",
      includeSystem: true,
      allowPartial: false,
    }
  );
  // highlight-end

  const response = await model.invoke(selectedMessages);

  // With LangGraph, we're able to return a single message, and LangGraph will concatenate
  // it to the existing list
  return { messages: [response] };
};


// Define a new graph
const workflow = new StateGraph(MessagesAnnotation)
// Define the two nodes we will cycle between
  .addNode("model", callModel)
  .addEdge(START, "model")
  .addEdge("model", END)

const app = workflow.compile({
  // Adding memory is straightforward in LangGraph!
  // Just pass a checkpointer to the compile method.
  checkpointer: new MemorySaver()
});

// The thread id is a unique key that identifies this particular conversation
// ---
// NOTE: this must be `thread_id` and not `threadId` as the LangGraph internals expect `thread_id`
// ---
const thread_id = uuidv4();
const config = { configurable: { thread_id }, streamMode: "values" as const };

const inputMessage = {
  role: "user",
  content: "hi! I'm bob",
}
for await (const event of await app.stream({ messages: [inputMessage] }, config)) {
  const lastMessage = event.messages[event.messages.length - 1];
  console.log(lastMessage.content);
}

// Here, let's confirm that the AI remembers our name!
const followUpMessage = {
  role: "user",
  content: "what was my name?",
}

// ---
// NOTE: You must pass the same thread id to continue the conversation
// we do that here by passing the same `config` object to the `.stream` call.
// ---
for await (const event of await app.stream({ messages: [followUpMessage] }, config)) {
  const lastMessage = event.messages[event.messages.length - 1];
  console.log(lastMessage.content);
}
# Output:
#   hi! I'm bob

#   Hello, Bob! How can I assist you today?

#   what was my name?

#   You mentioned that your name is Bob. How can I help you today?


"""
```{=mdx}
</details>
```

## Usage with a pre-built langgraph agent

This example shows usage of an Agent Executor with a pre-built agent constructed using the [createReactAgent](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph_prebuilt.createReactAgent.html) function.

If you are using one of the [old LangChain pre-built agents](https://js.langchain.com/v0.1/docs/modules/agents/agent_types/), you should be able
to replace that code with the new [LangGraph pre-built agent](https://langchain-ai.github.io/langgraphjs/how-tos/create-react-agent/) which leverages
native tool calling capabilities of chat models and will likely work better out of the box.

```{=mdx}
<details open>
```
"""

import { z } from "zod";
import { v4 as uuidv4 } from 'uuid';
import { BaseMessage, trimMessages } from "@langchain/core/messages";
import { tool } from "@langchain/core/tools";
import { ChatOpenAI } from "@langchain/openai";
import { MemorySaver } from "@langchain/langgraph";
import { createReactAgent } from "@langchain/langgraph/prebuilt";

const getUserAge = tool(
  (name: string): string => {
    // This is a placeholder for the actual implementation
    if (name.toLowerCase().includes("bob")) {
      return "42 years old";
    }
    return "41 years old";
  },
  {
    name: "get_user_age",
    description: "Use this tool to find the user's age.",
    schema: z.string().describe("the name of the user"),
  }
);

const memory = new MemorySaver();
const model2 = new ChatOpenAI({ model: "gpt-4o" });

// highlight-start
const stateModifier = async (messages: BaseMessage[]): Promise<BaseMessage[]> => {
  // We're using the message processor defined above.
  return trimMessages(
    messages,
    {
      tokenCounter: (msgs) => msgs.length, // <-- .length will simply count the number of messages rather than tokens
      maxTokens: 5, // <-- allow up to 5 messages.
      strategy: "last",
      // The startOn is specified
      // to make sure we do not generate a sequence where
      // a ToolMessage that contains the result of a tool invocation
      // appears before the AIMessage that requested a tool invocation
      // as this will cause some chat models to raise an error.
      startOn: "human",
      includeSystem: true, // <-- Keep the system message
      allowPartial: false,
    }
  );
};
// highlight-end

const app2 = createReactAgent({
  llm: model2,
  tools: [getUserAge],
  checkpointSaver: memory,
  // highlight-next-line
  messageModifier: stateModifier,
});

// The thread id is a unique key that identifies
// this particular conversation.
// We'll just generate a random uuid here.
const threadId2 = uuidv4();
const config2 = { configurable: { thread_id: threadId2 }, streamMode: "values" as const };

// Tell the AI that our name is Bob, and ask it to use a tool to confirm
// that it's capable of working like an agent.
const inputMessage2 = {
  role: "user",
  content: "hi! I'm bob. What is my age?",
}

for await (const event of await app2.stream({ messages: [inputMessage2] }, config2)) {
  const lastMessage = event.messages[event.messages.length - 1];
  console.log(lastMessage.content);
}

// Confirm that the chat bot has access to previous conversation
// and can respond to the user saying that the user's name is Bob.
const followUpMessage2 = {
  role: "user",
  content: "do you remember my name?",
};

for await (const event of await app2.stream({ messages: [followUpMessage2] }, config2)) {
  const lastMessage = event.messages[event.messages.length - 1];
  console.log(lastMessage.content);
}
# Output:
#   hi! I'm bob. What is my age?

#   

#   42 years old

#   Hi Bob! You are 42 years old.

#   do you remember my name?

#   Yes, your name is Bob! If there's anything else you'd like to know or discuss, feel free to ask.


"""
```{=mdx}
</details>
```

## LCEL: Add a preprocessing step

The simplest way to add complex conversation management is by introducing a pre-processing step in front of the chat model and pass the full conversation history to the pre-processing step.

This approach is conceptually simple and will work in many situations; for example, if using a [RunnableWithMessageHistory](/docs/how_to/message_history/) instead of wrapping the chat model, wrap the chat model with the pre-processor.

The obvious downside of this approach is that latency starts to increase as the conversation history grows because of two reasons:

1. As the conversation gets longer, more data may need to be fetched from whatever store your'e using to store the conversation history (if not storing it in memory).
2. The pre-processing logic will end up doing a lot of redundant computation, repeating computation from previous steps of the conversation.

```{=mdx}
:::caution

If you want to use a chat model's tool calling capabilities, remember to bind the tools to the model before adding the history pre-processing step to it!

:::
```

```{=mdx}
<details open>
```
"""

import { ChatOpenAI } from "@langchain/openai";
import { AIMessage, HumanMessage, SystemMessage, BaseMessage, trimMessages } from "@langchain/core/messages";
import { tool } from "@langchain/core/tools";
import { z } from "zod";

const model3 = new ChatOpenAI({ model: "gpt-4o" });

const whatDidTheCowSay = tool(
  (): string => {
    return "foo";
  },
  {
    name: "what_did_the_cow_say",
    description: "Check to see what the cow said.",
    schema: z.object({}),
  }
);

// highlight-start
const messageProcessor = trimMessages(
  {
    tokenCounter: (msgs) => msgs.length, // <-- .length will simply count the number of messages rather than tokens
    maxTokens: 5, // <-- allow up to 5 messages.
    strategy: "last",
    // The startOn is specified
    // to make sure we do not generate a sequence where
    // a ToolMessage that contains the result of a tool invocation
    // appears before the AIMessage that requested a tool invocation
    // as this will cause some chat models to raise an error.
    startOn: "human",
    includeSystem: true, // <-- Keep the system message
    allowPartial: false,
  }
);
// highlight-end

// Note that we bind tools to the model first!
const modelWithTools = model3.bindTools([whatDidTheCowSay]);

// highlight-next-line
const modelWithPreprocessor = messageProcessor.pipe(modelWithTools);

const fullHistory = [
  new SystemMessage("you're a good assistant, you always respond with a joke."),
  new HumanMessage("i wonder why it's called langchain"),
  new AIMessage('Well, I guess they thought "WordRope" and "SentenceString" just didn\'t have the same ring to it!'),
  new HumanMessage("and who is harrison chasing anyways"),
  new AIMessage("Hmmm let me think.\n\nWhy, he's probably chasing after the last cup of coffee in the office!"),
  new HumanMessage("why is 42 always the answer?"),
  new AIMessage("Because it's the only number that's constantly right, even when it doesn't add up!"),
  new HumanMessage("What did the cow say?"),
];

// We pass it explicitly to the modelWithPreprocessor for illustrative purposes.
// If you're using `RunnableWithMessageHistory` the history will be automatically
// read from the source that you configure.
const result = await modelWithPreprocessor.invoke(fullHistory);
console.log(result);
# Output:
#   AIMessage {

#     "id": "chatcmpl-AB6uzWscxviYlbADFeDlnwIH82Fzt",

#     "content": "",

#     "additional_kwargs": {

#       "tool_calls": [

#         {

#           "id": "call_TghBL9dzqXFMCt0zj0VYMjfp",

#           "type": "function",

#           "function": "[Object]"

#         }

#       ]

#     },

#     "response_metadata": {

#       "tokenUsage": {

#         "completionTokens": 16,

#         "promptTokens": 95,

#         "totalTokens": 111

#       },

#       "finish_reason": "tool_calls",

#       "system_fingerprint": "fp_a5d11b2ef2"

#     },

#     "tool_calls": [

#       {

#         "name": "what_did_the_cow_say",

#         "args": {},

#         "type": "tool_call",

#         "id": "call_TghBL9dzqXFMCt0zj0VYMjfp"

#       }

#     ],

#     "invalid_tool_calls": [],

#     "usage_metadata": {

#       "input_tokens": 95,

#       "output_tokens": 16,

#       "total_tokens": 111

#     }

#   }


"""
```{=mdx}
</details>
```

If you need to implement more efficient logic and want to use `RunnableWithMessageHistory` for now the way to achieve this
is to subclass from [BaseChatMessageHistory](https://api.js.langchain.com/classes/_langchain_core.chat_history.BaseChatMessageHistory.html) and
define appropriate logic for `addMessages` (that doesn't simply append the history, but instead re-writes it).

Unless you have a good reason to implement this solution, you should instead use LangGraph.
"""

"""
## Next steps

Explore persistence with LangGraph:

* [LangGraph quickstart tutorial](https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/)
* [How to add persistence ("memory") to your graph](https://langchain-ai.github.io/langgraphjs/how-tos/persistence/)
* [How to manage conversation history](https://langchain-ai.github.io/langgraphjs/how-tos/manage-conversation-history/)
* [How to add summary of the conversation history](https://langchain-ai.github.io/langgraphjs/how-tos/add-summary-conversation-history/)

Add persistence with simple LCEL (favor LangGraph for more complex use cases):

* [How to add message history](/docs/how_to/message_history/)

Working with message history:

* [How to trim messages](/docs/how_to/trim_messages)
* [How to filter messages](/docs/how_to/filter_messages/)
* [How to merge message runs](/docs/how_to/merge_message_runs/)

"""



================================================
FILE: docs/core_docs/docs/versions/migrating_memory/conversation_summary_memory.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Migrating off ConversationSummaryMemory or ConversationSummaryBufferMemory

Follow this guide if you're trying to migrate off one of the old memory classes listed below:


| Memory Type                          | Description                                                                                                                                          |
|---------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|
| `ConversationSummaryMemory`           | Continually summarizes the conversation history. The summary is updated after each conversation turn. The abstraction returns the summary of the conversation history. |
| `ConversationSummaryBufferMemory`     | Provides a running summary of the conversation together with the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit. |

Please follow the following [how-to guide on summarization](https://langchain-ai.github.io/langgraphjs/how-tos/add-summary-conversation-history/) in LangGraph. 

This guide shows how to maintain a running summary of the conversation while discarding older messages, ensuring they aren't re-processed during later turns.
"""



================================================
FILE: docs/core_docs/docs/versions/migrating_memory/index.mdx
================================================
---
sidebar_position: 1
---

# How to migrate to LangGraph memory

As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of LangGraph persistence to incorporate `memory` into their LangChain application.

- Users that rely on `RunnableWithMessageHistory` or `BaseChatMessageHistory` do **not** need to make any changes, but are encouraged to consider using LangGraph for more complex use cases.
- Users that rely on deprecated memory abstractions from LangChain 0.0.x should follow this guide to upgrade to the new LangGraph persistence feature in LangChain 0.3.x.

## Why use LangGraph for memory?

The main advantages of persistence in LangGraph are:

- Built-in support for multiple users and conversations, which is a typical requirement for real-world conversational AI applications.
- Ability to save and resume complex conversations at any point. This helps with:
  - Error recovery
  - Allowing human intervention in AI workflows
  - Exploring different conversation paths ("time travel")
- Full compatibility with both traditional [language models](/docs/concepts/text_llms) and modern [chat models](/docs/concepts/chat_models). Early memory implementations in LangChain weren't designed for newer chat model APIs, causing issues with features like tool-calling. LangGraph memory can persist any custom state.
- Highly customizable, allowing you to fully control how memory works and use different storage backends.

## Evolution of memory in LangChain

The concept of memory has evolved significantly in LangChain since its initial release.

### LangChain 0.0.x memory

Broadly speaking, LangChain 0.0.x memory was used to handle three main use cases:

| Use Case                             | Example                                                                                                                           |
| ------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------- |
| Managing conversation history        | Keep only the last `n` turns of the conversation between the user and the AI.                                                     |
| Extraction of structured information | Extract structured information from the conversation history, such as a list of facts learned about the user.                     |
| Composite memory implementations     | Combine multiple memory sources, e.g., a list of known facts about the user along with facts learned during a given conversation. |

While the LangChain 0.0.x memory abstractions were useful, they were limited in their capabilities and not well suited for real-world conversational AI applications. These memory abstractions lacked built-in support for multi-user, multi-conversation scenarios, which are essential for practical conversational AI systems.

Most of these implementations have been officially deprecated in LangChain 0.3.x in favor of LangGraph persistence.

### RunnableWithMessageHistory and BaseChatMessageHistory

:::note
Please see [How to use BaseChatMessageHistory with LangGraph](./chat_history), if you would like to use `BaseChatMessageHistory` (with or without `RunnableWithMessageHistory`) in LangGraph.
:::

As of LangChain v0.1, we started recommending that users rely primarily on [BaseChatMessageHistory](https://api.js.langchain.com/classes/_langchain_core.chat_history.BaseChatMessageHistory.html). `BaseChatMessageHistory` serves
as a simple persistence for storing and retrieving messages in a conversation.

At that time, the only option for orchestrating LangChain chains was via [LCEL](/docs/how_to/#langchain-expression-language-lcel). To incorporate memory with `LCEL`, users had to use the [RunnableWithMessageHistory](https://api.js.langchain.com/classes/_langchain_core.runnables.RunnableWithMessageHistory.html) interface. While sufficient for basic chat applications, many users found the API unintuitive and challenging to use.

As of LangChain v0.3, we recommend that **new** code takes advantage of LangGraph for both orchestration and persistence:

- Orchestration: In LangGraph, users define [graphs](https://langchain-ai.github.io/langgraphjs/concepts/low_level/) that specify the flow of the application. This allows users to keep using `LCEL` within individual nodes when `LCEL` is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.
- Persistence: Users can rely on LangGraph's persistence to store and retrieve data. LangGraph persistence is extremely flexible and can support a much wider range of use cases than the `RunnableWithMessageHistory` interface.

:::important
If you have been using `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do not need to make any changes. We do not plan on deprecating either functionality in the near future. This functionality is sufficient for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.
:::

## Migrations

:::info Prerequisites

These guides assume some familiarity with the following concepts:

- [LangGraph](https://langchain-ai.github.io/langgraphjs/)
- [v0.0.x Memory](https://js.langchain.com/v0.1/docs/modules/memory/)
- [How to add persistence ("memory") to your graph](https://langchain-ai.github.io/langgraphjs/how-tos/persistence/)

:::

### 1. Managing conversation history

The goal of managing conversation history is to store and retrieve the history in a way that is optimal for a chat model to use.

Often this involves trimming and / or summarizing the conversation history to keep the most relevant parts of the conversation while having the conversation fit inside the context window of the chat model.

Memory classes that fall into this category include:

| Memory Type                       | How to Migrate                                               | Description                                                                                                                                                                                                         |
| --------------------------------- | :----------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `ConversationTokenBufferMemory`   | [Link to Migration Guide](conversation_buffer_window_memory) | Keeps only the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit.                                                   |
| `ConversationSummaryMemory`       | [Link to Migration Guide](conversation_summary_memory)       | Continually summarizes the conversation history. The summary is updated after each conversation turn. The abstraction returns the summary of the conversation history.                                              |
| `ConversationSummaryBufferMemory` | [Link to Migration Guide](conversation_summary_memory)       | Provides a running summary of the conversation together with the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit. |

### 2. Extraction of structured information from the conversation history

Memory classes that fall into this category include:

| Memory Type       | Description                                                                                                                                                                                                    |
| ----------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `BaseEntityStore` | An abstract interface that resembles a key-value store. It was used for storing structured information learned during the conversation. The information had to be represented as an object of key-value pairs. |

And specific backend implementations of abstractions:

| Memory Type           | Description                                                                                              |
| --------------------- | -------------------------------------------------------------------------------------------------------- |
| `InMemoryEntityStore` | An implementation of `BaseEntityStore` that stores the information in the literal computer memory (RAM). |

These abstractions have not received much development since their initial release. The reason
is that for these abstractions to be useful they typically require a lot of specialization for a particular application, so these
abstractions are not as widely used as the conversation history management abstractions.

For this reason, there are no migration guides for these abstractions. If you're struggling to migrate an application
that relies on these abstractions, please pen an issue on the LangChain GitHub repository, explain your use case, and we'll try to provide more guidance on how to migrate these abstractions.

The general strategy for extracting structured information from the conversation history is to use a chat model with tool calling capabilities to extract structured information from the conversation history.
The extracted information can then be saved into an appropriate data structure (e.g., an object), and information from it can be retrieved and added into the prompt as needed.

### 3. Implementations that provide composite logic on top of one or more memory implementations

Memory classes that fall into this category include:

| Memory Type      | Description                                                                                                                    |
| ---------------- | ------------------------------------------------------------------------------------------------------------------------------ |
| `CombinedMemory` | This abstraction accepted a list of `BaseMemory` and fetched relevant memory information from each of them based on the input. |

These implementations did not seem to be used widely or provide significant value. Users should be able
to re-implement these without too much difficulty in custom code.

## Related Resources

Explore persistence with LangGraph:

- [LangGraph quickstart tutorial](https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/)
- [How to add persistence ("memory") to your graph](https://langchain-ai.github.io/langgraphjs/how-tos/persistence/)
- [How to manage conversation history](https://langchain-ai.github.io/langgraphjs/how-tos/manage-conversation-history/)
- [How to add summary of the conversation history](https://langchain-ai.github.io/langgraphjs/how-tos/add-summary-conversation-history/)

Add persistence with simple LCEL (favor langgraph for more complex use cases):

- [How to add message history](/docs/how_to/message_history/)

Working with message history:

- [How to trim messages](/docs/how_to/trim_messages)
- [How to filter messages](/docs/how_to/filter_messages/)
- [How to merge message runs](/docs/how_to/merge_message_runs/)



================================================
FILE: docs/core_docs/docs/versions/v0_2/index.mdx
================================================
---
sidebar_position: 1
sidebar_label: v0.2
---

# LangChain v0.2

LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x, as well as a list of deprecations and breaking changes.

:::note Reference

- [Migrating to Astream Events v2](/docs/versions/v0_2/migrating_astream_events)

:::

## Migration

This documentation will help you upgrade your code to LangChain `0.2.x.`. To prepare for migration, we first recommend you take the following steps:

1. install the 0.2.x versions of `@langchain/core`, langchain and upgrade to recent versions of other packages that you may be using (e.g. `@langchain/langgraph`, `@langchain/community`, `@langchain/openai`, etc.)
2. Verify that your code runs properly with the new packages (e.g., unit tests pass)
3. Install a recent version of `langchain-cli` , and use the tool to replace old imports used by your code with the new imports. (See instructions below.)
4. Manually resolve any remaining deprecation warnings
5. Re-run unit tests

### Upgrade to new imports

We created a tool to help migrate your code. This tool is still in **beta** and may not cover all cases, but
we hope that it will help you migrate your code more quickly.

The migration script has the following limitations:

1. It's limited to helping users move from old imports to new imports. It doesn't help address other deprecations.
2. It can't handle imports that involve `as` .
3. New imports are always placed in global scope, even if the old import that was replaced was located inside some local scope (e..g, function body).
4. It will likely miss some deprecated imports.

Here is an example of the import changes that the migration script can help apply automatically:

| From Package           | To Package                 | Deprecated Import                                                          | New Import                                                                       |
| ---------------------- | -------------------------- | -------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |
| `langchain`            | `@langchain/community`     | `import { UpstashVectorStore } from "langchain/vectorstores/upstash"`      | `import { UpstashVectorStore } from "@langchain/community/vectorstores/upstash"` |
| `@langchain/community` | `@langchain/openai`        | `import { ChatOpenAI } from "@langchain/community/chat_models/openai"`     | `import { ChatOpenAI } from "@langchain/openai"`                                 |
| `langchain`            | `@langchain/core`          | `import { Document } from "langchain/schema/document"`                     | `import { Document } from "@langchain/core/documents"`                           |
| `langchain`            | `@langchain/textsplitters` | `import { RecursiveCharacterTextSplitter } from "langchain/text_splitter"` | `import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters"`      |

#### Deprecation timeline

We have two main types of deprecations:

1. Code that was moved from `langchain` into another package (e.g, `@langchain/community`)

If you try to import it from `langchain`, it will fail since the entrypoint has been removed.

2. Code that has better alternatives available and will eventually be removed, so there's only a single way to do things. (e.g., `predictMessages` method in ChatModels has been deprecated in favor of `invoke`).

Many of these were marked for removal in 0.2. We have bumped the removal to 0.3.

#### Installation

:::note
The 0.2.X migration script is only available in version `0.0.14-rc.1` or later.
:::

```bash npm2yarn
npm i @langchain/scripts@0.0.14-rc.1
```

#### Usage

Given that the migration script is not perfect, you should make sure you have a backup of your code first (e.g., using version control like `git`).

For example, say your code still uses `import ChatOpenAI from "@langchain/community/chat_models/openai";`:

Invoking the migration script will replace this import with `import ChatOpenAI from "@langchain/openai";`.

```typescript
import { updateEntrypointsFrom0_x_xTo0_2_x } from "@langchain/scripts/migrations";

const pathToMyProject = "..."; // This path is used in the following glob pattern: `${projectPath}/**/*.{ts,tsx,js,jsx}`.

updateEntrypointsFrom0_x_xTo0_2_x({
  projectPath: pathToMyProject,
  shouldLog: true,
});
```

#### Other options

```typescript
updateEntrypointsFrom0_x_xTo0_2_x({
  projectPath: pathToMyProject,
  tsConfigPath: "tsconfig.json", // Path to the tsConfig file. This will be used to load all the project files into the script.
  testRun: true, // If true, the script will not save any changes, but will log the changes that would be made.
  files: ["..."], // A list of .ts file paths to check. If this is provided, the script will only check these files.
});
```



================================================
FILE: docs/core_docs/docs/versions/v0_2/migrating_astream_events.mdx
================================================
---
sidebar_position: 2
sidebar_label: Migrating to streamEvents v2
---

# Migrating to streamEvents v2

:::danger

This migration guide is a work in progress and is not complete.

:::

We've added a `v2` of the [`streamEvents`](/docs/how_to/streaming#using-stream-events) API with the release of `0.2.0`. You can see this [PR](https://github.com/langchain-ai/langchainjs/pull/5539/) for more details.

The `v2` version is a re-write of the `v1` version, and should be more efficient, with more consistent output for the events. The `v1` version of the API will be deprecated in favor of the `v2` version and will be removed in `0.4.0`.

Below is a list of changes between the `v1` and `v2` versions of the API.

### output for `on_chat_model_end`

In `v1`, the outputs associated with `on_chat_model_end` changed depending on whether the
chat model was run as a root level runnable or as part of a chain.

As a root level runnable the output was:

```ts
{
  data: {
    output: AIMessageChunk((content = "hello world!"), (id = "some id"));
  }
}
```

As part of a chain the output was:

```
{
  data: {
    output: {
      generations: [
        [
          {
            generation_info: None,
            message: AIMessageChunk(
                content="hello world!", id="some id"
            ),
            text: "hello world!",
          }
        ]
      ],
    }
  },
}
```

As of `v2`, the output will always be the simpler representation:

```ts
{
  data: {
    output: AIMessageChunk((content = "hello world!"), (id = "some id"));
  }
}
```

:::note
Non chat models (i.e., regular LLMs) will be consistently associated with the more verbose format for now.
:::

### output for `on_retriever_end`

`on_retriever_end` output will always return a list of `Documents`.

This was the output in `v1`:

```ts
{
  data: {
    output: {
      documents: [
        Document(...),
        Document(...),
        ...
      ]
    }
  }
}
```

And here is the new output for `v2`:

```ts
{
  data: {
    output: [
      Document(...),
      Document(...),
      ...
    ]
  }
}
```

### Removed `on_retriever_stream`

The `on_retriever_stream` event was an artifact of the implementation and has been removed.

Full information associated with the event is already available in the `on_retriever_end` event.

Please use `on_retriever_end` instead.

### Removed `on_tool_stream`

The `on_tool_stream` event was an artifact of the implementation and has been removed.

Full information associated with the event is already available in the `on_tool_end` event.

Please use `on_tool_end` instead.

### Propagating Names

Names of runnables have been updated to be more consistent.

If you're filtering by event names, check if you need to update your filters.



================================================
FILE: docs/core_docs/docs/versions/v0_3/index.mdx
================================================
---
sidebar_position: 0
sidebar_label: v0.3
---

# LangChain v0.3

_Last updated: 09.14.24_

## What's changed

- All LangChain packages now have `@langchain/core` as a peer dependency instead of a direct dependency to help avoid type errors around [core version conflicts](/docs/how_to/installation/#installing-integration-packages).
  - You will now need to explicitly install `@langchain/core` rather than relying on an internally resolved version from other packages.
- Callbacks are now backgrounded and non-blocking by default rather than blocking.
  - This means that if you are using e.g. LangSmith for tracing in a serverless environment, you will need to [await the callbacks to ensure they finish before your function ends](/docs/how_to/callbacks_serverless).
- Removed deprecated document loader and self-query entrypoints from `langchain` in favor of entrypoints in [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) and integration packages.
- Removed deprecated Google PaLM entrypoints from community in favor of entrypoints in [`@langchain/google-vertexai`](https://www.npmjs.com/package/@langchain/google-vertexai) and [`@langchain/google-genai`](https://www.npmjs.com/package/@langchain/google-genai).
- Deprecated using objects with a `"type"` as a [`BaseMessageLike`](https://v03.api.js.langchain.com/types/_langchain_core.messages.BaseMessageLike.html) in favor of the more OpenAI-like [`MessageWithRole`](https://v03.api.js.langchain.com/types/_langchain_core.messages.MessageFieldWithRole.html)

## What’s new

The following features have been added during the development of 0.2.x:

- Simplified tool definition and usage. Read more [here](https://blog.langchain.dev/improving-core-tool-interfaces-and-docs-in-langchain/).
- Added a [generalized chat model constructor](https://js.langchain.com/docs/how_to/chat_models_universal_init/).
- Added the ability to [dispatch custom events](https://js.langchain.com/docs/how_to/callbacks_custom_events/).
- Released LangGraph.js 0.2.0 and made it the [recommended way to create agents](https://js.langchain.com/docs/how_to/migrate_agent) with LangChain.js.
- Revamped integration docs and API reference. Read more [here](https://blog.langchain.dev/langchain-integration-docs-revamped/).

## How to update your code

If you're using `langchain` / `@langchain/community` / `@langchain/core` 0.0 or 0.1, we recommend that you first [upgrade to 0.2](https://js.langchain.com/v0.2/docs/versions/v0_2/).

If you're using `@langchain/langgraph`, upgrade to `@langchain/langgraph>=0.2.3`. This will work with either 0.2 or 0.3 versions of all the base packages.

Here is a complete list of all packages that have been released and what we recommend upgrading your version constraints to in your `package.json`.
Any package that now supports `@langchain/core` 0.3 had a minor version bump.

### Base packages

| Package                  | Latest | Recommended `package.json` constraint |
| ------------------------ | ------ | ------------------------------------- |
| langchain                | 0.3.0  | >=0.3.0 <0.4.0                        |
| @langchain/community     | 0.3.0  | >=0.3.0 <0.4.0                        |
| @langchain/textsplitters | 0.1.0  | >=0.1.0 <0.2.0                        |
| @langchain/core          | 0.3.0  | >=0.3.0 <0.4.0                        |

### Downstream packages

| Package              | Latest | Recommended `package.json` constraint |
| -------------------- | ------ | ------------------------------------- |
| @langchain/langgraph | 0.2.3  | >=0.2.3 <0.3                          |

### Integration packages

| Package                           | Latest | Recommended `package.json` constraint |
| --------------------------------- | ------ | ------------------------------------- |
| @langchain/anthropic              | 0.3.0  | >=0.3.0 <0.4.0                        |
| @langchain/aws                    | 0.1.0  | >=0.1.0 <0.2.0                        |
| @langchain/azure-cosmosdb         | 0.2.0  | >=0.2.0 <0.3.0                        |
| @langchain/azure-dynamic-sessions | 0.2.0  | >=0.2.0 <0.3.0                        |
| @langchain/baidu-qianfan          | 0.1.0  | >=0.1.0 <0.2.0                        |
| @langchain/cloudflare             | 0.1.0  | >=0.1.0 <0.2.0                        |
| @langchain/cohere                 | 0.3.0  | >=0.3.0 <0.4.0                        |
| @langchain/exa                    | 0.1.0  | >=0.1.0 <0.2.0                        |
| @langchain/google-genai           | 0.1.0  | >=0.1.0 <0.2.0                        |
| @langchain/google-vertexai        | 0.1.0  | >=0.1.0 <0.2.0                        |
| @langchain/google-vertexai-web    | 0.1.0  | >=0.1.0 <0.2.0                        |
| @langchain/groq                   | 0.1.1  | >=0.1.1 <0.2.0                        |
| @langchain/mistralai              | 0.1.0  | >=0.1.0 <0.2.0                        |
| @langchain/mixedbread-ai          | 0.1.0  | >=0.1.0 <0.2.0                        |
| @langchain/mongodb                | 0.1.0  | >=0.1.0 <0.2.0                        |
| @langchain/nomic                  | 0.1.0  | >=0.1.0 <0.2.0                        |
| @langchain/ollama                 | 0.1.0  | >=0.1.0 <0.2.0                        |
| @langchain/openai                 | 0.3.0  | >=0.3.0 <0.4.0                        |
| @langchain/pinecone               | 0.1.0  | >=0.1.0 <0.2.0                        |
| @langchain/qdrant                 | 0.1.0  | >=0.1.0 <0.2.0                        |
| @langchain/redis                  | 0.1.0  | >=0.1.0 <0.2.0                        |
| @langchain/weaviate               | 0.1.0  | >=0.1.0 <0.2.0                        |
| @langchain/yandex                 | 0.1.0  | >=0.1.0 <0.2.0                        |

Once you've updated to recent versions of the packages, you will need to explicitly install `@langchain/core` if you haven't already:

```bash npm2yarn
npm install @langchain/core
```

We also suggest checking your lockfile or running the [appropriate package manager command](/docs/how_to/installation/#installing-integration-packages) to make sure that your package manager only has one version of `@langchain/core` installed.

If you are currently running your code in a serverless environment (e.g., a Cloudflare Worker, Edge function, or AWS Lambda function)
and you are using LangSmith tracing or other callbacks, you will need to [await callbacks to ensure they finish before your function ends](/docs/how_to/callbacks_serverless).
Here's a quick example:

```ts
import { RunnableLambda } from "@langchain/core/runnables";
import { awaitAllCallbacks } from "@langchain/core/callbacks/promises";

const runnable = RunnableLambda.from(() => "hello!");

const customHandler = {
  handleChainEnd: async () => {
    await new Promise((resolve) => setTimeout(resolve, 2000));
    console.log("Call finished");
  },
};

const startTime = new Date().getTime();

await runnable.invoke({ number: "2" }, { callbacks: [customHandler] });

console.log(`Elapsed time: ${new Date().getTime() - startTime}ms`);

await awaitAllCallbacks();

console.log(`Final elapsed time: ${new Date().getTime() - startTime}ms`);
```

```
Elapsed time: 1ms
Call finished
Final elapsed time: 2164ms
```

You can also set `LANGCHAIN_CALLBACKS_BACKGROUND` to `"false"` to make all callbacks blocking:

```ts
process.env.LANGCHAIN_CALLBACKS_BACKGROUND = "false";

const startTimeBlocking = new Date().getTime();

await runnable.invoke({ number: "2" }, { callbacks: [customHandler] });

console.log(
  `Initial elapsed time: ${new Date().getTime() - startTimeBlocking}ms`
);
```

```
Call finished
Initial elapsed time: 2002ms
```



================================================
FILE: docs/core_docs/scripts/append_related_links.py
================================================
import itertools
import multiprocessing
import re
import sys
from pathlib import Path


def _generate_related_links_section(integration_type: str, notebook_name: str):
    concept_display_name = None
    concept_heading = None
    if integration_type == "chat":
        concept_display_name = "Chat model"
        concept_heading = "chat-models"
    elif integration_type == "llms":
        concept_display_name = "LLM"
        concept_heading = "llms"
    elif integration_type == "text_embedding":
        concept_display_name = "Embedding model"
        concept_heading = "embedding-models"
    elif integration_type == "document_loaders":
        concept_display_name = "Document loader"
        concept_heading = "document-loaders"
    elif integration_type == "vectorstores":
        concept_display_name = "Vector store"
        concept_heading = "vectorstores"
    elif integration_type == "retrievers":
        concept_display_name = "Retriever"
        concept_heading = "retrievers"
    elif integration_type == "tools":
        concept_display_name = "Tool"
        concept_heading = "tools"
    elif integration_type == "stores":
        concept_display_name = "Key-value store"
        concept_heading = "key-value-stores"
        # Special case because there are no key-value store how-tos yet
        return f"""## Related

- [{concept_display_name} conceptual guide](/docs/concepts/#{concept_heading})
"""
    else:
        return None
    return f"""## Related

- {concept_display_name} [conceptual guide](/docs/concepts/#{concept_heading})
- {concept_display_name} [how-to guides](/docs/how_to/#{concept_heading})
"""


def _process_path(doc_path: Path):
    content = doc_path.read_text()
    has_related = "## Related" in content
    pattern = r"docs/integrations/([^/]+)/([^/]+).mdx?"
    match = re.search(pattern, str(doc_path))
    if match and match.group(2) != "index" and not has_related:
        integration_type = match.group(1)
        notebook_name = match.group(2)
        related_links_section = _generate_related_links_section(
            integration_type, notebook_name
        )
        if related_links_section:
            content = content + "\n\n" + related_links_section
            doc_path.write_text(content)


if __name__ == "__main__":
    output_docs_dir = Path(sys.argv[1])

    mds = output_docs_dir.rglob("integrations/**/*.md")
    mdxs = output_docs_dir.rglob("integrations/**/*.mdx")
    paths = itertools.chain(mds, mdxs)
    # modify all md files in place
    with multiprocessing.Pool() as pool:
        pool.map(_process_path, paths)



================================================
FILE: docs/core_docs/scripts/check-broken-links.js
================================================
const { checkBrokenLinks } = require("@langchain/scripts/check_broken_links");

checkBrokenLinks("docs", {
  timeout: 10000,
  retryFailed: true,
});



================================================
FILE: docs/core_docs/scripts/code-block-loader.js
================================================
/* eslint-disable prefer-template */
/* eslint-disable no-param-reassign */
// eslint-disable-next-line import/no-extraneous-dependencies
const swc = require("@swc/core");
const path = require("path");
const fs = require("fs");

// Directories generated inside the API docs (excluding "modules").
const CATEGORIES = [
  "classes",
  "enums",
  "functions",
  "interfaces",
  "types",
  "variables",
];

/**
 * Edge cases where the import will not match the proper API ref path.
 * This is typically caused by a re-export, or an aliased export so we
 * must manually map the import to the correct path.
 */
const SYMBOL_EDGE_CASE_MAP = {
  InMemoryStore: {
    sources: ["langchain/storage/in_memory"],
    originalSource: "@langchain/core/stores",
    originalSymbolName: null,
  },
  ToolMessage: {
    sources: ["@langchain/core/messages"],
    originalSource: "@langchain/core/messages/tool",
    originalSymbolName: null,
  },
  zodToGeminiParameters: {
    sources: ["@langchain/google-vertexai/utils"],
    originalSource: "@langchain/google-common",
    originalSymbolName: null,
  },
  FunctionalTranslator: {
    sources: ["langchain/retrievers/self_query/functional"],
    originalSource: "@langchain/core/structured_query",
    originalSymbolName: null,
  },
  ChatMessageHistory: {
    sources: [
      "langchain/stores/message/in_memory",
      "@langchain/community/stores/message/in_memory",
    ],
    originalSource: "@langchain/core/chat_history",
    originalSymbolName: "InMemoryChatMessageHistory",
  },
  GeminiTool: {
    sources: ["@langchain/google-vertexai/types"],
    originalSource: "@langchain/google-common/types",
    originalSymbolName: null,
  },
  RecursiveCharacterTextSplitter: {
    sources: ["langchain/text_splitter"],
    originalSource: "@langchain/textsplitters",
    originalSymbolName: null,
  },
  CharacterTextSplitter: {
    sources: ["langchain/text_splitter"],
    originalSource: "@langchain/textsplitters",
    originalSymbolName: null,
  },
  TokenTextSplitter: {
    sources: ["langchain/text_splitter"],
    originalSource: "@langchain/textsplitters",
    originalSymbolName: null,
  },
  SupportedTextSplitterLanguages: {
    sources: ["langchain/text_splitter"],
    originalSource: "@langchain/textsplitters",
    originalSymbolName: null,
  },
};

/**
 * Symbols which will never exist in the API refs.
 *
 * This can be caused by re-exports from non LangChain
 * packages.
 */
const SYMBOLS_TO_SKIP_MAP = {
  LunaryHandler: {
    source: "@langchain/community/callbacks/handlers/lunary",
  },
  updateEntrypointsFrom0_0_xTo0_1_x: {
    source: "@langchain/scripts/migrations",
  },
};

/**
 *
 * @param {string|Buffer} content Content of the resource file
 * @param {object} [map] SourceMap data consumable by https://github.com/mozilla/source-map
 * @param {any} [meta] Meta data, could be anything
 */
async function webpackLoader(content, map, meta) {
  const cb = this.async();
  const BASE_URL = "https://api.js.langchain.com";

  if (!this.resourcePath.endsWith(".ts")) {
    cb(null, JSON.stringify({ content, imports: [] }), map, meta);
    return;
  }

  try {
    const module = await swc.parse(content, {
      isModule: true,
      filename: this.resourcePath,
      syntax: "typescript",
    });

    const imports = [];

    module.body.forEach((node) => {
      if (node.type === "ImportDeclaration") {
        const source = node.source.value;

        if (
          !source.startsWith("langchain") &&
          !source.startsWith("@langchain")
        ) {
          return;
        }

        node.specifiers.forEach((specifier) => {
          if (specifier.type === "ImportSpecifier") {
            const local = specifier.local.value;
            const imported = specifier.imported?.value ?? local;
            // Only push imports if the symbol & source is not in the skip map.
            if (
              !(
                imported in SYMBOLS_TO_SKIP_MAP &&
                SYMBOLS_TO_SKIP_MAP[imported].source === source
              )
            ) {
              imports.push({ local, imported, source });
            }
          } else {
            throw new Error("Unsupported import type");
          }
        });
      }
    });

    /**
     * Create a full path to the API ref docs file, given a "componentPath".
     * A "componentPath" is a string in the format of "category/module/symbol.html".
     *
     * @param {string} componentPath The path to the component in the API docs.
     * @returns {string} The path to the API docs file.
     */
    const getDocsPath = (componentPath) =>
      path.resolve("..", "api_refs", "public", componentPath);

    /**
     * Given an imported symbol and source, find the path to the API ref docs.
     * If no match is found, return null.
     *
     * @param {string} imported The name of the imported symbol. E.g. `ChatOpenAI`
     * @param {string} source The name of the package/module it was imported from. E.g. `@langchain/openai`
     * @returns {string | null} The path to the API docs file or null if not found.
     */
    const findApiRefPath = (imported, source) => {
      // Fix the source if it's an edge case.
      if (
        imported in SYMBOL_EDGE_CASE_MAP &&
        SYMBOL_EDGE_CASE_MAP[imported].sources.some((s) => s === source)
      ) {
        source = SYMBOL_EDGE_CASE_MAP[imported].originalSource;
        imported =
          SYMBOL_EDGE_CASE_MAP[imported].originalSymbolName ?? imported;
      }

      let cleanedSource = "";
      if (source.startsWith("@langchain/")) {
        cleanedSource = source
          .replace("@langchain/", "_langchain_")
          .replace(/(?<=_langchain_[^/]+)\//, ".")
          .replaceAll(/\//g, "_")
          .replaceAll(/-/g, "_");
      } else if (source.startsWith("langchain")) {
        cleanedSource = source
          .replace("langchain/", "langchain.")
          .replaceAll("/", "_")
          .replaceAll("-", "_");
      } else {
        throw new Error(
          `Invalid source: ${source}. Must be prefixed with one of "langchain/" or "@langchain/"`
        );
      }
      const componentPath = `${cleanedSource}.${imported}.html`;
      const componentIndexPath = `${cleanedSource}.index.${imported}.html`;

      /**
       * Defaults to null, reassigned to string if a match is found.
       * @type {null | string}
       */
      let actualPath = null;
      CATEGORIES.forEach((category) => {
        if (actualPath !== null) {
          return;
        }
        const fullPath = `${category}/${componentPath}`;
        const fullIndexPath = `${category}/${componentIndexPath}`;

        const pathExists = fs.existsSync(getDocsPath(fullPath));
        if (pathExists) {
          actualPath = fullPath;
          return;
        }
        const indexPathExists = fs.existsSync(getDocsPath(fullIndexPath));
        if (indexPathExists) {
          actualPath = fullIndexPath;
        }
      });

      return actualPath;
    };

    imports.forEach((imp) => {
      const { imported, source } = imp;

      if (source.startsWith("@langchain/langgraph")) {
        // TODO: Add support for verifying LangGraph API reference links so we can use exact URLs.
        imp.docs = "https://langchain-ai.github.io/langgraphjs/reference/";
        return;
      }

      const apiRefPath = findApiRefPath(imported, source);

      if (apiRefPath) {
        imp.docs = BASE_URL + "/" + apiRefPath;
      } else {
        // `this.resourcePath` is typically the absolute path. By splitting at "examples/"
        // we can get the relative path to the examples directory, making the error more readable.
        const cleanedResourcePath = this.resourcePath.includes("examples/")
          ? this.resourcePath.split("examples/")[1]
          : this.resourcePath;

        console.warn(
          {
            imported,
            source,
          },
          `examples/${cleanedResourcePath}: Could not find API refs link.`
        );
      }
    });

    cb(null, JSON.stringify({ content, imports }), map, meta);
  } catch (err) {
    cb(err);
  }
}

module.exports = webpackLoader;



================================================
FILE: docs/core_docs/scripts/quarto-build.js
================================================
const fs = require("node:fs");
const { glob } = require("glob");
const { execSync } = require("node:child_process");

const IGNORED_CELL_REGEX =
  /^``` *\w*?[\s\S]\/\/ ?@lc-docs-hide-cell[\s\S]*?^```/gm;
const LC_TS_IGNORE_REGEX = /\/\/ ?@lc-ts-ignore\n/g;

async function main() {
  const allIpynb = await glob("./docs/**/*.ipynb");

  const allRenames = allIpynb.flatMap((filename) => [
    filename.replace(".ipynb", ".md"),
    filename.replace(".ipynb", ".mdx"),
  ]);
  const pathToRootGitignore = ".gitignore";
  let gitignore = fs.readFileSync(pathToRootGitignore, "utf-8");
  gitignore = gitignore.split("# AUTO_GENERATED_DOCS")[0];
  gitignore += "# AUTO_GENERATED_DOCS\n";
  gitignore += allRenames.join("\n");
  fs.writeFileSync(pathToRootGitignore, gitignore);
  for (const renamedFilepath of allRenames) {
    if (fs.existsSync(renamedFilepath)) {
      let content = fs.readFileSync(renamedFilepath, "utf-8").toString();
      if (
        content.match(IGNORED_CELL_REGEX) ||
        content.match(LC_TS_IGNORE_REGEX)
      ) {
        content = content
          .replace(IGNORED_CELL_REGEX, "")
          .replace(LC_TS_IGNORE_REGEX, "");
        fs.writeFileSync(renamedFilepath, content);
      }
    }
  }

  try {
    /**
     * Run Prettier on all generated .ipynb -> .mdx because we don't
     * currently have another way to format code written in notebooks.
     */
    const command = `yarn prettier --write ${allRenames
      .filter((filename) => fs.existsSync(filename))
      .join(" ")}`;
    execSync(command);
  } catch (error) {
    console.error(
      {
        error,
        stdout: error?.stderr?.toString(),
      },
      "Failed to format notebooks"
    );
  }
}

main().catch((e) => {
  console.error(e);
  process.exit(1);
});



================================================
FILE: docs/core_docs/scripts/vercel_build.sh
================================================
#!/bin/bash

yum -y update
# Not sure we need all these
yum install gcc bzip2-devel libffi-devel zlib-devel wget tar gzip -y

# install quarto
wget -q https://github.com/quarto-dev/quarto-cli/releases/download/v1.3.450/quarto-1.3.450-linux-amd64.tar.gz
tar -xzf quarto-1.3.450-linux-amd64.tar.gz
export PATH=$PATH:$(pwd)/quarto-1.3.450/bin/

quarto render docs/


================================================
FILE: docs/core_docs/src/css/custom.css
================================================
/**
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 *
 * @format
 */

/**
 * Any CSS included here will be global. The classic template
 * bundles Infima by default. Infima is a CSS framework designed to
 * work well for content-centric websites.
 */

@font-face {
  font-family: 'Manrope';
  src: url('/fonts/Manrope-VariableFont_wght.ttf') format('truetype');
}
@font-face {
  font-family: 'Public Sans';
  src: url('/fonts/PublicSans-VariableFont_wght.ttf') format('truetype');
}

/* You can override the default Infima variables here. */
:root {
  --ifm-color-primary: #2e8555;
  --ifm-color-primary-dark: #29784c;
  --ifm-color-primary-darker: #277148;
  --ifm-color-primary-darkest: #205d3b;
  --ifm-color-primary-light: #33925d;
  --ifm-color-primary-lighter: #359962;
  --ifm-color-primary-lightest: #3cad6e;
  --ifm-font-weight-bold: 600;
  --ifm-code-font-size: 95%;
  --ifm-font-family-base: 'Public Sans';
  --ifm-menu-link-padding-horizontal: 0.5rem;
  --ifm-menu-link-padding-vertical: 0.5rem;
  --doc-sidebar-width: 275px !important;
  /* Code highlighting background color */
  --docusaurus-highlighted-code-line-bg: rgb(202, 203, 205);

}

/* For readability concerns, you should choose a lighter palette in dark mode. */
[data-theme="dark"] {
  --ifm-color-primary: #25c2a0;
  --ifm-color-primary-dark: #21af90;
  --ifm-color-primary-darker: #1fa588;
  --ifm-color-primary-darkest: #1a8870;
  --ifm-color-primary-light: #29d5b0;
  --ifm-color-primary-lighter: #32d8b4;
  --ifm-color-primary-lightest: #4fddbf;
  /* Code highlighting background color */
  --docusaurus-highlighted-code-line-bg: rgb(73, 73, 73);
}

nav, h1, h2, h3, h4 {
  font-family: 'Manrope';
}

.footer__links {
  margin-top: 1rem;
  margin-bottom: 3rem;
}

.footer__col {
  text-align: center;
}

.footer__copyright {
  opacity: 0.6;
}

.node-only,
.web-only,
.beta {
  position: relative;
}


.menu__list-item.node-only .menu__link {
  padding-right: 80px;
}

.node-only::after,
.beta::after,
.web-only::after {
  position: absolute;
  right: 0.35rem;
  top: 5px;
  content: "Node-only";
  background: #026e00;
  color: #fff;
  border-radius: 0.25rem;
  padding: 0 0.5rem;
  pointer-events: none;
  font-size: 0.85rem;
}

[data-theme="dark"] .node-only::after {
  background: #026e00;
  color: #fff;
}

/* Override `beta` color */
.beta::after {
  content: "Beta";
  color: #fff;
  background: #58006e;
  border: 1px solid #58006e;
}

/* Override `beta` color */
[data-theme="dark"] .beta::after {
  background: #58006e;
  color: #fff;
}

/* Override `web-only` color */
.web-only::after {
  content: "Web-only";
  color: #fff;
  background: #0a0072;
  border: 1px solid #0a0072;
}

/* Override `web-only` color */
[data-theme="dark"] .web-only::after {
  background: #0a0072;
  color: #fff;
}

.node-only-category,
.beta-category {
  position: relative;
}

.node-only-category::after,
.beta-category::after {
  position: absolute;
  right: 3rem;
  top: 5px;
  content: "Node-only";
  background: #026e00;
  color: #fff;
  border-radius: 0.25rem;
  padding: 0 0.5rem;
  pointer-events: none;
  font-size: 0.85rem;
}

[data-theme="dark"] .node-only-category::after {
  background: #026e00;
  color: #fff;
}

/* Override `beta` color */
.beta-category::after {
  content: "Beta";
  color: #58006e;
  border: 1px solid #58006e;
}

/* Override `beta` color */
[data-theme="dark"] .beta::after {
  background: #58006e;
  color: #fff;
}


.theme-code-block.language-python,
.theme-code-block.language-javascript,
.theme-code-block.language-js,
.theme-code-block.language-typescript,
.theme-code-block.language-ts {
  position: relative; /* Ensure this is set so the ::before pseudo-element is positioned relative to this element */
  padding-left: 4px;
  border: 1px solid var(--ifm-color-primary-darkest);
}

.theme-code-block.language-python::before,
.theme-code-block.language-javascript::before,
.theme-code-block.language-js::before,
.theme-code-block.language-typescript::before,
.theme-code-block.language-ts::before {
  content: "";
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  width: 3px;
  border-top-left-radius: 4px;
  border-bottom-left-radius: 4px;
  background-color: var(--ifm-color-primary-light);
  z-index: 1;
}

.theme-doc-sidebar-menu > .theme-doc-sidebar-item-category:not(:first-of-type),
.theme-doc-sidebar-menu > .theme-doc-sidebar-item-link,
.theme-doc-sidebar-menu > .theme-doc-sidebar-item-link.theme-doc-sidebar-item-link-level-1:not(:first-of-type) {
  margin-top: 1rem;
}

.theme-doc-sidebar-menu .theme-doc-sidebar-item-link,
.theme-doc-sidebar-menu .theme-doc-sidebar-item-category {
  margin-top: 0;
  padding-bottom: 0;
  padding-top: 0;
}

.theme-doc-sidebar-menu .theme-doc-sidebar-item-category > ul {
  margin-top: 0;
}

.theme-doc-sidebar-menu .theme-doc-sidebar-item-link a,
.theme-doc-sidebar-menu .theme-doc-sidebar-item-category a {
  margin-top: 0;
}

.theme-doc-sidebar-item-category, .theme-doc-sidebar-menu > .theme-doc-sidebar-item-link {
  font-size: 1rem;
  font-weight: 700;
}

.theme-doc-sidebar-item-category button:before {
  height: 1rem;
  width: 1.25rem;
}

.theme-doc-sidebar-item-link, .theme-doc-sidebar-item-category .theme-doc-sidebar-item-category {
  font-size: .9rem;
  font-weight: 500;
}

.theme-doc-sidebar-item-category > div > a {
  flex: 1 1 0;
  overflow: hidden;
  word-break: break-word;
}

.theme-doc-sidebar-item-category > div > button {
  opacity: 0.5;
}

/* Hack for "More" style caret buttons */
.theme-doc-sidebar-item-category > div > a::after {
  opacity: 0.5;
}

.markdown {
  line-height: 2em;
}

.markdown > h2 {
  margin-top: 2rem;
  border-bottom-color: var(--ifm-color-primary);
  border-bottom-width: 2px;
  padding-bottom: 1rem;
}

.markdown > :not(h2) +  h3 {
  margin-top: 1rem;
}

.markdown > h4 {
  margin-bottom: 0.2rem;
  font-weight: 600;
}

.markdown > h4:has(+ table) {
  margin-bottom: 0.4rem;
}

.markdown > h5 {
  margin-bottom: 0.2rem;
  font-weight: 600;
}

.hidden {
  display: none !important;
}

.header-github-link:hover {
  opacity: 0.6;
}

.header-github-link::before {
  content: '';
  width: 24px;
  height: 24px;
  display: flex;
  background: url("data:image/svg+xml,%3Csvg viewBox='0 0 24 24' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12'/%3E%3C/svg%3E")
    no-repeat;
}

[data-theme="dark"] .header-github-link::before {
  background: url("data:image/svg+xml,%3Csvg viewBox='0 0 24 24' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath fill='white' d='M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12'/%3E%3C/svg%3E") no-repeat;
}

.announcementBar_node_modules-\@docusaurus-theme-classic-lib-theme-AnnouncementBar-styles-module {
  height: 40px !important;
  font-size: 20px !important;
}

[data-theme='dark'] .announcementBar_node_modules-\@docusaurus-theme-classic-lib-theme-AnnouncementBar-styles-module {
  background-color: #1b1b1b;
  color: #fff;
}

[data-theme='dark'] .announcementBar_node_modules-\@docusaurus-theme-classic-lib-theme-AnnouncementBar-styles-module button {
  color: #fff;
}

.announcement-bar-text {
  font-size: 16px;
}

div[class^='announcementBar_'] {
  padding: 20px 0; 
}



================================================
FILE: docs/core_docs/src/pages/index.js
================================================
/**
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 *
 * @format
 */

import React from "react";
import { Redirect } from "@docusaurus/router";
import useBaseUrl from "@docusaurus/useBaseUrl";

export default function Home() {
  return <Redirect to={useBaseUrl("/docs/introduction/")} />;
}



================================================
FILE: docs/core_docs/src/theme/ChatModelTabs.js
================================================
/* eslint-disable react/jsx-props-no-spreading, react/destructuring-assignment */
import React from "react";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import CodeBlock from "@theme-original/CodeBlock";
import Npm2Yarn from "@theme/Npm2Yarn";
import Admonition from "@theme/Admonition";

function InstallationInfo({ children }) {
  return (
    <>
      <Admonition type="tip">
        <p>
          See{" "}
          <a href="/docs/how_to/installation/#installing-integration-packages">
            this section for general instructions on installing integration
            packages
          </a>
          .
        </p>
      </Admonition>
      <Npm2Yarn>{children}</Npm2Yarn>
    </>
  );
}

const DEFAULTS = {
  openaiParams: `{\n  model: "gpt-4o-mini",\n  temperature: 0\n}`,
  anthropicParams: `{\n  model: "claude-3-5-sonnet-20240620",\n  temperature: 0\n}`,
  fireworksParams: `{\n  model: "accounts/fireworks/models/llama-v3p1-70b-instruct",\n  temperature: 0\n}`,
  mistralParams: `{\n  model: "mistral-large-latest",\n  temperature: 0\n}`,
  groqParams: `{\n  model: "llama-3.3-70b-versatile",\n  temperature: 0\n}`,
  vertexParams: `{\n  model: "gemini-1.5-flash",\n  temperature: 0\n}`,
};

const MODELS_WSO = ["openai", "anthropic", "mistral", "groq", "vertex"];

/**
 * @typedef {Object} ChatModelTabsProps - Component props.
 * @property {string} [openaiParams] - Parameters for OpenAI chat model. Defaults to `"{\n  model: "gpt-3.5-turbo",\n  temperature: 0\n}"`
 * @property {string} [anthropicParams] - Parameters for Anthropic chat model. Defaults to `"{\n  model: "claude-3-sonnet-20240229",\n  temperature: 0\n}"`
 * @property {string} [fireworksParams] - Parameters for Fireworks chat model. Defaults to `"{\n  model: "accounts/fireworks/models/firefunction-v1",\n  temperature: 0\n}"`
 * @property {string} [mistralParams] - Parameters for Mistral chat model. Defaults to `"{\n  model: "mistral-large-latest",\n  temperature: 0\n}"`
 * @property {string} [groqParams] - Parameters for Groq chat model. Defaults to `"{\n  model: "mixtral-8x7b-32768",\n  temperature: 0\n}"`
 * @property {string} [vertexParams] - Parameters for Google VertexAI chat model. Defaults to `"{\n  model: "gemini-1.5-pro",\n  temperature: 0\n}"`
 *
 * @property {boolean} [hideOpenai] - Whether or not to hide OpenAI chat model.
 * @property {boolean} [hideAnthropic] - Whether or not to hide Anthropic chat model.
 * @property {boolean} [hideFireworks] - Whether or not to hide Fireworks chat model.
 * @property {boolean} [hideMistral] - Whether or not to hide Mistral chat model.
 * @property {boolean} [hideGroq] - Whether or not to hide Mistral chat model.
 * @property {boolean} [hideVertex] - Whether or not to hide Mistral chat model.
 *
 * @property {string} [customVarName] - Custom variable name for the model. Defaults to `"model"`.
 * @property {boolean} [onlyWso] - Only display models which have `withStructuredOutput` implemented.
 */

/**
 * @param {ChatModelTabsProps} props - Component props.
 */
export default function ChatModelTabs(props) {
  const { customVarName, additionalDependencies } = props;

  const llmVarName = customVarName ?? "model";

  const openaiParams = props.openaiParams ?? DEFAULTS.openaiParams;
  const anthropicParams = props.anthropicParams ?? DEFAULTS.anthropicParams;
  const fireworksParams = props.fireworksParams ?? DEFAULTS.fireworksParams;
  const mistralParams = props.mistralParams ?? DEFAULTS.mistralParams;
  const groqParams = props.groqParams ?? DEFAULTS.groqParams;
  const vertexParams = props.vertexParams ?? DEFAULTS.vertexParams;
  const providers = props.providers ?? [
    "groq",
    "openai",
    "anthropic",
    "fireworks",
    "mistral",
    "vertex",
  ];

  const tabs = {
    openai: {
      value: "openai",
      label: "OpenAI",
      default: true,
      text: `import { ChatOpenAI } from "@langchain/openai";\n\nconst ${llmVarName} = new ChatOpenAI(${openaiParams});`,
      envs: `OPENAI_API_KEY=your-api-key`,
      dependencies: "@langchain/openai",
    },
    anthropic: {
      value: "anthropic",
      label: "Anthropic",
      default: false,
      text: `import { ChatAnthropic } from "@langchain/anthropic";\n\nconst ${llmVarName} = new ChatAnthropic(${anthropicParams});`,
      envs: `ANTHROPIC_API_KEY=your-api-key`,
      dependencies: "@langchain/anthropic",
    },
    fireworks: {
      value: "fireworks",
      label: "FireworksAI",
      default: false,
      text: `import { ChatFireworks } from "@langchain/community/chat_models/fireworks";\n\nconst ${llmVarName} = new ChatFireworks(${fireworksParams});`,
      envs: `FIREWORKS_API_KEY=your-api-key`,
      dependencies: "@langchain/community",
    },
    mistral: {
      value: "mistral",
      label: "MistralAI",
      default: false,
      text: `import { ChatMistralAI } from "@langchain/mistralai";\n\nconst ${llmVarName} = new ChatMistralAI(${mistralParams});`,
      envs: `MISTRAL_API_KEY=your-api-key`,
      dependencies: "@langchain/mistralai",
    },
    groq: {
      value: "groq",
      label: "Groq",
      default: false,
      text: `import { ChatGroq } from "@langchain/groq";\n\nconst ${llmVarName} = new ChatGroq(${groqParams});`,
      envs: `GROQ_API_KEY=your-api-key`,
      dependencies: "@langchain/groq",
    },
    vertex: {
      value: "vertex",
      label: "VertexAI",
      default: false,
      text: `import { ChatVertexAI } from "@langchain/google-vertexai";\n\nconst ${llmVarName} = new ChatVertexAI(${vertexParams});`,
      envs: `GOOGLE_APPLICATION_CREDENTIALS=credentials.json`,
      dependencies: "@langchain/google-vertexai",
    },
  };

  const displayedTabs = (props.onlyWso ? MODELS_WSO : providers).map(
    (provider) => tabs[provider]
  );

  return (
    <div>
      <h3>Pick your chat model:</h3>
      <Tabs groupId="modelTabs">
        {displayedTabs.map((tab) => (
          <TabItem value={tab.value} label={tab.label} key={tab.value}>
            <h4>Install dependencies</h4>
            <InstallationInfo>
              {[tab.dependencies, additionalDependencies].join(" ")}
            </InstallationInfo>
            <h4>Add environment variables</h4>
            <CodeBlock language="bash">{tab.envs}</CodeBlock>
            <h4>Instantiate the model</h4>
            <CodeBlock language="typescript">{tab.text}</CodeBlock>
          </TabItem>
        ))}
      </Tabs>
    </div>
  );
}



================================================
FILE: docs/core_docs/src/theme/EmbeddingTabs.js
================================================
/* eslint-disable react/jsx-props-no-spreading, react/destructuring-assignment */
import React from "react";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import CodeBlock from "@theme-original/CodeBlock";
import Npm2Yarn from "@theme/Npm2Yarn";

const DEFAULTS = {
  openaiParams: `{\n  model: "text-embedding-3-large"\n}`,
  azureParams: `{\n  azureOpenAIApiEmbeddingsDeploymentName: "text-embedding-ada-002"\n}`,
  awsParams: `{\n  model: "amazon.titan-embed-text-v1"\n}`,
  vertexParams: `{\n  model: "text-embedding-004"\n}`,
  mistralParams: `{\n  model: "mistral-embed"\n}`,
  cohereParams: `{\n  model: "embed-english-v3.0"\n}`,
};

/**
 * @typedef {Object} EmbeddingTabsProps - Component props.
 * @property {string} [openaiParams]
 * @property {string} [azureParams]
 * @property {string} [awsParams]
 * @property {string} [vertexParams]
 * @property {string} [mistralParams]
 * @property {string} [cohereParams]
 *
 * @property {boolean} [hideOpenai]
 * @property {boolean} [hideAzure]
 * @property {boolean} [hideAws]
 * @property {boolean} [hideVertex]
 * @property {boolean} [hideMistral]
 * @property {boolean} [hideCohere]
 *
 * @property {string} [customVarName] - Custom variable name for the model. Defaults to `"embeddings"`.
 */

/**
 * @param {EmbeddingTabsProps} props - Component props.
 */
export default function EmbeddingTabs(props) {
  const { customVarName } = props;

  const embeddingsVarName = customVarName ?? "embeddings";

  const openaiParams = props.openaiParams ?? DEFAULTS.openaiParams;
  const azureParams = props.azureParams ?? DEFAULTS.azureParams;
  const awsParams = props.awsParams ?? DEFAULTS.awsParams;
  const vertexParams = props.vertexParams ?? DEFAULTS.vertexParams;
  const mistralParams = props.mistralParams ?? DEFAULTS.mistralParams;
  const cohereParams = props.cohereParams ?? DEFAULTS.cohereParams;
  const providers = props.providers ?? [
    "openai",
    "azure",
    "aws",
    "vertex",
    "mistral",
    "cohere",
  ];

  const tabs = {
    openai: {
      value: "openai",
      label: "OpenAI",
      default: true,
      text: `import { OpenAIEmbeddings } from "@langchain/openai";\n\nconst ${embeddingsVarName} = new OpenAIEmbeddings(${openaiParams});`,
      envs: `OPENAI_API_KEY=your-api-key`,
      dependencies: "@langchain/openai",
    },
    azure: {
      value: "azure",
      label: "Azure",
      default: false,
      text: `import { AzureOpenAIEmbeddings } from "@langchain/openai";\n\nconst ${embeddingsVarName} = new AzureOpenAIEmbeddings(${azureParams});`,
      envs: `AZURE_OPENAI_API_INSTANCE_NAME=<YOUR_INSTANCE_NAME>\nAZURE_OPENAI_API_KEY=<YOUR_KEY>\nAZURE_OPENAI_API_VERSION="2024-02-01"`,
      dependencies: "@langchain/openai",
    },
    aws: {
      value: "aws",
      label: "AWS",
      default: false,
      text: `import { BedrockEmbeddings } from "@langchain/aws";\n\nconst ${embeddingsVarName} = new BedrockEmbeddings(${awsParams});`,
      envs: `BEDROCK_AWS_REGION=your-region`,
      dependencies: "@langchain/aws",
    },
    vertex: {
      value: "vertex",
      label: "VertexAI",
      default: false,
      text: `import { VertexAIEmbeddings } from "@langchain/google-vertexai";\n\nconst ${embeddingsVarName} = new VertexAIEmbeddings(${vertexParams});`,
      envs: `GOOGLE_APPLICATION_CREDENTIALS=credentials.json`,
      dependencies: "@langchain/google-vertexai",
    },
    mistral: {
      value: "mistral",
      label: "MistralAI",
      default: false,
      text: `import { MistralAIEmbeddings } from "@langchain/mistralai";\n\nconst ${embeddingsVarName} = new MistralAIEmbeddings(${mistralParams});`,
      envs: `MISTRAL_API_KEY=your-api-key`,
      dependencies: "@langchain/mistralai",
    },
    cohere: {
      value: "cohereParams",
      label: "Cohere",
      default: false,
      text: `import { CohereEmbeddings } from "@langchain/cohere";\n\nconst ${embeddingsVarName} = new CohereEmbeddings(${cohereParams});`,
      envs: `COHERE_API_KEY=your-api-key`,
      dependencies: "@langchain/cohere",
    },
  };

  const displayedTabs = providers.map((provider) => tabs[provider]);

  return (
    <div>
      <h3>Pick your embedding model:</h3>
      <Tabs groupId="modelTabs">
        {displayedTabs.map((tab) => (
          <TabItem value={tab.value} label={tab.label} key={tab.value}>
            <h4>Install dependencies</h4>
            <Npm2Yarn>{tab.dependencies}</Npm2Yarn>
            <CodeBlock language="bash">{tab.envs}</CodeBlock>
            <CodeBlock language="typescript">{tab.text}</CodeBlock>
          </TabItem>
        ))}
      </Tabs>
    </div>
  );
}



================================================
FILE: docs/core_docs/src/theme/FeatureTables.js
================================================
/* eslint-disable import/no-extraneous-dependencies */
/* eslint-disable prefer-template */
import React from "react";
import { useCurrentSidebarCategory } from "@docusaurus/theme-common";
import { useDocById } from "@docusaurus/theme-common/internal";

const FEATURE_TABLES = {
  chat: {
    link: "/docs/integrations/chat",
    columns: [
      {
        title: "Provider",
        formatter: (item) => <a href={item.link}>{item.name}</a>,
      },
      {
        title: <a href="/docs/how_to/tool_calling">Tool calling</a>,
        formatter: (item) => (item.tool_calling ? "✅" : "❌"),
      },
      {
        title: <a href="/docs/how_to/structured_output/">Structured output</a>,
        formatter: (item) => (item.structured_output ? "✅" : "❌"),
      },
      {
        title: "JSON mode",
        formatter: (item) => (item.json_mode ? "✅" : "❌"),
      },
      { title: "Local", formatter: (item) => (item.local ? "✅" : "❌") },
      {
        title: <a href="/docs/how_to/multimodal_inputs/">Multimodal</a>,
        formatter: (item) => (item.multimodal ? "✅" : "❌"),
      },
      {
        title: "Package",
        formatter: (item) => <a href={item.apiLink}>{item.package}</a>,
      },
    ],
    items: [
      {
        name: "ChatAnthropic",
        package: "langchain-anthropic",
        link: "anthropic/",
        structured_output: true,
        tool_calling: true,
        json_mode: false,
        multimodal: true,
        local: false,
        apiLink:
          "https://api.python.langchain.com/en/latest/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html#langchain_anthropic.chat_models.ChatAnthropic",
      },
      {
        name: "ChatMistralAI",
        package: "langchain-mistralai",
        link: "mistralai/",
        structured_output: true,
        tool_calling: true,
        json_mode: false,
        multimodal: false,
        local: false,
        apiLink:
          "https://api.python.langchain.com/en/latest/chat_models/langchain_mistralai.chat_models.ChatMistralAI.html#langchain_mistralai.chat_models.ChatMistralAI",
      },
      {
        name: "ChatFireworks",
        package: "langchain-fireworks",
        link: "fireworks/",
        structured_output: true,
        tool_calling: true,
        json_mode: true,
        multimodal: false,
        local: false,
        apiLink:
          "https://api.python.langchain.com/en/latest/chat_models/langchain_fireworks.chat_models.ChatFireworks.html#langchain_fireworks.chat_models.ChatFireworks",
      },
      {
        name: "AzureChatOpenAI",
        package: "langchain-openai",
        link: "azure_chat_openai/",
        structured_output: true,
        tool_calling: true,
        json_mode: true,
        multimodal: true,
        local: false,
        apiLink:
          "https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.azure.AzureChatOpenAI.html#langchain_openai.chat_models.azure.AzureChatOpenAI",
      },
      {
        name: "ChatOpenAI",
        package: "langchain-openai",
        link: "openai/",
        structured_output: true,
        tool_calling: true,
        json_mode: true,
        multimodal: true,
        local: false,
        apiLink:
          "https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI",
      },
      {
        name: "ChatTogether",
        package: "langchain-together",
        link: "together/",
        structured_output: true,
        tool_calling: true,
        json_mode: true,
        multimodal: false,
        local: false,
        apiLink:
          "https://api.python.langchain.com/en/latest/chat_models/langchain_together.chat_models.ChatTogether.html#langchain_together.chat_models.ChatTogether",
      },
      {
        name: "ChatVertexAI",
        package: "langchain-google-vertexai",
        link: "google_vertex_ai_palm/",
        structured_output: true,
        tool_calling: true,
        json_mode: false,
        multimodal: true,
        local: false,
        apiLink:
          "https://api.python.langchain.com/en/latest/chat_models/langchain_google_vertexai.chat_models.ChatVertexAI.html#langchain_google_vertexai.chat_models.ChatVertexAI",
      },
      {
        name: "ChatGoogleGenerativeAI",
        package: "langchain-google-genai",
        link: "google_generative_ai/",
        structured_output: true,
        tool_calling: true,
        json_mode: false,
        multimodal: true,
        local: false,
        apiLink:
          "https://api.python.langchain.com/en/latest/chat_models/langchain_google_genai.chat_models.ChatGoogleGenerativeAI.html#langchain_google_genai.chat_models.ChatGoogleGenerativeAI",
      },
      {
        name: "ChatGroq",
        package: "langchain-groq",
        link: "groq/",
        structured_output: true,
        tool_calling: true,
        json_mode: true,
        multimodal: false,
        local: false,
        apiLink:
          "https://api.python.langchain.com/en/latest/chat_models/langchain_groq.chat_models.ChatGroq.html#langchain_groq.chat_models.ChatGroq",
      },
      {
        name: "ChatCohere",
        package: "langchain-cohere",
        link: "cohere/",
        structured_output: true,
        tool_calling: true,
        json_mode: false,
        multimodal: false,
        local: false,
        apiLink:
          "https://api.python.langchain.com/en/latest/chat_models/langchain_cohere.chat_models.ChatCohere.html#langchain_cohere.chat_models.ChatCohere",
      },
      {
        name: "ChatBedrock",
        package: "langchain-aws",
        link: "bedrock/",
        structured_output: true,
        tool_calling: true,
        json_mode: false,
        multimodal: false,
        local: false,
        apiLink:
          "https://api.python.langchain.com/en/latest/chat_models/langchain_aws.chat_models.bedrock.ChatBedrock.html#langchain_aws.chat_models.bedrock.ChatBedrock",
      },
      {
        name: "ChatHuggingFace",
        package: "langchain-huggingface",
        link: "huggingface/",
        structured_output: true,
        tool_calling: true,
        json_mode: false,
        multimodal: false,
        local: true,
        apiLink:
          "https://api.python.langchain.com/en/latest/chat_models/langchain_huggingface.chat_models.huggingface.ChatHuggingFace.html#langchain_huggingface.chat_models.huggingface.ChatHuggingFace",
      },
      {
        name: "ChatNVIDIA",
        package: "langchain-nvidia-ai-endpoints",
        link: "nvidia_ai_endpoints/",
        structured_output: true,
        tool_calling: true,
        json_mode: false,
        multimodal: false,
        local: true,
        apiLink:
          "https://api.python.langchain.com/en/latest/chat_models/langchain_nvidia_ai_endpoints.chat_models.ChatNVIDIA.html#langchain_nvidia_ai_endpoints.chat_models.ChatNVIDIA",
      },
      {
        name: "ChatOllama",
        package: "langchain-ollama",
        link: "ollama/",
        structured_output: true,
        tool_calling: true,
        json_mode: true,
        multimodal: false,
        local: true,
        apiLink:
          "https://api.python.langchain.com/en/latest/chat_models/langchain_ollama.chat_models.ChatOllama.html#langchain_ollama.chat_models.ChatOllama",
      },
      {
        name: "ChatLlamaCpp",
        package: "langchain-community",
        link: "llamacpp",
        structured_output: true,
        tool_calling: true,
        json_mode: false,
        multimodal: false,
        local: true,
        apiLink:
          "https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.llamacpp.ChatLlamaCpp.html#langchain_community.chat_models.llamacpp.ChatLlamaCpp",
      },
      {
        name: "ChatAI21",
        package: "langchain-ai21",
        link: "ai21",
        structured_output: true,
        tool_calling: true,
        json_mode: false,
        multimodal: false,
        local: false,
        apiLink:
          "https://api.python.langchain.com/en/latest/chat_models/langchain_ai21.chat_models.ChatAI21.html#langchain_ai21.chat_models.ChatAI21",
      },
      {
        name: "ChatUpstage",
        package: "langchain-upstage",
        link: "upstage",
        structured_output: true,
        tool_calling: true,
        json_mode: false,
        multimodal: false,
        local: false,
        apiLink:
          "https://api.python.langchain.com/en/latest/chat_models/langchain_upstage.chat_models.ChatUpstage.html#langchain_upstage.chat_models.ChatUpstage",
      },
      {
        name: "Arcjet Redact",
        package: "langchain-community",
        link: "arcjet",
        structured_output: false,
        tool_calling: false,
        json_mode: false,
        multimodal: false,
        local: true,
      },
    ],
  },
  llms: {
    link: "/docs/integrations/llms",
    columns: [
      {
        title: "Provider",
        formatter: (item) => <a href={item.link}>{item.name}</a>,
      },
      {
        title: "Package",
        formatter: (item) => <a href={item.apiLink}>{item.package}</a>,
      },
    ],
    items: [
      {
        name: "AI21LLM",
        link: "ai21",
        package: "langchain-ai21",
        apiLink:
          "https://api.python.langchain.com/en/latest/llms/langchain_ai21.llms.AI21LLM.html#langchain_ai21.llms.AI21LLM",
      },
      {
        name: "Arcjet Redact",
        link: "arcjet",
        package: "langchain-community",
      },
      {
        name: "AnthropicLLM",
        link: "anthropic",
        package: "langchain-anthropic",
        apiLink:
          "https://api.python.langchain.com/en/latest/llms/langchain_anthropic.llms.AnthropicLLM.html#langchain_anthropic.llms.AnthropicLLM",
      },
      {
        name: "AzureOpenAI",
        link: "azure_openai",
        package: "langchain-openai",
        apiLink:
          "https://api.python.langchain.com/en/latest/llms/langchain_openai.llms.azure.AzureOpenAI.html#langchain_openai.llms.azure.AzureOpenAI",
      },
      {
        name: "BedrockLLM",
        link: "bedrock",
        package: "langchain-aws",
        apiLink:
          "https://api.python.langchain.com/en/latest/llms/langchain_aws.llms.bedrock.BedrockLLM.html#langchain_aws.llms.bedrock.BedrockLLM",
      },
      {
        name: "CohereLLM",
        link: "cohere",
        package: "langchain-cohere",
        apiLink:
          "https://api.python.langchain.com/en/latest/llms/langchain_cohere.llms.Cohere.html#langchain_cohere.llms.Cohere",
      },
      {
        name: "FireworksLLM",
        link: "fireworks",
        package: "langchain-fireworks",
        apiLink:
          "https://api.python.langchain.com/en/latest/llms/langchain_fireworks.llms.Fireworks.html#langchain_fireworks.llms.Fireworks",
      },
      {
        name: "OllamaLLM",
        link: "ollama",
        package: "langchain-ollama",
        apiLink:
          "https://api.python.langchain.com/en/latest/llms/langchain_ollama.llms.OllamaLLM.html#langchain_ollama.llms.OllamaLLM",
      },
      {
        name: "OpenAILLM",
        link: "openai",
        package: "langchain-openai",
        apiLink:
          "https://api.python.langchain.com/en/latest/llms/langchain_openai.llms.base.OpenAI.html#langchain_openai.llms.base.OpenAI",
      },
      {
        name: "TogetherLLM",
        link: "together",
        package: "langchain-together",
        apiLink:
          "https://api.python.langchain.com/en/latest/llms/langchain_together.llms.Together.html#langchain_together.llms.Together",
      },
      {
        name: "VertexAILLM",
        link: "google_vertexai",
        package: "langchain-google_vertexai",
        apiLink:
          "https://api.python.langchain.com/en/latest/llms/langchain_google_vertexai.llms.VertexAI.html#langchain_google_vertexai.llms.VertexAI",
      },
    ],
  },
  text_embedding: {
    link: "/docs/integrations/text_embedding",
    columns: [
      {
        title: "Provider",
        formatter: (item) => <a href={item.link}>{item.name}</a>,
      },
      {
        title: "Package",
        formatter: (item) => <a href={item.apiLink}>{item.package}</a>,
      },
    ],
    items: [
      {
        name: "AzureOpenAI",
        link: "azureopenai",
        package: "langchain-openai",
        apiLink:
          "https://api.python.langchain.com/en/latest/embeddings/langchain_openai.embeddings.azure.AzureOpenAIEmbeddings.html#langchain_openai.embeddings.azure.AzureOpenAIEmbeddings",
      },
      {
        name: "Ollama",
        link: "ollama",
        package: "langchain-ollama",
        apiLink:
          "https://api.python.langchain.com/en/latest/embeddings/langchain_ollama.embeddings.OllamaEmbeddings.html#langchain_ollama.embeddings.OllamaEmbeddings",
      },
      {
        name: "AI21",
        link: "ai21",
        package: "langchain-ai21",
        apiLink:
          "https://api.python.langchain.com/en/latest/embeddings/langchain_ai21.embeddings.AI21Embeddings.html#langchain_ai21.embeddings.AI21Embeddings",
      },
      {
        name: "Fake",
        link: "fake",
        package: "langchain-core",
        apiLink:
          "https://api.python.langchain.com/en/latest/embeddings/langchain_core.embeddings.fake.FakeEmbeddings.html#langchain_core.embeddings.fake.FakeEmbeddings",
      },
      {
        name: "OpenAI",
        link: "openai",
        package: "langchain-openai",
        apiLink:
          "https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI",
      },
      {
        name: "Together",
        link: "together",
        package: "langchain-together",
        apiLink:
          "https://api.python.langchain.com/en/latest/embeddings/langchain_together.embeddings.TogetherEmbeddings.html#langchain_together.embeddings.TogetherEmbeddings",
      },
      {
        name: "Fireworks",
        link: "fireworks",
        package: "langchain-fireworks",
        apiLink:
          "https://api.python.langchain.com/en/latest/embeddings/langchain_fireworks.embeddings.FireworksEmbeddings.html#langchain_fireworks.embeddings.FireworksEmbeddings",
      },
      {
        name: "MistralAI",
        link: "mistralai",
        package: "langchain-mistralai",
        apiLink:
          "https://api.python.langchain.com/en/latest/embeddings/langchain_mistralai.embeddings.MistralAIEmbeddings.html#langchain_mistralai.embeddings.MistralAIEmbeddings",
      },
      {
        name: "Cohere",
        link: "cohere",
        package: "langchain-cohere",
        apiLink:
          "https://api.python.langchain.com/en/latest/embeddings/langchain_cohere.embeddings.CohereEmbeddings.html#langchain_cohere.embeddings.CohereEmbeddings",
      },
    ],
  },
  document_retrievers: {
    link: "docs/integrations/retrievers",
    columns: [
      {
        title: "Retriever",
        formatter: (item) => <a href={item.link}>{item.name}</a>,
      },
      {
        title: "Self-host",
        formatter: (item) => (item.selfHost ? "✅" : "❌"),
      },
      {
        title: "Cloud offering",
        formatter: (item) => (item.cloudOffering ? "✅" : "❌"),
      },
      {
        title: "Package",
        formatter: (item) => <a href={item.apiLink}>{item.package}</a>,
      },
    ],
    items: [
      {
        name: "AmazonKnowledgeBasesRetriever",
        link: "bedrock",
        selfHost: false,
        cloudOffering: true,
        apiLink:
          "https://api.python.langchain.com/en/latest/retrievers/langchain_aws.retrievers.bedrock.AmazonKnowledgeBasesRetriever.html",
        package: "langchain_aws",
      },
      {
        name: "AzureAISearchRetriever",
        link: "azure_ai_search",
        selfHost: false,
        cloudOffering: true,
        apiLink:
          "https://api.python.langchain.com/en/latest/retrievers/langchain_community.retrievers.azure_ai_search.AzureAISearchRetriever.html",
        package: "langchain_community",
      },
      {
        name: "ElasticsearchRetriever",
        link: "elasticsearch_retriever",
        selfHost: true,
        cloudOffering: true,
        apiLink:
          "https://api.python.langchain.com/en/latest/retrievers/langchain_elasticsearch.retrievers.ElasticsearchRetriever.html",
        package: "langchain_elasticsearch",
      },
      {
        name: "MilvusCollectionHybridSearchRetriever",
        link: "milvus_hybrid_search",
        selfHost: true,
        cloudOffering: false,
        apiLink:
          "https://api.python.langchain.com/en/latest/retrievers/langchain_milvus.retrievers.milvus_hybrid_search.MilvusCollectionHybridSearchRetriever.html",
        package: "langchain_milvus",
      },
      {
        name: "VertexAISearchRetriever",
        link: "google_vertex_ai_search",
        selfHost: false,
        cloudOffering: true,
        apiLink:
          "https://api.python.langchain.com/en/latest/vertex_ai_search/langchain_google_community.vertex_ai_search.VertexAISearchRetriever.html",
        package: "langchain_google_community",
      },
    ],
  },
  external_retrievers: {
    link: "docs/integrations/retrievers",
    columns: [
      {
        title: "Retriever",
        formatter: (item) => <a href={item.link}>{item.name}</a>,
      },
      { title: "Source", formatter: (item) => item.source },
      {
        title: "Package",
        formatter: (item) => <a href={item.apiLink}>{item.package}</a>,
      },
    ],
    items: [
      {
        name: "ArxivRetriever",
        link: "arxiv",
        source: (
          <>
            Scholarly articles on <a href="https://arxiv.org/">arxiv.org</a>
          </>
        ),
        apiLink:
          "https://api.python.langchain.com/en/latest/retrievers/langchain_community.retrievers.arxiv.ArxivRetriever.html",
        package: "langchain_community",
      },
      {
        name: "TavilySearchAPIRetriever",
        link: "tavily",
        source: "Internet search",
        apiLink:
          "https://api.python.langchain.com/en/latest/retrievers/langchain_community.retrievers.tavily_search_api.TavilySearchAPIRetriever.html",
        package: "langchain_community",
      },
      {
        name: "WikipediaRetriever",
        link: "wikipedia",
        source: (
          <>
            <a href="https://www.wikipedia.org/">Wikipedia</a> articles
          </>
        ),
        apiLink:
          "https://api.python.langchain.com/en/latest/retrievers/langchain_community.retrievers.wikipedia.WikipediaRetriever.html",
        package: "langchain_community",
      },
    ],
  },
  document_loaders: {
    link: "docs/integrations/loaders",
    columns: [],
    items: [],
  },
  vectorstores: {
    link: "docs/integrations/vectorstores",
    columns: [
      {
        title: "Vectorstore",
        formatter: (item) => <a href={item.link}>{item.name}</a>,
      },
      {
        title: "Delete by ID",
        formatter: (item) => (item.deleteById ? "✅" : "❌"),
      },
      {
        title: "Filtering",
        formatter: (item) => (item.filtering ? "✅" : "❌"),
      },
      {
        title: "Search by Vector",
        formatter: (item) => (item.searchByVector ? "✅" : "❌"),
      },
      {
        title: "Search with score",
        formatter: (item) => (item.searchWithScore ? "✅" : "❌"),
      },
      { title: "Async", formatter: (item) => (item.async ? "✅" : "❌") },
      {
        title: "Passes Standard Tests",
        formatter: (item) => (item.passesStandardTests ? "✅" : "❌"),
      },
      {
        title: "Multi Tenancy",
        formatter: (item) => (item.multiTenancy ? "✅" : "❌"),
      },
      {
        title: "IDs in add Documents",
        formatter: (item) => (item.idsInAddDocuments ? "✅" : "❌"),
      },
      {
        title: "Local/Cloud",
        formatter: (item) => (item.local ? "Local" : "Cloud"),
      },
    ],
    items: [
      {
        name: "AstraDBVectorStore",
        link: "astradb",
        deleteById: true,
        filtering: true,
        searchByVector: true,
        searchWithScore: true,
        async: true,
        passesStandardTests: false,
        multiTenancy: false,
        local: true,
        idsInAddDocuments: false,
      },
      {
        name: "Chroma",
        link: "chroma",
        deleteById: true,
        filtering: true,
        searchByVector: true,
        searchWithScore: true,
        async: true,
        passesStandardTests: false,
        multiTenancy: false,
        local: true,
        idsInAddDocuments: false,
      },
      {
        name: "Clickhouse",
        link: "clickhouse",
        deleteById: true,
        filtering: true,
        searchByVector: false,
        searchWithScore: true,
        async: false,
        passesStandardTests: false,
        multiTenancy: false,
        local: true,
        idsInAddDocuments: false,
      },
      {
        name: "CouchbaseVectorStore",
        link: "couchbase",
        deleteById: true,
        filtering: true,
        searchByVector: false,
        searchWithScore: true,
        async: true,
        passesStandardTests: false,
        multiTenancy: false,
        local: true,
        idsInAddDocuments: false,
      },
      {
        name: "ElasticsearchStore",
        link: "elasticsearch",
        deleteById: true,
        filtering: true,
        searchByVector: true,
        searchWithScore: false,
        async: true,
        passesStandardTests: false,
        multiTenancy: false,
        local: true,
        idsInAddDocuments: false,
      },
      {
        name: "FAISS",
        link: "faiss",
        deleteById: true,
        filtering: true,
        searchByVector: true,
        searchWithScore: true,
        async: true,
        passesStandardTests: false,
        multiTenancy: false,
        local: true,
        idsInAddDocuments: false,
      },
      {
        name: "InMemoryVectorStore",
        link: "in_memory",
        deleteById: true,
        filtering: true,
        searchByVector: false,
        searchWithScore: true,
        async: true,
        passesStandardTests: false,
        multiTenancy: false,
        local: true,
        idsInAddDocuments: false,
      },
      {
        name: "mariadb",
        link: "mariadb",
        deleteById: true,
        filtering: true,
        searchByVector: true,
        searchWithScore: true,
        async: true,
        passesStandardTests: false,
        multiTenancy: false,
        local: true,
        idsInAddDocuments: false,
      },
      {
        name: "Milvus",
        link: "milvus",
        deleteById: true,
        filtering: true,
        searchByVector: false,
        searchWithScore: true,
        async: true,
        passesStandardTests: false,
        multiTenancy: false,
        local: true,
        idsInAddDocuments: false,
      },
      {
        name: "MongoDBAtlasVectorSearch",
        link: "mongodb_atlas",
        deleteById: true,
        filtering: true,
        searchByVector: false,
        searchWithScore: false,
        async: true,
        passesStandardTests: false,
        multiTenancy: false,
        local: true,
        idsInAddDocuments: false,
      },
      {
        name: "PGVector",
        link: "pg_vector",
        deleteById: true,
        filtering: true,
        searchByVector: true,
        searchWithScore: true,
        async: true,
        passesStandardTests: false,
        multiTenancy: false,
        local: true,
        idsInAddDocuments: false,
      },
      {
        name: "PineconeVectorStore",
        link: "pinecone",
        deleteById: true,
        filtering: true,
        searchByVector: true,
        searchWithScore: false,
        async: true,
        passesStandardTests: false,
        multiTenancy: false,
        local: true,
        idsInAddDocuments: false,
      },
      {
        name: "QdrantVectorStore",
        link: "qdrant",
        deleteById: true,
        filtering: true,
        searchByVector: true,
        searchWithScore: true,
        async: true,
        passesStandardTests: false,
        multiTenancy: false,
        local: true,
        idsInAddDocuments: false,
      },
      {
        name: "Redis",
        link: "redis",
        deleteById: true,
        filtering: true,
        searchByVector: true,
        searchWithScore: true,
        async: true,
        passesStandardTests: false,
        multiTenancy: false,
        local: true,
        idsInAddDocuments: false,
      },
    ],
  },
};

const DEPRECATED_DOC_IDS = [
  "integrations/chat/anthropic_tools",
  "integrations/chat/baidu_wenxin",
  "integrations/chat/google_palm",
  "integrations/chat/ni_bittensor",
  "integrations/chat/ollama_functions",
  "integrations/chat/prompt_layer_openai",
  "integrations/llms/google_palm",
  "integrations/llms/ni_bittensor",
  "integrations/llms/prompt_layer_openai",
  "integrations/text_embedding/google_palm",
  "integrations/retrievers/chatgpt-retriever-plugin",
  "integrations/tools/aiplugin-tool",
  "integrations/tools/zapier_agent",
];

function toTable(columns, items) {
  const headers = columns.map((col) => col.title);
  return (
    <table>
      <thead>
        <tr>
          {headers.map((header, i) => (
            // eslint-disable-next-line react/no-array-index-key
            <th key={`header-${i}`}>{header}</th>
          ))}
        </tr>
      </thead>
      <tbody>
        {items.map((item, i) => (
          // eslint-disable-next-line react/no-array-index-key
          <tr key={`row-${i}`}>
            {columns.map((col, j) => (
              // eslint-disable-next-line react/no-array-index-key
              <td key={`cell-${i}-${j}`}>{col.formatter(item)}</td>
            ))}
          </tr>
        ))}
      </tbody>
    </table>
  );
}

export function CategoryTable({ category }) {
  const cat = FEATURE_TABLES[category];
  const rtn = toTable(cat.columns, cat.items);
  return rtn;
}

export function ItemTable({ category, item }) {
  const cat = FEATURE_TABLES[category];
  const row = cat.items.find((i) => i.name === item);
  if (!row) {
    throw new Error(`Item ${item} not found in category ${category}`);
  }
  const rtn = toTable(cat.columns, [row]);
  return rtn;
}

function truncate(str, n) {
  return str.length > n ? str.substring(0, n - 1) + "..." : str;
}

export function IndexTable() {
  const { items } = useCurrentSidebarCategory();

  const rows = items
    .filter(
      (item) =>
        !item.docId?.endsWith?.("/index") &&
        !DEPRECATED_DOC_IDS.includes(item.docId)
    )
    .map((item) => ({
      ...item,
      // eslint-disable-next-line react-hooks/rules-of-hooks
      description: useDocById(item.docId ?? undefined)?.description,
    }));
  const rtn = toTable(
    [
      {
        title: "Name",
        formatter: (item) => <a href={item.href}>{item.label}</a>,
      },
      {
        title: "Description",
        formatter: (item) => truncate(item.description ?? "", 70),
      },
    ],
    rows
  );
  return rtn;
}



================================================
FILE: docs/core_docs/src/theme/Feedback.js
================================================
/* eslint-disable no-return-assign, react/jsx-props-no-spreading, no-console */
import React, { useState, useEffect } from "react";
import { createClient } from "@supabase/supabase-js";
import useDocusaurusContext from "@docusaurus/useDocusaurusContext";
import { v4 as uuidv4 } from "uuid";

const useCookie = () => {
  /**
   * Function to set a cookie
   * @param {string} name The name of the cookie to set
   * @param {string} value The value of the cookie
   * @param {number} days the number of days until the cookie expires
   */
  const setCookie = (name, value, days) => {
    const d = new Date();
    d.setTime(d.getTime() + days * 24 * 60 * 60 * 1000);
    const expires = `expires=${d.toUTCString()}`;
    document.cookie = `${name}=${value};${expires};path=/`;
  };

  /**
   * Function to get a cookie by name
   * @param {string} name The name of the cookie to get
   * @returns {string} The value of the cookie
   */
  const getCookie = (name) => {
    const ca = document.cookie.split(";");
    const caLen = ca.length;
    const cookieName = `${name}=`;
    let c;

    for (let i = 0; i < caLen; i += 1) {
      c = ca[i].replace(/^\s+/g, "");
      if (c.indexOf(cookieName) === 0) {
        return c.substring(cookieName.length, c.length);
      }
    }
    return "";
  };

  /**
   * Function to check cookie existence
   * @param {string} name The name of the cookie to check for
   * @returns {boolean} Whether or not the cookie exists
   */
  const checkCookie = (name) => {
    const cookie = getCookie(name);
    if (cookie !== "") {
      return true;
    }
    return false;
  };

  return { setCookie, checkCookie };
};

function SvgThumbsUp() {
  return (
    <svg
      xmlns="http://www.w3.org/2000/svg"
      fill="none"
      viewBox="0 0 24 24"
      strokeWidth="1.5"
      stroke="#166534"
      style={{ width: "24px", height: "24px" }}
    >
      <path
        strokeLinecap="round"
        strokeLinejoin="round"
        d="M6.633 10.25c.806 0 1.533-.446 2.031-1.08a9.041 9.041 0 0 1 2.861-2.4c.723-.384 1.35-.956 1.653-1.715a4.498 4.498 0 0 0 .322-1.672V2.75a.75.75 0 0 1 .75-.75 2.25 2.25 0 0 1 2.25 2.25c0 1.152-.26 2.243-.723 3.218-.266.558.107 1.282.725 1.282m0 0h3.126c1.026 0 1.945.694 2.054 1.715.045.422.068.85.068 1.285a11.95 11.95 0 0 1-2.649 7.521c-.388.482-.987.729-1.605.729H13.48c-.483 0-.964-.078-1.423-.23l-3.114-1.04a4.501 4.501 0 0 0-1.423-.23H5.904m10.598-9.75H14.25M5.904 18.5c.083.205.173.405.27.602.197.4-.078.898-.523.898h-.908c-.889 0-1.713-.518-1.972-1.368a12 12 0 0 1-.521-3.507c0-1.553.295-3.036.831-4.398C3.387 9.953 4.167 9.5 5 9.5h1.053c.472 0 .745.556.5.96a8.958 8.958 0 0 0-1.302 4.665c0 1.194.232 2.333.654 3.375Z"
      />
    </svg>
  );
}

function SvgThumbsDown() {
  return (
    <svg
      xmlns="http://www.w3.org/2000/svg"
      fill="none"
      viewBox="0 0 24 24"
      strokeWidth="1.5"
      stroke="#991b1b"
      style={{ width: "24px", height: "24px" }}
    >
      <path
        strokeLinecap="round"
        strokeLinejoin="round"
        d="M7.498 15.25H4.372c-1.026 0-1.945-.694-2.054-1.715a12.137 12.137 0 0 1-.068-1.285c0-2.848.992-5.464 2.649-7.521C5.287 4.247 5.886 4 6.504 4h4.016a4.5 4.5 0 0 1 1.423.23l3.114 1.04a4.5 4.5 0 0 0 1.423.23h1.294M7.498 15.25c.618 0 .991.724.725 1.282A7.471 7.471 0 0 0 7.5 19.75 2.25 2.25 0 0 0 9.75 22a.75.75 0 0 0 .75-.75v-.633c0-.573.11-1.14.322-1.672.304-.76.93-1.33 1.653-1.715a9.04 9.04 0 0 0 2.86-2.4c.498-.634 1.226-1.08 2.032-1.08h.384m-10.253 1.5H9.7m8.075-9.75c.01.05.027.1.05.148.593 1.2.925 2.55.925 3.977 0 1.487-.36 2.89-.999 4.125m.023-8.25c-.076-.365.183-.75.575-.75h.908c.889 0 1.713.518 1.972 1.368.339 1.11.521 2.287.521 3.507 0 1.553-.295 3.036-.831 4.398-.306.774-1.086 1.227-1.918 1.227h-1.053c-.472 0-.745-.556-.5-.96a8.95 8.95 0 0 0 .303-.54"
      />
    </svg>
  );
}

/**
 * Generated type for the Supabase DB schema.
 * @typedef {import('../supabase').Database} Database
 */

const FEEDBACK_COOKIE_PREFIX = "feedbackSent";
/** @type {Database["public"]["Enums"]["project_type"]} */
const LANGCHAIN_PROJECT_NAME = "langchain_js_docs";

/**
 * @returns {Promise<string>}
 */
const getIpAddress = async () => {
  const response = await fetch("https://api.ipify.org?format=json");
  return (await response.json()).ip;
};

export default function Feedback() {
  const { setCookie, checkCookie } = useCookie();
  const [feedbackId, setFeedbackId] = useState(null);
  const [feedbackSent, setFeedbackSent] = useState(false);
  const [feedbackDetailsSent, setFeedbackDetailsSent] = useState(false);
  const { siteConfig } = useDocusaurusContext();
  const [pathname, setPathname] = useState("");

  /** @param {"good" | "bad"} feedback */
  const handleFeedback = async (feedback) => {
    if (process.env.NODE_ENV !== "production") {
      console.log("Feedback (dev)");
      return;
    }

    const cookieName = `${FEEDBACK_COOKIE_PREFIX}_${window.location.pathname}`;
    if (checkCookie(cookieName)) {
      return;
    }

    /** @type {Database} */
    const supabase = createClient(
      siteConfig.customFields.supabaseUrl,
      siteConfig.customFields.supabasePublicKey
    );
    try {
      const ipAddress = await getIpAddress();
      const rowId = uuidv4();
      setFeedbackId(rowId);
      /**
       * "id" and "created_at" are automatically generated by Supabase
       * @type {Omit<Database["public"]["Tables"]["feedback"]["Row"], "id" | "created_at">}
       */
      const params = {
        id: rowId,
        is_good: feedback === "good",
        url: window.location.pathname,
        user_ip: ipAddress,
        project: LANGCHAIN_PROJECT_NAME,
      };

      const { error } = await supabase.from("feedback").insert(params);
      if (error) {
        throw error;
      }
    } catch (e) {
      console.error("Failed to send feedback", e);
      return;
    }

    // Set a cookie to prevent feedback from being sent multiple times
    setCookie(cookieName, window.location.pathname, 1);
    setFeedbackSent(true);
  };

  const handleFeedbackDetails = async (e) => {
    e.preventDefault();
    if (!feedbackId) {
      setFeedbackDetailsSent(true);
      return;
    }
    const details = e.target.elements
      .namedItem("details")
      ?.value.slice(0, 1024);
    if (!details) {
      return;
    }
    const supabase = createClient(
      siteConfig.customFields.supabaseUrl,
      siteConfig.customFields.supabasePublicKey
    );
    const { error } = await supabase.from("feedback_details").insert({
      feedback_id: feedbackId,
      details,
    });
    if (error) {
      console.error("Failed to add feedback details", error);
      return;
    }
    setFeedbackDetailsSent(true);
  };

  useEffect(() => {
    if (typeof window !== "undefined") {
      // If the cookie exists, set feedback sent to
      // true so the user can not send feedback again
      // (cookies exp in 24hrs)
      const cookieName = `${FEEDBACK_COOKIE_PREFIX}_${window.location.pathname}`;
      setFeedbackSent(checkCookie(cookieName));
      setPathname(window.location.pathname);
    }
  }, []);

  const defaultFields = {
    style: {
      display: "flex",
      alignItems: "center",
      paddingTop: "10px",
      paddingBottom: "10px",
      paddingLeft: "22px",
      paddingRight: "22px",
      border: "1px solid gray",
      borderRadius: "6px",
      gap: "10px",
      cursor: "pointer",
      fontSize: "16px",
      fontWeight: "600",
    },
    onMouseEnter: (e) => (e.currentTarget.style.backgroundColor = "#f0f0f0"),
    onMouseLeave: (e) =>
      (e.currentTarget.style.backgroundColor = "transparent"),
    onMouseDown: (e) => (e.currentTarget.style.backgroundColor = "#d0d0d0"),
    onMouseUp: (e) => (e.currentTarget.style.backgroundColor = "#f0f0f0"),
  };

  const newGithubIssueURL = pathname
    ? `https://github.com/langchain-ai/langchainjs/issues/new?assignees=&labels=03+-+Documentation&projects=&template=documentation.yml&title=DOC%3A+%3CIssue+related+to+${pathname}%3E`
    : "https://github.com/langchain-ai/langchainjs/issues/new?assignees=&labels=03+-+Documentation&projects=&template=documentation.yml&title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E";
  return (
    <div style={{ display: "flex", flexDirection: "column" }}>
      <hr />
      {feedbackSent ? (
        <>
          <h4>Thanks for your feedback!</h4>
          {!feedbackDetailsSent && feedbackId && (
            <form
              style={{ display: "flex", flexDirection: "column" }}
              onSubmit={handleFeedbackDetails}
            >
              <h4>Do you have any specific comments?</h4>
              <textarea
                name="details"
                style={{ width: "480px", height: "120px" }}
              />
              <button
                style={{
                  width: "72px",
                  marginLeft: "408px",
                  marginTop: "12px",
                }}
                type="submit"
              >
                Submit
              </button>
            </form>
          )}
        </>
      ) : (
        <>
          <h4>Was this page helpful?</h4>
          <div style={{ display: "flex", gap: "5px" }}>
            <div
              {...defaultFields}
              role="button" // Make it recognized as an interactive element
              tabIndex={0} // Make it focusable
              onKeyDown={async (e) => {
                // Handle keyboard interaction
                if (e.key === "Enter" || e.key === " ") {
                  e.preventDefault();
                  await handleFeedback("good");
                }
              }}
              onClick={async (e) => {
                e.preventDefault();
                await handleFeedback("good");
              }}
            >
              <SvgThumbsUp />
            </div>
            <div
              {...defaultFields}
              role="button" // Make it recognized as an interactive element
              tabIndex={0} // Make it focusable
              onKeyDown={async (e) => {
                // Handle keyboard interaction
                if (e.key === "Enter" || e.key === " ") {
                  e.preventDefault();
                  await handleFeedback("bad");
                }
              }}
              onClick={async (e) => {
                e.preventDefault();
                await handleFeedback("bad");
              }}
            >
              <SvgThumbsDown />
            </div>
          </div>
        </>
      )}
      <br />
      <h4>
        You can also leave detailed feedback{" "}
        <a target="_blank" href={newGithubIssueURL} rel="noreferrer">
          on GitHub
        </a>
        .
      </h4>
    </div>
  );
}



================================================
FILE: docs/core_docs/src/theme/NotFound.js
================================================
/* eslint-disable import/no-extraneous-dependencies */
/* eslint-disable no-nested-ternary */
import React from "react";
import { translate } from "@docusaurus/Translate";
import { PageMetadata } from "@docusaurus/theme-common";
import Layout from "@theme/Layout";

import { useLocation } from "react-router-dom";

function LegacyBadge() {
  return <span className="badge badge--secondary">LEGACY</span>;
}

const suggestedLinks = {
  "/docs/additional_resources/tutorials/expression_language_cheatsheet/": {
    canonical: "/docs/how_to/lcel_cheatsheet/",
    alternative: [
      "/v0.1/docs/additional_resources/tutorials/expression_language_cheatsheet/",
    ],
  },
  "/docs/ecosystem/": {
    canonical: "/docs/integrations/platforms/",
    alternative: ["/v0.1/docs/ecosystem/"],
  },
  "/docs/ecosystem/integrations/": {
    canonical: "/docs/integrations/platforms/",
    alternative: ["/v0.1/docs/ecosystem/integrations/"],
  },
  "/docs/ecosystem/integrations/databerry/": {
    canonical: "/docs/integrations/platforms/",
    alternative: ["/v0.1/docs/ecosystem/integrations/databerry/"],
  },
  "/docs/ecosystem/integrations/helicone/": {
    canonical: "/docs/integrations/platforms/",
    alternative: ["/v0.1/docs/ecosystem/integrations/helicone/"],
  },
  "/docs/ecosystem/integrations/lunary/": {
    canonical: "/docs/integrations/platforms/",
    alternative: ["/v0.1/docs/ecosystem/integrations/lunary/"],
  },
  "/docs/ecosystem/integrations/makersuite/": {
    canonical: "/docs/integrations/platforms/",
    alternative: ["/v0.1/docs/ecosystem/integrations/makersuite/"],
  },
  "/docs/ecosystem/integrations/unstructured/": {
    canonical: "/docs/integrations/document_loaders/file_loaders/unstructured/",
    alternative: ["/v0.1/docs/ecosystem/integrations/unstructured/"],
  },
  "/docs/ecosystem/langserve/": {
    canonical:
      "https://api.js.langchain.com/classes/_langchain_core.runnables_remote.RemoteRunnable.html",
    alternative: ["/v0.1/docs/ecosystem/langserve/"],
  },
  "/docs/expression_language/": {
    canonical: "/docs/how_to/#langchain-expression-language-lcel",
    alternative: ["/v0.1/docs/expression_language/"],
  },
  "/docs/expression_language/cookbook/": {
    canonical: "/docs/how_to/#langchain-expression-language-lcel",
    alternative: ["/v0.1/docs/expression_language/cookbook/"],
  },
  "/docs/expression_language/cookbook/adding_memory/": {
    canonical: "/docs/how_to/message_history",
    alternative: ["/v0.1/docs/expression_language/cookbook/adding_memory/"],
  },
  "/docs/expression_language/cookbook/agents/": {
    canonical: "/docs/how_to/agent_executor",
    alternative: ["/v0.1/docs/expression_language/cookbook/agents/"],
  },
  "/docs/expression_language/cookbook/multiple_chains/": {
    canonical: "/docs/how_to/parallel",
    alternative: ["/v0.1/docs/expression_language/cookbook/multiple_chains/"],
  },
  "/docs/expression_language/cookbook/prompt_llm_parser/": {
    canonical: "/docs/tutorials/llm_chain",
    alternative: ["/v0.1/docs/expression_language/cookbook/prompt_llm_parser/"],
  },
  "/docs/expression_language/cookbook/retrieval/": {
    canonical: "/docs/tutorials/rag",
    alternative: ["/v0.1/docs/expression_language/cookbook/retrieval/"],
  },
  "/docs/expression_language/cookbook/sql_db/": {
    canonical: "/docs/tutorials/sql_qa",
    alternative: ["/v0.1/docs/expression_language/cookbook/sql_db/"],
  },
  "/docs/expression_language/cookbook/tools/": {
    canonical: "/docs/how_to/tool_calling/",
    alternative: ["/v0.1/docs/expression_language/cookbook/tools/"],
  },
  "/docs/expression_language/get_started/": {
    canonical: "/docs/how_to/sequence",
    alternative: ["/v0.1/docs/expression_language/get_started/"],
  },
  "/docs/expression_language/how_to/map/": {
    canonical: "/docs/how_to/cancel_execution/",
    alternative: ["/v0.1/docs/expression_language/how_to/map/"],
  },
  "/docs/expression_language/how_to/message_history/": {
    canonical: "/docs/how_to/message_history",
    alternative: ["/v0.1/docs/expression_language/how_to/message_history/"],
  },
  "/docs/expression_language/how_to/routing/": {
    canonical: "/docs/how_to/routing",
    alternative: ["/v0.1/docs/expression_language/how_to/routing/"],
  },
  "/docs/expression_language/how_to/with_history/": {
    canonical: "/docs/how_to/message_history",
    alternative: ["/v0.1/docs/expression_language/how_to/with_history/"],
  },
  "/docs/expression_language/interface/": {
    canonical: "/docs/how_to/lcel_cheatsheet",
    alternative: ["/v0.1/docs/expression_language/interface/"],
  },
  "/docs/expression_language/streaming/": {
    canonical: "/docs/how_to/streaming",
    alternative: ["/v0.1/docs/expression_language/streaming/"],
  },
  "/docs/expression_language/why/": {
    canonical: "/docs/concepts/#langchain-expression-language",
    alternative: ["/v0.1/docs/expression_language/why/"],
  },
  "/docs/get_started/": {
    canonical: "/docs/introduction/",
    alternative: ["/v0.1/docs/get_started/"],
  },
  "/docs/get_started/installation/": {
    canonical: "/docs/tutorials/",
    alternative: ["/v0.1/docs/get_started/installation/"],
  },
  "/docs/get_started/introduction/": {
    canonical: "/docs/tutorials/",
    alternative: ["/v0.1/docs/get_started/introduction/"],
  },
  "/docs/get_started/quickstart/": {
    canonical: "/docs/tutorials/",
    alternative: ["/v0.1/docs/get_started/quickstart/"],
  },
  "/docs/guides/": {
    canonical: "/docs/how_to/",
    alternative: ["/v0.1/docs/guides/"],
  },
  "/docs/guides/debugging/": {
    canonical: "/docs/how_to/debugging",
    alternative: ["/v0.1/docs/guides/debugging/"],
  },
  "/docs/guides/deployment/": {
    canonical: "https://langchain-ai.github.io/langgraph/cloud/",
    alternative: ["/v0.1/docs/guides/deployment/"],
  },
  "/docs/guides/deployment/nextjs/": {
    canonical: "https://github.com/langchain-ai/langchain-nextjs-template",
    alternative: ["/v0.1/docs/guides/deployment/nextjs/"],
  },
  "/docs/guides/deployment/sveltekit/": {
    canonical: "https://github.com/langchain-ai/langchain-nextjs-template",
    alternative: ["/v0.1/docs/guides/deployment/sveltekit/"],
  },
  "/docs/guides/evaluation/": {
    canonical:
      "https://docs.smith.langchain.com/tutorials/Developers/evaluation",
    alternative: ["/v0.1/docs/guides/evaluation/"],
  },
  "/docs/guides/evaluation/comparison/": {
    canonical:
      "https://docs.smith.langchain.com/tutorials/Developers/evaluation",
    alternative: ["/v0.1/docs/guides/evaluation/comparison/"],
  },
  "/docs/guides/evaluation/comparison/pairwise_embedding_distance/": {
    canonical:
      "https://docs.smith.langchain.com/tutorials/Developers/evaluation",
    alternative: [
      "/v0.1/docs/guides/evaluation/comparison/pairwise_embedding_distance/",
    ],
  },
  "/docs/guides/evaluation/comparison/pairwise_string/": {
    canonical:
      "https://docs.smith.langchain.com/tutorials/Developers/evaluation",
    alternative: ["/v0.1/docs/guides/evaluation/comparison/pairwise_string/"],
  },
  "/docs/guides/evaluation/examples/": {
    canonical:
      "https://docs.smith.langchain.com/tutorials/Developers/evaluation",
    alternative: ["/v0.1/docs/guides/evaluation/examples/"],
  },
  "/docs/guides/evaluation/examples/comparisons/": {
    canonical:
      "https://docs.smith.langchain.com/tutorials/Developers/evaluation",
    alternative: ["/v0.1/docs/guides/evaluation/examples/comparisons/"],
  },
  "/docs/guides/evaluation/string/": {
    canonical:
      "https://docs.smith.langchain.com/tutorials/Developers/evaluation",
    alternative: ["/v0.1/docs/guides/evaluation/string/"],
  },
  "/docs/guides/evaluation/string/criteria/": {
    canonical:
      "https://docs.smith.langchain.com/tutorials/Developers/evaluation",
    alternative: ["/v0.1/docs/guides/evaluation/string/criteria/"],
  },
  "/docs/guides/evaluation/string/embedding_distance/": {
    canonical:
      "https://docs.smith.langchain.com/tutorials/Developers/evaluation",
    alternative: ["/v0.1/docs/guides/evaluation/string/embedding_distance/"],
  },
  "/docs/guides/evaluation/trajectory/": {
    canonical:
      "https://docs.smith.langchain.com/tutorials/Developers/evaluation",
    alternative: ["/v0.1/docs/guides/evaluation/trajectory/"],
  },
  "/docs/guides/evaluation/trajectory/trajectory_eval/": {
    canonical:
      "https://docs.smith.langchain.com/tutorials/Developers/evaluation",
    alternative: ["/v0.1/docs/guides/evaluation/trajectory/trajectory_eval/"],
  },
  "/docs/guides/extending_langchain/": {
    canonical: "/docs/how_to/#custom",
    alternative: ["/v0.1/docs/guides/extending_langchain/"],
  },
  "/docs/guides/fallbacks/": {
    canonical: "/docs/how_to/fallbacks",
    alternative: ["/v0.1/docs/guides/fallbacks/"],
  },
  "/docs/guides/langsmith_evaluation/": {
    canonical:
      "https://docs.smith.langchain.com/tutorials/Developers/evaluation",
    alternative: ["/v0.1/docs/guides/langsmith_evaluation/"],
  },
  "/docs/guides/migrating/": {
    canonical: "https://js.langchain.com/v0.1/docs/guides/migrating/",
    alternative: ["/v0.1/docs/guides/migrating/"],
  },
  "/docs/integrations/chat_memory/": {
    canonical: "/docs/integrations/memory",
    alternative: ["/v0.1/docs/integrations/chat_memory/"],
  },
  "/docs/integrations/chat_memory/astradb/": {
    canonical: "/docs/integrations/memory/astradb",
    alternative: ["/v0.1/docs/integrations/chat_memory/astradb/"],
  },
  "/docs/integrations/chat_memory/cassandra/": {
    canonical: "/docs/integrations/memory/cassandra",
    alternative: ["/v0.1/docs/integrations/chat_memory/cassandra/"],
  },
  "/docs/integrations/chat_memory/cloudflare_d1/": {
    canonical: "/docs/integrations/memory/cloudflare_d1",
    alternative: ["/v0.1/docs/integrations/chat_memory/cloudflare_d1/"],
  },
  "/docs/integrations/chat_memory/convex/": {
    canonical: "/docs/integrations/memory/convex",
    alternative: ["/v0.1/docs/integrations/chat_memory/convex/"],
  },
  "/docs/integrations/chat_memory/dynamodb/": {
    canonical: "/docs/integrations/memory/dynamodb",
    alternative: ["/v0.1/docs/integrations/chat_memory/dynamodb/"],
  },
  "/docs/integrations/chat_memory/firestore/": {
    canonical: "/docs/integrations/memory/firestore",
    alternative: ["/v0.1/docs/integrations/chat_memory/firestore/"],
  },
  "/docs/integrations/chat_memory/ipfs_datastore/": {
    canonical: "/docs/integrations/memory/ipfs_datastore",
    alternative: ["/v0.1/docs/integrations/chat_memory/ipfs_datastore/"],
  },
  "/docs/integrations/chat_memory/momento/": {
    canonical: "/docs/integrations/memory/momento",
    alternative: ["/v0.1/docs/integrations/chat_memory/momento/"],
  },
  "/docs/integrations/chat_memory/mongodb/": {
    canonical: "/docs/integrations/memory/mongodb",
    alternative: ["/v0.1/docs/integrations/chat_memory/mongodb/"],
  },
  "/docs/integrations/chat_memory/motorhead_memory/": {
    canonical: "/docs/integrations/memory/motorhead_memory",
    alternative: ["/v0.1/docs/integrations/chat_memory/motorhead_memory/"],
  },
  "/docs/integrations/chat_memory/planetscale/": {
    canonical: "/docs/integrations/memory/planetscale",
    alternative: ["/v0.1/docs/integrations/chat_memory/planetscale/"],
  },
  "/docs/integrations/chat_memory/postgres/": {
    canonical: "/docs/integrations/memory/postgres",
    alternative: ["/v0.1/docs/integrations/chat_memory/postgres/"],
  },
  "/docs/integrations/chat_memory/redis/": {
    canonical: "/docs/integrations/memory/redis",
    alternative: ["/v0.1/docs/integrations/chat_memory/redis/"],
  },
  "/docs/integrations/chat_memory/upstash_redis/": {
    canonical: "/docs/integrations/memory/upstash_redis",
    alternative: ["/v0.1/docs/integrations/chat_memory/upstash_redis/"],
  },
  "/docs/integrations/chat_memory/xata/": {
    canonical: "/docs/integrations/memory/xata",
    alternative: ["/v0.1/docs/integrations/chat_memory/xata/"],
  },
  "/docs/integrations/chat_memory/zep_memory/": {
    canonical: "/docs/integrations/memory/zep_memory",
    alternative: ["/v0.1/docs/integrations/chat_memory/zep_memory/"],
  },
  "/docs/integrations/document_compressors/": {
    canonical: "/docs/integrations/document_transformers",
    alternative: ["/v0.1/docs/integrations/document_compressors/"],
  },
  "/docs/integrations/llms/togetherai/": {
    canonical: "/docs/integrations/llms/together",
    alternative: ["/v0.1/docs/integrations/llms/togetherai/"],
  },
  "/docs/integrations/llms/fake/": {
    canonical:
      "https://api.js.langchain.com/classes/_langchain_core.utils_testing.FakeLLM.html",
    alternative: ["/v0.1/docs/integrations/llms/fake/"],
  },
  "/docs/integrations/retrievers/vectorstore/": {
    canonical: "/docs/how_to/vectorstore_retriever",
    alternative: ["/v0.1/docs/integrations/retrievers/vectorstore/"],
  },
  "/docs/integrations/vectorstores/azure_cosmosdb/": {
    canonical: "/docs/integrations/vectorstores/azure_cosmosdb_mongodb",
    alternative: ["/v0.1/docs/integrations/vectorstores/azure_cosmosdb/"],
  },
  "/docs/langgraph/": {
    canonical: "https://langchain-ai.github.io/langgraphjs/",
    alternative: ["/v0.1/docs/langgraph/"],
  },
  "/docs/modules/": {
    canonical: "/docs/concepts/",
    alternative: ["/v0.1/docs/modules/"],
  },
  "/docs/modules/agents": {
    canonical: "/docs/concepts/agents/",
    alternative: ["/v0.1/docs/modules/agents/"],
  },
  "/docs/modules/agents/concepts/": {
    canonical: "/docs/concepts/agents/",
    alternative: ["/v0.1/docs/modules/agents/concepts/"],
  },
  "/docs/modules/agents/agent_types/": {
    canonical: "/docs/how_to/migrate_agent/",
    alternative: ["/v0.1/docs/modules/agents/agent_types/"],
  },
  "/docs/modules/agents/how_to/agent_structured/": {
    canonical: "/docs/how_to/migrate_agent/",
    alternative: ["/v0.1/docs/modules/agents/how_to/agent_structured/"],
  },
  "/docs/modules/agents/how_to/max_iterations/": {
    canonical: "/docs/how_to/migrate_agent/",
    alternative: ["/v0.1/docs/modules/agents/how_to/max_iterations/"],
  },
  "/docs/modules/agents/how_to/streaming/": {
    canonical: "/docs/how_to/migrate_agent/",
    alternative: ["/v0.1/docs/modules/agents/how_to/streaming/"],
  },
  "/docs/modules/agents/quick_start/": {
    canonical: "https://langchain-ai.github.io/langgraphjs//",
    alternative: ["/v0.1//docs/modules/agents/quick_start/"],
  },
  "/docs/modules/agents/agent_types/chat_conversation_agent/": {
    canonical: "/docs/how_to/migrate_agent",
    alternative: [
      "/v0.1/docs/modules/agents/agent_types/chat_conversation_agent/",
    ],
  },
  "/docs/modules/agents/agent_types/openai_assistant/": {
    canonical: "/docs/how_to/migrate_agent",
    alternative: ["/v0.1/docs/modules/agents/agent_types/openai_assistant/"],
  },
  "/docs/modules/agents/agent_types/openai_functions_agent/": {
    canonical: "/docs/how_to/migrate_agent",
    alternative: [
      "/v0.1/docs/modules/agents/agent_types/openai_functions_agent/",
    ],
  },
  "/docs/modules/agents/agent_types/openai_tools_agent/": {
    canonical: "/docs/how_to/migrate_agent",
    alternative: ["/v0.1/docs/modules/agents/agent_types/openai_tools_agent/"],
  },
  "/docs/modules/agents/agent_types/plan_and_execute/": {
    canonical: "/docs/how_to/migrate_agent",
    alternative: ["/v0.1/docs/modules/agents/agent_types/plan_and_execute/"],
  },
  "/docs/modules/agents/agent_types/react/": {
    canonical: "/docs/how_to/migrate_agent",
    alternative: ["/v0.1/docs/modules/agents/agent_types/react/"],
  },
  "/docs/modules/agents/agent_types/structured_chat/": {
    canonical: "/docs/how_to/migrate_agent",
    alternative: ["/v0.1/docs/modules/agents/agent_types/structured_chat/"],
  },
  "/docs/modules/agents/agent_types/tool_calling/": {
    canonical: "/docs/how_to/migrate_agent",
    alternative: ["/v0.1/docs/modules/agents/agent_types/tool_calling/"],
  },
  "/docs/modules/agents/agent_types/xml_legacy/": {
    canonical: "/docs/how_to/migrate_agent",
    alternative: ["/v0.1/docs/modules/agents/agent_types/xml_legacy/"],
  },
  "/docs/modules/agents/agent_types/xml/": {
    canonical: "/docs/how_to/migrate_agent",
    alternative: ["/v0.1/docs/modules/agents/agent_types/xml/"],
  },
  "/docs/modules/agents/how_to/callbacks/": {
    canonical: "/docs/how_to/#callbacks",
    alternative: ["/v0.1/docs/modules/agents/how_to/callbacks/"],
  },
  "/docs/modules/agents/how_to/cancelling_requests/": {
    canonical: "/docs/how_to/cancel_execution",
    alternative: ["/v0.1/docs/modules/agents/how_to/cancelling_requests/"],
  },
  "/docs/modules/agents/how_to/custom_agent/": {
    canonical:
      "https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/",
    alternative: ["/v0.1/docs/modules/agents/how_to/custom_agent/"],
  },
  "/docs/modules/agents/how_to/custom_llm_agent/": {
    canonical:
      "https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/",
    alternative: ["/v0.1/docs/modules/agents/how_to/custom_llm_agent/"],
  },
  "/docs/modules/agents/how_to/custom_llm_chat_agent/": {
    canonical:
      "https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/",
    alternative: ["/v0.1/docs/modules/agents/how_to/custom_llm_chat_agent/"],
  },
  "/docs/modules/agents/how_to/custom_mrkl_agent/": {
    canonical:
      "https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/",
    alternative: ["/v0.1/docs/modules/agents/how_to/custom_mrkl_agent/"],
  },
  "/docs/modules/agents/how_to/handle_parsing_errors/": {
    canonical:
      "https://langchain-ai.github.io/langgraphjs/how-tos/tool-calling-errors/",
    alternative: ["/v0.1/docs/modules/agents/how_to/handle_parsing_errors/"],
  },
  "/docs/modules/agents/how_to/intermediate_steps/": {
    canonical:
      "https://langchain-ai.github.io/langgraphjs/how-tos/stream-values/",
    alternative: ["/v0.1/docs/modules/agents/how_to/intermediate_steps/"],
  },
  "/docs/modules/agents/how_to/logging_and_tracing/": {
    canonical:
      "https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph",
    alternative: ["/v0.1/docs/modules/agents/how_to/logging_and_tracing/"],
  },
  "/docs/modules/agents/how_to/timeouts/": {
    canonical: "/docs/how_to/cancel_execution/",
    alternative: ["/v0.1/docs/modules/agents/how_to/timeouts/"],
  },
  "/docs/modules/agents/tools/": {
    canonical:
      "https://langchain-ai.github.io/langgraphjs/how-tos/tool-calling/",
    alternative: ["/v0.1/docs/modules/agents/tools/"],
  },
  "/docs/modules/agents/tools/dynamic/": {
    canonical: "/docs/how_to/custom_tools/",
    alternative: ["/v0.1/docs/modules/agents/tools/dynamic/"],
  },
  "/docs/modules/agents/tools/how_to/agents_with_vectorstores/": {
    canonical: "/docs/how_to/custom_tools",
    alternative: [
      "/v0.1/docs/modules/agents/tools/how_to/agents_with_vectorstores/",
    ],
  },
  "/docs/modules/agents/tools/toolkits/": {
    canonical: "/docs/how_to/tools_builtin",
    alternative: ["/v0.1/docs/modules/agents/tools/toolkits/"],
  },
  "/docs/modules/callbacks/how_to/background_callbacks/": {
    canonical: "/docs/how_to/callbacks_backgrounding",
    alternative: ["/v0.1/docs/modules/callbacks/how_to/background_callbacks/"],
  },
  "/docs/modules/callbacks/how_to/create_handlers/": {
    canonical: "/docs/how_to/custom_callbacks",
    alternative: ["/v0.1/docs/modules/callbacks/how_to/create_handlers/"],
  },
  "/docs/modules/callbacks/how_to/creating_subclasses/": {
    canonical: "/docs/how_to/custom_callbacks",
    alternative: ["/v0.1/docs/modules/callbacks/how_to/creating_subclasses/"],
  },
  "/docs/modules/callbacks/how_to/tags/": {
    canonical: "/docs/how_to/#callbacks",
    alternative: ["/v0.1/docs/modules/callbacks/how_to/tags/"],
  },
  "/docs/modules/callbacks/how_to/with_listeners/": {
    canonical: "/docs/how_to/#callbacks",
    alternative: ["/v0.1/docs/modules/callbacks/how_to/with_listeners/"],
  },
  "/docs/modules/chains/": {
    canonical: "/docs/how_to/sequence",
    alternative: ["/v0.1/docs/modules/chains/"],
  },
  "/docs/modules/chains/additional/analyze_document/": {
    canonical:
      "https://js.langchain.com/v0.1/docs/modules/chains/additional/analyze_document/",
    alternative: ["/v0.1/docs/modules/chains/additional/analyze_document/"],
  },
  "/docs/modules/chains/additional/constitutional_chain/": {
    canonical:
      "https://js.langchain.com/v0.1/docs/modules/chains/additional/constitutional_chain/",
    alternative: ["/v0.1/docs/modules/chains/additional/constitutional_chain/"],
  },
  "/docs/modules/chains/additional/cypher_chain/": {
    canonical: "/docs/tutorials/graph",
    alternative: ["/v0.1/docs/modules/chains/additional/cypher_chain/"],
  },
  "/docs/modules/chains/additional/moderation/": {
    canonical:
      "https://js.langchain.com/v0.1/docs/modules/chains/additional/moderation/",
    alternative: ["/v0.1/docs/modules/chains/additional/moderation/"],
  },
  "/docs/modules/chains/additional/multi_prompt_router/": {
    canonical:
      "https://js.langchain.com/v0.1/docs/modules/chains/additional/multi_prompt_router/",
    alternative: ["/v0.1/docs/modules/chains/additional/multi_prompt_router/"],
  },
  "/docs/modules/chains/additional/multi_retrieval_qa_router/": {
    canonical: "/docs/how_to/multiple_queries",
    alternative: [
      "/v0.1/docs/modules/chains/additional/multi_retrieval_qa_router/",
    ],
  },
  "/docs/modules/chains/additional/openai_functions/": {
    canonical: "/docs/how_to/tool_calling",
    alternative: ["/v0.1/docs/modules/chains/additional/openai_functions/"],
  },
  "/docs/modules/chains/additional/openai_functions/extraction/": {
    canonical: "/docs/tutorials/extraction",
    alternative: [
      "/v0.1/docs/modules/chains/additional/openai_functions/extraction/",
    ],
  },
  "/docs/modules/chains/additional/openai_functions/openapi/": {
    canonical:
      "https://js.langchain.com/v0.1/docs/modules/chains/additional/openai_functions/openapi/",
    alternative: [
      "/v0.1/docs/modules/chains/additional/openai_functions/openapi/",
    ],
  },
  "/docs/modules/chains/additional/openai_functions/tagging/": {
    canonical: "/docs/tutorials/extraction",
    alternative: [
      "/v0.1/docs/modules/chains/additional/openai_functions/tagging/",
    ],
  },
  "/docs/modules/chains/document/": {
    canonical:
      "https://api.js.langchain.com/functions/langchain.chains_combine_documents.createStuffDocumentsChain.html",
    alternative: ["/v0.1/docs/modules/chains/document/"],
  },
  "/docs/modules/chains/document/map_reduce/": {
    canonical:
      "https://js.langchain.com/v0.1/docs/modules/chains/document/map_reduce/",
    alternative: ["/v0.1/docs/modules/chains/document/map_reduce/"],
  },
  "/docs/modules/chains/document/refine/": {
    canonical:
      "https://js.langchain.com/v0.1/docs/modules/chains/document/refine/",
    alternative: ["/v0.1/docs/modules/chains/document/refine/"],
  },
  "/docs/modules/chains/document/stuff/": {
    canonical:
      "https://js.langchain.com/v0.1/docs/modules/chains/document/stuff/",
    alternative: ["/v0.1/docs/modules/chains/document/stuff/"],
  },
  "/docs/modules/chains/foundational/llm_chain/": {
    canonical: "/docs/tutorials/llm_chain",
    alternative: ["/v0.1/docs/modules/chains/foundational/llm_chain/"],
  },
  "/docs/modules/chains/how_to/debugging/": {
    canonical: "/docs/how_to/debugging",
    alternative: ["/v0.1/docs/modules/chains/how_to/debugging/"],
  },
  "/docs/modules/chains/how_to/memory/": {
    canonical: "/docs/how_to/qa_chat_history_how_to",
    alternative: ["/v0.1/docs/modules/chains/how_to/memory/"],
  },
  "/docs/modules/chains/popular/api/": {
    canonical: "https://js.langchain.com/v0.1/docs/modules/chains/popular/api/",
    alternative: ["/v0.1/docs/modules/chains/popular/api/"],
  },
  "/docs/modules/chains/popular/chat_vector_db_legacy/": {
    canonical: "/docs/tutorials/rag",
    alternative: ["/v0.1/docs/modules/chains/popular/chat_vector_db_legacy/"],
  },
  "/docs/modules/chains/popular/chat_vector_db/": {
    canonical: "/docs/tutorials/rag",
    alternative: ["/v0.1/docs/modules/chains/popular/chat_vector_db/"],
  },
  "/docs/modules/chains/popular/sqlite_legacy/": {
    canonical: "/docs/tutorials/sql_qa",
    alternative: ["/v0.1/docs/modules/chains/popular/sqlite_legacy/"],
  },
  "/docs/modules/chains/popular/sqlite/": {
    canonical: "/docs/tutorials/sql_qa",
    alternative: ["/v0.1/docs/modules/chains/popular/sqlite/"],
  },
  "/docs/modules/chains/popular/structured_output/": {
    canonical: "/docs/how_to/structured_output",
    alternative: ["/v0.1/docs/modules/chains/popular/structured_output/"],
  },
  "/docs/modules/chains/popular/summarize/": {
    canonical: "/docs/tutorials/summarization",
    alternative: ["/v0.1/docs/modules/chains/popular/summarize/"],
  },
  "/docs/modules/chains/popular/vector_db_qa_legacy/": {
    canonical: "/docs/tutorials/rag",
    alternative: ["/v0.1/docs/modules/chains/popular/vector_db_qa_legacy/"],
  },
  "/docs/modules/chains/popular/vector_db_qa/": {
    canonical: "/docs/tutorials/rag",
    alternative: ["/v0.1/docs/modules/chains/popular/vector_db_qa/"],
  },
  "/docs/modules/data_connection/": {
    canonical: "/docs/concepts/rag",
    alternative: ["/v0.1/docs/modules/data_connection/"],
  },
  "/docs/modules/data_connection/document_loaders/": {
    canonical: "/docs/concepts/document_loaders",
    alternative: ["/v0.1/docs/modules/data_connection/document_loaders/"],
  },
  "/docs/modules/data_connection/document_loaders/csv/": {
    canonical: "/docs/integrations/document_loaders/file_loaders/csv/",
    alternative: ["/v0.1/docs/modules/data_connection/document_loaders/csv/"],
  },
  "/docs/modules/data_connection/document_loaders/custom/": {
    canonical: "/docs/how_to/document_loader_custom/",
    alternative: [
      "/v0.1/docs/modules/data_connection/document_loaders/custom/",
    ],
  },
  "/docs/modules/data_connection/document_loaders/file_directory/": {
    canonical: "/docs/integrations/document_loaders/file_loaders/directory/",
    alternative: [
      "/v0.1/docs/modules/data_connection/document_loaders/file_directory/",
    ],
  },
  "/docs/modules/data_connection/document_loaders/json/": {
    canonical: "/docs/integrations/document_loaders/file_loaders/json/",
    alternative: ["/v0.1/docs/modules/data_connection/document_loaders/json/"],
  },
  "/docs/modules/data_connection/document_loaders/pdf/": {
    canonical: "/docs/integrations/document_loaders/file_loaders/pdf/",
    alternative: ["/v0.1/docs/modules/data_connection/document_loaders/pdf/"],
  },
  "/docs/modules/data_connection/document_transformers/": {
    canonical: "/docs/concepts/text_splitters/",
    alternative: ["/v0.1/docs/modules/data_connection/document_transformers/"],
  },
  "/docs/modules/data_connection/document_transformers/character_text_splitter/":
    {
      canonical: "/docs/how_to/character_text_splitter/",
      alternative: [
        "/v0.1/docs/modules/data_connection/document_transformers/character_text_splitter/",
      ],
    },
  "/docs/modules/data_connection/document_transformers/code_splitter/": {
    canonical: "/docs/how_to/code_splitter/",
    alternative: [
      "/v0.1/docs/modules/data_connection/document_transformers/code_splitter/",
    ],
  },
  "/docs/modules/data_connection/document_transformers/recursive_text_splitter/":
    {
      canonical: "/docs/how_to/recursive_text_splitter/",
      alternative: [
        "/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/",
      ],
    },
  "/docs/modules/data_connection/document_loaders/creating_documents/": {
    canonical: "/docs/concepts#document",
    alternative: [
      "/v0.1/docs/modules/data_connection/document_loaders/creating_documents/",
    ],
  },
  "/docs/modules/data_connection/document_transformers/contextual_chunk_headers/":
    {
      canonical:
        "/docs/how_to/parent_document_retriever/#with-contextual-chunk-headers",
      alternative: [
        "/v0.1/docs/modules/data_connection/document_transformers/contextual_chunk_headers/",
      ],
    },
  "/docs/modules/data_connection/document_transformers/custom_text_splitter/": {
    canonical: "/docs/how_to/#text-splitters",
    alternative: [
      "/v0.1/docs/modules/data_connection/document_transformers/custom_text_splitter/",
    ],
  },
  "/docs/modules/data_connection/document_transformers/token_splitter/": {
    canonical: "/docs/how_to/split_by_token",
    alternative: [
      "/v0.1/docs/modules/data_connection/document_transformers/token_splitter/",
    ],
  },
  "/docs/modules/data_connection/experimental/graph_databases/neo4j/": {
    canonical: "/docs/tutorials/graph",
    alternative: [
      "/v0.1/docs/modules/data_connection/experimental/graph_databases/neo4j/",
    ],
  },
  "/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai/":
    {
      canonical:
        "https://js.langchain.com/v0.1/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai/",
      alternative: [
        "/v0.1/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai/",
      ],
    },
  "/docs/modules/data_connection/retrievers/custom/": {
    canonical: "/docs/how_to/custom_retriever",
    alternative: ["/v0.1/docs/modules/data_connection/retrievers/custom/"],
  },
  "/docs/modules/data_connection/retrievers/matryoshka_retriever/": {
    canonical: "/docs/how_to/reduce_retrieval_latency",
    alternative: [
      "/v0.1/docs/modules/data_connection/retrievers/matryoshka_retriever/",
    ],
  },
  "/docs/modules/data_connection/retrievers/multi-query-retriever/": {
    canonical: "/docs/how_to/multiple_queries",
    alternative: [
      "/v0.1/docs/modules/data_connection/retrievers/multi-query-retriever/",
    ],
  },
  "/docs/modules/data_connection/retrievers/multi-vector-retriever/": {
    canonical: "/docs/how_to/multi_vector",
    alternative: [
      "/v0.1/docs/modules/data_connection/retrievers/multi-vector-retriever/",
    ],
  },
  "/docs/modules/data_connection/retrievers/parent-document-retriever/": {
    canonical: "/docs/how_to/parent_document_retriever",
    alternative: [
      "/v0.1/docs/modules/data_connection/retrievers/parent-document-retriever/",
    ],
  },
  "/docs/modules/data_connection/retrievers/self_query/chroma-self-query/": {
    canonical: "/docs/integrations/retrievers/self_query/chroma",
    alternative: [
      "/v0.1/docs/modules/data_connection/retrievers/self_query/chroma-self-query/",
    ],
  },
  "/docs/modules/data_connection/retrievers/self_query/hnswlib-self-query/": {
    canonical: "/docs/integrations/retrievers/self_query/hnswlib",
    alternative: [
      "/v0.1/docs/modules/data_connection/retrievers/self_query/hnswlib-self-query/",
    ],
  },
  "/docs/modules/data_connection/retrievers/self_query/memory-self-query/": {
    canonical: "/docs/integrations/retrievers/self_query/memory",
    alternative: [
      "/v0.1/docs/modules/data_connection/retrievers/self_query/memory-self-query/",
    ],
  },
  "/docs/modules/data_connection/retrievers/self_query/pinecone-self-query/": {
    canonical: "/docs/integrations/retrievers/self_query/pinecone",
    alternative: [
      "/v0.1/docs/modules/data_connection/retrievers/self_query/pinecone-self-query/",
    ],
  },
  "/docs/modules/data_connection/retrievers/self_query/qdrant-self-query/": {
    canonical: "/docs/integrations/retrievers/self_query/qdrant",
    alternative: [
      "/v0.1/docs/modules/data_connection/retrievers/self_query/qdrant-self-query/",
    ],
  },
  "/docs/modules/data_connection/retrievers/self_query/supabase-self-query/": {
    canonical: "/docs/integrations/retrievers/self_query/supabase",
    alternative: [
      "/v0.1/docs/modules/data_connection/retrievers/self_query/supabase-self-query/",
    ],
  },
  "/docs/modules/data_connection/retrievers/self_query/vectara-self-query/": {
    canonical: "/docs/integrations/retrievers/self_query/vectara",
    alternative: [
      "/v0.1/docs/modules/data_connection/retrievers/self_query/vectara-self-query/",
    ],
  },
  "/docs/modules/data_connection/retrievers/self_query/weaviate-self-query/": {
    canonical: "/docs/integrations/retrievers/self_query/weaviate",
    alternative: [
      "/v0.1/docs/modules/data_connection/retrievers/self_query/weaviate-self-query/",
    ],
  },
  "/docs/modules/data_connection/retrievers/similarity-score-threshold-retriever/":
    {
      canonical:
        "https://api.js.langchain.com/classes/langchain.retrievers_score_threshold.ScoreThresholdRetriever.html",
      alternative: [
        "/v0.1/docs/modules/data_connection/retrievers/similarity-score-threshold-retriever/",
      ],
    },
  "/docs/modules/data_connection/text_embedding/api_errors/": {
    canonical:
      "https://js.langchain.com/v0.1/docs/modules/data_connection/text_embedding/api_errors/",
    alternative: [
      "/v0.1/docs/modules/data_connection/text_embedding/api_errors/",
    ],
  },
  "/docs/modules/data_connection/text_embedding/caching_embeddings/": {
    canonical: "/docs/how_to/caching_embeddings",
    alternative: [
      "/v0.1/docs/modules/data_connection/text_embedding/caching_embeddings/",
    ],
  },
  "/docs/modules/data_connection/text_embedding/rate_limits/": {
    canonical:
      "https://js.langchain.com/v0.1/docs/modules/data_connection/text_embedding/rate_limits/",
    alternative: [
      "/v0.1/docs/modules/data_connection/text_embedding/rate_limits/",
    ],
  },
  "/docs/modules/data_connection/text_embedding/timeouts/": {
    canonical:
      "https://js.langchain.com/v0.1/docs/modules/data_connection/text_embedding/timeouts/",
    alternative: [
      "/v0.1/docs/modules/data_connection/text_embedding/timeouts/",
    ],
  },
  "/docs/modules/data_connection/vectorstores/custom/": {
    canonical:
      "https://js.langchain.com/v0.1/docs/modules/data_connection/vectorstores/custom/",
    alternative: ["/v0.1/docs/modules/data_connection/vectorstores/custom/"],
  },
  "/docs/modules/experimental/": {
    canonical: "https://js.langchain.com/v0.1/docs/modules/experimental/",
    alternative: ["/v0.1/docs/modules/experimental/"],
  },
  "/docs/modules/experimental/mask/": {
    canonical:
      "https://api.js.langchain.com/modules/langchain.experimental_masking.html",
    alternative: ["/v0.1/docs/modules/experimental/mask/"],
  },
  "/docs/modules/experimental/prompts/custom_formats/": {
    canonical:
      "https://api.js.langchain.com/classes/langchain.experimental_prompts_handlebars.HandlebarsPromptTemplate.html",
    alternative: ["/v0.1/docs/modules/experimental/prompts/custom_formats/"],
  },
  "/docs/modules/memory/chat_messages/custom/": {
    canonical:
      "https://js.langchain.com/v0.1/docs/modules/memory/chat_messages/custom/",
    alternative: ["/v0.1/docs/modules/memory/chat_messages/custom/"],
  },
  "/docs/modules/memory/types/buffer_memory_chat/": {
    canonical:
      "https://api.js.langchain.com/classes/langchain.memory.BufferMemory.html",
    alternative: ["/v0.1/docs/modules/memory/types/buffer_memory_chat/"],
  },
  "/docs/modules/memory/types/buffer_window/": {
    canonical:
      "https://api.js.langchain.com/classes/langchain.memory.BufferWindowMemory.html",
    alternative: ["/v0.1/docs/modules/memory/types/buffer_window/"],
  },
  "/docs/modules/memory/types/entity_summary_memory/": {
    canonical:
      "https://api.js.langchain.com/classes/langchain.memory.EntityMemory.html",
    alternative: ["/v0.1/docs/modules/memory/types/entity_summary_memory/"],
  },
  "/docs/modules/memory/types/multiple_memory/": {
    canonical:
      "https://api.js.langchain.com/classes/langchain.memory.CombinedMemory.html",
    alternative: ["/v0.1/docs/modules/memory/types/multiple_memory/"],
  },
  "/docs/modules/memory/types/summary_buffer/": {
    canonical:
      "https://api.js.langchain.com/classes/langchain.memory.ConversationSummaryBufferMemory.html",
    alternative: ["/v0.1/docs/modules/memory/types/summary_buffer/"],
  },
  "/docs/modules/memory/types/summary/": {
    canonical:
      "https://api.js.langchain.com/classes/langchain.memory.ConversationSummaryMemory.html",
    alternative: ["/v0.1/docs/modules/memory/types/summary/"],
  },
  "/docs/modules/memory/types/vectorstore_retriever_memory/": {
    canonical:
      "https://api.js.langchain.com/classes/langchain.memory.VectorStoreRetrieverMemory.html",
    alternative: [
      "/v0.1/docs/modules/memory/types/vectorstore_retriever_memory/",
    ],
  },
  "/docs/modules/model_io/chat/caching/": {
    canonical: "/docs/how_to/chat_model_caching",
    alternative: ["/v0.1/docs/modules/model_io/chat/caching/"],
  },
  "/docs/modules/model_io/chat/cancelling_requests/": {
    canonical: "/docs/how_to/cancel_execution",
    alternative: ["/v0.1/docs/modules/model_io/chat/cancelling_requests/"],
  },
  "/docs/modules/model_io/chat/custom_chat/": {
    canonical: "/docs/how_to/custom_chat",
    alternative: ["/v0.1/docs/modules/model_io/chat/custom_chat/"],
  },
  "/docs/modules/model_io/chat/dealing_with_api_errors/": {
    canonical: "/docs/how_to/fallbacks",
    alternative: ["/v0.1/docs/modules/model_io/chat/dealing_with_api_errors/"],
  },
  "/docs/modules/model_io/chat/dealing_with_rate_limits/": {
    canonical: "/docs/how_to/fallbacks",
    alternative: ["/v0.1/docs/modules/model_io/chat/dealing_with_rate_limits/"],
  },
  "/docs/modules/model_io/chat/subscribing_events/": {
    canonical: "/docs/how_to/custom_callbacks",
    alternative: ["/v0.1/docs/modules/model_io/chat/subscribing_events/"],
  },
  "/docs/modules/model_io/chat/timeouts/": {
    canonical: "/docs/how_to/custom_callbacks",
    alternative: ["/v0.1/docs/modules/model_io/chat/timeouts/"],
  },
  "/docs/modules/model_io/llms/cancelling_requests/": {
    canonical: "/docs/how_to/cancel_execution",
    alternative: ["/v0.1/docs/modules/model_io/llms/cancelling_requests/"],
  },
  "/docs/modules/model_io/llms/dealing_with_api_errors/": {
    canonical: "/docs/how_to/fallbacks",
    alternative: ["/v0.1/docs/modules/model_io/llms/dealing_with_api_errors/"],
  },
  "/docs/modules/model_io/llms/dealing_with_rate_limits/": {
    canonical: "/docs/how_to/fallbacks",
    alternative: ["/v0.1/docs/modules/model_io/llms/dealing_with_rate_limits/"],
  },
  "/docs/modules/model_io/llms/subscribing_events/": {
    canonical: "/docs/how_to/custom_callbacks",
    alternative: ["/v0.1/docs/modules/model_io/llms/subscribing_events/"],
  },
  "/docs/modules/model_io/llms/timeouts/": {
    canonical: "/docs/how_to/cancel_execution",
    alternative: ["/v0.1/docs/modules/model_io/llms/timeouts/"],
  },
  "/docs/modules/model_io/output_parsers/types/bytes/": {
    canonical:
      "https://api.js.langchain.com/modules/_langchain_core.output_parsers.html",
    alternative: ["/v0.1/docs/modules/model_io/output_parsers/types/bytes/"],
  },
  "/docs/modules/model_io/output_parsers/types/combining_output_parser/": {
    canonical:
      "https://api.js.langchain.com/classes/langchain.output_parsers.CombiningOutputParser.html",
    alternative: [
      "/v0.1/docs/modules/model_io/output_parsers/types/combining_output_parser/",
    ],
  },
  "/docs/modules/model_io/output_parsers/types/csv/": {
    canonical:
      "https://api.js.langchain.com/classes/_langchain_core.output_parsers.CommaSeparatedListOutputParser.html",
    alternative: ["/v0.1/docs/modules/model_io/output_parsers/types/csv/"],
  },
  "/docs/modules/model_io/output_parsers/types/custom_list_parser/": {
    canonical:
      "https://api.js.langchain.com/classes/_langchain_core.output_parsers.CustomListOutputParser.html",
    alternative: [
      "/v0.1/docs/modules/model_io/output_parsers/types/custom_list_parser/",
    ],
  },
  "/docs/modules/model_io/output_parsers/types/http_response/": {
    canonical:
      "https://api.js.langchain.com/classes/langchain.output_parsers.HttpResponseOutputParser.html",
    alternative: [
      "/v0.1/docs/modules/model_io/output_parsers/types/http_response/",
    ],
  },
  "/docs/modules/model_io/output_parsers/types/json_functions/": {
    canonical:
      "https://api.js.langchain.com/classes/langchain.output_parsers.JsonOutputFunctionsParser.html",
    alternative: [
      "/v0.1/docs/modules/model_io/output_parsers/types/json_functions/",
    ],
  },
  "/docs/modules/model_io/output_parsers/types/structured/": {
    canonical: "/docs/how_to/structured_output/",
    alternative: [
      "/v0.1/docs/modules/model_io/output_parsers/types/structured/",
    ],
  },
  "/docs/modules/model_io/output_parsers/types/string/": {
    canonical:
      "https://api.js.langchain.com/classes/_langchain_core.output_parsers.StringOutputParser.html",
    alternative: ["/v0.1/docs/modules/model_io/output_parsers/types/string/"],
  },
  "/docs/modules/model_io/prompts/example_selector_types/": {
    canonical: "/docs/how_to/#example-selectors",
    alternative: [
      "/v0.1/docs/modules/model_io/prompts/example_selector_types/",
    ],
  },
  "/docs/modules/model_io/prompts/example_selector_types/length_based/": {
    canonical: "/docs/how_to/example_selectors_length_based",
    alternative: [
      "/v0.1/docs/modules/model_io/prompts/example_selector_types/length_based/",
    ],
  },
  "/docs/modules/model_io/prompts/example_selector_types/similarity/": {
    canonical: "/docs/how_to/example_selectors_similarity",
    alternative: [
      "/v0.1/docs/modules/model_io/prompts/example_selector_types/similarity/",
    ],
  },
  "/docs/modules/model_io/prompts/few_shot/": {
    canonical: "/docs/how_to/few_shot_examples",
    alternative: ["/v0.1/docs/modules/model_io/prompts/few_shot/"],
  },
  "/docs/modules/model_io/prompts/pipeline/": {
    canonical: "/docs/how_to/prompts_composition",
    alternative: ["/v0.1/docs/modules/model_io/prompts/pipeline/"],
  },
  "/docs/production/deployment/": {
    canonical: "https://langchain-ai.github.io/langgraph/cloud/",
    alternative: ["/v0.1/docs/production/deployment/"],
  },
  "/docs/production/tracing/": {
    canonical:
      "https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain",
    alternative: ["/v0.1/docs/production/tracing/"],
  },
  "/docs/use_cases/agent_simulations/": {
    canonical:
      "https://js.langchain.com/v0.1/docs/use_cases/agent_simulations/",
    alternative: ["/v0.1/docs/use_cases/agent_simulations/"],
  },
  "/docs/use_cases/agent_simulations/generative_agents/": {
    canonical:
      "https://js.langchain.com/v0.1/docs/use_cases/agent_simulations/generative_agents/",
    alternative: ["/v0.1/docs/use_cases/agent_simulations/generative_agents/"],
  },
  "/docs/use_cases/agent_simulations/violation_of_expectations_chain/": {
    canonical:
      "https://js.langchain.com/v0.1/docs/use_cases/agent_simulations/violation_of_expectations_chain/",
    alternative: [
      "/v0.1/docs/use_cases/agent_simulations/violation_of_expectations_chain/",
    ],
  },
  "/docs/use_cases/api/": {
    canonical: "https://js.langchain.com/v0.1/docs/use_cases/api/",
    alternative: ["/v0.1/docs/use_cases/api/"],
  },
  "/docs/use_cases/autonomous_agents/": {
    canonical:
      "https://js.langchain.com/v0.1/docs/use_cases/autonomous_agents/",
    alternative: ["/v0.1/docs/use_cases/autonomous_agents/"],
  },
  "/docs/use_cases/autonomous_agents/auto_gpt/": {
    canonical:
      "https://js.langchain.com/v0.1/docs/use_cases/autonomous_agents/auto_gpt/",
    alternative: ["/v0.1/docs/use_cases/autonomous_agents/auto_gpt/"],
  },
  "/docs/use_cases/autonomous_agents/baby_agi/": {
    canonical:
      "https://js.langchain.com/v0.1/docs/use_cases/autonomous_agents/baby_agi/",
    alternative: ["/v0.1/docs/use_cases/autonomous_agents/baby_agi/"],
  },
  "/docs/use_cases/autonomous_agents/sales_gpt/": {
    canonical:
      "https://js.langchain.com/v0.1/docs/use_cases/autonomous_agents/sales_gpt/",
    alternative: ["/v0.1/docs/use_cases/autonomous_agents/sales_gpt/"],
  },
  "/docs/use_cases/graph/construction/": {
    canonical: "/docs/tutorials/graph",
    alternative: ["/v0.1/docs/use_cases/graph/construction/"],
  },
  "/docs/use_cases/media/": {
    canonical: "/docs/how_to/multimodal_prompts",
    alternative: ["/v0.1/docs/use_cases/media/"],
  },
  "/docs/use_cases/query_analysis/how_to/constructing_filters/": {
    canonical: "/docs/tutorials/query_analysis",
    alternative: [
      "/v0.1/docs/use_cases/query_analysis/how_to/constructing_filters/",
    ],
  },
  "/docs/use_cases/tabular/": {
    canonical: "/docs/tutorials/sql_qa",
    alternative: ["/v0.1/docs/use_cases/tabular/"],
  },
};

export default function NotFound() {
  const location = useLocation();
  const pathname = location.pathname.endsWith("/")
    ? location.pathname
    : `${location.pathname}/`; // Ensure the path matches the keys in suggestedLinks
  const { canonical, alternative } = suggestedLinks[pathname] || {};

  return (
    <>
      <PageMetadata
        title={translate({
          id: "theme.NotFound.title",
          message: "Page Not Found",
        })}
      />
      <Layout>
        <main className="container margin-vert--xl">
          <div className="row">
            <div className="col col--6 col--offset-3">
              <h1 className="hero__title">
                {canonical
                  ? "Page Moved"
                  : alternative
                  ? "Page Removed"
                  : "Page Not Found"}
              </h1>
              {canonical ? (
                <h3>
                  You can find the new location <a href={canonical}>here</a>.
                </h3>
              ) : alternative ? (
                <p>The page you were looking for has been removed.</p>
              ) : (
                <p>We could not find what you were looking for.</p>
              )}
              {alternative && (
                <p>
                  <details>
                    <summary>Alternative pages</summary>
                    <ul>
                      {alternative.map((alt, index) => (
                        // eslint-disable-next-line react/no-array-index-key
                        <li key={index}>
                          <a href={alt}>{alt}</a>
                          {alt.startsWith("/v0.1/") && (
                            <>
                              {" "}
                              <LegacyBadge />
                            </>
                          )}
                        </li>
                      ))}
                    </ul>
                  </details>
                </p>
              )}
              <p>
                Please contact the owner of the site that linked you to the
                original URL and let them know their link{" "}
                {canonical
                  ? "has moved."
                  : alternative
                  ? "has been removed."
                  : "is broken."}
              </p>
            </div>
          </div>
        </main>
      </Layout>
    </>
  );
}



================================================
FILE: docs/core_docs/src/theme/Npm2Yarn.js
================================================
import React from "react";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import CodeBlock from "@theme-original/CodeBlock";

// Substitute component for Jupyter notebooks since Quarto has trouble
// parsing built-in npm2yarn markdown blocks
export default function Npm2Yarn({ children }) {
  return (
    <Tabs groupId="npm2yarn">
      <TabItem value="npm" label="npm">
        <CodeBlock language="bash">npm i {children}</CodeBlock>
      </TabItem>
      <TabItem value="yarn" label="yarn" default>
        <CodeBlock language="bash">yarn add {children}</CodeBlock>
      </TabItem>
      <TabItem value="pnpm" label="pnpm">
        <CodeBlock language="bash">pnpm add {children}</CodeBlock>
      </TabItem>
    </Tabs>
  );
}



================================================
FILE: docs/core_docs/src/theme/People.js
================================================
import React from "react";
import PeopleData from "../../data/people.yml";

function renderPerson({ login, avatarUrl, url }) {
  return (
    <div
      key={`person:${login}`}
      style={{
        display: "flex",
        flexDirection: "column",
        alignItems: "center",
        padding: "18px",
      }}
    >
      <a href={url} target="_blank" rel="noreferrer">
        <img
          src={avatarUrl}
          alt={`Avatar for ${login}`}
          style={{ borderRadius: "50%", width: "128px", height: "128px" }}
        />
      </a>
      <a
        href={url}
        target="_blank"
        rel="noreferrer"
        style={{ fontSize: "18px", fontWeight: "700" }}
      >
        @{login}
      </a>
    </div>
  );
}

export default function People({ type, count }) {
  let people = PeopleData[type] ?? [];
  if (count !== undefined) {
    people = people.slice(0, parseInt(count, 10));
  }
  const html = people.map((person) => renderPerson(person));
  return (
    <div
      style={{
        display: "flex",
        flexWrap: "wrap",
        padding: "10px",
        justifyContent: "space-around",
      }}
    >
      {html}
    </div>
  );
}



================================================
FILE: docs/core_docs/src/theme/RedirectAnchors.js
================================================
// eslint-disable-next-line no-unused-vars
import React from "react";

function RedirectAnchors() {
  if (typeof window === "undefined") return null;

  // get # anchor from url
  const lookup = {
    "#conceptual-guide": "/docs/concepts",
    "#architecture": "/docs/concepts/architecture",
    "#langchaincore": "/docs/concepts/architecture/#langchaincore",
    "#langchain": "/docs/concepts/architecture/#langchain",
    "#langchaincommunity": "/docs/concepts/architecture/#langchaincommunity",
    "#partner-packages": "/docs/concepts/architecture/#integration-packages",
    "#langgraph": "/docs/concepts/architecture/#langchainlanggraph",
    "#langsmith": "/docs/concepts/architecture/#langsmith",
    "#langchain-expression-language-lcel": "/docs/concepts/lcel",
    "#langchain-expression-language": "/docs/concepts/lcel",
    "#runnable-interface": "/docs/concepts/runnables",
    "#components": "/docs/concepts/",
    "#chat-models": "/docs/concepts/chat_models",
    "#multimodality": "/docs/concepts/multimodality",
    "#llms": "/docs/concepts/chat_models",
    "#messages": "/docs/concepts/messages",
    "#message-types": "/docs/concepts/messages",
    "#humanmessage": "/docs/concepts/messages/#humanmessage",
    "#aimessage": "/docs/concepts/messages/#aimessage",
    "#systemmessage": "/docs/concepts/messages/#systemmessage",
    "#toolmessage": "/docs/concepts/messages/#toolmessage",
    "#legacy-functionmessage":
      "/docs/concepts/messages/#legacy-functionmessage",
    "#prompt-templates": "/docs/concepts/prompt_templates",
    "#string-prompttemplates": "/docs/concepts/prompt_templates",
    "#chatprompttemplates": "/docs/concepts/prompt_templates",
    "#messagesplaceholder": "/docs/concepts/prompt_templates",
    "#example-selectors": "/docs/concepts/example_selectors",
    "#output-parsers": "/docs/concepts/output_parsers",
    "#chat-history": "/docs/concepts/chat_history",
    "#documents":
      "https://api.js.langchain.com/classes/_langchain_core.documents.Document.html",
    "#document":
      "https://api.js.langchain.com/classes/_langchain_core.documents.Document.html",
    "#document-loaders": "/docs/concepts/document_loaders",
    "#text-splitters": "/docs/concepts/text_splitters",
    "#embedding-models": "/docs/concepts/embedding_models",
    "#vector-stores": "/docs/concepts/vectorstores",
    "#vectorstore": "/docs/concepts/vectorstores",
    "#retrievers": "/docs/concepts/retrievers",
    "#keyvalue-stores": "/docs/concepts/key_value_stores",
    "#interface": "/docs/concepts/runnables",
    "#tools": "/docs/concepts/tools",
    "#invoke-with-just-the-arguments": "/docs/concepts/tools",
    "#invoke-with-toolcall": "/docs/concepts/tools",
    "#best-practices": "/docs/concepts/tools/#best-practices",
    "#related": "/docs/concepts/tools",
    "#toolkits": "/docs/concepts/toosl/#toolkits",
    "#initialize-a-toolkit": "/docs/concepts/toosl/#toolkits",
    "#get-list-of-tools": "/docs/concepts/toosl/#toolkits",
    "#agents": "/docs/concepts/agents",
    "#react-agents": "/docs/concepts/agents",
    "#callbacks": "/docs/concepts/callbacks",
    "#callback-events": "/docs/concepts/callbacks/#callback-events",
    "#callback-handlers": "/docs/concepts/callbacks/#callback-handlers",
    "#passing-callbacks": "/docs/concepts/callbacks/#passing-callbacks",
    "#techniques": "/docs/concepts/",
    "#streaming": "/docs/concepts/streaming",
    "#stream": "/docs/concepts/streaming#stream",
    "#streamevents": "/docs/concepts/streaming#streamevents",
    "#tokens": "/docs/concepts/tokens",
    "#functiontool-calling": "/docs/concepts/tool_calling",
    "#tool-usage": "/docs/concepts/tool_calling",
    "#structured-output": "/docs/concepts/structured_outputs",
    "#withstructuredoutput": "/docs/concepts/structured_outputs",
    "#raw-prompting": "/docs/concepts/structured_outputs",
    "#json-mode": "/docs/concepts/structured_outputs/#json-mode",
    "#tool-calling-structuredoutputtoolcalling":
      "/docs/concepts/structured_outputs",
    "#fewshot-prompting": "/docs/concepts/few_shot_prompting",
    "#1-generating-examples":
      "/docs/concepts/few_shot_prompting/#1-generating-examples",
    "#2-number-of-examples":
      "/docs/concepts/few_shot_prompting/#2-number-of-examples",
    "#3-selecting-examples":
      "/docs/concepts/few_shot_prompting/#3-selecting-examples",
    "#4-formatting-examples":
      "/docs/concepts/few_shot_prompting/#4-formatting-examples",
    "#retrieval": "/docs/concepts/retrieval",
    "#query-translation": "/docs/concepts/retrieval/#query-re-writing",
    "#routing": "/docs/concepts/",
    "#query-construction": "/docs/concepts/retrieval/#query-construction",
    "#indexing": "/docs/concepts/retrieval/",
    "#postprocessing": "/docs/concepts/retrieval/",
    "#generation": "/docs/concepts/rag",
    "#text-splitting": "/docs/concepts/text_splitting",
    "#evaluation": "/docs/concepts/evaluation",
    "#tracing": "/docs/concepts/tracing",
    "#few-shot-prompting": "/docs/concepts/few_shot_prompting",
  };

  const hash = window?.location?.hash;
  if (hash) {
    if (lookup[hash]) {
      window.location.href = lookup[hash];
      return null;
    }
  }

  return null;
}

export default RedirectAnchors;



================================================
FILE: docs/core_docs/src/theme/VectorStoreTabs.js
================================================
import React from "react";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import CodeBlock from "@theme-original/CodeBlock";
import Npm2Yarn from "@theme/Npm2Yarn";

export default function VectorStoreTabs(props) {
  const { customVarName } = props;

  const vectorStoreVarName = customVarName ?? "vectorStore";

  const tabItems = [
    {
      value: "Memory",
      label: "Memory",
      text: `import { MemoryVectorStore } from "langchain/vectorstores/memory";\n\nconst ${vectorStoreVarName} = new MemoryVectorStore(embeddings);`,
      dependencies: "langchain",
      default: true,
    },
    {
      value: "Chroma",
      label: "Chroma",
      text: `import { Chroma } from "@langchain/community/vectorstores/chroma";\n\nconst ${vectorStoreVarName} = new Chroma(embeddings, {\n  collectionName: "a-test-collection",\n});`,
      dependencies: "@langchain/community",
      default: true,
    },
    {
      value: "FAISS",
      label: "FAISS",
      text: `import { FaissStore } from "@langchain/community/vectorstores/faiss";\n\nconst ${vectorStoreVarName} = new FaissStore(embeddings, {});`,
      dependencies: "@langchain/community",
      default: false,
    },
    {
      value: "MongoDB",
      label: "MongoDB",
      text: `import { MongoDBAtlasVectorSearch } from "@langchain/mongodb"
import { MongoClient } from "mongodb";

const client = new MongoClient(process.env.MONGODB_ATLAS_URI || "");
const collection = client
  .db(process.env.MONGODB_ATLAS_DB_NAME)
  .collection(process.env.MONGODB_ATLAS_COLLECTION_NAME);

const ${vectorStoreVarName} = new MongoDBAtlasVectorSearch(embeddings, {
  collection: collection,
  indexName: "vector_index",
  textKey: "text",
  embeddingKey: "embedding",
});`,
      dependencies: "@langchain/mongodb",
      default: false,
    },
    {
      value: "PGVector",
      label: "PGVector",
      text: `import { PGVectorStore } from "@langchain/community/vectorstores/pgvector";

const ${vectorStoreVarName} = await PGVectorStore.initialize(embeddings, {})`,
      dependencies: "@langchain/community",
      default: false,
    },
    {
      value: "Pinecone",
      label: "Pinecone",
      text: `import { PineconeStore } from "@langchain/pinecone";
import { Pinecone as PineconeClient } from "@pinecone-database/pinecone";

const pinecone = new PineconeClient();
const ${vectorStoreVarName} = new PineconeStore(embeddings, {
  pineconeIndex,
  maxConcurrency: 5,
});`,
      dependencies: "@langchain/pinecone",
      default: false,
    },
    {
      value: "Qdrant",
      label: "Qdrant",
      text: `import { QdrantVectorStore } from "@langchain/qdrant";

const ${vectorStoreVarName} = await QdrantVectorStore.fromExistingCollection(embeddings, {
  url: process.env.QDRANT_URL,
  collectionName: "langchainjs-testing",
});`,
      dependencies: "@langchain/qdrant",
      default: false,
    },
  ];

  return (
    <div>
      <h3>Pick your vector store:</h3>
      <Tabs groupId="vectorStoreTabs">
        {tabItems.map((tab) => (
          <TabItem value={tab.value} label={tab.label} key={tab.value}>
            <h4>Install dependencies</h4>
            <Npm2Yarn>{tab.dependencies}</Npm2Yarn>
            <CodeBlock language="typescript">{tab.text}</CodeBlock>
          </TabItem>
        ))}
      </Tabs>
    </div>
  );
}



================================================
FILE: docs/core_docs/src/theme/CodeBlock/index.js
================================================
/* eslint-disable react/jsx-props-no-spreading */
import React from "react";
import CodeBlock from "@theme-original/CodeBlock";

function Imports({ imports }) {
  return (
    <div
      style={{
        paddingTop: "1.3rem",
        background: "var(--prism-background-color)",
        color: "var(--prism-color)",
        marginTop: "calc(-1 * var(--ifm-leading) - 5px)",
        marginBottom: "var(--ifm-leading)",
        boxShadow: "var(--ifm-global-shadow-lw)",
        borderBottomLeftRadius: "var(--ifm-code-border-radius)",
        borderBottomRightRadius: "var(--ifm-code-border-radius)",
      }}
    >
      <h4 style={{ paddingLeft: "0.65rem", marginBottom: "0.45rem" }}>
        API Reference:
      </h4>
      <ul style={{ paddingBottom: "1rem" }}>
        {imports.map(({ imported, source, docs }) => (
          <li key={imported}>
            <a href={docs}>
              <span>{imported}</span>
            </a>{" "}
            from <code>{source}</code>
          </li>
        ))}
      </ul>
    </div>
  );
}

export default function CodeBlockWrapper({ children, ...props }) {
  if (typeof children === "string") {
    return <CodeBlock {...props}>{children}</CodeBlock>;
  }

  return (
    <>
      <CodeBlock {...props}>{children.content}</CodeBlock>
      <Imports imports={children.imports} />
    </>
  );
}



================================================
FILE: docs/core_docs/src/theme/DocItem/Paginator/index.js
================================================
import React from "react";
import Paginator from "@theme-original/DocItem/Paginator";
import Feedback from "../../Feedback";

export default function PaginatorWrapper(props) {
  return (
    <>
      <Feedback />
      <Paginator {...props} />
    </>
  );
}



================================================
FILE: docs/core_docs/src/theme/DocPaginator/index.js
================================================
import React from "react";
import DocPaginator from "@theme-original/DocPaginator";

const BLACKLISTED_PATHS = ["/docs/how_to/", "/docs/tutorials/"];

export default function DocPaginatorWrapper(props) {
  const [shouldHide, setShouldHide] = React.useState(false);

  React.useEffect(() => {
    if (typeof window === "undefined") return;
    const currentPath = window.location.pathname;
    if (BLACKLISTED_PATHS.some((path) => currentPath.includes(path))) {
      setShouldHide(true);
    }
  }, []);

  if (!shouldHide) {
    // eslint-disable-next-line react/jsx-props-no-spreading
    return <DocPaginator {...props} />;
  }
  return null;
}



================================================
FILE: docs/core_docs/src/theme/DocVersionBanner/index.js
================================================
// Swizzled class to show custom text for canary version.
// Should be removed in favor of the stock implementation.\
/* eslint-disable react/jsx-props-no-spreading, import/no-extraneous-dependencies */

import React from "react";
import clsx from "clsx";
import useDocusaurusContext from "@docusaurus/useDocusaurusContext";
import Link from "@docusaurus/Link";
import Translate from "@docusaurus/Translate";
import {
  useActivePlugin,
  useDocVersionSuggestions,
} from "@docusaurus/plugin-content-docs/client";
import { ThemeClassNames } from "@docusaurus/theme-common";
import {
  useDocsPreferredVersion,
  useDocsVersion,
} from "@docusaurus/theme-common/internal";

function UnreleasedVersionLabel({ siteTitle, versionMetadata }) {
  return (
    <Translate
      id="theme.docs.versions.unreleasedVersionLabel"
      description="The label used to tell the user that he's browsing an unreleased doc version"
      values={{
        siteTitle,
        versionLabel: <b>{versionMetadata.label}</b>,
      }}
    >
      {
        "This is unreleased documentation for {siteTitle}'s {versionLabel} version."
      }
    </Translate>
  );
}

function UnmaintainedVersionLabel({ siteTitle, versionMetadata }) {
  return (
    <Translate
      id="theme.docs.versions.unmaintainedVersionLabel"
      description="The label used to tell the user that he's browsing an unmaintained doc version"
      values={{
        siteTitle,
        versionLabel: <b>{versionMetadata.label}</b>,
      }}
    >
      {
        "This is documentation for {siteTitle} {versionLabel}, which is no longer actively maintained."
      }
    </Translate>
  );
}

const BannerLabelComponents = {
  unreleased: UnreleasedVersionLabel,
  unmaintained: UnmaintainedVersionLabel,
};

function BannerLabel({ siteTitle, versionMetadata }) {
  const BannerLabelComponent = BannerLabelComponents[versionMetadata.banner];
  return (
    <BannerLabelComponent
      siteTitle={siteTitle}
      versionMetadata={versionMetadata}
    />
  );
}

function LatestVersionSuggestionLabel({ versionLabel, to, onClick }) {
  return (
    <Translate
      id="theme.docs.versions.latestVersionSuggestionLabel"
      description="The label used to tell the user to check the latest version"
      values={{
        versionLabel,
        latestVersionLink: (
          <b>
            <Link to={to} onClick={onClick}>
              <Translate
                id="theme.docs.versions.latestVersionLinkLabel"
                description="The label used for the latest version suggestion link label"
              >
                this version
              </Translate>
            </Link>
          </b>
        ),
      }}
    >
      {
        "For the current stable version, see {latestVersionLink} ({versionLabel})."
      }
    </Translate>
  );
}

function DocVersionBannerEnabled({ className, versionMetadata }) {
  const {
    siteConfig: { title: siteTitle },
  } = useDocusaurusContext();
  const { pluginId } = useActivePlugin({ failfast: true });
  const getVersionMainDoc = (version) =>
    version.docs.find((doc) => doc.id === version.mainDocId);
  const { savePreferredVersionName } = useDocsPreferredVersion(pluginId);
  const { latestDocSuggestion, latestVersionSuggestion } =
    useDocVersionSuggestions(pluginId);
  // Try to link to same doc in latest version (not always possible), falling
  // back to main doc of latest version
  const latestVersionSuggestedDoc =
    latestDocSuggestion ?? getVersionMainDoc(latestVersionSuggestion);
  return (
    <div
      className={clsx(
        className,
        ThemeClassNames.docs.docVersionBanner,
        "alert alert--warning margin-bottom--md"
      )}
      role="alert"
    >
      <div>
        <BannerLabel siteTitle={siteTitle} versionMetadata={versionMetadata} />
      </div>
      <div className="margin-top--md">
        <LatestVersionSuggestionLabel
          versionLabel={latestVersionSuggestion.label}
          to={latestVersionSuggestedDoc.path}
          onClick={() => savePreferredVersionName(latestVersionSuggestion.name)}
        />
      </div>
    </div>
  );
}

// function LatestDocVersionBanner({ className, versionMetadata }) {
//   const {
//     siteConfig: { title: siteTitle },
//   } = useDocusaurusContext();
//   const { pluginId } = useActivePlugin({ failfast: true });
//   const getVersionMainDoc = (version) =>
//     version.docs.find((doc) => doc.id === version.mainDocId);
//   const { savePreferredVersionName } = useDocsPreferredVersion(pluginId);
//   const { latestDocSuggestion, latestVersionSuggestion } =
//     useDocVersionSuggestions(pluginId);
//   // Try to link to same doc in latest version (not always possible), falling
//   // back to main doc of latest version
//   const latestVersionSuggestedDoc =
//     latestDocSuggestion ?? getVersionMainDoc(latestVersionSuggestion);
//   const canaryPath = `/docs/0.2.x/${latestVersionSuggestedDoc.path.slice(
//     "/docs/".length
//   )}`;
//   return (
//     <div
//       className={clsx(
//         className,
//         ThemeClassNames.docs.docVersionBanner,
//         "alert alert--info margin-bottom--md"
//       )}
//       role="alert"
//     >
//       <div>
//         <Translate
//           id="theme.docs.versions.unmaintainedVersionLabel"
//           description="The label used to encourage the user to view the experimental 0.2.x version"
//           values={{
//             siteTitle,
//             versionLabel: <b>{versionMetadata.label}</b>,
//           }}
//         >
//           {
//             "This is a stable version of documentation for {siteTitle}'s version {versionLabel}."
//           }
//         </Translate>
//       </div>
//       <div className="margin-top--md">
//         <Translate
//           id="theme.docs.versions.latestVersionSuggestionLabel"
//           description="The label used to tell the user to check the experimental version"
//           values={{
//             versionLabel: <b>{versionMetadata.label}</b>,
//             latestVersionLink: (
//               <b>
//                 <Link
//                   to={canaryPath}
//                   onClick={() => savePreferredVersionName("0.2.x")}
//                 >
//                   <Translate
//                     id="theme.docs.versions.latestVersionLinkLabel"
//                     description="The label used for the latest version suggestion link label"
//                   >
//                     this experimental version
//                   </Translate>
//                 </Link>
//               </b>
//             ),
//           }}
//         >
//           {
//             "You can also check out {latestVersionLink} for an updated experience."
//           }
//         </Translate>
//       </div>
//     </div>
//   );
// }

export default function DocVersionBanner({ className }) {
  const versionMetadata = useDocsVersion();
  if (versionMetadata.banner) {
    return (
      <DocVersionBannerEnabled
        className={className}
        versionMetadata={versionMetadata}
      />
    );
  }
  if (versionMetadata.isLast) {
    // Uncomment when we are ready to direct people to new build
    // return (
    //   <LatestDocVersionBanner
    //     className={className}
    //     versionMetadata={versionMetadata}
    //   />
    // );
    return null;
  }
  return null;
}



================================================
FILE: docs/core_docs/static/llms.txt
================================================
# LangChain JavaScript

This guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.

We recommend that you go through at least one of the [Tutorials](https://js.langchain.com/docs/tutorials/) before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.

The conceptual guide does not cover step-by-step instructions or specific implementation examples — those are found in the [How-to guides](https://js.langchain.com/docs/how_to/) and [Tutorials](https://js.langchain.com/docs/tutorials/). For detailed reference material, please see the [API reference](https://api.js.langchain.com/).

## Concepts

- **[Why LangChain?](https://js.langchain.com/docs/concepts/why_langchain/)**: Overview of the value that LangChain provides.
- **[Architecture](https://js.langchain.com/docs/concepts/architecture/)**: How packages are organized in the LangChain ecosystem.
- **[Chat models](https://js.langchain.com/docs/concepts/chat_models/)**: LLMs exposed via a chat API that process sequences of messages as input and output a message.
- **[Messages](https://js.langchain.com/docs/concepts/messages/)**: The unit of communication in chat models, used to represent model input and output.
- **[Chat history](https://js.langchain.com/docs/concepts/chat_history/)**: A conversation represented as a sequence of messages, alternating between user messages and model responses.
- **[Tools](https://js.langchain.com/docs/concepts/tools/)**: A function with an associated schema defining the function's name, description, and the arguments it accepts.
- **[Tool calling](https://js.langchain.com/docs/concepts/tool_calling/)**: A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message.
- **[Structured output](https://js.langchain.com/docs/concepts/structured_outputs/)**: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.
- **[Memory](https://langchain-ai.github.io/langgraphjs/concepts/memory/)**: Information about a conversation that is persisted so that it can be used in future conversations.
- **[Multimodality](https://js.langchain.com/docs/concepts/multimodality/)**: The ability to work with data that comes in different forms, such as text, audio, images, and video.
- **[Runnable interface](https://js.langchain.com/docs/concepts/runnables/)**: The base abstraction that many LangChain components and the LangChain Expression Language are built on.
- **[Streaming](https://js.langchain.com/docs/concepts/streaming/)**: LangChain streaming APIs for surfacing results as they are generated.
- **[LangChain Expression Language (LCEL)](https://js.langchain.com/docs/concepts/lcel/)**: A syntax for orchestrating LangChain components. Most useful for simpler applications.
- **[Document loaders](https://js.langchain.com/docs/concepts/document_loaders/)**: Load a source as a list of documents.
- **[Retrieval](https://js.langchain.com/docs/concepts/retrieval/)**: Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query.
- **[Text splitters](https://js.langchain.com/docs/concepts/text_splitters/)**: Split long text into smaller chunks that can be individually indexed to enable granular retrieval.
- **[Embedding models](https://js.langchain.com/docs/concepts/embedding_models/)**: Models that represent data such as text or images in a vector space.
- **[Vector stores](https://js.langchain.com/docs/concepts/vectorstores/)**: Storage of and efficient search over vectors and associated metadata.
- **[Retriever](https://js.langchain.com/docs/concepts/retrievers/)**: A component that returns relevant documents from a knowledge base in response to a query.
- **[Retrieval Augmented Generation (RAG)](https://js.langchain.com/docs/concepts/rag/)**: A technique that enhances language models by combining them with external knowledge bases.
- **[Agents](https://js.langchain.com/docs/concepts/agents/)**: Use a [language model](https://js.langchain.com/docs/concepts/chat_models/) to choose a sequence of actions to take. Agents can interact with external resources via [tool](https://js.langchain.com/docs/concepts/tools/).
- **[Prompt templates](https://js.langchain.com/docs/concepts/prompt_templates/)**: Component for factoring out the static parts of a model "prompt" (usually a sequence of messages). Useful for serializing, versioning, and reusing these static parts.
- **[Output parsers](https://js.langchain.com/docs/concepts/output_parsers/)**: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily useful prior to the general availability of [tool calling](https://js.langchain.com/docs/concepts/tool_calling/) and [structured outputs](https://js.langchain.com/docs/concepts/structured_outputs/).
- **[Few-shot prompting](https://js.langchain.com/docs/concepts/few_shot_prompting/)**: A technique for improving model performance by providing a few examples of the task to perform in the prompt.
- **[Example selectors](https://js.langchain.com/docs/concepts/example_selectors/)**: Used to select the most relevant examples from a dataset based on a given input. Example selectors are used in few-shot prompting to select examples for a prompt.
- **[Callbacks](https://js.langchain.com/docs/concepts/callbacks/)**: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more.
- **[Tracing](https://js.langchain.com/docs/concepts/tracing/)**: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications.
- **[Evaluation](https://js.langchain.com/docs/concepts/evaluation/)**: The process of assessing the performance and effectiveness of AI applications. This involves testing the model's responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications.

## Glossary

- **[AIMessageChunk](https://js.langchain.com/docs/concepts/messages/#aimessagechunk)**: A partial response from an AI message. Used when streaming responses from a chat model.
- **[AIMessage](https://js.langchain.com/docs/concepts/messages/#aimessage)**: Represents a complete response from an AI model.
- **[StructuredTool](https://api.js.langchain.com/classes/_langchain_core.tools.StructuredTool.html/)**: The base class for all tools in LangChain.
- **[batch](https://js.langchain.com/docs/concepts/runnables/)**: Use to execute a runnable with batch inputs a Runnable.
- **[bindTools](https://js.langchain.com/docs/concepts/tool_calling/#tool-binding)**: Allows models to interact with tools.
- **[Caching](https://js.langchain.com/docs/concepts/chat_models/#caching)**: Storing results to avoid redundant calls to a chat model.
- **[Context window](https://js.langchain.com/docs/concepts/chat_models/#context-window)**: The maximum size of input a chat model can process.
- **[Conversation patterns](https://js.langchain.com/docs/concepts/chat_history/#conversation-patterns)**: Common patterns in chat interactions.
- **[Document](https://api.js.langchain.com/classes/_langchain_core.documents.Document.html/)**: LangChain's representation of a document.
- **[Embedding models](https://js.langchain.com/docs/concepts/embedding_models/)**: Models that generate vector embeddings for various data types.
- **[HumanMessage](https://js.langchain.com/docs/concepts/messages/#humanmessage)**: Represents a message from a human user.
- **[input and output types](https://js.langchain.com/docs/concepts/runnables/#input-and-output-types)**: Types used for input and output in Runnables.
- **[Integration packages](https://js.langchain.com/docs/concepts/architecture/#integration-packages)**: Third-party packages that integrate with LangChain.
- **[invoke](https://js.langchain.com/docs/concepts/runnables/)**: A standard method to invoke a Runnable.
- **[JSON mode](https://js.langchain.com/docs/concepts/structured_outputs/#json-mode)**: Returning responses in JSON format.
- **[@langchain/community](https://js.langchain.com/docs/concepts/architecture/#langchaincommunity)**: Community-driven components for LangChain.
- **[@langchain/core](https://js.langchain.com/docs/concepts/architecture/#langchaincore)**: Core langchain package. Includes base interfaces and in-memory implementations.
- **[langchain](https://js.langchain.com/docs/concepts/architecture/#langchain)**: A package for higher level components (e.g., some pre-built chains).
- **[@langchain/langgraph](https://js.langchain.com/docs/concepts/architecture/#langchainlanggraph)**: Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows.
- **[Managing chat history](https://js.langchain.com/docs/concepts/chat_history/#managing-chat-history)**: Techniques to maintain and manage the chat history.
- **[OpenAI format](https://js.langchain.com/docs/concepts/messages/#openai-format)**: OpenAI's message format for chat models.
- **[Propagation of RunnableConfig](https://js.langchain.com/docs/concepts/runnables/#propagation-of-runnableconfig)**: Propagating configuration through Runnables.
- **[RemoveMessage](https://js.langchain.com/docs/concepts/messages/#removemessage)**: An abstraction used to remove a message from chat history, used primarily in LangGraph.
- **[role](https://js.langchain.com/docs/concepts/messages/#role)**: Represents the role (e.g., user, assistant) of a chat message.
- **[RunnableConfig](https://js.langchain.com/docs/concepts/runnables/#runnableconfig)**: Use to pass run time information to Runnables (e.g., `runName`, `runId`, `tags`, `metadata`, `maxConcurrency`, `recursionLimit`, `configurable`).
- **[Standard parameters for chat models](https://js.langchain.com/docs/concepts/chat_models/#standard-parameters)**: Parameters such as API key, `temperature`, and `maxTokens`,
- **[stream](https://js.langchain.com/docs/concepts/streaming/)**: Use to stream output from a Runnable or a graph.
- **[Tokenization](https://js.langchain.com/docs/concepts/tokens/#how-tokens-work-in-language-models)**: The process of converting data into tokens and vice versa.
- **[Tokens](https://js.langchain.com/docs/concepts/tokens/)**: The basic unit that a language model reads, processes, and generates under the hood.
- **[Tool artifacts](https://js.langchain.com/docs/concepts/tools/#tool-artifacts)**: Add artifacts to the output of a tool that will not be sent to the model, but will be available for downstream processing.
- **[Tool binding](https://js.langchain.com/docs/concepts/tool_calling/#tool-binding)**: Binding tools to models.
- **[`tool`](https://js.langchain.com/docs/concepts/tools/)**: Function for creating tools in LangChain.
- **[Toolkits](https://js.langchain.com/docs/concepts/tools/#toolkits)**: A collection of tools that can be used together.
- **[ToolMessage](https://js.langchain.com/docs/concepts/messages/#toolmessage)**: Represents a message that contains the results of a tool execution.
- **[Vector stores](https://js.langchain.com/docs/concepts/vectorstores/)**: Datastores specialized for storing and efficiently searching vector embeddings.
- **[withStructuredOutput](https://js.langchain.com/docs/concepts/structured_outputs/#structured-output-method)**: A helper method for chat models that natively support [tool calling](https://js.langchain.com/docs/concepts/tool_calling/) to get structured output matching a given schema specified via Zod, JSON schema or a function.


## Integrations

### Chat Models

Key chat model integrations include:

- [BedrockChat](https://js.langchain.com/docs/integrations/chat/bedrock/)
- [ChatBedrockConverse](https://js.langchain.com/docs/integrations/chat/bedrock_converse/)
- [ChatAnthropic](https://js.langchain.com/docs/integrations/chat/anthropic/)
- [ChatCloudflareWorkersAI](https://js.langchain.com/docs/integrations/chat/cloudflare_workersai/)
- [ChatCohere](https://js.langchain.com/docs/integrations/chat/cohere/)
- [ChatFireworks](https://js.langchain.com/docs/integrations/chat/fireworks/)
- [ChatGoogleGenerativeAI](https://js.langchain.com/docs/integrations/chat/google_generativeai/)
- [ChatVertexAI](https://js.langchain.com/docs/integrations/chat/google_vertex_ai/)
- [ChatGroq](https://js.langchain.com/docs/integrations/chat/groq/)
- [ChatMistralAI](https://js.langchain.com/docs/integrations/chat/mistral/)
- [ChatOllama](https://js.langchain.com/docs/integrations/chat/ollama/)
- [ChatOpenAI](https://js.langchain.com/docs/integrations/chat/openai/)
- [ChatTogetherAI](https://js.langchain.com/docs/integrations/chat/togetherai/)
- [ChatXAI](https://js.langchain.com/docs/integrations/chat/xai/)

Other chat model integrations can be found at [chat model integrations](https://js.langchain.com/docs/integrations/chat/)

## How-to guides

Here you'll find answers to “How do I….?” types of questions.
These guides are _goal-oriented_ and _concrete_; they're meant to help you complete a specific task.
For conceptual explanations see [Conceptual Guides](https://js.langchain.com/docs/concepts/).
For end-to-end walkthroughs see [Tutorials](https://js.langchain.com/docs/tutorials/).
For comprehensive descriptions of every class and function see [API Reference](https://api.js.langchain.com/).

### Installation

- [How to: install LangChain packages](https://js.langchain.com/docs/how_to/installation/)

### Key features

This highlights functionality that is core to using LangChain.

- [How to: return structured data from an LLM](https://js.langchain.com/docs/how_to/structured_output/)
- [How to: use a chat model to call tools](https://js.langchain.com/docs/how_to/tool_calling/)
- [How to: stream runnables](https://js.langchain.com/docs/how_to/streaming/)
- [How to: debug your LLM apps](https://js.langchain.com/docs/how_to/debugging/)

### LangChain Expression Language (LCEL)

LangChain Expression Language is a way to create arbitrary custom chains. It is built on the [`Runnable`](https://api.js.langchain.com/classes/langchain_core.runnables.Runnable.html/) protocol.

[**LCEL cheatsheet**](https://js.langchain.com/docs/how_to/lcel_cheatsheet/): For a quick overview of how to use the main LCEL primitives.

- [How to: chain runnables](https://js.langchain.com/docs/how_to/sequence/)
- [How to: stream runnables](https://js.langchain.com/docs/how_to/streaming/)
- [How to: invoke runnables in parallel](https://js.langchain.com/docs/how_to/parallel/)
- [How to: attach runtime arguments to a runnable](https://js.langchain.com/docs/how_to/binding/)
- [How to: run custom functions](https://js.langchain.com/docs/how_to/functions/)
- [How to: pass through arguments from one step to the next](https://js.langchain.com/docs/how_to/passthrough/)
- [How to: add values to a chain's state](https://js.langchain.com/docs/how_to/assign/)
- [How to: add message history](https://js.langchain.com/docs/how_to/message_history/)
- [How to: route execution within a chain](https://js.langchain.com/docs/how_to/routing/)
- [How to: add fallbacks](https://js.langchain.com/docs/how_to/fallbacks/)
- [How to: cancel execution](https://js.langchain.com/docs/how_to/cancel_execution/)

### Components

These are the core building blocks you can use when building applications.

#### Prompt templates

[Prompt Templates](https://js.langchain.com/docs/concepts/prompt_templates/) are responsible for formatting user input into a format that can be passed to a language model.

- [How to: use few shot examples](https://js.langchain.com/docs/how_to/few_shot_examples/)
- [How to: use few shot examples in chat models](https://js.langchain.com/docs/how_to/few_shot_examples_chat/)
- [How to: partially format prompt templates](https://js.langchain.com/docs/how_to/prompts_partial/)
- [How to: compose prompts together](https://js.langchain.com/docs/how_to/prompts_composition/)

#### Example selectors

[Example Selectors](https://js.langchain.com/docs/concepts/example_selectors/) are responsible for selecting the correct few shot examples to pass to the prompt.

- [How to: use example selectors](https://js.langchain.com/docs/how_to/example_selectors/)
- [How to: select examples by length](https://js.langchain.com/docs/how_to/example_selectors_length_based/)
- [How to: select examples by semantic similarity](https://js.langchain.com/docs/how_to/example_selectors_similarity/)
- [How to: select examples from LangSmith few-shot datasets](https://js.langchain.com/docs/how_to/example_selectors_langsmith/)

#### Chat models

[Chat Models](https://js.langchain.com/docs/concepts/chat_models/) are newer forms of language models that take messages in and output a message.

- [How to: do function/tool calling](https://js.langchain.com/docs/how_to/tool_calling/)
- [How to: get models to return structured output](https://js.langchain.com/docs/how_to/structured_output/)
- [How to: cache model responses](https://js.langchain.com/docs/how_to/chat_model_caching/)
- [How to: create a custom chat model class](https://js.langchain.com/docs/how_to/custom_chat/)
- [How to: get log probabilities](https://js.langchain.com/docs/how_to/logprobs/)
- [How to: stream a response back](https://js.langchain.com/docs/how_to/chat_streaming/)
- [How to: track token usage](https://js.langchain.com/docs/how_to/chat_token_usage_tracking/)
- [How to: pass tool outputs to chat models](https://js.langchain.com/docs/how_to/tool_results_pass_to_model/)
- [How to: stream tool calls](https://js.langchain.com/docs/how_to/tool_streaming/)
- [How to: few shot prompt tool behavior](https://js.langchain.com/docs/how_to/tools_few_shot/)
- [How to: force a specific tool call](https://js.langchain.com/docs/how_to/tool_choice/)
- [How to: disable parallel tool calling](https://js.langchain.com/docs/how_to/tool_calling_parallel/)
- [How to: init any model in one line](https://js.langchain.com/docs/how_to/chat_models_universal_init/)

#### Messages

[Messages](https://js.langchain.com/docs/concepts/###message-types) are the input and output of chat models. They have some `content` and a `role`, which describes the source of the message.

- [How to: trim messages](https://js.langchain.com/docs/how_to/trim_messages/)
- [How to: filter messages](https://js.langchain.com/docs/how_to/filter_messages/)
- [How to: merge consecutive messages of the same type](https://js.langchain.com/docs/how_to/merge_message_runs/)

#### LLMs

What LangChain calls [LLMs](https://js.langchain.com/docs/concepts/text_llms/) are older forms of language models that take a string in and output a string.

- [How to: cache model responses](https://js.langchain.com/docs/how_to/llm_caching/)
- [How to: create a custom LLM class](https://js.langchain.com/docs/how_to/custom_llm/)
- [How to: stream a response back](https://js.langchain.com/docs/how_to/streaming_llm/)
- [How to: track token usage](https://js.langchain.com/docs/how_to/llm_token_usage_tracking/)

#### Output parsers

[Output Parsers](https://js.langchain.com/docs/concepts/output_parsers/) are responsible for taking the output of an LLM and parsing into more structured format.

- [How to: use output parsers to parse an LLM response into structured format](https://js.langchain.com/docs/how_to/output_parser_structured/)
- [How to: parse JSON output](https://js.langchain.com/docs/how_to/output_parser_json/)
- [How to: parse XML output](https://js.langchain.com/docs/how_to/output_parser_xml/)
- [How to: try to fix errors in output parsing](https://js.langchain.com/docs/how_to/output_parser_fixing/)

#### Document loaders

[Document Loaders](https://js.langchain.com/docs/concepts/document_loaders/) are responsible for loading documents from a variety of sources.

- [How to: load CSV data](https://js.langchain.com/docs/how_to/document_loader_csv/)
- [How to: load data from a directory](https://js.langchain.com/docs/how_to/document_loader_directory/)
- [How to: load PDF files](https://js.langchain.com/docs/how_to/document_loader_pdf/)
- [How to: write a custom document loader](https://js.langchain.com/docs/how_to/document_loader_custom/)
- [How to: load HTML data](https://js.langchain.com/docs/how_to/document_loader_html/)
- [How to: load Markdown data](https://js.langchain.com/docs/how_to/document_loader_markdown/)

#### Text splitters

[Text Splitters](https://js.langchain.com/docs/concepts/text_splitters/) take a document and split into chunks that can be used for retrieval.

- [How to: recursively split text](https://js.langchain.com/docs/how_to/recursive_text_splitter/)
- [How to: split by character](https://js.langchain.com/docs/how_to/character_text_splitter/)
- [How to: split code](https://js.langchain.com/docs/how_to/code_splitter/)
- [How to: split by tokens](https://js.langchain.com/docs/how_to/split_by_token/)

#### Embedding models

[Embedding Models](https://js.langchain.com/docs/concepts/embedding_models/) take a piece of text and create a numerical representation of it.

- [How to: embed text data](https://js.langchain.com/docs/how_to/embed_text/)
- [How to: cache embedding results](https://js.langchain.com/docs/how_to/caching_embeddings/)

#### Vector stores

[Vector stores](https://js.langchain.com/docs/concepts/#vectorstores) are databases that can efficiently store and retrieve embeddings.

- [How to: create and query vector stores](https://js.langchain.com/docs/how_to/vectorstores/)

#### Retrievers

[Retrievers](https://js.langchain.com/docs/concepts/retrievers/) are responsible for taking a query and returning relevant documents.

- [How to: use a vector store to retrieve data](https://js.langchain.com/docs/how_to/vectorstore_retriever/)
- [How to: generate multiple queries to retrieve data for](https://js.langchain.com/docs/how_to/multiple_queries/)
- [How to: use contextual compression to compress the data retrieved](https://js.langchain.com/docs/how_to/contextual_compression/)
- [How to: write a custom retriever class](https://js.langchain.com/docs/how_to/custom_retriever/)
- [How to: combine the results from multiple retrievers](https://js.langchain.com/docs/how_to/ensemble_retriever/)
- [How to: generate multiple embeddings per document](https://js.langchain.com/docs/how_to/multi_vector/)
- [How to: retrieve the whole document for a chunk](https://js.langchain.com/docs/how_to/parent_document_retriever/)
- [How to: generate metadata filters](https://js.langchain.com/docs/how_to/self_query/)
- [How to: create a time-weighted retriever](https://js.langchain.com/docs/how_to/time_weighted_vectorstore/)
- [How to: reduce retrieval latency](https://js.langchain.com/docs/how_to/reduce_retrieval_latency/)

#### Indexing

Indexing is the process of keeping your vectorstore in-sync with the underlying data source.

- [How to: reindex data to keep your vectorstore in-sync with the underlying data source](https://js.langchain.com/docs/how_to/indexing/)

#### Tools

LangChain [Tools](https://js.langchain.com/docs/concepts/tools/) contain a description of the tool (to pass to the language model) as well as the implementation of the function to call.

- [How to: create tools](https://js.langchain.com/docs/how_to/custom_tools/)
- [How to: use built-in tools and toolkits](https://js.langchain.com/docs/how_to/tools_builtin/)
- [How to: use chat models to call tools](https://js.langchain.com/docs/how_to/tool_calling/)
- [How to: pass tool outputs to chat models](https://js.langchain.com/docs/how_to/tool_results_pass_to_model/)
- [How to: few shot prompt tool behavior](https://js.langchain.com/docs/how_to/tools_few_shot/)
- [How to: pass run time values to tools](https://js.langchain.com/docs/how_to/tool_runtime/)
- [How to: handle tool errors](https://js.langchain.com/docs/how_to/tools_error/)
- [How to: force a specific tool call](https://js.langchain.com/docs/how_to/tool_choice/)
- [How to: disable parallel tool calling](https://js.langchain.com/docs/how_to/tool_calling_parallel/)
- [How to: access the `RunnableConfig` object within a custom tool](https://js.langchain.com/docs/how_to/tool_configure/)
- [How to: stream events from child runs within a custom tool](https://js.langchain.com/docs/how_to/tool_stream_events/)
- [How to: return artifacts from a tool](https://js.langchain.com/docs/how_to/tool_artifacts/)
- [How to: convert Runnables to tools](https://js.langchain.com/docs/how_to/convert_runnable_to_tool/)
- [How to: add ad-hoc tool calling capability to models](https://js.langchain.com/docs/how_to/tools_prompting/)

#### Agents

:::note

For in depth how-to guides for agents, please check out [LangGraph](https://langchain-ai.github.io/langgraphjs/) documentation.

:::

- [How to: use legacy LangChain Agents (AgentExecutor)](https://js.langchain.com/docs/how_to/agent_executor/)
- [How to: migrate from legacy LangChain agents to LangGraph](https://js.langchain.com/docs/how_to/migrate_agent/)

#### Callbacks

[Callbacks](https://js.langchain.com/docs/concepts/callbacks/) allow you to hook into the various stages of your LLM application's execution.

- [How to: pass in callbacks at runtime](https://js.langchain.com/docs/how_to/callbacks_runtime/)
- [How to: attach callbacks to a module](https://js.langchain.com/docs/how_to/callbacks_attach/)
- [How to: pass callbacks into a module constructor](https://js.langchain.com/docs/how_to/callbacks_constructor/)
- [How to: create custom callback handlers](https://js.langchain.com/docs/how_to/custom_callbacks/)
- [How to: await callbacks in serverless environments](https://js.langchain.com/docs/how_to/callbacks_serverless/)
- [How to: dispatch custom callback events](https://js.langchain.com/docs/how_to/callbacks_custom_events/)

#### Custom

All of LangChain components can easily be extended to support your own versions.

- [How to: create a custom chat model class](https://js.langchain.com/docs/how_to/custom_chat/)
- [How to: create a custom LLM class](https://js.langchain.com/docs/how_to/custom_llm/)
- [How to: write a custom retriever class](https://js.langchain.com/docs/how_to/custom_retriever/)
- [How to: write a custom document loader](https://js.langchain.com/docs/how_to/document_loader_custom/)
- [How to: create custom callback handlers](https://js.langchain.com/docs/how_to/custom_callbacks/)
- [How to: define a custom tool](https://js.langchain.com/docs/how_to/custom_tools/)
- [How to: dispatch custom callback events](https://js.langchain.com/docs/how_to/callbacks_custom_events/)

#### Generative UI

- [How to: build an LLM generated UI](https://js.langchain.com/docs/how_to/generative_ui/)
- [How to: stream agentic data to the client](https://js.langchain.com/docs/how_to/stream_agent_client/)
- [How to: stream structured output to the client](https://js.langchain.com/docs/how_to/stream_tool_client/)

#### Multimodal

- [How to: pass multimodal data directly to models](https://js.langchain.com/docs/how_to/multimodal_inputs/)
- [How to: use multimodal prompts](https://js.langchain.com/docs/how_to/multimodal_prompts/)
- [How to: call tools with multimodal data](https://js.langchain.com/docs/how_to/tool_calls_multimodal/)

### Use cases

These guides cover use-case specific details.

#### Q&A with RAG

Retrieval Augmented Generation (RAG) is a way to connect LLMs to external sources of data.
For a high-level tutorial on RAG, check out [this guide](https://js.langchain.com/docs/tutorials/rag/).

- [How to: add chat history](https://js.langchain.com/docs/how_to/qa_chat_history_how_to/)
- [How to: stream](https://js.langchain.com/docs/how_to/qa_streaming/)
- [How to: return sources](https://js.langchain.com/docs/how_to/qa_sources/)
- [How to: return citations](https://js.langchain.com/docs/how_to/qa_citations/)
- [How to: do per-user retrieval](https://js.langchain.com/docs/how_to/qa_per_user/)

#### Extraction

Extraction is when you use LLMs to extract structured information from unstructured text.
For a high level tutorial on extraction, check out [this guide](https://js.langchain.com/docs/tutorials/extraction/).

- [How to: use reference examples](https://js.langchain.com/docs/how_to/extraction_examples/)
- [How to: handle long text](https://js.langchain.com/docs/how_to/extraction_long_text/)
- [How to: do extraction without using function calling](https://js.langchain.com/docs/how_to/extraction_parse/)

#### Chatbots

Chatbots involve using an LLM to have a conversation.
For a high-level tutorial on building chatbots, check out [this guide](https://js.langchain.com/docs/tutorials/chatbot/).

- [How to: manage memory](https://js.langchain.com/docs/how_to/chatbots_memory/)
- [How to: do retrieval](https://js.langchain.com/docs/how_to/chatbots_retrieval/)
- [How to: use tools](https://js.langchain.com/docs/how_to/chatbots_tools/)

#### Query analysis

Query Analysis is the task of using an LLM to generate a query to send to a retriever.
For a high-level tutorial on query analysis, check out [this guide](https://js.langchain.com/docs/tutorials/rag/#query-analysis).

- [How to: add examples to the prompt](https://js.langchain.com/docs/how_to/query_few_shot/)
- [How to: handle cases where no queries are generated](https://js.langchain.com/docs/how_to/query_no_queries/)
- [How to: handle multiple queries](https://js.langchain.com/docs/how_to/query_multiple_queries/)
- [How to: handle multiple retrievers](https://js.langchain.com/docs/how_to/query_multiple_retrievers/)
- [How to: construct filters](https://js.langchain.com/docs/how_to/query_constructing_filters/)
- [How to: deal with high cardinality categorical variables](https://js.langchain.com/docs/how_to/query_high_cardinality/)

#### Q&A over SQL + CSV

You can use LLMs to do question answering over tabular data.
For a high-level tutorial, check out [this guide](https://js.langchain.com/docs/tutorials/sql_qa/).

- [How to: use prompting to improve results](https://js.langchain.com/docs/how_to/sql_prompting/)
- [How to: do query validation](https://js.langchain.com/docs/how_to/sql_query_checking/)
- [How to: deal with large databases](https://js.langchain.com/docs/how_to/sql_large_db/)

#### Q&A over graph databases

You can use an LLM to do question answering over graph databases.
For a high-level tutorial, check out [this guide](https://js.langchain.com/docs/tutorials/graph/).

- [How to: map values to a database](https://js.langchain.com/docs/how_to/graph_mapping/)
- [How to: add a semantic layer over the database](https://js.langchain.com/docs/how_to/graph_semantic/)
- [How to: improve results with prompting](https://js.langchain.com/docs/how_to/graph_prompting/)
- [How to: construct knowledge graphs](https://js.langchain.com/docs/how_to/graph_constructing/)

### [LangGraph.js](https://langchain-ai.github.io/langgraphjs/)

LangGraph.js is an extension of LangChain aimed at
building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.

LangGraph.js documentation is currently hosted on a separate site.
You can peruse [LangGraph.js how-to guides here](https://langchain-ai.github.io/langgraphjs/how-tos/).

### [LangSmith](https://docs.smith.langchain.com/)

LangSmith allows you to closely trace, monitor and evaluate your LLM application.
It seamlessly integrates with LangChain and LangGraph.js, and you can use it to inspect and debug individual steps of your chains as you build.

LangSmith documentation is hosted on a separate site.
You can peruse [LangSmith how-to guides here](https://docs.smith.langchain.com/how_to_guides/), but we'll highlight a few sections that are particularly
relevant to LangChain below:

#### Evaluation

<span data-heading-keywords="evaluation,evaluate"></span>

Evaluating performance is a vital part of building LLM-powered applications.
LangSmith helps with every step of the process from creating a dataset to defining metrics to running evaluators.

To learn more, check out the [LangSmith evaluation how-to guides](https://docs.smith.langchain.com/how_to_guides/#evaluation).

#### Tracing

<span data-heading-keywords="trace,tracing"></span>

Tracing gives you observability inside your chains and agents, and is vital in diagnosing issues.

- [How to: trace with LangChain](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain/)
- [How to: add metadata and tags to traces](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain/#add-metadata-and-tags-to-traces)

You can see general tracing-related how-tos [in this section of the LangSmith docs](https://docs.smith.langchain.com/how_to_guides/tracing/).



================================================
FILE: docs/core_docs/static/robots.txt
================================================
User-agent: *

Sitemap: https://js.langchain.com/sitemap.xml/


================================================
FILE: docs/core_docs/static/.nojekyll
================================================



================================================
FILE: docs/core_docs/static/fonts/Manrope-VariableFont_wght.ttf
================================================
[Non-text file]


================================================
FILE: docs/core_docs/static/fonts/PublicSans-VariableFont_wght.ttf
================================================
[Non-text file]


================================================
FILE: docs/core_docs/static/img/graph_chain.webp
================================================
[Non-text file]


================================================
FILE: docs/core_docs/static/img/langchain_stack_feb_2024.webp
================================================
[Non-text file]



================================================
FILE: docs/core_docs/static/js/google_analytics.js
================================================
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());

gtag('config', 'G-TVSL7JBE9Y');


